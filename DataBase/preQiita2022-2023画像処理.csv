index,Unnamed: 0,title,tags,url,body,like,create_at
0,0,Elixir Evision と Nx で画像のコントラスト強調（ヒストグラム平坦化）を実装する,画像処理,https://qiita.com/RyoWakabayashi/items/bcd5d5940711e4480467,   はじめにバッチリメイクをしたはずなのに写真ではイマイチ印象に残らない そんなとき ありますよね  スクリーンショット       png  その問題 解決できますそう  Elixir ならねというわけでヒストグラム平均化で画像のコントラストを強調しますもちろん Elixir   Nx   Evision の組み合わせです当然のように Livebook で実行します実装の全量はこちら   実行環境Livebook    の Docker　イメージを元にしたコンテナで動かしましたコンテナ定義はこちらを参照   セットアップ必要なモジュールをインストールします  Req  元画像のダウンロード  Evision  画像処理  Kino  結果表示  Nx  行列演算実行自体に必要なのは Nx と Evision　だけです   画像のダウンロードいつものように Lenna さんを呼びます  スクリーンショット       png  これが元画像になります   グレースケールの場合まずは単純なグレースケールの場合について実装します画像をグレースケールに変換します  スクリーンショット       png  このグレーススケールの各ピクセルの値についてヒストグラムを作ってみますヒストグラムとは 値毎の件数をグラフ化したものです画像の場合    真っ白 から   真っ黒 の値がそれぞれ画像内に何回出現したか を表します例えば   灰色 近辺の件数が高く   や  近辺の件数が少ない   ヒストグラムが偏っている場合 画像全体が同じような灰色になっている ということなので コントラストが弱いと言えます逆に  から  まで満遍なく件数が同じくらいある   ヒストグラムが平坦になっている場合 真っ白な部分も真っ黒な部分も灰色の部分も同じくらいある ということになるのでコントラストが高いと言えますヒストグラムを見ると   あたりと  あたりの値が多いことが分かりますそして  や  の近辺は明らかに少ないですでは ヒストグラムを平坦にしてみましょう  スクリーンショット       png  画像がくっきりしたように見えます元画像と並べてみましょう  スクリーンショット       png  明らかにヒストグラム平坦化した方がくっきり コントラストが強くなっていますね平坦化した画像のヒストグラムを確認してみましょう完全な平坦ではないですが 元と比較すると凹凸が小さくなっています   カラーの場合カラーの場合は少し難しくなります Evision equalizeHist  はグレースケール チャネル数 色数 にしか使えませんかといって R G B それぞれについてヒストグラムを平坦化すると 色味が変わってしまいますなので 画像を BGR 色空間から HSV 色空間に変換します   note infoEvison OpenCV では RGB ではなく BGR を使っていますたまたま最近そのことについて記事を書いている方がいました OpenCV のピクセル形式が BGR である理由  BGR は青 Blue  緑 Green  赤 Red の頭文字で それぞれの原色がどれくらい強いかで色を表現しますHSV は色相 Hue  彩度 Saturation  明度 Value の頭文字で 色の種類 鮮やかさ 明るさの組み合わせで色を表現します色の種類や鮮やかさは変えずに明るさだけ変えたいので  HSV から V だけ取り出してヒストグラム平坦化してから  HS とくっ付ければいいわけです  スクリーンショット       png  HSV に変換しても画像を表示するときには BGR として表示しようとするため なんだかミステリアスな画像になっていますHSV から V を取り出すので  Nx の行列にしてから 　次元目の  列目を抜き出しますこの値は明るさだけを取り出しているので グレースケール画像のように表示できます  スクリーンショット       png  この V に対して  Evision equalizeHist  を実行します少し気をつけないといけないのは 次元数を下げてから実行し その後に戻すことです    次元数を下げる  縦  横        縦  横      次元数を上げる  縦  横      縦  横      スクリーンショット       png  この時点でコントラストが強調されたのが分かります最後に HS と V を結合します  スクリーンショット       png  カラーでもくっきりしています元画像と並べて見ましょう  スクリーンショット       png  明確にコントラストが強調されていますね   まとめVegaLite だとヒストグラムも簡単に作れるので 平坦化の概念が理解しやすかったです   参考サイト,4,2022-12-28
1,1,Elixir Evision と Nx で画像の明るさ調整（ガンマ補正）を実装する,画像処理,https://qiita.com/RyoWakabayashi/items/ab9857ddd79cbbcf6506,   はじめに画像が暗くて見づらい 印象が悪く見えてしまう そんなとき ありますよね  dark png  その問題 解決できますそう  Elixir ならねというわけでガンマ補正で画像の明るさを調整しますもちろん Elixir   Nx   Evision の組み合わせです当然のように Livebook で実行します実装の全量はこちら   実行環境Livebook    の Docker　イメージを元にしたコンテナで動かしましたコンテナ定義はこちらを参照   セットアップ必要なモジュールをインストールします  Req  元画像のダウンロード  Evision  画像処理  Kino  結果表示  Nx  行列演算実行自体に必要なのは Nx と Evision　だけです行列演算を見やすくしたいので Nx Defn をインポートしておきますこれで  defn  内で演算子による行列演算が記述できます   画像のダウンロードいつものように Lenna さんを呼びますこれが元画像になります  スクリーンショット       png     ガンマ補正ガンマ補正モジュールを以下のように定義します GammaCorrection correct  に画像と gamma パラメータを渡すことで明るさ補正ができますgamma    のとき明るくなります中身は後で解説しますでは実行してみましょう元画像とガンマ補正結果を並べてみます  スクリーンショット       png  明るくなりましたね次は   から   の範囲でガンマ補正をかけてみます  スクリーンショット       png  左上 gamma     から右下 gamma     に行くにつれて どんどん明るくなるのがわかると思います   演算内容の確認ガンマ補正で使っているテンソルの中身を見てみましょう  スクリーンショット       png  うーん これだけを見てもなんともですねgamma を変化させながらグラフにして見ましょうこのグラフの src が元の明るさ  dst が変換後の明るさと捉えてくださいgamma    のグラフを見ると  src が小さい値のときから dst の値がかなり大きくなっていますgamma    のときは src   dst ですgamma     のとき  src が  を越えないとほとんど dst が大きくなりません Evision lut  は Look Up Table という関数で テーブルを元に値を変換します変換例  src 変換元                 変換テーブル                  長さ                                     というように変換する  dst 変換後               なので  gamma    の変換テーブルを使えば小さい値 暗い色 でも大きい値 明るい値 に変換される結果 画像が明るくなります逆に gamma     の変換テーブルを使うと大きい値でも小さい値に変換されるため 画像が暗くなります   まとめ思った以上にガンマ補正がシンプルに実装できましたElixir だとやっぱり書くのが楽しいですね,4,2022-12-28
2,2,2次元 Wavelet 行列を用いた定数時間メディアンフィルタ日本語解説（SIGGRAPH Asia 2022 Best Paper）,画像処理,https://qiita.com/TumoiYorozu/items/a8193e5ecd957a1b9b0d,初めましての方は初めまして 私は東京大学大学院 情報理工学系研究科で博士年生をしている学生です 今回はこの論文の簡単な日本語解説と 論文では書かれていないちょっとした裏話などをしていきたいと思います    プロジェクトページ     SIGGRAPH Asia のプレゼン概要ページ     ACM DIGIRAL LIBRARY     GitHub のコード  良ければこの Qiita 記事のいいね ストック GitHub の Star をしていただけると今後のモチベーションに繋がります また ACM DIGIRAL LIBRARY から論文 PDF をダウンロードしていただけると DLカウンタが増えて嬉しいです また SIGGRAPH Asia での発表の際に用意した 分のプレゼン映像も公開しています 良ければ高評価 チャンネル登録よろしくお願いします    なにをしたの  Abstract    note info  メディアンフィルタと言う 様々な処理に用いられている古典画像フィルタの高速化をした  これまでソート法かヒストグラム法の手法があったが 全く新たな第三の手法を提案  競プロでよく用いられているウェーブレット行列 Wavelet Matrix をベースに 画像処理に使えるように拡張した  既存手法でどちらも不得意だった  HDR 高色ビット  や 中 巨大フィルタサイズ でも効率的に扱える  並列化もサポートし GPU  CUDA  で既存手法と比較し   倍の高速化を達成  ACM SIGGRAPH の開催する コンピュータグラフィックスのトップカンファ SIGGRAPH Asia  のテクニカルペーパーに採択 Best Paper Award を受賞こちらのツイートでも簡単に研究の概要 貢献を紹介しています    目次 Outline 以下の順序で紹介していきます  章は論文の内容を日本語で解説 一部補足などを行っています  章は今回の研究にまつわる背景や考察を行っています  ウェーブレット行列の話や  提案手法 の話は アルゴリズムや競技プログラミングのバックグラウンドが無いと読むのが大変かもしれません 辛いなと思ったら斜め読みして雰囲気だけでもつかんでいただけたらと思います      研究の背景と 既存の手法  メディアンフィルタについての紹介をします      ウェーブレット行列  アイディアのベースとなったウェーブレット行列の説明です      提案手法  ウェーブレット行列を拡張して 次元整数配列 ＝画像 を扱えるようにします      評価 Result   実際の速度の結果や メモリ使用量の説明をします      制約と今後の課題  Limitation   Future Work です      この研究の経緯  この研究のアイディアのきっかけなどを紹介します      査読の感触  査読で査読者からの評価や突っ込まれた所を紹介します      Best Pepar Award をいただけた事の分析  自己分析をしています      まとめ       研究の背景と 既存の手法    メディアンフィルタとは 数ある画像処理フィルターの一つに メディアンフィルタ 中央値フィルタ という物があります これはモノクロ画像のあるピクセルに対して 上下左右  r  ピクセルの領域の中の中央値の値を採用していくフィルタです RGB などのカラー画像に関しては色チャンネル毎に処理します   image png   年に考案された非常に古典的なフィルタであり 画像処理の教科書にも載っている定番のフィルタです 画像処理の定番ライブラリである OpenCV にも  medianBlur   として実装されていますし Intel による画像処理ライブラリ Intel IPP にも  FilterMedian   として実装されています また Photoshop の 中央値 メディアン フィルター  GIMP の メディアンぼかし  Premiere Pro や After Effects の ミディアン など 各種画像 動画編集ソフトでも標準的に実装されています 工学的にはノイズ除去フィルタとして使われたり ぼかしフィルタの一つとしても使われています   image png  また 映像制作をする人の間では 文字やロゴなどの簡単にできる溶解表現としても使われてもいます 他にも 美肌化フィルタの一部工程など 様々な用途の構成要素として メディアンフィルタやその派生が使われています     主なメディアンフィルタの実装これまでのメディアンフィルタの主な実装方法としては 主に以下の通りがありました   ソートベースの手法  ヒストグラムベースの手法     ソートベースの手法ソートベースの手法では ピクセルの対象範囲をソートして順番に並べることにより中央値を求めます シンプルな手法ですが特に   imes   といった小さなフィルタのときは最速の手法です しかしフィルタの半径を大きくしようとした場合 フィルタの辺サイズを n  としたとき  n   個の要素をソートしないといけなくなります 単純にソートした場合はピクセルあたり  O n  \log n   の時間がかかってしまいます 中央値を求めるのに全てソートする必要は無いと言う発想で  クイックセレクト  と言うアルゴリズムを使うことで平均  O n    で  median of medians   というアルゴリズムで最悪計算量も  O n    にすることが出来ますが どちらにしろフィルタサイズが大きくなると非常に遅くなってしまいます また 最近のソートベースの手法は  ソーティングネットワーク  と呼ばれる つの値をmin maxを用いて交換することでソートする手法がメジャーです．クイックソートなどと違って if を使わず min max 命令でどんな値が来ても同様の手順でソートができるので 特に GPU などの条件分岐に弱いアーキテクチャでも相性が良いのですが 多数の値を効率よくソートできるソーティングネットワークというものは未解決問題です また 単純には  O n   \log n     の計算量がかかってしまいます 隣り合うピクセル同士で処理を共通化することにより計算量を下げることは出来ますが やはりフィルタサイズの上昇には弱い手法となっています      ヒストグラムベースの手法bit画像の場合 各ピクセルの値域は になります あるフィルタ区間でそれぞれの値が何回登場したかをカウントする配列 ヒグトグラム を用意すると 隣のピクセルの計算をするときにヒストグラムを再利用して 差分のピクセルの値だけカウントを更新することで  ヒストグラムを操作するのに  回のメモリアクセスが必要になりますが フィルタサイズに関わらずに中央値を計算することができます これだけでも ピクセルあたり計算量  O n   のメディアンフィルタが実現可能です 更に 列ごとに連続 n マスのヒストグラムを用意し 行ごとに n 個の列ヒストグラムを加減算することで ピクセルあたり  O    の時間 つまり定数時間のメディアンフィルタが実現できます OpenCV の メディアンフィルタはこの方式が採用されています しかし 定数時間でフィルタリングできるとは言え  n  に対して定数というだけであり ピクセル値のビット数  b  に対しては  O  b   の時間がかかります 近年 High Dynamic Range  HDR  画像という  bit int や bit float といった高ビットの色空間で作業することも多く それらの画像でメディアンフィルタを行う場合はこのヒストグラム法では現実的ではありません 実際 OpenCV のメディアンフィルタは   bit 画像 かつフィルタサイズが一定以上 の場合はサポートされていません      既存手法のまとめと ウェーブレット行列を使った第三の手法 提案手法 これらの特性から 主にフィルタ半径が小さい場合はソート法 色ビット数が小さい場合はヒストグラム法が用いられてきました しかし 古くから利用され研究されてきたメディアンフィルタですが   フィルタ半径が大きい かつ HDR画像   の場合は どちらの手法も適さないものでした ここで 提案手法では   HDRに対応しつつ フィルタ半径に影響されない ウェーブレット行列を用いた全く新たな第三の手法   を提案しました            ソート法   ヒストグラム法   提案手法    計算量   約 O r        O         O         HDR     Yes    No      Yes        ウェーブレット行列ウェーブレット行列とは 長さ  n  の 次元の  b  bit 整数配列に対して  O nb   の時間の前処理を行うことで以下のような区間クエリを求められるデータ構造です   Quantile L  R  k    配列の区間   L  R   の中で  k  番目に小さい値を答える  RangeFreq L  R  x  y    配列の区間   L  R   の中で 値域が   x  y   の要素数を答える単純に考えると どちらの操作も計算量は  O R L   になりそうですが ウェーブレット行列はこれらのクエリを 区間の長さや中身 答えにかかわらず  O b   の時間で答えることができます 更に メモリ使用量に関しても十分省メモリに実装することができ 真面目に実装すると  nb   o nb    bit  のスペース つまりもともとの数列を表すのに必要な  nb  bit 分と  o nb   bit の領域しか使用しません ウェーブレット行列とは ウェーブレット木というものを 答えられるクエリや計算量はそのままに 効率化したものです ウェーブレット行列を知るためにはウェーブレット木から学習することをおすすめします 以下の記事がおすすめです    ウェーブレット木の世界   一応 この知識がなくても分かるようには説明を構成していますが 知っておいたほうが理解の助けになるでしょう なお 画像処理分野でよく用いられるウェーブレット変換とは 直接的な関係はありません ウェーブレット行列はこれらの特性から データベース検索やDNA解析 競技プログラミングの世界で有名なデータ構造でした 次に ウェーブレット行列の作り方を 例とともに簡単に紹介します     構築ウェーブレット行列は クエリに答える前に構築を行います 構築では 各値を進数で考えていき 上のビットから   で安定ソートを行っていきます 手順としては 基数ソート   と似ています 違うのは 基数ソートでは下のビットから注目していきますが ウェーブレット行列の構築では上のビットから注目していきます 長さ  の bit整数配列                    を例に 実際に構築の手順を見ていきます まず 上段では 下からビット目が  なのか  なのかで青とオレンジに色分けされています これを 同じ色同士の順序は変えずに 青は前方 オレンジは後方に並び替えます 次に中段では 下からビット目が  なのか  なのかで青とオレンジに色分けされています これを同様に並び替えます 最後に下段では 最下位ビットが  なのか  なのかで青とオレンジに色分けされ 同様に並び替えられています 基数ソートと処理が似ていると言いましたが 最終的なデータを見ると 進数の上下の位を反転させる Bit Reverse する とソート済み配列になっていることに気づくと思います   image png  ウェーブレット行列は このソート時の 並び替える直前の青とオレンジ つまり のビットだけを記録します   image png  この青かオレンジかのビット配列だけを用いて 各クエリを省メモリに高速に答えます     Quantile クエリ   区間の中で k 番目に小さな値を求める次に Quantile クエリの計算方法について紹介します 例として 区間の 番目の要素の中から   番目に小さな値を図例を使いつつ求めてみます なお この図ではわかりやすさの為に数字も振っていますが 前章のとおりウェーブレット行列では青であるのか オレンジであるのかしか分かりません まず 上のビットから考えていきます 図を見るとこの中に青の個数は  つあります 青の値はオレンジの値より小さいということを考えると この中で 番目に小さな値というものはオレンジの値であることがわかります 更にいうとオレンジの中で番小さな値が 目的のものと言うことが分かります ここで 特定区間中の青の個数を求める必要が出てきますが  O   で求める効率的な方法を 後章  で説明します 次の段に移ります 段目の検索区間の要素は青かオレンジかで前後に分かれました オレンジの値に注目したいので つに分かれたうちオレンジの区間に注目します この区間移動の計算は 段目全体の青の個数と 段目の検索区間より右の青の個数 段目の検索区間内の青の個数を用いることで計算できます 分かれた段目の検索区間に含まれる値は     の通りです この検索区間では   だけが青であり   がオレンジになります この検索区間で青の個数を数えると  つです 段目の検索区間で 番小さな値が知りたいので 目的の値は青 つまり  か  であると言うことが分かります 同様の手順で次の検索区間 つまり段目で青に分類された値が集まる段目の区間に移ります 段目の検索区間では 値は  か  の通りであり であれば青 であればオレンジになっています この区間で青の個数を数えると 個です 目的はこの区間で番目に小さな値で 青は個なので 求めていた答えはオレンジであることが分かります この段目の検索が終わった時点で 答えになる値というのは  ただ一つに絞られました これによって 答えは  と求めることができます   image png  今回は例を使って説明していきましたが 配列や検索区間の長さがどんなに長くても 中のデータがどの様な値でも 整列されてても偏りがあっても   bit 値の配列であれば  ステップでこの様に求めることが出来ます ここで重要なのは ステップ目で答えになりうる値の範囲を   の 個から   の  個へ ステップ目で   の 個へ ステップ目で  の 個へ ステップごとに半分にしていった事により 定数ステップ数で求められたことです     RangeFreq クエリ   区間の中で特定値域のものの個数を求める次に 区間の中で特定値域のものの個数を求める RangeFreq クエリの計算方法です 区間  L R  で値域が  x  y  の 個数を求める RangeFreq L  R  x  y  を 値域が    x  の 個数を求める LowFreq L  R  x  関数を用いて 以下のように変形します RangeFreq L  R  x  y    LowFreq L  R  y    LowFreq L  R  x この様に RangeFreq クエリは 値が x 未満の個数をカウントする LowFreq を用いる事で実装可能です LowFreq の計算方法を 例として区間の 番目の要素の中から  未満の個数を求める方法を 図例を使いつつ求めてみます ここで重要なのは この  x  を二進数で上のビットから注目して考えることと  未満の個数 を求めるということは           の  種類の値のカウントを行うわけですが            と      の  ブロックに分けてカウントするのがポイントです まずはじめに 上段から見ていきます     x   の最上位ビットは                 なので  です 注目しているビットが  のときは 答えを格納する変数  Ans  に 区間内の青の個数を加算します この例では 青の値というのは区間内の            の値の個数をカウントしていることになります 次の検索区間に移るときは 注目しているビットがであれば青 左 の方向に であればオレンジ 右 に進みます 今回は  だったのでオレンジの方向に進みます 中段に関して    の ビット目 は  です 注目しているビットが  のときは  Ans  には何もせずに 青の方向に移動します 最後に下段    の 最下位ビットは  です  なのでこの区間中の青の個数を  Ans  に加算します この区間の青というのは      の個数なので 最終的に  Ans  には       の種類の値の個数が 漏れも重複も無くカウントできたことになります   image png      BitVectorさて ここまで Quantile クエリと RangeFreq クエリについて説明してきました 説明の中で指定区間内の青の数 ビットがの要素数 を数える操作が頻出していましたが 区間  L R  のカウンティングには愚直には  O R L   の時間がかかってしまいます 長さが  n  の bool 配列に対して 事前に  O n   かけて累積和テーブル  A  を用意しておくことにより 区間  L R  のカウンティングは  A R    A L   を計算すればいいだけであり  O    の時間で取得できます   image png  めでたしめでたしとしたいところですが 累積和テーブル  A  は  n  が int で表現できる範囲だとして  n  ビット必要になります 元々の bool 配列のサイズが  n  ビットであることを考えると 少し大きいです これはこれで良いのですが もう少し効率の良い方法を考えます ここで 長さ  w  ごとに  チャンクとして チャンクごとにそれ以前の累積和と チャンク内の bool を保持します PopCount命令 進数で見たときにビットがの数をカウントする命令 で扱いやすい様に  w     か  を一般に使用します 以下の図では  w  として説明しています   image png  ある地点までの累積和を求めるのは 対象チャンクに書かれている累積和値と チャンクの対象ビットまでの PopCount の合計を計算すれば求められます 上の図では先頭ビットの PopCount を  \lfloor wfloor    番チャンクに書かれている    と   \bmod w     ということで対象チャンクのビット目までの PopCount の合計ということで  と求めています 実際 先頭ビットの青  の個数は  個です チャンク内の  x  ビット目までの PopCount は  x  に応じてシフト演算や BZHI 命令を使用して必要な長さに切り取り PopCount 命令  CUDA では    popc  GCC では    builtin popcount  など を使用して計算できます このとき必要な空間は チャンクごとの累積和に  n w  ビット チャンク内の bool として全体で  n  ビット必要になります  w  として全体で  n  ビットとなり 大幅な効率化が出来ました 近年のアーキテクチャでは計算がメモリ速度律速になることが多く メモリ使用量の削減は速度の面でもメリットがあります 今回はチャンクを段階で分割しましたが 段階で分割することにより メモリ使用量を  n   o n   ビットに削減することができ 簡潔ビットベクトル 完備辞書  BitVector と呼ばれています この簡潔とは 実装がシンプルと言う意味ではなく  計算機科学の用語で情報理論的下界に 近い 領域量だけを使いつつ  他の圧縮形式とは異なり 効率的に質問を受け付けることができるデータ構造  を指します 段回チャンクの場合は  O n   の空間が必要になるので  簡潔  ではなく  コンパクト  と呼ばれます この程度の完備辞書の用途の場合は 無理に簡潔にしてメモリ使用量を少し減らすよりも 段階のコンパクト実装にするほうがメモリアクセス回数を回から回に減らす事によるメリットが上回り 一般に高速になります 簡潔ビットベクトルについてより詳しく知りたい方は 以下の記事を御覧ください   簡潔ビットベクトル 完備辞書  Tiramister s Blog       提案手法：次元整数配列を扱えるウェーブレット行列この様にウェーブレット行列では 次元整数配列に対しての様々な区間クエリを 効率的に求めることが可能とわかりました 特に 区間の k 番目の数を求める Quantile クエリを用いることで k を奇数長区間の半分の値にすることで中央値を求めることが可能です しかし 画像データのような次元整数配列に対して応用を考えた時 そのままではうまくいきません 以下の図の様に 次元整数配列を 次元整数配列に平坦化してみましょう 次元に平坦化する時 もともとのマスの x 座標も一緒にメモしておきます 図で言う各マスの右下の三角内の x index のこと  この時 元々の  次元配列における矩形区間を そのまま  次元配列での区間にそのまま言い換えようとしても 途中に不要なマスが含まれてしまうのでうまくいきません 図の例でいうと 次元の矩形区間では x 座標が   であるものの中で  番目の値を求めたいわけですが 次元にそのまま当てはめようとすると不要なx座標が  であるものも含まれてしまいます   image png  ここで 次元のウェーブレット行列での操作を振り返ってみます 次元のときは 範囲中の青のマスの個数を数えるのが重要な操作でした 次元のときに x座標が不要なものは無視してカウントしたいです   image png  ここで 青のマスはそのまま x 座標 オレンジのときは  W    画像の幅 つまりいかなる x 座標よりも大きな値  に置き換えた新たな数列を考えます これに対して 区間内の   の値の個数を数えることで x座標が であった青のマスの個数を数えることができます この操作は 次元ウェーブレット行列における RangeFreq と同じ操作になります つまり 内部に  次元ウェーブレット行列を保持しておくことで  次元整数配列に対するウェーブレット行列が実現可能なのです     次元ウェーブレット行列の構築前章のアイディアをもとに 次元ウェーブレット行列の構築を考えてみます まず 次元配列を x座標を添付して平坦化します そして それぞれの値を x座標を付加したまま次元ウェーブレット行列の構築をしたときと同様にビット数分並び替えを行います   image png  次元ウェーブレット行列のときは 並び替えた数値の青かオレンジかの   のみを保存していました 次元ウェーブレット行列では 青のときはx座標 オレンジのときは  W  に並び替えた数列を作ります クエリ処理の時 この数列に対して RangeFreq を行いたいので この数列で次元ウェーブレット行列の構築を行います  b  bit 値の値を扱う次元ウェーブレット行列では  b  本の次元ウェーブレット行列を内部に持って クエリに答えていきます   image png      次元 Quantile クエリ   区間の中で k 番目に小さな値を求めるでは実際に 次元整数配列における  Quantile クエリを考えて行きます 例として x マスの入力画像に対してx座標が  y座標が のマスの矩形区間の中央値を求めてみます なお 偶数個要素の中央値は本来  n   番目と n   番目の値の平均を採用するのが一般的ですが 今回のメディアンフィルタでは簡単のために  n   番目の値だけを採用することにします 今回の例の場合                の中で  番目に小さな値を求めます 次元ウェーブレット行列のときと同様に 上のビットから考えていきます まず 最初の検索区間です RangeFreq クエリを使えば 対象区間中の   の値の個数が分かります 実際に D RangeFreq でx座標   の個数をカウントすると  になります  もとの数列の対象区間を思い出すと                     なので 実際に 対象区間中の   の値の個数は個であることが分かります 番目に小さな値を求めたかったので 答えは後半のオレンジにあることがわかるので検索区間を移動し この中で番目に小さな値を求めます 次に段目です この区間で x座標   の個数をカウントすると  です これは もとの数列における   の値の個数に相当します                    なので一致します 答えは前半の青にあることがわかるので検索区間を移動します 最後に段目です この区間で  番小さな値を求めます x座標   の個数をカウントすると  で これはもとの数列における  の個数です 番小さなものを求めたかったので 答えは前半の青 つまり  に絞られます ステップ目が終了し  bit 二次元配列であったのでこれで手順は終了です                の中で  番目に小さな値は  ということが分かりました   image png      計算量 W imes H  の二次元配列を一次元配列に平坦化すると 長さが  WH  の一次元配列になります  b  bitの値を持つ二次元ウェーブレット行列は 内部に値域が  W で長さが  WH  の一次元ウェーブレット行列を  b  本持っていました 長さが  n  で値域が    m   の一次元ウェーブレット行列の構築時間は  O n\log m   であったので 二次元ウェーブレット行列の全体の構築に必要な時間は  O bWH\log W   になります 空間計算量に関しては Compact な Bit Vector を使用する  b  bit 値で長さが  n  の一次元ウェーブレット行列は  O bn   の空間が必要であるので 二次元ウェーブレット行列の全体で  O bWH \log W   の空間が必要になります     bit float への対応これまでの説明では 入力は整数であることを前提としていました しかし 画像処理では ピクセルを float で処理するシーンも多々あります float 型が一般的に用いられている IEEE  形式であることを仮定すると 非負の画素しか登場しない場合は float  のビットパターンを uint  として解釈しても nan を除いて値の大小関係が保存されるので問題ありません しかし 負の値の画素が含まれる場合 符号ビットに応じて追加の処理が必要になります ここで 一般に入力画像の画素数は一般に       以下であることを利用します  縦横   imes   pxのときに ちょうど      画素 入力の画素全体を 値と座標をペアにソートして それぞれの値が何番目に小さな値なのかで置き換えます  このとき 値の重複を削除する必要性はありません 例えば   imes   サイズの画像の場合 各ピクセルは           の値に変換されます これは  bit 整数を扱うウェーブレット行列として処理することができ bit として扱うよりも     だけ高速化されます 最後に テーブル引きをして元の float の値に戻せば完成です ソートやテーブル引きのコストが増えてしまいますが ソートなどは非常に研究されつくされ高速に行うことができるので 非負の画素しか登場しない場合でもソートを行った方が高速になります     矩形窓以外への対応これまでの例では 軸に沿った四角形の範囲で中央値を求めている例になります しかしメディアンフィルタの派生として 六角形や八角形範囲の中央値を取る事で より綺麗な結果を得られる事があります 今回のウェーブレット行列を使った手法でも 時間やメモリが倍必要になってしまいますが六角形範囲 倍必要になりますが八角形範囲 倍必要になりますが十二角形範囲にフィルター窓を拡張することが出来ます 詳しくは論文の p Appendix B を御覧ください また  六角形範囲の実装例  もあります 六角形であれば四角形よりも十分円形に近いので実用的でしょう      評価  Result 今回の二次元 Wavelet Matrix を用いたメディアンフィルタを CUDA で実装し NVidia 製 GPU 上で実行できるようにし評価を行いました 環境としては以下のとおりです 詳しくは  こちらのGitHub上の情報   を確認ください × ピクセルの K サイズの  bit int   bit int   bit float 画像を用意し 画素数を固定してフィルター半径を変えてそれぞれのフィルタ速度を比較しました 単位は横軸がフィルタ半径 px であり 縦軸がスループット Mega 画素  つまり秒間に何百万ピクセル相当を処理できたかを表しています つまり高いほど良い成績であり  Mega 画素を達成した場合は x 画素を msで × 画素を msで処理できたということを表します 比較に用いた手法を簡単に紹介します    Adams     ソーティングネットワークを用いたソートベース最速の実装です SIGGRAPH  に発表されました dynamic はソーティングネットワークを実行時にインタプリタ的に構築 実行しています static はコンパイル時にフィルタ半径を固定することにより 特化したバイナリを Halide を用いて生成します 非常に高速ですが半径のコードをコンパイル生成するのに分 半径の生成に分かかり それ以上は当環境ではコンパイルが不可能でした    Array Fire   オープンソースの画像処理ライブラリです    OpenCV   オープンソースの画像処理ライブラリです CUDA用はヒストグラムベースの手法が使われており NVIDIA Research の Green 氏が 年に実装  論文を公開  しています 提案手法の Ours はウェーブレット行列の構築と中央値計算の両方を含んで計測しています Runtime Only は 構築にかかった時間を無視し 構築済みのウェーブレット行列を利用したときを想定した時間です 同じ入力画像に対してフィルタ半径を変えるなどのユースケースにおいては構築済みウェーブレット行列を使い回すことができます 実際の結果は以下の通りです 提案手法はフィルタ半径が変化しても計算時間が変わらない 定数時間フィルタリングを実現しています ビット画像のとき 提案手法は半径以上で最速となっています また 半径の時はソート法最速の Adams 氏の手法と比べ  倍高速でした 計算時間が半径に依存しない OpenCV のヒストグラム法と比較すると どの半径のときでも  倍程度以上高速でした   bit 画像の場合 それぞれ半径   以上で提案手法が最速となりました 半径のとき 次点の Adams 氏の手法と比べそれぞれ 倍 倍高速でした ヒストグラム法による OpenCV では bit 以外対応していませんでしたが 提案手法はこの様な HDR 画像に対しても定数時間フィルタリングを実現しました 同じ入力画像に対してフィルタリングを想定した提案手法の Runtime Only では ビット画像のとき半径  以上で全手法の中で最速の手法となりました     メモリ消費量Full HD K サイズのそれぞれのモノクロ画像に対し 各色ビット数でウェーブレット行列を構築するときに消費したメモリ使用量  MB  は以下の通りでした bit の p サイズであれば     MB  のメモリがあれば使用可能であることがわかります Kサイズになったとしても         MB  と およそ画素数に比例したメモリ使用量でフィルタリングが可能です bit の時は bit の倍程度のメモリ使用量でした bit Float の場合 一旦画素のソートを行ってから番号を振り直しているので bit の 倍よりはメモリ消費量を抑えられています K bit のとき 合計で   GBのメモリが必要でした 一部の GPU ではビデオメモリが不足する可能性がありますが K HDR を扱うような環境では相応の GPU を使用していることを期待すると現実的な消費量と言えるでしょう また ウェーブレット行列の構築は CPU 側で行い 実際のフィルタリングはデータ構造を GPU に転送してから行うヘテロジニアスな手法を使用すれば GPU 側のメモリは  MB ですみ CPU と GPU の両計算資源を有効活用できる可能性もあります      制約と今後の課題    メモリ使用量bit の RGB チャンネル pサイズ の場合 約  MB のメモリが必要です しかしK bit カラーの場合 全チャンネル同時に処理をしようとすると約 GBのメモリが必要になります K 画素レベルであればチャンネル毎に処理しても実装にもよりますが約万から万のスレッドが発生するので 現代のGPUのスレッドを十分有効活用できる量ではあります しかし いくら K HDR 自体高コストな内容だとしても 少し大きすぎるかもしれません 現在 メモリ使用量を約半分にするマイナーアップデート手法が発見できたので 実装が完了次第プロジェクトページなどで成果を追記します     CPU 版の実装今回 CUDA 向けの効率的な実装は行い 実際に高速に動作することを確認しましたが CPU 版の効率的な実装は 時間が足りないのもあり 行いませんでした なお 可読性を優先した現在の CPU 版の実装は OpenCV と比較して数十倍です ウェーブレット行列ではビットベクトルの操作が非常に重要であり 進数でビットがの数を求める PopCount を非常に多用します CUDA ではハードウェアによる    popc  命令があります CPU でも    builtin popcount  や   mm popcnt u  命令を使用した popcount 命令はあるのですがこれはスカラ命令であり AVX や AVX  の様な複数の値を同時に処理する SIMD 化を行おうとしたら使用できません AVX  にも AVX VPOPCNTDQ に対応した CPU であればベクトル popcount があるのですが AMD Zen や 省電力向けモバイルノートに使用されている Intel IceLake 系列でしか使用できず 個人向けハイエンドデスクトップ向けの Raptor Lake  Core i K等 では利用できません CPU 版の効率的な実装をしてくれる方を募集しています      正確な メディアンフィルタ以外へのウェーブレット行列の応用今回 今まで次元整数配列に利用されてきたウェーブレット行列を次元整数配列に適応可能な様に拡張し 画像処理にもウェーブレット行列が利用可能であることを示しました しかし メディアンフィルタ以外への応用はあまり目処が立っていません グラフィックスの研究者にウェーブレット行列を知っていただく事で コンピュータビジョン系を含む他の画像処理への応用を期待しています また メディアンフィルタは様々な派生が存在し 中には正確なメディアンフィルタよりも特定用途でより良い結果を得られるものが数多くあります それらの手法の中にはウェーブレット行列を利用することでより高速化できる可能性もあります      この研究の経緯ここまでは論文に書かれている事の日本語解説   一部補足を行ってきました ここからは論文には書かれていない研究の背景を紹介します 私自身の紹介を少しすると 高校時代から情報オリンピックや AtCoder などの競技プログラミングを始めました    現在  私の AtCoder は黄色  であり AtCoder 内では上位約    順位は世界位 日本人位になります  AtCoder 社長 chokudai さんのブログ  によると  研究職 研究開発などや 高度なアルゴリズムを要求される開発現場で重宝されます  と紹介されているランクです 個人的には DP やグラフ系の問題が苦手気味ですが  ACM ICPC と言う国際大学対抗プログラミングコンテスト   では 人でチームと言う特徴と 重実装 幾何を専門に担当し 運もあり 年大会の World Finals に出場しています ウェーブレット行列はコンピューターグラフィックス分野の研究者の間では無名なデータ構造ですが 競技プログラミングでは上級者の中では非常に有名なデータ構造であり 知っていました また 競プロ以外の趣味として 高校時代から映像制作を行っています  AtCoder社のPV  を自主制作したり  ICPC アジア地区横浜大会のオープニング映像  を制作したりしています また  自主制作の二次制作ミュージックビデオ  がニコニコ動画トップページに紹介され殿堂入り目前だったり ニコニコ超会議に自分の映像が展示 受賞されたこともあります 映像制作の経験を積む中で メディアンフィルタが数多くの表現を構成する要素のつとしてよく使用されていることや メディアンフィルタの処理が非常に遅いと言うことを知っていました そして私が学部年生のとき 年 に ウェーブレット行列を使えばメディアンフィルタが高速化できるということに気づきました つまり映像制作と言う知見から問題発見を行い 競プロの知見から問題解決を行ったとも言えます 年までは別の研究を行っていたのでネタストックとして取っておいたのですが そちらも一段落したのでこの研究 実装を本格化しました ずっとストックしていた理由の一つに ウェーブレット行列は計算量こそ良いものの 定数倍が少し遅いと言う経験があり 理論的な計算量が減らせても実際のフィルタ速度も速くなるのか確認が持てなかった事が挙げられます そのため 最初のうちは SIGGRAPH よりのランクが少し下の EGSR  Eurographics Symposium on Rendering  という学会で発表することを目指していました しかし CUDAの実装 チューニングを進めた結果 GPU上で既存手法の 倍以上高速化できることが判明し SIGGRAPH Asia へ変更しました      査読の感触SIGGRAPH Asia  の Technical Paper では人の査読者がアサインされ   Strong Accept     Accept     Weak Accept     Weak Reject     Reject     Strong Reject   の段階で評価されました 私の最初の評価は               でした 主にメディアンフィルタと言う古典フィルタの全く新たな実装を提案した事が特に評価された感触です 論文の説明に関しては ページ数の制約もあり ウェーブレット木の説明ではなくいきなりウェーブレット行列の説明から始めざるを得なかった点で分かりにくい 不十分と意見もありました 一部の査読者は論文だけでは分からなかったが ウェーブレット行列の説明をしているブログを読んだことで理解できたとコメントいただきました 大きなツッコミとして CPU 版の実装は可読性を重視したものしか用意していなかったのですが それを指して CPU 版の実装を動かしたら非常に遅かったと言う意見がありました それに対しては CPU 版の効率的な実装は Future Work ということにして納得していただきました リバッタル 反駁 を返したあとの評価では               と人評価を上げていただけました   とした査読者も特に大きなマイナスがあると言うわけではなく 専門分野でなかったので高評価をつけられないといった印象でした 平均評価としては    なので 高い評価をいただくことが出来ました 個人的には Strong Accept    を出す査読者はいなかったが Best Pepar Award をいただくことが出来たのが少し意外と感じました      Best Pepar Award をいただけた事の分析Best Pepar Award の連絡が来たのは SIGGRAPH Asia の始まる前日でした ボスと韓国へ前着し 電車に乗っているときにメールで連絡が来ました 実はウェーブレット行列を拡張して次元整数配列を扱えるようにするというアイディア自体はシンプルなものであり  競プロerの中には気づいている人も居ました   このブログでは  次元空間の点群 を扱っていますが これは 次元整数配列 を扱うと言うのと同義です 正直 論文を書いている最中にこのブログを発見してかなり焦りました しかし このブログの公開日 そしてブログ中で拡張ウェーブレット行列が使えると示して   TLE   出している問題の提出日が    です それに対して私はこの問題に対して 同じく拡張ウェーブレット行列により   TLE   や  AC   を    に行っていました なので新規性に関して万が一ケチをつけられたとしても一応私の方が先ということで進めました  もっとも 論文成果としての主張がされていなければ この程度では独自性を疑われることはほぼ無いでしょう 例えば高速フーリエ変換の発見は 年に Cooley と Tukey が発見した事になっていますが 実は  年頃からガウスを始めとする一部の人は独自に同様の手法を発見していたと言われています  このことから ラマヌジャン的な突拍子の無い超定理を発見した事による評価 では無いと自己分析しています 自己分析では 以下の点が評価されたと考えています   ソート法とヒストグラム法が主流だった古典フィルタのメディアンフィルタの 全く新たな第の手法を提案した点  別の分野で研究されてきたウェーブレット行列をグラフィックス分野に持ち込んだ点  単に輸入しただけではなく  少しの 独自性を用いて拡張している点  GPU において 実際に 倍以上高速化された点これが達成できた要因としては 競技プログラミングの知識と言う問題解決力の貢献が非常に大きかったでしょう      まとめいかがでしたでしょうか ACM SIGGRAPH と言うコンピュータ グラッフィックス分野のトップカンファレンスの Best Paper Award と言う大変光栄で貴重な賞をいただいたので 記念して日本語解説記事を作成しました 今回の Award は運や 論文執筆を手伝っていただいた指導教員の力も大きかったと考えていますが アイディア自体は意外とシンプルで 人を超えていないと出来ない業績では無いと思います 特に私の場合は 映像制作の経験による問題発見と 競プロの経験による問題解決力が重要だったと振り返っています 競プロが研究解決における万能の要素とは決して考えてはいませんが 多方面の興味を持つ事で この様な異分野技術を用いて新たな問題解決ができると思います ぜひこの記事が修士 博士を始めとする研究生活の知見として活かせたらと思います 最後に良ければ この記事のいいね ストックをお願いします ,30,2022-12-24
4,4,PythonによるX線回折顕微法（位相回復）,画像処理,https://qiita.com/Ratdotl/items/aa256f87a68f76281874,  X線回折顕微法 位相回復 とは全てを理解するのは大変ですので ここでは簡単に説明します 詳しく知りたい方は 下のサイトをご覧ください 本記事は こちらを参考にしております X線を試料に照射すると 回折したX線がスクリーンに現れます 回折したX線は 複素数のデータを持っています しかし スクリーンに現れている像 観測可能なもの は 実数部分の強度のみで位相の部分は観測できません 試料の電子密度分布 オリジナルのデータ をフーリエ変換して絶対値の乗を取ったものです ゆえに 位相のデータは消えてしまいます これでは 元に戻そうと逆フーリエ変換をしても位相のデータがないために元には戻りません これを元に戻す手法が 今回紹介するX線回折顕微法 位相回復 というものです 回復の流れは下の概略図です   Group     png  逆空間の制約は 位相データが失われることを示しています 実空間の制約は 最も肝である位相を回復させるためのアルゴリズムを適用することを示しています そのアルゴリズムは Fienup により提案された HIO  hybrid input output  アルゴリズムというものです 下の式がそのアルゴリズムの内容です    png  Sはサポートと呼ばれるもので 実空間においてデータが存在している領域を示します βの値は 電子密度を減らす度合いを決めるパラメータです 上で説明したことを繰り返すと下のように回復していきます 回復していく様子元の画像  test png  単に逆フーリエ変換して戻した画像  abc    png  現時点ではどう回復していくのかわかりずらいかと思いますので 下に進んでプログラムを書きながら理解してもらえたらと思います   プログラム   ライブラリの導入つのライブラリを使用します それぞれについて簡単に説明します NumPyは 多次元配列を効率的に扱うライブラリです  Pythonの標準ライブラリではないが 科学技術計算や機械学習など ベクトルや行列の演算が多用される分野では ほぼ必須のライブラリです Imageioは アニメーション画像 ボリュームデータ 科学的フォーマットなど さまざまな画像データの読み書きを簡単に行うためのPythonライブラリです mathは 数学計算用の変数や関数 定数が入っているライブラリです  それでは 以下のコードを追加してください これでライブラリの導入は完了です    画像の取り込み位相回復に用いるオリジナルの白黒画像を用意します 白黒である理由は X線を当ててスクリーンに現れるものは電子密度の高低だけの情報であるからです 理論上はカラー画像を用いてもできますが 実験で用いないというのと 少し複雑になるといいう理由でここでは取り扱いません    note白黒の場合は pxの情報がつの値で表現される それに対しカラーの場合は RGBの色 すなわちつの値で表現される つまり 白黒に比べてカラーは操作が倍になってしまう    note warnサイズが大きすぎると PCが重くなりますので 小さいものを推奨します ここでは この画像を用います pxpxのPNG画像です   pngと bmpの画像は動作確認しています    test png  準備ができたら こちらのコードを追加します    pydata   imageio imread  画像のパス   ファイルのパス の部分には 画像のパスを入れてください  print data  で中身を見てみます  data の中身 一部    note 要素数が大きすぎて省略表現になる場合は こちらをファイル上部に加えてください すべて表示されるようになります 上のような大量の数字が格納されています これは 画像のピクセルごとの白黒の濃淡の度合いを示す値を格納しています 白黒の濃淡を までの段階で表現しています       と表示されます これは 行列の形が×であることを意味しており pxpxの画像であることと一致しています これで画像の取り込みについては完了です    オーバーサンプリングを模擬したパッド画像の作成オーバーサンプリングを模擬したパッド画像の作成を行います サンプル画像をフーリエ変換し 位相回復を始める元のデータを作成します 下の画像の→の部分です   Group     png  次に そのデータをオーバーサンプリングします フーリエ変換された逆空間のデータは 実空間に比べて の大きさであるため 元に戻すためにデータ数を倍以上に増やす必要があります 以下のコードを追加してください 何をしているのか見てみます まず 以下のコードです   という値が入っています  len   関数は 引数に指定したオブジェクトの長さや要素の数を取得します よって  source の要素の長さを pad len に代入しているということです 次に 以下のコードです  padded の中身は 中央付近に data の行列があり その回りがで囲まれています  print padded shape  で行列の形は ×であるとわかります つまり 縦 横倍ずつののオーバーサンプリングになっています  np pad   関数は 第一引数に置いたオブジェクトに対し その外側に第二引数で指定した数だけ 第三引数で指定したモードで 第四引数で指定した値を付け加えるというものです 第二引数の  a b   c d  は aが上 bが下 cが左 dが右にどれだけ加えるかを決めます 第四引数においては 加える値を決めます よって ここでは data の行列の上下左右に pad len    ずつを加えています 以下に分かりやすい例を示します 値を変えて理解を深めてみてください  x の中身最後に 以下のコードです    pyft   fft fft padded  print ft  で中身を見ます    e        j           e   j    e   j       e   j    e   j   e   j     e   j   e   j    e   j       e   j    e    j   e    j     e   j   e   j    e   j       e    j    e    j   e   j  padded とは 明らかに違う値が入っています  fft fft x  関数は  x の値をフーリエ変換します よって ここでは padded 内のすべての値をフーリエ変換しています 以上で オーバーサンプリングを模擬したパッド画像の作成が完了です    回折パターンのシミュレートフーリエ変換した画像データの絶対値を取り 下の画像の F K  を作成します   Group     png  以下のコードを追加してください    pyg   np abs ft np abs x 関数は xの絶対値を返します 次に 以下のコードを追加してください 後で使うので  padded の長さを取得しておきます 以上で 回折パターンのシミュレートは完了です    画像の位置とパディングの位置を把握する サポートとその外の判定 サンプル画像のある所 サポート を ない所をとして サポート領域か その外かを判定するためのマスクを作成します 以下のコードを追加してください ×の行列で 要素がすべてになっています np ones  関数は 要素がすべての行列を作成する関数です 次に 以下のコードを追加してください 要素がすべての×の行列の上下左右をでパディングして ×の行列を作成しています 以上で 画像の位置とパディングの位置の把握 サポートとその外の判定 は完了です    ランダムな位相情報を使用した初期推測位相回復を始めるときの一番最初の位相はランダムに与える必要があります 以下のコードでランダムな位相を作成します  G K  にランダム位相e  iφK をかけて G K を作成しています 以上で ランダムな位相情報を使用した初期推測は完了です    反復回数の設定下の画像のサイクルを繰り返すことでより元の画像の復元ができます   Group     png  ここでは は回繰り返します 以下のコードを追加してください    pyn   以上で 反復回数の設定は完了です    回復する時の係数の設定下の画像のβの値を設定します    png  理論的には  が最適だとわかっています ここでは 特に意味はないですが に設定してみます 以下のコードを追加してください    pyb    以上で 回復する時の係数の設定は完了です    回復準備は整ったので 下の画像のサイクルを作成します   Group     png  以下のコードを追加してください    py 前回の結果pre   Nonefor s in range  n       フーリエドメイン制約を適用する     実空間制約を適用する             画像領域が正であること             サポート領域の強度をゼロに近づける     進行状況を画像で保存する                      str s     任意の画像の拡張子   pre         print s 何をしているのか見ていきます まず 以下のコードです    pypre   Nonepreは前回の回復結果です Noneで初期化しておきます 次に 以下のコードです    pyfor s in range  n  サイクル全体の繰り返しを行っています 次に 以下のコードです 下の画像のピンクの部分の操作です   Group     png  np angle z 関数は 複素数zの偏角を返します 次に 以下のコードです fft ifft x 関数は xの逆フーリエ変換した値を返します np real z 関数は 複素数zの実数部の値を返します if文の所は 回目のサイクルの時は個前の結果が存在しないため 補完しています 次に 以下のコードです 下の画像の緑の部分の操作です   Group     png  具体的には 下の画像の計算を行っています 上側は temp   invです それ以外が下側の計算です 次に 以下のコードです    pypre   tempphase   fft fft inv 次のサイクルへの移行部分です preに関してはそのまま次のサイクルに使用したいため フーリエ変換する必要はないです invはフーリエ変換します 最後に 以下のコードです                       str s     任意の画像の拡張子   pre         print s 回復の途中経過の画像preを書き出しています  保存先のパス の所に保存したいフォルダのパスを入力してください  任意の画像の拡張子 の所に  bmp や  png などを入力してください 以上で回復の操作は完了です    ソースコード全体 ここから画像の取り込みdata   imageio imread  画像のパス   ここからオーバーサンプリングを模擬したパッド画像の作成 ここから回折パターンのシミュレート ここから画像の位置とパディングの位置を把握する サポートとその外の判定  ここからランダムな位相情報を使用した初期推測 ここから反復回数の設定n    ここから回復する時の係数の設定b     ここから回復 前回の結果pre   Nonefor s in range  n       フーリエドメイン制約を適用する     実空間制約を適用する             画像領域が正であること             サポート領域の強度をゼロに近づける     進行状況を画像で保存するせっかくなので 回復されていく様をGifにします コードファイルと同じフォルダ内に画像を設置して 以下のコードを実行します  画像を入れるリストpictures    画像を入れる gifアニメを出力する  画像の拡張子  には ご自分の作成した画像の拡張子を入力してください   おわりに無事に画像は回復されましたか 反復数を増やすなど色々つついて遊んでみてください コード自体は短いのですが 原理を理解するのは骨が折れます 生物や医療の分野でホットな技術のようです   参考,6,2022-12-23
7,7,luajitでガウシアンぼかし,画像処理,https://qiita.com/metaphysical_bard/items/f556c7450c56ac901c13,この記事は アドカレ  に参加しています   luajitでガウシアンぼかし 前回の記事  の延長線上にあることをやります    ガウシアンぼかしとはガウシアンぼかしとは  ガウス関数  ガウス関数 で重みを付けたぼかし処理のことです   image png  ガウス関数は中心近いほど値が大きくなり 中心から離れるほど値がに近くなっていきます これをぼかし処理に適用すると 近くにあるピクセルほど重みが強くなり 遠くにあるピクセルの影響は微小になります    アルゴリズム先人の方達が素晴らしいアルゴリズムを共有してくださっています take 言語不問で実装する高速な移動平均処理   フィルタ処理の高速化アルゴリズム 縦横に処理を分ける    gaussian フィルタ   画像フィルター：ガウスぼかし  ←これすき   コード anmとして使用できます    luablur anm	ガウシアンぼかしです for i  ha do  taにガウス関数で重みを付けるfor i  ha do  taの合計値が になるようにするlocal qt     ぼかす際の変数を入れるlocal af ffi new  Pixel       横方向  縦方向   おわりに実はこれの gpu ver   をc   ampで書いてみたのですが 上手く高速化できませんでした gpu 難しいですね ,0,2022-12-20
8,8,luajitでボックスブラー,画像処理,https://qiita.com/metaphysical_bard/items/ef14aea4e6941c04613d,この記事は アドカレ  に参加しています   luajitでボックスブラーluajitでボックスブラーをやっていきます とはいえ luajitもスクリプト言語 速度はあまり期待できないです    ボックスブラーとはボックスブラーとはその名の通りボックス 箱 の形でぼかしていく画像処理です 任意ピクセルとその周囲の平均から色を求めていきます    アルゴリズム先人の方達が素晴らしいアルゴリズムを共有してくださっています 今回はそれをそのまま使います  言語不問で実装する高速な移動平均処理   フィルタ処理の高速化アルゴリズム 縦横に処理を分ける    gaussian フィルタ  要点は二つあります  二次元的な処理を一次元的な処理 縦と横の二回 に分ける  移動平均処理で 計算回数を減らす    コード anmとして使用できます    luablur anm	ボックスブラーです   横方向にぼかしていく  縦方向にぼかしていく   おわりにluajitでボックスブラーをやりました 実際に使用してみると分かりますが とても重いです 実用性を考慮するならば素直にdllでマルチスレッドとかsimdとかしたほうがいいです ところで gpuで高速化できるとか聞いたのですが 本当なんですかね… 最近オンボの限界を感じます ,0,2022-12-20
9,9,ノードエディタ形式の画像処理ツール「Image-Processing-Node-Editor」,画像処理,https://qiita.com/Kazuhito/items/03741785301f5f47caa2,   note infoこの記事はOpenCV Advent Calendar の日目の記事です   はじめに趣味でノードエディタ形式の画像処理ツール Image Processing Node Editor を作りました その紹介の記事です 中身にOpenCVガッツリ使っているからアドカレOKですよね    ガッツリ使っているという意味では GUI部分の  DearPyGui   のほうがガッツリ使っているかもしれませんが   Image Processing Node Editor とは以下のように ノードを接続していくことで 処理結果を可視化しながら画像処理が行えるツールです 以下のような特徴があります   主要な処理は全てPython ※ライブラリ部分除く  各処理を可視化しながら画像処理が試せる    自作ノードの追加が容易  だと信じている   記事書くために見直していましたが イマイチ複雑ですわ     デフォルトでいくつかのAI機能を搭載正直言えば 似たようなツールは既に結構あるので    自己満足   自作だから内部を理解していて改造しやすい 程度のツールです 僕の僕による僕のためのツール  動作例    線画抽出入力画像に対して線画抽出を行うデモです ビデオ入力 Videoノード  → ぼかし Blurノード  → キャニー Cannyノード  → 二値化 Thresholdノード デモ動画には  NHKクリエイティブ ライブラリー  の 冬のえさ場   ヤマガラ シジュウカラ アトリ  を使用しています     低照度画像補正からの手検出ほぼ真っ暗の入力画像に対して 低照度画像補正を行い 補正した画像から手検出を行うデモです Webカメラ入力 WebCamノード  → 左右反転 Flipノード  → 低照度画像補正 Low Light Image Enhancementノード  → 手検出 Pose Estimationノード  → RGBヒストグラム表示 RGB Histgramノード     マルチオブジェクトトラッキング入力画像に対して 物体検出を行いマルチオブジェクトトラッキングを行うデモです ビデオ入力 Videoノード  → 物体検出 Object Detectionノード  → マルチオブジェクトトラッキング MOTノード ※MOTノードは試験的なノードデモ動画には  NHKクリエイティブ ライブラリー  の イギリス ウースターのエルガー像  を使用しています     車載動画に対する複数種の画像解析入力画像に対して 複数の画像解析を行い実行結果を結合するデモです ビデオ入力 Videoノード  → 以下解析処理 → 画像結合 Image Concatノード  → FPS計測 FPSノード Pythonのコードを扱うデモです ノードへの入力画像は input image  ノードからの出力画像は output image の変数に格納されています   インストール方法GitHubリポジトリの  README installation   を参照ください からあげさんの  画像処理ツール Image Processing Node Editor インストール方法   も参考になると思います 素晴らしい記事をありがとうございます  基本的な操作基本的な操作は以下のつです     ノード生成メニューから作成したいノードを選びクリック        ノード接続出力端子をドラッグして入力端子に接続端子に設定された型同士のみ接続可能    ノード削除削除したいノードを選択した状態で Del キー      ノードGitHubリポジトリの  README node   を参照ください ざっくり言うと以下の種別のノードがあります     画像入力に関わるノード Input Node   image png      画像処理に関わるノード Process Node   image png      ディープラーニングに関わるノード Deep Learning Node   image png      解析に関わるノード Analysis Node   image png      描画に関わるノード Draw Node   image png      その他 上記以外のノード Other Node   image png      試験的なノード Preview Release Node   image png      試験的なノード ※別リポジトリ公開  image png    エクスポート インポートノードエディターのノードの接続状態をjsonファイルでエクスポートすることが出来ます エクスポートしたjsonファイルはインポートして ノードの接続状態を復元することが出来ます 初期設計をミスって ノードを一度も追加していない状況でしかインポート出来ない問題がありますが     動作概要細かい処理などは省いていますが ノードエディタとノード動作をザックリ書くと以下のような感じです    mermaidsequenceDiagram    メイン処理   メイン処理  コンフィグ読み込み       メイン処理   メイン処理  フォント DearPyGUI初期化    メイン処理   ノードエディター  ノードエディター生成       ノードエディター   ノードエディター  GUI初期化    loop ノード格納ディレクトリから動的インポート        ノードエディター   ノード  ノードのインスタンスを生成        ノードエディター   ノードエディター  ノードのラベル名をメニューに反映    loop ノードエディター上の各ノード        メイン処理   ノード  ノード update          opt ノード追加イベント            ノードエディター   ノード  ノード add node          opt ノード削除イベント            ノードエディター   ノード  ノード close          opt エクスポートイベント        opt インポートイベント    opt プログラム終了        loop ノードエディター上の各ノード            メイン処理   ノード  ノード close    ノード追加方法ノードは  Image Processing Node Editor node   配下のディレクトリに Pythonスクリプトを置くと 動的インポートされます ノードの実装は DpgNodeABCクラスを継承したNodeクラスを作成し その各メソッドを作りこんでいく感じです 例えば ぼかしノードは以下のような実装になっています           タグ名          初期化用黒画像          テクスチャ登録          ノード              入力端子              画像              カーネルサイズ              処理時間          接続情報確認                  接続タグ取得                  値更新                  画像取得元のノード名 ID付き を取得          画像取得          カーネルサイズ          計測開始          計測終了          描画ちょっと追加で説明を書いていきます コメントを細かめに追記    インポート部分  指定したDearPyGuiパーツから値を取得 設定するためのラッパー関数  ノード用の抽象クラス  OpenCV形式の画像データからDearPyGui用のテクスチャに変換する関数    画像処理部分特筆すべきことは無いのですが   image process   はクラスメソッドじゃないので この関数名である必要はありませんupdate  の中でべた書きしても問題無し     クラス変数部分   pythonclass Node DpgNodeABC        ノードのバージョン      インポート時にバージョンが異なると読み込みエラーとなる      ノードのラベル      メニューバーに表示される      ノードのタグ名      ノードエディタ内でノードを識別するために使用される      システム内でユニークな名称になっていなければいけない 重複するとエラー       クラス内で使用する定数定義      システム上 特別な意味を持つわけではないので必要に応じて作成    コンストラクタ部分特に無し ノード追加時ではなく ノードエディタ起動時に実行したい処理があれば書く ただし ここに処理を追加するとノードの使用有無に関わらずノードエディタの起動時間が長くなるため注意     ノード追加イベント部分主にノードのGUIや初期化に関わる処理を書く部分 基本的にはDearPyGuiのパーツを順次追加している個所なのですが   ノードとノードの入出力名をユニークにするための命名を 型名とコロン区切りの文字列を手でゴリゴリ書く形にしてしまったことを若干後悔   もっと自動的な仕組みを考えるか 共通関数化しておけば良かった          タグ名          ノード名は ノードID ＋   ＋ ノードタグ名 のルールで作成し          システム上ユニークにする          ノードの入力名は ユニークなタグ名 ＋   ＋ 型名 ＋   ＋ InputXX           ここで指定した型と同じ型のノード出力のみ接続できる           InputXX は任意の名称で可          ノードの入力の値名は ユニークなタグ名 ＋   ＋ 型名 ＋   ＋ InputXXValue           ノードの出力名は ユニークなタグ名 ＋   ＋ 型名 ＋   ＋ OutputXX           ここで指定した型と同じ型のノード入力のみ接続できる          ノードの出力の値名は ユニークなタグ名 ＋   ＋ 型名 ＋   ＋ OutputXXValue           初期化用黒画像          テクスチャ登録          ノード              入力端子              画像              カーネルサイズ              処理時間    ノード更新イベント作成されたノードの先頭から順に呼び出される ノードの表示更新や画像処理を行う部分 ノード追加と同様 タグの解釈部分をもう少し関数化すれば良かったと後悔中   それぞれ以下の情報が入っている   connection list：自ノードに関わる  ノードの出力  ノードの入力  のペアのリスト　　   node result dict：物体検出結果などの画像以外の情報　　           接続情報確認                  接続タグ取得                  値更新                  画像取得元のノード名 ID付き を取得          画像取得          カーネルサイズ          計測開始          計測終了          描画    ノード削除イベントノードの削除時の後処理を書く部分 ノード追加時に何かリソースを確保した場合などは ここに解放処理を書く     エクスポートイベントエクスポート時の処理を書く部分 エクスポート時に保持したいGUI上の設定値などをDict形式で返却する     インポートイベントインポート時の処理を書く部分 インポート時に復元したいGUI上の設定値などがあれば エクスポートイベントとあわせて書く 長々と色々書きましたが   追加したい処理に近いノードのスクリプトをコピーして改造するのが良いかも  その他  からあげさんに ノード追加のプルリクエストいただきました   airpocketさんに 自作のアルファブレンドノードの作例をご連絡いただきました   あと 風の噂ですが   ROSのPub Subを受け取って連携する改造をされている方もいるとか   ROSとの連携もかなり面白い使い方だと思いますね  ちなみに僕は仕事で自作モデルを組み込んで検証ツールにしています,109,2022-12-20
10,10,[1年前を振り返って]ハピネススカウターをレビューする,画像処理,https://qiita.com/ymd65536/items/c82744e7ed1215047891,   年前を振り返って もうちょっと細かくハピネススカウターを作ってみる   はじめにこの記事では年前 年月日 にAWS上に作成したLINE botをレビューしていく記事です もちろん 過去の自分は知る由もないことです 当時気づけなかった観点や実装できなかったことを反省して実際に検討して実装していきます 要は 昔の記事  について実装と運用面から見ていく記事です ではいってみよう     何を作ったか行でまずは何を作ったかですが 端的に言えば  Amazon Rekognitionの画像認識技術を用いて表情を分析し 数値化して返信するLINE botを作りました というものです アイデアは拾い物ですが 中の実装は完全オリジナルという代物です 動作イメージ    技術的には何使ってるん  そんな代物をどうやってそして何で作ったのかって話をします 具体的には以下の技術です   AWS     　AWS Lambda     　Amazon DynamoDB     　Amazon S     　Amazon Rekognition     　Amazon API Gateway REST API   言語      Python  LINE API      Messaging APIはい 要するにサーバーレスアーキテクチャのLINE botです とてもシンプルに言えば アプリケーションが提供するAPIをAWS上で実行できるように調整して他のAWSサービスと連携した ということになりそうです 昨今では最も基本的でかつ運用にまで持っていくにはいろんな試行錯誤が必要な内容になります ただ作るだけならおそらく誰にでもできる内容かと思いますがこれを実際の運用までに持っていくのであれば それなりにセキュリティ対策とかアーキテクチャはこれで良いのかとかコストはどうするんだとか監視はどうするんだとか色々考えるべきことがあります いくつか観点がありますが そもそもここがイケてないよねという 良くないイケていない部分をピックアップしていきます    LINE botのここがイケてないよ   実装編    同じ情報がつのテーブルに保存されている DB設計 構築 記事内ではDynamoDBテーブルをつ作成しており 同じ意味持つ情報がつのテーブルに保存されています 具体的にはユーザーIDとImageIDです   image png  LINEからのメッセージを受信した時にユーザーIDとImageIDのみを保存していますが これらだけだとデータとしてはあまり価値がありません わかるとしてもせいぜい ユーザー数くらいのものだと思います ユーザーIDとImageIDだけでなくメッセージを受信した時のイベント情報を保存するということであれば 意味のあるデータ になるとは思いますが そうでない場合は 意味のないデータ になる為 費用対効果は高くありません データを集めるのであれば 何かしら 意味のあるデータセットを構築する ようにした方が建設的です 見極めが難しいですが データに対するドメインをしっかり把握しておくことが見極めることにつながります      解決策各テーブルの用途を定めた後 テーブルをつにしてデータを集約する この時パーティションキーとソートキーを見直します 具体的にはパーティションキーはLINEのユーザーID ソートキーはLINEが作成するImageIDとします   image png  理由：パーティションキーにLINEのユーザーID ソートキーをImageIDとすることでどのユーザーどれくらい画像を送信しているかを計測できるからということとキャパシティ管理もシンプルになるからです   image png      ソースコードに冗長な部分が多い 考慮が足りていないソースコード内で環境変数を参照していますが 中身のチェックがないです   〜省略〜  〜省略〜また 変数についても配列アクセスを複数回実行していますが 変数一つにまとめられる部分があります 例えば  画像分析用のLambda  でl を見てみるとこんな感じで配列へのアクセスが実行されていますが これはどう見ても修正の余地があります     解決策変数のチェックを入れる 変数をまとめる 可能であれば 次に説明するFlexMessageと合わせて改善すると良いです     FlexMessageを表現するjson文字列がベタ書きLINEのFlexMessageをJSONで表現していますが エントリーポイントのあるLambdaにベタで書いています 例えば  画像分析用のLambda  でl を見てみると  〜省略〜    解決策新しくディレクトリを作成して定数値として分けるなどの処理をしましょう 具体的にはlibディレクトリを作成して設計原則 単一責任の原則 を意識します 例えば  emotion flexmessage というメソッドであれば  LINE 用のモジュールを作成するなどしておくとデバッグの時にどの機能に問題があるのかをモジュール単位で見極めることができます     zipで圧縮してアップロード しかもサイズが大きいLambdaにアップロードするファイルが大きいです また zipでアップロードされていますが容量が大きいのでソースコードを確認しにくいです そもそもですが  事前準備にあるこの手順  ってなんか面倒だと思いますよね 複数のプラットフォームで改修することもあるので動作確認のタイミングで困ることもあると思います      解決策コンテナデプロイに変更する AWS Lambdaはzipで利用する以外でECRにpushされたコンテナを参照することでコンテナによるコンピューティングが可能となります こうすることでコンテナランタイムで動作するようになるのでプラットフォーム依存にならず アプリケーションの起動設定もまとめることができるようになります Apple Sillicon MacBookとMicrosoft SurfaceBookで異なるOS アーキテクチャの端末を持っているので今回はアーキテクチャごとにマルチステージビルドを書くようにしました 本番用Dockerfileなのか検証用Dockerfileなのかという部分はあるかと思いますが現状 このDockerfileは環境毎に別の動作をするような想定はなく 環境はアカウント毎に分離できていれば 問題ありません    このLINE bot ここがイケてないよ   運用編    再構築のしやすさなし 現在はマネージドコンソールをポチポチしながら作成する感じです      解決策具体案としてはAWS CLIによるデプロイ手順を作ります Lambdaの作成 API Gatewayの作成をコマンドラインできるようにします なお Lambdaはコンテナベースで開発する為 ECRへのpushもコマンドラインでやります 究極を言うとSAM サーバーレスアプリケーションモデル を使うと良いですが 現段階の改善ではそこまで不要である 認識です 理由：SAMを使う場合はコマンドラインによる作成がメインとなる 現状はマネージドコンソールでやっている為 まずはマネージドコンソールによる開発から脱却することを目指すことが良いから   note info補足：zipを使いつつ AWS CLIでアプリケーションを更新することもできます     アラームと監視の設計がない結婚式の披露宴だけ動けばそれで問題ないという要件からアラームと監視の設計をしていませんが このLINE botをある程度長い期間 例えば ヶ月以上 稼働させるといくつか課題があることに気づくと思います 具体的には以下のつです   各AWSサービスの利用制限  クウォータ    AWSのサービスの正常性確認 各サービスのヘルスチェック   LINE APIの回数制限   note info補足： アラームは何のために出すのか と サービス監視なのかインフラの監視なのか も重要です      解決策サーバーレスのアラーム設計については以下の項目について監視すると良いです   API Gatewayの呼び出し回数  httpステータスのエラー  Lambdaのメモリ使用率を監視  DynamoDBのスロットリング  クォータを消費しきったときに発生します   各AWSサービスの利用制限 クウォータ また LINE APIにはAPIの利用回数制限があり 監視すべき項目としてはAWSからLINEアプリへの push メッセージの回数です LINE APIで利用中のメッセージを取得する場合は 当月のメッセージ利用状況を取得する  を実行することで取得できます ゆえに  定期監視でDBに当月のメッセージ利用状況を記録する  累計のメッセージ数が通を超えたらシステムが混み合っているreply メッセージを返信するように改修するという動きをとると安心して使えるLINE botになるかと思います     コスト管理と見積もりがないLINE botを運用していくにしても利用料金に関する記述がないので初学者にとってはとてもハードルが高いと思われます 料金の度合いを感じ取ることのできる内容があると良いとは思うので月に 回くらい使ってみたらどうかについて考えてみます ※Pricing Calculatorを利用     料金AWS Lambda 画像処理用 リクエスト回数：回 月稼働日数：  日 月 メモリ：MB実行時間：秒   txtLambda costs   Without Free Tier  monthly     USDAWS Lambda LINE bot返信用 リクエスト回数：回 月稼働日数：  日 月 メモリ：MB実行時間：秒   txtLambda costs   Without Free Tier  monthly     USDAmazon API Gateway 回 月   APIs    日 月    txtREST API cost  monthly     USDAmazon RekognitionDetectFace：  枚　   日 月    txtImage pricing  monthly     USDAmazon Sストレージ容量  枚 月  ×   日 月  ×  MB 枚  ×   GB 換算 PUT リクエスト   回 月    日 月 GET リクエスト  回 月    日 月    txtS Standard cost  monthly     USDAmazon DynamoDB書き込み要求  回 月   Functions    日 月    txtTotal Monthly cost    USD   note info補足 ユーザー数は〜人 月に回の実行で計算しています Lambdaを秒に設定している理由としてはAmazon Rekognitionの処理を返す前にLambdaが終了する可能性があるので長くしています  検討の余地あり API Gatewayのタイムアウトは秒なのでその点についても注意する     デバッグの方法が定められていない特に記載はしていませんが 記事内で利用されているSバケットに対して画像をPUTすることでAWS Lambdaを発火することが可能です がしかし 明文化されていません      解決策ローカルデバッグはせず クラウド上で全てデバッグする テストする項目としては  Webhookの動作確認 LINE bot  AWS   受信用Lambdaの動作確認 Webhook 実行時にLINE ユーザーIDで返信をする   FlexMessageを返すLambdaはSのPUTイベントを使って動作確認をする 画像にはLINE ユーザーIDを付加する    note infoAWS XRayを導入するとアプリケーション上でどこに遅延が発生しているか分かるのでより詳細にデバッグしたい場合は導入を検討すると良いです 例えば Lambdaの料金を設定するときは平均の実行時間がわかっていると良いです AWS XRayではそういうパフォーマンスの部分を調査することができます    まとめ他にも考慮すべきことがいろいろとあります 例えば   保存した画像の保存期間を設定しておく      ストレージクラスなど  同時実行数を考慮しておく  同時接続数やユーザー数を意識する      今回は 人くらいを想定しています  Lambdaの仕様を意識したコードにする events データの精査など 年前に作ったものを見て思うのが 作ったままで終わっているというところです 結婚式の披露宴で出す出し物なのでその場限りで動作すれば良いという側面はありますがもう少し上手く作り込んでいれば LINE APIだけでなくいろんなAPIにも対応できるので少し勿体無いかなと思いました また  実務経験ありで作っている人 のと 実務経験なしで作っている人 との間では大きな差があると感じました 特に 実務経験ありの人だとサービスクウォータの部分やAPI実行回数制限 データベースのキャパシティマネジメントなどなど実務においては重要な観点だなと思いました    リポジトリ linebot ブランチで絶賛改修中です ,4,2022-12-20
11,11,cv2 Select Multiple ROIsの自作,画像処理,https://qiita.com/youichi_io/items/2424e5c79af54ccb119a,  はじめにマウスクリックで画像から複数のROI Boxを描画します ROI Region of Interest とは 画像データのうち 操作の対象として選ぶ領域のことです  対象領域  注目領域  関心領域 などともいいます 例えば 画像の一部分にだけ処理を施す場合にROIという言葉を使います   キャプチャ JPG    既存の関数を利用した場合opencvを使用すると比較的簡単に複数のROI Boxを描画可能です cv selectROIs  を使用します 事前にモジュールのインストールが必要です pip install opencv python適当な画像で試してみます コードは以下の通り   画像ファイルのパスpath    abc JPG   画像読み込み  複数領域の指定  boxを描画した画像の表示  キー入力で閉じるgifでは分かりにくいですが ROI boxをつ描画するごとにスペースキーで領域選択を確定しています 少し面倒  そしてつ目以降のROI boxを描画しているときに前のboxが消えてしまいます このあたりが不便   cv selectROIsの自作事前にモジュールのインストールが必要です       参考　          複数のバウンディングボックスの座標情報をreturn スペースキーもしくはエンターキーで画像を閉じる  画像ファイルのパスpath    abc JPG   画像読み込み      boxを描画した画像の表示      Escキー入力で閉じるROI boxをつ描画するごとに入れていた確定処理が必要なくなり つ目以降のROI boxを描画しているときに前のboxが残るようになりました 右クリックで初期化も可能です スペースキーで複数の領域選択を確定します 複数のROI box描画中の四角の色がすべて同じなのが気になるところ 改善の余地ありです   応用例はこちら  参考,3,2022-12-20
12,12,【完走賞ゲット-19】パワポ（Office系のソフトで OK）を使って画像の背景除去を行って p5.js で扱う,画像処理,https://qiita.com/youtoy/items/4ca0e09e00a6bfa97687,   はじめにこちらは  完走賞ゲットのため小ネタを毎日投稿しようとチャレンジする Advent Calendar    の 日目の記事です    今回の内容今回の内容は タイトルの通りで それを行った過程や結果は以下のとおりです 最終結果となる p js で背景透過がされた PNG画像を読み込んで表示させた状態は 以下の通りです   最終結果  また p js　のキャンバス上で表示している画像ができるまでの過程 パワポで背景除去を行っている過程 は 以下の動画の通りです この事例では  意図的に 背景分離が行いやすそうな状態にした  というのはあるのですが ※ こちらは後述  活用できそうな性能ではありそうです 簡単な画像加工をパワポで行うことがある自分としては とてもありがたい話です    背景除去機能の話今回 パワポ上で使った背景除去について 公式の情報は以下のとおりです ●Office で画像の背景を削除する   Microsoft サポート以下の画像をパワポ上に読み込ませて 上記の背景除去を行いました   背景除去を行う対象にした画像     p js でお試し用に作ったプログラムp js で PNG画像を読み込んで表示した際 p js Web Editor上で書いたプログラムを掲載しておきます 単純な 画像の読み込みと表示を行う p js のプログラム ではありますが        元の画像を少し切り出して使った話上で  意図的に 背景分離が行いやすそうな状態にした と書いていた話の補足です 今回の例で用いた画像は Twitter で自分が数日前にツイートしていた画像の一部分です そして 元画像は以下の通りです 写真の上のほうにうつっている製品の箱が フローリングの色に似ています 見るからに 画像処理による分離に悪影響を与えそうな感じです   そして 最初は背景除去機能のお試しを この画像全体を対象にやってみたのですが さすがに写真の上のほうの箱 フローリングの色に似ているもの を残そうとすると 苦戦しました 実際の利用時にそういったパターンも混じってくることがあるとはいえ もっとシンプルな事例も多々あると思われたため 最終的なお試しは元画像の左下あたりを切り出したもので試しました ,1,2022-12-19
13,13,ChatGPTで学校のレポートをどれだけ楽できるか,画像処理,https://qiita.com/quantumshiro/items/45801039de4cc0db2251,  はじめにこの記事は大阪工業大学の日目の記事です みなさんC演習Iは難しかったですか そこで一番難しかったであろう画像圧縮のレポートを退治していきたいと思います今回用いるのはChat GPTですChat GPTは色々説明でてるので今はググってください　あとで付け加えます  runLength圧縮とはランレングス圧縮とは 最も基本的な圧縮アルゴリズムの一つで 連続して現れる符号を 繰り返しの回数を表す値に置き換える方式 圧縮によって内容を損なわない可逆圧縮を行う     ChatGPTに聞いてみたChatGPTとは AIと対話ができるものである これを使って実際にレポートを手抜きしてみた   質問   回答   質問   回答   質問   回答   できているか確認  image png    結果今回はchatGPTにrunLength圧縮の例を最初に与えた上で紹介したが これを行わなかった場合 c  での実装例を見せてもらうときに文法がおかしくなってしまったり途中で書くのを止めてしまうことがあった 実装例を書いてもらうときにc言語で書いてくださいと指示したつもりがc  で書かれてしまった clangのところをcだけにしたらc言語で書いてくれるがなぜかtry againしても毎回network errorが起きてしまった   参考文献   IT用語辞典 ランレングス圧縮,0,2022-12-18
14,14,CNNを用いて動物の画像認識を行ってみた,画像処理,https://qiita.com/risaQiita55/items/459a794a5f988f3ddf2e,最近 プログラミングスクールで機械学習による画像認識を学び始めたプログラミング初学者です もし田舎で動物と遭遇した時 動物を識別できれば便利ではないだろうかと思い 今回 CNNを用いて アライグマ ハクビシン ヒグマ シカ イノシシ イタチの種類の動物の画像認識できる簡単なアプリを作ってみました 学習してきた内容の復習にもなっており こうしておけばよかった等の反省点も少し記載していくので よろしくお願いします 画像を訓練データと検証データに分類モデルの定義モデルの学習と評価実装結果の可視化アプリの作成まとめGoogle ColaboratoryVisual Studio Codeまずは 必要な量の画像を収集 保存を行います 画像収集したものを以下に保存されるように 最初にcdコマンドを実行しておきます    python cd   content drive My Drive seikabutu キーワード検索で画像データを簡単に収集できる  をインストールし必要なライブラリをインポートしました    python incrawlerのインストール pip install icrawler pythonライブラリの incrawler でBing用クローラーのモジュールをインポート globをインポート以下のコードは 検索から画像収集する際使えるコードだと教えていただいたものです 詳しいことは分かりませんが 簡単に画像収集する際 便利なコードなようです 使う人によって変える箇所は 主に点 ①に 通常ネット検索したい時と同じように 検索したいワードを日本語で入力します ②に 検索で得た画像を入れるフォルダ名を任意で入力します ここは ローマ字でフォルダ内に何が入っているか分かるような言葉なら 何を入力してもいいようです ③に ダウンロードする最大枚数を設定します ここでは と設定しました  何度か試しましたが で設定すると弱ダウンロードできました にしても同じ結果だったので で設定しました     python 動物種類の検索リストの生成search words     イタチ   ハクビシン   アライグマ   イノシシ   鹿   ヒグマ   検索画像を保存する時のフォルダ名の設定 検索した画像を名前を設定したフォルダに入れていく   Bing用クローラーの生成   クロール キーワード検索による画像収集 の実行      max num                        ダウンロードする画像の最大枚数枚に設定search words内のヒグマで検索をかけている箇所 最初は クマで検索をかけていました ヒグマに検索ワードを変更した理由を 次の項目  画像の選定 で説明しています ＞＞画像データをキーワード検索で効率的に収集する方法 Python icrawler のBing検索  各動物枚弱の画像が集めましたが モデルの精度に関わる不要なデータもありましたので 削除を行いました ．何枚も同じ画像がある．動物が複数映っている．イラストなど明らかに本物の動物でないこの選定により ある種類の動物画像だけ 極端にデータが減ってしまいました  クマ です キャラクター等 デザイン画像が多く 削除データが多くなってしまいました クマとつくキャラクターが多いことを考慮し 具体的なクマの種類に絞って検索をかけていたら 検索のかけなおしと画像の選定の二度手間が防げたかなと思います 検索ワードを一部変えたいとき 以下を実行すれば 他の種類が再ダウンロードされずにすみ 時短になると教えていただきました 注意点として 保存する時のフォルダ名 dir names だけ 使ってないフォルダ名に一旦変えておく必要があります    python 検索リストの生成    Bing用クローラーの生成    クロール キーワード検索による画像収集 の実行      max num                        ダウンロードする画像の最大枚数収集 選定した画像をgoogleColaboratoryに読み込み サイズ変更などの処理をして 訓練データと検証データに分類します 各動物の画像をを用い リスト式でgoogleColaboratoryに読み込みます その中に必要のないデータがあったため を使って取り除きました print len path    のコードは 本来必要ありませんが 画像が何枚あるのか知るために 書いています  画像の枚数画像の処理を行い 各動物のデータをずつに揃えます 枚ずつにした理由は つあります ①データはなるべく多い方がよく 最低でもずつはデータがあった方がよいため②画像認証の結果に偏りが出ないためにデータ数を揃える必要があるため   python 動物ごとのリスト作成img num          画像データの数  rgb型   サイズに変えて つずつ取り出し リストに入れている以下 画像処理について アライグマの部分だけ取り出して細かく説明していきます  枚ずつ取り出し画像処理を行うまず でアライグマの画像処理ができるように読み込みます で b g r 型になっているのを切り離し で r g b 型に結合する で x にサイズ変更を行い のアライグマの空リストに追加していくこれを で枚目の画像データまで枚ずつ取り出し繰り返します 同じことを他の動物の画像データでも行っています 画像データを r g b 型に変える理由は CNNを用いる場合  r g b 型である必要があるためです また サイズを変更を行う理由は サイズがバラバラだと実装したモデルできちんとした学習が行えないからです 動物の種類ごとに分けていた画像データを Numpy形式に変えて一つにまとめます 画像データをコンピューターが認識できるように 動物ごとに の数字を振って で ランダムに並び替えます 並び替えたデータの内  をtrainとして訓練データに 残りの を検証データに分類します  種類ごとに番号を付ける ランダムにデータを入れ替え＞＞numpy random permutation – 配列の要素をランダムに並べ替えた新しい配列を生成 　　　CNNを用いてモデルを実装し VGGの転移学習を行います CNNとは  Convolutional Neural Network の略であり 日本では  畳み込みニューラルネットワーク とも呼ばれています いくつもの深い層が重なったニューラルネットワークであり 主に画像認識でよく使われているネットワークです VGGとは ImageNetという大量の画像セットでカテゴリの分類を学習したモデルです 転移学習とは 機械学習の手法の一つであり  別のタスクで学習された知識を別の領域の学習に適用させる技術 のことを指します つまり 画像認識でよく使われるネットワークを用いて 学習済みのモデルに独自モデルを結合し 学習の精度 効率をあげるモデルを実装していきます   出力結果の固定  VGGモデルを作る  全結合層モデルを構築  学習済みのvggと構築した別のモデルを連結する  vggの重みの固定  モデルのコンパイルモデルの実装コードを細かく分けて説明していきます 初学者のため ふんわりした説明になっているかもしれません    python  VGGモデルを作るVGGを今回の画像データに合わせて実装します は r g b 型のカラー画像という事です は VGGの全結合層を使わないという意味です 全結合層部分は この後で実装します は 重みはImageNetで学習したものを使いますという意味です    python  全結合層のモデルを構築vGGで次元化されたものを 全結合層では次元で読み込むために で平滑化します を使って モデルを層ずつ定義していきます のコードは 基本的にお決まりのコードであり 数値やsigmoidの部分を場合によって変えていきます 数値の部分は の何乗かの数字 activation は最初 leru とするのが ベーシックなやり方だと教わりました sigmoidやreluは活性化関数です は 過学習を防ぐためのコードで 学習したものを 削除しています は 今回種類の動物の画像認識を行うため 数値をと設定し activation 活性化関数 は 画像が種類の内どれかというのを 確率での表示に変換してくれるsoftmaxに設定します    python  学習済みのvggと構築した別のモデルを連結する     でVGGと全結合層のモデル定義が完了したので つを連結します データをインプットする部分をとします    python  vggの重みの固定連結したつのモデルを実行する際 VGGで学習済みだった 重み が変わらないように 上記のコードで層目まで固定します    python  モデルのコンパイル最後にモデルをコンパイル モデルの学習方法を決定 し モデルの生成が終了します を adamに設定します 最適化関数は 様々な種類があるが 今回はadamが良さそうだと講師の方に教えていただきました 今後 色々試していきたいです は accuracy 正解率 と設定します 他の評価関数も存在しますが 今回のような分類の場合 accuracyと設定することが多いそうです ＞＞画像認識でよく聞く CNN とは 仕組みや特徴をから解説＞＞ 転移学習 学習済みVGG による転移学習を行う方法 PyTorch 　　　　　　＞＞転移学習とは AI実装でよく聞くファインチューニングとの違いも紹介　　　　　　訓練データを利用してモデルの学習を行い その後 検証データを利用してモデルの評価を行います    python  モデルの学習で モデルの学習が行えます は モデルに一度に入力するデータの数を設定します の何乗かで設定するのが 基本的に良いそうです 今回は と設定します 訓練データが xx  あるので 回目ですべてのデータの学習が完了します 設定する値は 訓練データの数を考慮して適切に決める必要があります 値が大きすぎると 学習が早くなるが 精度は下がります 反対に 値が小さすぎると 学習時間が長くなりすぎてしまいます は モデルの学習回数を設定します 多く設定すれば 学習率は上がりそうですが 多くしすぎると過学習を引き起こしてしまいます 逆に少なすぎると きちんとした学習が行われないので 適切な回数を設定する必要があります は 検証データの設定です は モデルの学習過程の表示方法を設定できます   表示しない  表示  結果のみ表示 となります 今回は の表示にしているので 表示内容を以下に添付しています 検証用データを使って モデルの評価を行っていきます    python  モデルの検証で モデルの評価が行えます ＞＞ニューラルネットワークのトレーニングに最適なバッチサイズについてモデルの実装結果をを用いてグラフで分かりやすく表示します 上記のように を用いてグラフの表示を行います の変動を表したグラフです は 予測に検証データを用いた場合のlossの値になります こちらも小さくなっていくのが理想です   スクリーンショット 新実装の可視化 png  グラフの結果から loss値やval loss値が段々と減っており学習が上手くいっている事が分かります もし val loss値が下がらずloss値を上回るようになってしまうと 過学習が起こっているサインなので そこで学習を止める必要があります    python モデルの評価の可視化print  validation loss     \n validation accuracy       format scores  以下 検証データを用いたモデルの実装結果です validation loss   validation accuracy   検証データを用いた時のモデルの予測に対する正解率 が約 になりました 回やって  回外れる計算なので ある程度良い結果ではないでしょうか ＞＞モデルの可視化＞＞機械学習のlossが下がらない原因は 精度低下を抑える工夫を解説Flaskを用いて アプリの作成を行いました   スクリーンショット アプリ  png  田舎で動物に遭遇した時や 動物に畑を荒らされた時 何の動物か分かれば対応や対策ができるかと思い 田舎によく居そうな動物種類の画像認識アプリを作成しました 画像認識の精度はある程度良くなりましたが 種類以外の動物を読み込んだ時も 種類の内のどれかに分類されてしまうので 種類とそれ以外で分類させるか 分類する動物の種類を増やすという改善が必要だと感じました そのためには 更にデータを収集 選定する必要があるので その知識も深めていく必要があると実感しました 今後も学習を進め 今回作ったアプリの改善をしつつ より分類が難しいものにも挑戦していきたいです ＞＞CNN画像認識で　野菜識別＞＞Niziuのメンバーを機械学習で分類してみた,0,2022-12-16
15,15,OpenCVで上下のあるサンプルの角度を取得する,画像処理,https://qiita.com/okm_6880/items/655f52d56514f1e75fc9,この記事は OpenCV Advent Calendar   の日目の記事です 日目は OpenCV pythonをdispatchから読み解く  を寄稿頂きました 本記事は異形かつ上下非対称の形状を持ったサンプルの回転角取得についてまとめたものです    はじめにOpenCVではキャプチャー画像から対象物の角度を測定することができますが 丸や四角など基本図形の回転角を測定するのは容易でも実際に測定したいサンプルの多くは異形だったりします これが上下 または左右 対象であればよいのですが 非対称の場合困ったことが起きます 例として下図のスプーンのような図形の場合を考えます スプーンの掬う側  つぼ と言うらしいです が真上の時を°とした場合下図は°となります   image png  これをcv minAreaRectで角度を取得すると  image png  °となりました これを上下にひっくり返します ° °で°になるはずですが…  image png  …°ですね 上下の認識が無いため cv minAreaRectのみでは °の範囲でしか測定できません 結果 製品の角度を取得したのち設定した角度に製品を揃える という処理をしたい場合 上下がバラバラになったものが流れてくることになります この問題について解決します    結論画像モーメント cv moments を使います mu   cv moments  検査対象   False   画像モーメントの取得x y  int mu  m   mu  m      int mu  m   mu  m     重心座標dist   np linalg norm box    x y    外接矩形の左上 重心までの距離dist   np linalg norm box    x y    外接矩形の右下 重心までの距離 distが大きい場合 スプーンのつぼは下にあるので角度を °するこれをラズパイに組み込んでリアルタイム測定してみます   環境  raspberry pi bit bullseye   opencv contrib python：     カメラ：Raspberry Pi カメラモジュール   image png    測定方法°に傾けたスプーンのような画像とArucoマーカーを紙に印刷したものを測定対象としました Arucoマーカーを使った検査面の作成については 過去の記事  を参照してください    PNG  カメラを固定していなかったり紙が歪んで貼られたりしていますが 検出角度は精度良く取れているのが確認できます   tf gif  次に冒頭で失敗した°のサンプルですが   PNG  きちんと認識されました    PNG  もちろん スプーンが下の場合を°としたい場合も対応できます その場合 とすればOKです   プログラム全文おまけで載せておきます       テキスト取得      パラメータ    size     表示画像サイズ＝ARマーカー間の実寸×size    th      閾値の初期値              時計回りで左上から順に表示画像の座標をmに格納              二値化  画面作成root   tk Tk     Windowroot title  回転角の取得  root geometry  x    ラベル  テキストボックス  ボタン  おわりに上下のあるサンプルの回転角が取れない問題を解決できました 具体的にどのようなケースで役立つのかについてはまた 来年 まとめたいと思います 明日 日目の記事は小川メソッドでおなじみの  kaizen nagoya  さんの記事です ,13,2022-12-16
16,16,【フレーム補間】撮った映像をAIでスローモーションにして遊ぼう【Python】,画像処理,https://qiita.com/keisuke-okb/items/852070dfd5a3fef9192d,  はじめに唐突ですが    映像撮ったはいいけど カクカクしてて見づらい       動きが速すぎてスローモーションで撮りたかった       そもそもスローモーションで動画撮る機材とかないし と思ったことはありませんか   それ AIで解決できます    フレーム補間に特化したAIを使って 好きな動画をスローモーションにして遊ぶ方法をご紹介します    動作確認環境以下の環境で動作することを確認しました   項目       OS    Windows  ver H    CPU    intel Core i      GPU    NVIDIA GeForce RTX  Ti  VRAM GB     メインメモリ   GB     CUDA        Python    WinPython        PyTorch       cu      そもそも フレーム補間 タスクとは動画は   静止画を高速にコマ送りすることで動く映像になっています   そのため コマの時間間隔が広いとそれだけ動きがカクカクします 秒間のコマ数 よくフレームレート ○○fps frames per second と言います は 通常fpsです 最近動画編集ソフトやYouTubeではfpsがデフォルト化してきています ダンス動画のMVなどでは あえてフレームレートを通常より下げて フレーム程度 ダンスの臨場感や激しさを表現することも多いです   用途   一般的なフレームレート    地上波放送 ビデオカメラ    fps  インターレース方式    注  により視覚的にはfpsに見える     アニメ    fps      映画    fps     YouTubeなどの配信動画     fps     音楽 ダンスなどのMV     fps     スマホで撮る動画     fps    注：インターレース／プログレッシブ方式についての説明は割愛します  フレーム補間 とは 動画を構成する大量のフレームについて 文字通り フレーム間を補間する タスクのことです   image png  動画を構成するすべてのフレームの間を前後フレームから推測し 元の動画に組み込めば   通常よりも倍フレームが多い動画ができあがることになります    元動画の総フレーム数を n とすると正確には n  フレーム 補間後の動画のフレームレートを 元動画のフレームレートに揃えれば 疑似的なxスローモーション動画 になり 倍のフレームレートにすれば なめらか動画 を作ることができます    note info  ポイント  よくフレームレート フレーム補間の業界では なめらかに動く ことを    ぬるぬる    と表現します  動画 ぬるぬる化 などで検索すると最先端のフレーム補間技術 アプリがたくさん出てきますので調べてみてください   image png  この記事では Pythonを用いて   動画をスローモーション化   したいと思います     利用する技術の簡単な解説この記事では以下のGitHubリポジトリを利用させていただきました このリポジトリは 以下の論文が主にベースになっています 以下 引用している図はこの論文が出典です    ネットワーク構造  image png  詳しくは解説しませんが この技術もU Netをベースにした画像処理ネットワークになっています   U Net万能      note warn  ポイント  U Netは単純な畳み込み構造で視覚的にもわかりやすいですが 研究が進むにつれ近年は複雑で高精度なモデルが主流になっています 前後のフレームを入力とし その前後間の動きの差分をAIで推測することで間のフレームを導きだします   image png  フレーム補間向けに適したデータセットを使って学習したモデルが配布されています 今回はこれを用いて実際に動画をスローモーション化してみたいと思います     実行準備     Python実行環境の準備  GitHubリポジトリから最新版をダウンロードし 任意のフォルダに配置します   学習済みモデルのダウンロードGitHubリポジトリにある説明から モデルをダウンロード 任意の場所に配置してください   インストールしていないライブラリがあればインストールしてください    powershellpip install torch torchvision   note warn  ポイント  GPUで動作させる場合 PyTorchの公式サイトをご覧ください      動画変換ライブラリffmpegの準備動画からフレームを抜き出したり フレームから動画を再構成するなどのタスクで ffmpeg を利用します こちらのサイトからffmpegをダウンロードし 中身のexeファイルを任意のフォルダに格納してください    note info  ポイント  ffmpegは動画変換などで大変重宝するツールです このフォルダを環境変数に登録しておくと 他のアプリやツールの実行時にアクセスが簡単になりおすすめです      変換する動画の用意変換したい動画を任意のフォルダに用意してください 今回は以下のような動画を用意しました   卵を割る動画     egg セグメント  gif    ランニング動画     run セグメント  gif    水面に泡が出る動画     water セグメント  gif      スローモーションにしてみよう以下の起動パラメータでコマンドを実行するだけで スローモーション動画ができあがります CPUやGPU環境に応じて時間がかかるので少し待ちましょう      起動パラメータの意味  パラメータ   意味      ffmpeg    ffmpegの実行ファイルがあるフォルダを指定        video    変換したい動画ファイル      sf    フレーム補間のファクター 元動画の総フレームの何倍にするか         checkpoint   学習済みモデルファイルの指定       fps     補間後の動画のフレームレート 元動画と同じフレームレートを指定すればスローモーションに 元動画×sfにすれば元の再生速度を維持しなめらか動画になります       output    補間後の動画の保存先       実行結果GIFだとわかりにくいですが 黄身の激しい動きが滑らかになっています 白身の粘り気も伝わってきます 特に動きが激しい手の動きはぶれてしまっていますが スローモーションにすると体の動きがなめらかになっていることがわかります 動きの推測が難しいためか不自然に動いている場所もありますが 細かい泡を見るとなめらかに動いているのが分かります     さいごに今回はAIで動画をスローモーションにする方法をご紹介しました こちらのリポジトリも手元の動画で学習できるプログラムが用意されていますので ご興味がありましたらぜひお試しください ,31,2022-12-16
17,17,PowerShellで空間フィルタリング(画像処理)を実装してみた,画像処理,https://qiita.com/n_osa/items/069e732d29c708d27ef7,  はじめに   note infoFUJITSU Advent Calendar  日目の記事です 今年は画像処理の勉強に力を入れてました なので 学んだことのアウトプットとして画像処理の記事を書きます また 去年勉強したPowerShellでコーディングもします この記事では PowerShellで 空間フィルタリング という画像処理を実装します 記事の最後には コピペしてそのまま実行できるコードをのせてますので 適当な画像で空間フィルタリングを動かしてみると楽しいかもしれません   空間フィルタリングとは空間フィルタリングは 出力画像の画素値を求めるのに 注目画素の周囲の画素の値を使って計算します 入力画像とフィルタを重ね合わせて 重なった画素のフィルタの値と入力画像の画素値の掛け算の合計を出力画像の画素値とします これを入力画像の全ての画素に対して行います   filter png  空間フィルタリングを適用することで 画像をぼかしたり エッジを抽出したりといった画像処理を実現できます    note infoこの記事では xの空間フィルタのみ説明していますが 一般にはもっと大きな空間フィルタも使用できます   実装方法   画像の 端 の考慮画像の端でも特殊な考慮をしなくていいように 以下のように入力画像の周囲をで囲ったビットマップ配列を作り その配列に対してフィルタを適用します フィルタを適用する時の画素位置のインデックスは で囲った分のずれを考慮する必要があります   bitmap png     ベースとするコード以下のコードをベースにして フィルタリングの関数 Xxx Filter関数 のみを各種空間フィルタリングの関数で置き換えます    powershell  コマンドライン引数Param      in file    入力画像ファイルのパス     out file   出力画像ファイルのパスAdd Type  AssemblyName System Drawing  ビットマップ配列を作成する関数      ビットマップ配列を作成 はR G Bをそれぞれ保持する必要があるため      bitmap   New Object  System Int         width     height        で初期化      入力画像の画素値をコピー              Rの画素値をコピー              Gの画素値をコピー              Bの画素値をコピー  フィルタリングの関数      実装はフィルタごとに異なる  入力画像を読み込み  出力画像のオブジェクトを作成  ビットマップ配列を作成する関数を呼び出し  フィルタリングの関数を適用  出力画像をPNGで保存  オブジェクトを破棄  各種空間フィルタリングの説明とフィルタリング関数の実装   平均化フィルタ平均化フィルタは フィルタをかける領域内の画素値の平均を求めます このフィルタをかけることで 画像をぼかす 平滑化 の効果を実現できます フィルタのサイズをxより大きくすると より平滑化の効果が強くなります 入力画像出力画像実装   ガウシアンフィルタガウシアンフィルタは 平均化に正規分布 ガウス分布 に基づく重みを付け フィルタの中心に近いほど大きな重みを付けます 平均化フィルタに比べ より滑らかな平滑化の効果を実現できます 入力画像出力画像実装   横方向の微分フィルタ微分フィルタは 注目画素の隣接画素間の差を求めることにより 画像のエッジを抽出します    note infoデジタル画像の画素値は連続ではないため 微分ではなく 差分 を求めます 注目画素と隣接画素との差を求めるやり方もあります 横方向の微分フィルタは 横方向の差を求め 縦のエッジを抽出します 入力画像出力画像実装                  画素値がより小さくなった場合の考慮   縦方向の微分フィルタ縦方向の微分フィルタは 横方向の微分フィルタと同じ考え方で縦方向の差を求め 横方向のエッジを抽出します 入力画像出力画像実装                  画素値がより小さくなった場合の考慮   ラプラシアンフィルタラプラシアンフィルタは 横方向の次微分と縦方向の次微分を同時に行い 方向に依存しないエッジを抽出します 入力画像出力画像実装                  画素値がより小さくなった場合の考慮                  画素値がより大きくなった場合の考慮  各種空間フィルタリングの実行可能コードここのコードは ファイルに保存してWindowsのコマンドプロンプトやPowerShellのプロンプトで以下のようにして実行できます   PowerShell exe  ExecutionPolicy RemoteSigned  File  保存したファイルのフルパス  入力画像ファイルのフルパス   出力画像ファイルのフルパス    note warnコードのファイルは  ps  画像ファイルは  png の拡張子を付けます 平均化フィルタ   powershell  コマンドライン引数Param      in file    入力画像ファイルのパス     out file   出力画像ファイルのパスAdd Type  AssemblyName System Drawing  ビットマップ配列を作成する関数      ビットマップ配列を作成     bitmap   New Object  System Int         width     height        で初期化      入力画像の画素値をコピー              Rの画素値をコピー              Gの画素値をコピー              Bの画素値をコピー  平均化フィルタの関数  入力画像を読み込み  出力画像のオブジェクトを作成  ビットマップ配列を作成する関数を呼び出し  平均化フィルタの関数を適用  出力画像をPNGで保存  オブジェクトを破棄ガウシアンフィルタ   powershell  コマンドライン引数Param      in file    入力画像ファイルのパス     out file   出力画像ファイルのパスAdd Type  AssemblyName System Drawing  ビットマップ配列を作成する関数      ビットマップ配列を作成     bitmap   New Object  System Int         width     height        で初期化      入力画像の画素値をコピー              Rの画素値をコピー              Gの画素値をコピー              Bの画素値をコピー  ガウシアンフィルタの関数  入力画像を読み込み  出力画像のオブジェクトを作成  ビットマップ配列を作成する関数を呼び出し  ガウシアンフィルタの関数を適用  出力画像をPNGで保存  オブジェクトを破棄横方向の微分フィルタ   powershell  コマンドライン引数Param      in file    入力画像ファイルのパス     out file   出力画像ファイルのパスAdd Type  AssemblyName System Drawing  ビットマップ配列を作成する関数      ビットマップ配列を作成     bitmap   New Object  System Int         width     height        で初期化      入力画像の画素値をコピー              Rの画素値をコピー              Gの画素値をコピー              Bの画素値をコピー  横方向の微分フィルタの関数                  画素値がより小さくなった場合の考慮  入力画像を読み込み  出力画像のオブジェクトを作成  ビットマップ配列を作成する関数を呼び出し  横方向の微分フィルタの関数を適用  出力画像をPNGで保存  オブジェクトを破棄縦方向の微分フィルタ   powershell  コマンドライン引数Param      in file    入力画像ファイルのパス     out file   出力画像ファイルのパスAdd Type  AssemblyName System Drawing  ビットマップ配列を作成する関数      ビットマップ配列を作成     bitmap   New Object  System Int         width     height        で初期化      入力画像の画素値をコピー              Rの画素値をコピー              Gの画素値をコピー              Bの画素値をコピー  縦方向の微分フィルタの関数                  画素値がより小さくなった場合の考慮  入力画像を読み込み  出力画像のオブジェクトを作成  ビットマップ配列を作成する関数を呼び出し  縦方向の微分フィルタの関数を適用  出力画像をPNGで保存  オブジェクトを破棄ラプラシアンフィルタ   powershell  コマンドライン引数Param      in file    入力画像ファイルのパス     out file   出力画像ファイルのパスAdd Type  AssemblyName System Drawing  ビットマップ配列を作成する関数      ビットマップ配列を作成     bitmap   New Object  System Int         width     height        で初期化      入力画像の画素値をコピー              Rの画素値をコピー              Gの画素値をコピー              Bの画素値をコピー  ラプラシアンフィルタの関数                  画素値がより小さくなった場合の考慮                  画素値がより大きくなった場合の考慮  入力画像を読み込み  出力画像のオブジェクトを作成  ビットマップ配列を作成する関数を呼び出し  ラプラシアンフィルタの関数を適用  出力画像をPNGで保存  オブジェクトを破棄  参考リンク 画像ファイルの読み込み  画素単位のアクセス  画像のファイル出力 の処理はこちらの記事を参考にさせて頂きました 画像処理はこちらの書籍で勉強しました 画像処理全般を学べてとてもいい本です ,7,2022-12-14
18,18,【社内ハッカソン】 社内カメラでCTO検出機を作ってみた,画像処理,https://qiita.com/takumi2786/items/194734fbd4d51dfc2498,この記事は セーフィー株式会社 Advent Calendar    の月日の記事です  tada  tada  tada セーフィーでエンジニアとして働いている伊林です セーフィーではこれまで AI領域の開発やエッジアプリの開発 サーバーサイドの開発などいろいろな開発をしてきました 今回は 中でも愛着深い   解析プラットフォーム  というプロダクトに関する記事です 解析プラットフォームとは セーフィーのカメラの映像に任意の機械学習モデルをデプロイ出来るという 非常に夢のあるプロダクトです   詳細はこちら  今回は 社内ハッカソンの形式で 解析プラットフォームに載せる画像処理アルゴリズムを開発してみました    ハッカソンのテーマ弊社CTOの森本を検知するアルゴリズムを構築することが今回のテーマです 多忙な森本さんを捕まえたい人にとっては便利なサービスかもしれません 笑また最終的には 成果物を解析プラットフォームにサービスとしてデプロイすることを目指します ハッカソンの内容については  こちら  の記事に詳しく記載されています    利用したアルゴリズム自分以外のハッカソン参加メンバーは 分類モデルを学習させることで検出器を作ろうとした方が多かったですが 前から興味のあったFaceNetを試してみることにしました 具体的には  物体検出により顔画像を切り抜く  FaceNetにより くり抜いた画像を対象の人物かどうか判別するという二段構成で推論を行います   ハッカソン参加メンバーの記事      MTCNNMTCNNとは 顔検出専用の物体検出モデルです 特に再学習などはせず 既成のものを使いました     FaceNet今回は 学習済みの顔認識モデルとして有名なモデルです FaceNetとは 簡潔にいうと 人物顔をその人特有の特徴量ベクトルに変換することができるモデル です この特徴量ベクトルを比較することで  画像同士が似ているか似ていないか  ≒  同一人物かどうか を  学習を必要とせずに  判別することができます ↑ このようなFaceNetの特徴を利用すれば    任意の人物を映像から検索するサービス   を作ることもおそらく可能です  夢が広がりますね dizzy MCTNNとFaceNetの実装は 以下のレポジトリを利用しました FaceNetの論文はこちら    ベース画像以下の枚の画像と入力画像を比較することで 対象人物かどうかを判別します ※一枚でもある程度判別できましたが 念のため枚用意しました  ベース画像    ベース画像     評価結果評価は 社内カメラを利用して収集した画像で行いました   Accuracy   Precition  Recall   見逃しは若干あるものの Precitionはかなり高いようです おそらく 顔が正面から写っていないと正しく検出されずらいのではないかなと思います     正しく予測できているもの  TP     間違って予測したもの  FP     見逃したもの  FN    解析PFへの組み込み思いのほか簡単に うまくCTOを予測できる検出器ができたため 今度は解析プラットフォームへサービスとしてデプロイしてみます 解析PFにモデルを組み込むためには 指定のフォーマットでDockerイメージを作成する必要があります 具体的には 以下のような作業を行いました   作成したコードを実行できるように Dockerfileのベースイメージを変えたり 必要なライブラリをインストールする  サンプルコード内の画像処理をおこなうメソッドを実装するまた 検出結果に応じてメールやSlack通知を送信したり 任意のWeb Hookを実行させることができます 今回は Slackに検知結果を通知してみました 過検出が若干ありましたが 概ねうまく動いていそうです tada   スクリーンショット       png    検知されたCTOの様子     まとめ今回は社内ハッカソンという形で 社内カメラを使って画像処理のアルゴリズムを作成し 解析プラットフォームへデプロイしてみました 解析プラットフォームはまだ公開前ですが 現在の段階でも簡単にアルゴリズムをSafieのカメラへ繋げられるので 面白かったです 今後 思いついたアルゴリズムを簡単に試せるようになっていくと 良いアイデアやサービスがたくさん生まれる気がしました 今回はCTO検出器でしたが 他にも作ってみたいものはたくさんあるので このような取り組は今後も続けていきたいです ,4,2022-12-13
21,21,【画像加工】remove.bg【自動画像切り抜き】,画像処理,https://qiita.com/NekoCrocus/items/19f9704d68e8412995b8,   remove bgドラッグ ドロップで画像の背景を自動で削除してくれるサイトです    使い方    remove bg   にアクセスします    jpg    加工したい画像をドラッグ ドロップします    jpg    ダウンロードボタンで画像を保存します    jpg     サンプル,0,2022-12-08
22,22,日本語対応軽量OCR PaddleOCRを試してみた ※GPU未使用,画像処理,https://qiita.com/sakamoto1209/items/59288cd88133852d2e9e,この投稿はKDDIテクノロジーアドベントカレンダーの日目の記事となります 普段は自然言語処理担当ですが別分野にチャレンジしてみました 本記事は環境構築〜検証までをから丁寧に解説するよう心がけております ※Baiduが公開している日本語OCRモデルを元にして実行しています ※日本語OCRモデルのファインチューニングなどは行っておりません   環境情報Ubuntu    LTSIntel Core i H ※GPU未使用PaddlePaddle　   PaddleOCR　             環境構築方法今回はGPUを使用しない前提での環境構築を目的としています     □DockerインストールDockerのインストール方法については下記を参考にしてください     □PaddlePaddleイメージコンテナの起動PaddleOCRはPaddlePaddle上で動作するため先にPaddlePaddleをインストールしていきます    bashdocker pull paddlepaddle paddle    cpudocker run   name ocrtest  v   ※   sample  it paddlepaddle paddle    cpu  bash  nameについては省略可能もしくは任意の名称で問題ありませんがここではocrtestとしています ※画像が配置されている任意の絶対パス    □PaddleOCRインストール   bashpython  m pip install paddlepaddle  i pip install  paddleocr     途中でrequirementsのバージョン不一致によるエラーが出る可能性がありますが問題ありません ※任意の画像ファイル名実行までの手順はたったこれだけです     □参考 PaddlePaddle  はDockerイメージが公開されてますのでGPUを利用されたい方はDockerHubより適切なイメージを選択してください   サンプルおよび結果出力ログフォーマットは  バウンディングボックスの座標     テキスト   および  認識信頼度   注 の順に出力されます 注：精度ではなく学習モデルの推論の 自信 を表すような数値のため注意下記の実行時間はすべて 秒程度でした       ①   白地にテキストを貼り付けて画像サンプルとした例 横書き ppocr INFO                                       行きかふ年もまた旅人なり      ppocr INFO                                       月日は百代の過客にして       ppocr INFO                                       向の上に生涯を浮力パ      ppocr INFO                                       馬の口とらへて老を迎こる者は      ppocr INFO                                       日々旅にして 旅をすみかとす      ppocr INFO                                       古人も多く旅に死せるあり      精度的にはもうひと頑張り欲しかったところです 一部間違ってしまっている箇所については信頼度が を切るなどしていたため相関が見受けられました       ①   白地にテキストを貼り付けて画像サンプルとした例 縦書き ppocr INFO                                       月日は目代の理透にしで      ppocr INFO                                       布さかみ中ちまた転人なり       横書きじゃないと一気に認識精度が悪くなりますね          ②   フォントを変えた画像例信頼度が 切ってしまいましたが特に問題なく認識出来ておりますね       ②   フォントを変えた画像例 その斜めになると弱いですね…       ③   手書き画像例あえてリラックマ もっとひどい精度になると考えてました 画像下部のRilakkumaの文字が認識されていないのは残念でした…       ③   手書き画像例　その あけましておめでとう が しナましておめでしう と認識されてしまいました… 手書きは厳しい模様    まとめ縦書きは残念でしたが横書きできちんとした書体という条件ならば十分な認識精度を示してくれました PaddleOCRはGPU無しでも動く軽量なライブラリですのでご興味ある方は是非ご活用ください いつかファインチューニングの方法も紹介したいと思います ,9,2022-12-08
23,23,αチャンネル付き画像の合成方法について,画像処理,https://qiita.com/nagayoshi-yuki/items/466462ef033cb29ca8f1,   はじめにこの記事は  フリュー Advent Calendar    の日目の記事となります   画像に関しての前提条件  画像はラスタ画像  色空間はRGB  RGB値及びα値は   に正規化されているとする    目次   αチャンネルとは   αチャンネルとは    αチャンネル付き画像の合成   αチャンネル付き画像の合成    αチャンネルとはαチャンネルとは 別名 不透明度 などとも呼ばれその画像を置いたときに 背景がどれだけ透けて見えるかを表します 現実的には 透過率 と近い関係にあります 例えば 下図のような位置関係でボールを見たとして赤色のガラスの透過率が         の場合 黄色のボールはどのように見えるでしょうか   image png  透過率  の場合は 完全に透明なガラスとなるので ボールはこのように見えます   image png  透過率  の場合は 光を 通すガラスになるので ボールはこのように見えます 全体的に赤みが増します ボールも 若干赤みがかった色になるのが分かるかと思います     png  透過率  の場合は 光を完全に通さないので ボールは見えません     png  現実的な例えにするために 透過率での説明を行いましたが αチャンネルは  不  透明度になるので 透過率とは逆で に近づくほど 不透明で背景の色により影響を及ぼすようになります    αチャンネル付き画像の合成これから出てくる計算式に含まれる 各変数と説明の対応表は以下の通りです  変数名   説明    b  rgb     背景画素値   b α        背景α値      f  rgb     前景画素値   f α        前景α値        αブレンドまず初めに 単純なαチャンネル付き画像同士の合成方法であるαブレンドについて解説します αブレンドの計算式は以下ですこの式から   前景 背景の画素値が 出力にどの程度影響を及ぼすのかがα値によって決定されている  ことが分かります  f  rgb  は f α 分適用されており  b  rgb  は b α     f α  分適用されています なお  out α   の場合   out  rgb      としていますが実際にはα値がの場合は完全に透明で何も見えない  影響を与えない ため  out  rgb   の値は   の任意の値です では この out α はどのように決まっているのでしょうか 二つのαチャンネル付き画像を合成したときに見える色と透過の関係は以下の図のようになります   image png   f αb α は 前景と背景二つの色の影響を受けた色のα値です  f α   b α  は 前景の色の影響を受け 背景の色の影響を受けなかった色のα値です  b α   f α  は 背景の色の影響を受け 前景の色の影響を受けなかった色のα値です     f α    b α  は 二つの色の影響を全く受けなかった色のα値です  つまり 背景のその向こうにある色 この時 少なくとも前景と背景のどちらかの色の影響を受けているのがこのつを足し合わせることで α値を求めることができます 実際に足し合わせてみましょう となります これは初めに示したαブレンドの式と同じであることが分かります     一般的な合成次は もっと一般的に使用できる式を導出してみましょう 本質的には αブレンドも αブレンド以外の式も変わりません 今回もこの画像をもとに説明します   image png  上図において 前景と背景の二つの色の影響を受けているのは f αb α です その点で 二つの画像の合成結果が受ける影響力 α値 は f αb α であることが分かります 二つの画像の合成結果を B f  rgb  b  rgb    背景の更に後ろにある色を C  rgb  としたとき目に見える色は以下の式で表すことができます また α値に関してはとなります 画像の合成結果 B f  rgb  b  rgb   の例を挙げるとといったものがあります まとめると 二つのαチャンネル付き画像を通して見える最終的なα値及び画素値はとなります しかし ここで求めたいのは  二つのαチャンネル付き画像の合成結果  ですので   C  rgb  に関する項目は不要  です つまりα値は αブレンドの式同様となり画素値はとなります  out α で割られている理由は  C  rgb  にかかっていた    f α    b α  分の色が消えたため out α で割り戻さないと 全体的に暗くなってしまうためです 式をまとめると 二つのαチャンネル付き画像の合成の式はで表すことができます 検算としてαブレンドの B f  rgb  b  rgb   を入れてみて αブレンドの式と同じになるかを確かめてみます 上の方の表で示しましたが αブレンドの B f  rgb  b  rgb   は以下の式です つまり 前景で上書きということです なお  out α に関しては 同じであることが自明なので検算は省略します となり これはαブレンドの out α の式と等しいので 正しそうであることが分かりました    まとめαチャンネル付き画像同士の合成の一般式はで表すことができる 代表的な合成方法における B f  rgb  b  rgb   はとなる ,2,2022-12-07
24,24,C++で画素データの代入を最適化する,画像処理,https://qiita.com/furyu_taky/items/20c1a19d8370cbd10c50,  はじめにこんにちは   フリュー Advent Calendar    も日目になります 今日はグッと低階層な話として 画像処理で使用する画素データの代入を最適化する話を取り上げてみようと思います 使用する言語はC  です   bitの画素データを代入するみなさん 画素データってどういうふうに扱っていますか ここでは試しに シンプルなbitのARGB色構造体を想定してみましょう    C   bitの色構造体このデータ構造体に 色の値を代入する事を考えます 最もわかりやすい形での実装は メンバにそれぞれコピーするような形になると思います    C   bitの色構造体に値を代入するstruct ARGB       中略 では これを使って画素ごとの値計算を行う場合のことを想定してみます 次のようなコードを見てみましょう    C   色構造体を使った処理の例       なんやかやあってa r g bの値が決まる     ARGB color     color SetColor  a  r  g  b          ここでcolorを使った処理をする 画素ごとにA R G Bの値をそれぞれ計算し ARGBのcolor構造体を作った上で何かの処理をします この処理を手元の環境で実行してみたところ 所要時間は  ms  になりました  回実行した合計 ではこのSetColorを高速化してみましょう    C   バイト単位でのまとめて代入struct ARGB       中略 ARGBをバイトの変数とみなし ビットシフトでバイト整数 uint t を作ることでコピー回数を減らしています    note warnこれはリトルエンディアンを想定しているのでバイトごとに降順で配置しています アーキテクチャによっては読み替える必要があります これを採用して前述の処理を動かしてみたところ 所要時間は  ms  になりました 見事倍以上の高速化ができました めでたしめでたし…     所要時間   回コピー式 ms   bitの画素データを代入するでは次は bitの画素を扱う場合を考えてみましょう 同じような構造体を考えてみます    C   bitの色構造体と代入前提として 構造体のアラインメントがバイト単位の場合は特に考える必要はありません 構造体にバイトのパディングがあるため 計バイトの構造体とみなすことでbit画素のときと同じ方法が使えます しかし アラインメントがバイト単位を想定したパディングのないbit画素の場合はどうでしょう バイト整数のうち バイトをマスクしてコピーする方法が考えられますが…   C   ◆危険◆bit色構造体のまとめて代入struct RGB       中略 しかし この方法には問題があります 構造体のサイズはバイトですが uint tはバイトですから コピー先が構造体の範囲をはみ出す恐れがあります 運が良ければ何も起きない可能性もありますが さすがにこれではいけません 正しい範囲にのみコピーする方法を考える必要があります たとえば次のような方法はどうでしょうか    C   memcpyを使った代入struct RGB       中略 いったんバイトの変数に値をまとめ memcpyでバイトコピーします これならコピーされる範囲を超えることはありませんし 安全です では速度はどうでしょうか なんとなく memcpy関数呼び出しによるオーバーヘッドが気になるかもしれません 実際に測ってみましょう bit画素のときと同様のやりかたで計測したところ次のようになりました      所要時間   回コピー式 ms  memcpy式 ms  ※回実行した合計 …ということで これでも十分高速になることがわかりました 何事も実際に測ってみることが大事ですね なぜこれで速くなるのでしょうか memcpyは展開されて早くなる あるいは変数を回コピーすることがmemcpyのオーバーヘッドよりも遅い など 理由はいくつか考えられます 機会があれば更に追いかけてみたいところです   おわりに  低階層の最適化は 意外なところで成果が出ることもある   先入観を置いておいて 何事も測ってみることが大事 みなさんもこれを機に 画素データ処理の最適化を志していただければと思います それではまた 良い最適化ライフを ,4,2022-12-07
25,25,録画映像の検索機能を作りたい,画像処理,https://qiita.com/hsmtta/items/84179f9967f2fa370be9,  はじめに会社でアドベントカレンダーをやることになったのでQiitaに初投稿します 私の勤めているセーフィー株式会社は ネットワークカメラの映像をクラウドに録画するサービスを提供している会社です クラウド上に長時間 カメラあたり週間 年 の動画が保存されているという特徴があります 多数のカメラ 長時間録画データからユーザが自身の興味関心のある映像に効率的にアクセスするには 検索する仕組みが必要不可欠と考えています 今のところ  映像の動き 背景差分    人物の映り込み 物体検出    音の大きさ をもとに 興味のある時点にアクセスする仕組みがありますが さらにいろいろなアクセス方法が提供できると面白そうだと思っています 今回は 画像をクエリにして類似する映像を検索する方法を考えてみました まだ自由研究程度のレベルなので つっこみお待ちしています    note warnセーフィーのサービスおよび開発業務では お客様の映像データにお客様の許可なくアクセスすることはありません   やったこと以下の方針で考えてみます   なんらかの方法で特徴量空間にエンコードする   特徴量空間で近傍探索する特徴量空間にエンコードする方法としては   表現学習済みのニューラルネットワークを使う  特徴量を自分で設計するのどちらから検討しようか考えました 表現学習済みのニューラルネットを使った場合 意味 semantic の抽出は優れているかもしれませんが 画像選択の根拠の説明が難しいかもしれないと思いました また 表現学習の仕方によって特徴量が変わってくるのも難しそうです ということで まずは自分で特徴量を設計してみることにしました 似たような色の映像が抽出できたら役に立つかも しれないので 色相とトーンでパレット  特徴量 を作ってみます パレットに含まれる色のピクセル数でヒストグラムを作ります 特徴量がうまく機能するのか確認するために k meansでカテゴリにクラスタリングし PCAで次元に埋め込んで可視化してみます  写真は私が適当に撮影したものです   image png  目論見どおり うまく緑系 青系 茶系の種類に分類されています 緑に分類されているmountain path pngですが 空  青色 が多いため青に近い場所に位置しているのも良さそうです 一方で 青色の少なそうなrobot png  地平ジュンこ ですが 無彩色のパレットが不足しているため グレーが青に分類されてしまっています   まとめ画像検索のお試しとして 色をベースに特徴量を作り クラスタリングしてうまく動作しているか確認をしてみました 筋は悪くなさそうだとは思いましたが いくつか改善したい点もでてきました   無彩色系の色のパレットも必要  Perceptual uniformなカラーパレットのサンプリングが必要  特徴量同士の距離の取り方 cosine 類似度      \phi \cdot \phi     は類似するカラーパレットの類似度が考慮されないので 色空間の距離を重みに入れた方が良さそう他の手法も色々と試してみたいですね ,1,2022-12-06
26,26,OpenCVで写真をドット絵調にする,画像処理,https://qiita.com/tsukemono/items/50730cd0e3b7c119bc58,  概要ドット絵とはレトロゲーム等に見られる描画方法で  低スペックのゲーム機内でそれっぽいものを表現する手法です 現在ではゲーム内の表現手法の一つとして確立しています 入れた画像をドット絵調にするコードを書きました beforeafter  ドット絵の特徴ドット絵の特徴として  低画素  色数が絞ってある カラーパレット   わかりやすい画調が挙げられます それぞれに対し  ｢画像の縮小｣  ｢K means｣  ｢輪郭描画 ハイコントラスト｣で対応させます   プログラムの流れ   背景削除まず用意した画像の背景を削除します powerpointの｢背景を削除｣という機能が便利で  よく使っています 一応OpenCVにも cv grabCut という背景を削除する関数が用意されています アルファチャンネル 透過情報 込みの状態で読み込むためにはとすればOKです     低画素   画像の縮小まずは  画像を縮小させ  画素が少ない状態にします 画像がカクカクになったのがわかるかと思います    色数を絞る   K means画像一枚に使われる色数を絞ります 頻度が多い色はひとまとまりにして同じ色に上書きするイメージです そのようなクラスタリングを実現させるために  教師なしクラスタリング手法の一つであるK meansを行います 各画素がBGRではられた次元空間上の点とみなし  その距離の近いものにまとめますOpenCV内にもK meansがあるようですが  今回はscikit learnで色を絞っていきます 透過度が高い箇所が悪さしないように  ちょっと複雑なコードになっています    python 透過二値変換 カラーパレット削減   わかりやすい画調   輪郭描画･ハイコントラスト輪郭取得はCanny法が有名で  それっぽい感じになるのでオススメハイコントラストは画像の色を線形変換すればOKです    python  コントラスト  輪郭 表示,8,2022-12-06
27,27,YOLOv7やーる（Python3.9、Windows10）,画像処理,https://qiita.com/SatoshiGachiFujimoto/items/5c872e1f577fecc8c86d,  Windows PC  Python yolov ptをダウンロードpython detect py   weights yolov pt   conf     img size    source yourvideo mppython detect py   weights yolov pt   conf     img size    source inference images horses jpgcpuだと行けたwaitKey  python detect py   weights yolov pt   conf     img size    source inference images horses jpg   view img   device cpu結果はここに入る,2,2022-12-03
29,29,画像と周波数領域と1/fゆらぎとsaliency,画像処理,https://qiita.com/KTaskn/items/9911a47f31787420da40,  はじめにこの記事は  LIGHTzアドベントカレンダー  の日目の記事です  KTasknこと たすく です LIGHTzは業務に支障がでない範囲で自由な働き方ができる風土が整っています 私は現在 そんな弊社で働きながら大学院のの修士回生 年目     として データサイエンス コンピュータビジョンの研究に励んでいます というのは 去年のアドベントカレンダーの書き出しをそのままコピってきたのですが 今年のアドベントカレンダーでは そのコンピュータビジョンを研究 勉強するうえで おもしろかった内容を投稿していきたいと思っています．  画像のフーリエ変換みなさん フーリエ変換はしたことありますか 毎日していますか 関数を周波数領域に変換するアレです．フーリエ変換だけで教科書一冊ある奥の深そうな話なのでその話はすみませんが割愛します． ウィキペディアの周波数領域の記事  に記載の記事がとてもわかりやすい．いろいろな関数は 単純な波の足し合わせで表現できるんですね．上の例だと 次元な値なんですが 実は次元の画像でも同じことができるというのが今回のお話でして 下の画像は私が昨夜ぽちぽちとドット絵エディタで打ったMegamanのドット絵になります．これだって実は波の足し合わせで表せるんですよ．二次元の波ってなんじゃという話なんですが 上の画像を構成する波が下の画像ら 抜粋 ． 幅 高さ     の画像ならx の波の足し合わせで表現されます．  波図 png  Pythonと OpenCV  を使うと下記のコードで確認することができます．  画像を開いて グレースケールにする  画像を表示  高速フーリエ変換する  周波数領域を描画          周波数ごとに波形をとりだす          で初期化した周波数行列を作成する          周波数帯 i  j をとりだし 行列に埋め込む          周波数領域    時間領域 画像 に変換する          表示する          あとで足し合わせるためにリストに保存しておく  リストに保存した波の画像を足し合わせて表示するdisplay Image fromarray np uint np abs np sum images  axis        時間領域に再変換ifimage   np fft ifft fimage  realdisplay Image fromarray np uint ifimage     風景と被写体と 周波数領域と fゆらぎそんな周波数領域なのですが 不思議なことに自然な写真だと 周波数とパワーが綺麗に反比例することが知られており 世の中で \frac   f  なんていわれる法則に従います．下の画像だと オレンジ色の線が実際の周波数に対するパワーの値で 青い線が \frac   f  のガイドになります．かなり線にのっかっているのがわかります．  image png  風景写真だとガイドに対して上振れにする傾向になります．  image png    image png  逆に接写だと下振れの傾向になります  image png    image png  被写体と風景の割合がちょうどバランスされたなんともなしな写真の場合 綺麗に青線にのります．  image png    image png    saliencyそんな周波数領域を使って 写真にうつっている重要な領域をとりだせないかという論文がありまして  \frac   f  の領域が自然な領域なら  \frac   f  以外の部分をとりだせば それは写真の中で重要な物体なんじゃないかという技術がございます． HOU  Xiaodi  ZHANG  Liqing  Saliency detection  A spectral residual approach  In   IEEE Conference on computer vision and pattern recognition  Ieee    p        image png    image png    image png    image png    image png    image png  上つの風景写真ぽいのではうまくいっている気がしますね．接写写真だと微妙な感じです．牛の画像はサッパリですが ネコの方はうまくいっている気がします．  image png  二条城なんかはかなりうまくいっている感じになりました．,11,2022-12-02
30,30,【画像認識】AIひろゆきがスプラトゥーン3のキルデス報告してくれるアプリ作った話,画像処理,https://qiita.com/Kazuma_Kikuya/items/71644455015a30a01571,  TL DRこの投稿はOpenCV Advent Calendar の日目の投稿です Python   OpenCVを利用して ニンテンドーSwitchに接続したキャプチャボードからスプラトゥーンの映像を取得 解析し  ひろゆきがキルデス報告くれるアプリを作りました   通話しながらの対抗戦でよくある スシやり 等のキル報告や スペシャルの報告を  例のAIひろゆき  が読み上げてくれます まるでひろゆきと一緒にスプラをプレイしているような感覚を楽しめるというアプリです ついでに各プレイヤーの生存秒数ゲージ デス回数カウンター 復帰までの推定予測ガイド等を視覚化するGUIも実装しました年前にもスプラで似たようなプレイヤーのデス検知 カウンターアプリを作りましたが スプラではブキの分類 SPの検知や音声読み上げという更にブラッシュアップさせることができました ソースはGitHubにすべて公開してます ただしテンプレートマッチング用の画像は含みません これに関しては後に対応いたします どういったアルゴリズム 手法 openCVのメソッドの利用方法などを簡単にではありますが本記事で書ける範囲で解説していきたいと思います   スプラトゥーンの画面から把握できることまずスプラトゥーンはvsのプレイヤーがそれぞれブキを持ち インクで塗り合うオンライン対戦ゲームです 対戦中 イカランプ  記事内ではイカアイコンと呼びます と呼ばれる画面上部にある 人のプレイヤーの持ち武器や生存中か スペシャル使用可能かなどが判別できるイカorタコの形をしたアイコンが表示されます   affdddfeedefdedafeefdfffddddee png  イカアイコンから把握できる事は     誰がデスしたか デスしてるのか      誰が なんのスペシャルウェポンが貯まったか メインブキからスペシャルが把握可能なので       デス回数      スペシャルウェポン使用回数      スペシャルウェポン抱え落ちしたか      生存秒数      デスしてる秒数　→　復帰までの推定秒数      ↑によってWIPEOUT詐欺も見抜ける   …等など デス回数などは人間では一々数えてらんないので把握することが難しいですが 画像認識によって自動カウントすればそんなことも可能です またマップ画面から イカ忍者 ステジャン 対物 積んだブキも把握し読み上げるようにしました 試してませんがマップから現状の塗り面積を割合を推定するという機能も作れそうな気がします もう一つ 実装は難しそうですが   敵のジャンプマーカーのプレイヤー名とマップのプレイヤー名／ブキアイコンを紐づけることで ジャンプしてくる敵がなんのブキか読み上げとか出来たら理想だったな      アルゴリズムを考える   マスク付きテンプレートマッチングで 対戦開始時に人のブキを分類する音声で読み上げするにはデスしたアイコンがなんのブキなのか プログラム側で把握しなければなりません またブキが把握できれば持っているスペシャルウェポンの名前も把握できます   アイコン画像の認識にはマスク付きテンプレートマッチングを利用します   バトル開始時に度だけ つのアイコンの座標を切り抜きそれぞれにテンプレートマッチングを行います テンプレートマッチングについてのコードはこちらの記事に掲載しています  現在   種類のブキがあり ヒーローシュータ含めて スコープは含めない テンプレートマッチングの総当りで正確に分類できています   個のアイコンに対して総当たりでマッチングしても 秒はかかりません しかし今後のブキ追加で今の倍増えたとすると精度 速度がどうなるのか 気になるところですね    位置 スケールを考慮したイカアイコンの検出  対戦開始時を除くと 対戦中は画面上部のイカアイコンのスケールと位置は可変であり 戦況が優勢なチームのアイコンが大きく 劣勢のチームが小さく表示されます   戦況に応じて段階のスケール変更があり 時には黒いバーをはみ出すように表示されます でアイコン位置を取得することもアイコンはスケール変更されるので毎フレームテンプレートマッチング 前作スプラトゥーンでこのアプリを作ったとき  はRGB or HSVの範囲を指定することで任意の色の領域を取得できるcv inRange  を利用してアイコンの並ぶ下レイヤにある黒いバーを検出し その検出した矩形領域の間にある画像をアイコンとして認識させました   しかし今作はUIがリッチになりすぎて当時の実装をそのまま移植することが難しくなりました   主な要因は イカアイコンがぼんやりと光るようになり 下レイヤのバーに色移りするようになったことです 色移りのせいで inRange  で指定色範囲のマスク取得が簡単ではなくなりました かと言ってHSVの範囲を広げることで黒いバーの領域を拾おうとすると ブキアイコンの一部など他のノイズを多く拾ってしまいます しかもスプラのチームカラーは毎回ランダムで 以上の組み合わせがあったはず……ゲーム側の設定で 青と黄 色固定ができるため色固定を前提に色範囲を指定することにしました 色固定じゃないと 様々なしきい値決めが大変になりそうなので     inRange  とconnectedComponentsWithStats  による解析手法ということで黒いバーに青 黄色掛かったHSV値を設定しinRange  でマスクの取得をするようにしました ゲーム画面はxでキャプチャされていることが前提で バーの表示位置は固定なので切り抜きは定数で行います この画像からバーの領域を切り取った画像を作成し その画像に対してinRange  で黄or青に色移りしたバーの領域を取得します  結果 検出領域を緑で塗った  nofilter inRange png  さて 取得したマスクにはバーではない混じってます アイコンのスケール変更したときに不要なマスクが増えたりもします これをcv connectedComponentsWithStats  による連結成分を分析することで絞り込んでいきます この関数についてはこの記事がわかりやすいと思います 右端の凹んだ箇所を除くと切り取った画像の最下部に接地しているという法則があるように見えます  ドライブワイパー等はみ出すブキのせいで 接地面のpx数はブキによって変わります  諸々考慮した条件が下記  外接矩形のwidthが指定範囲内  面積が一定px以上  連結成分の外接矩形のtopが一定以下  最下部の面積px数が一定以上これを満たす連結成分に絞り込み かつ外接矩形のleft right情報から矩形マスクを作成して緑で塗りつぶすと アイコン間のバーの領域だけうまく拾えました   img png  コードにすると下記のようになります   バーの領域のみ切り取る  黄色が色移りしたバーの色領域のマスク取得 ※キャプボによって色の出方は変わると思うのでHSV値は適宜変更してくださいyellow   cv inRange hsv                   青色が色移りしたバーの色領域のマスク取得 ※キャプボによって色の出方は変わると思うのでHSV値は適宜変更してくださいblue     cv inRange hsv                  つのマスク足し合わせ  表示用 マスクを入力画像に反映img   img copy  img mask          cv imshow  nofilter inRange   img   モルフォロジー変換でノイズ除去  labelsに画像 statsに外接矩形 面積が格納されます  マスクをクリアmask fill    連結成分分析を行う アイコン間のバー領域のみ抽出する          凹みの無いバーの領域の時               外接矩形のwidthが指定範囲内               面積が一定px以上               連結成分の外接矩形のtopが一定以下               最下部の面積px数が一定以上              条件満たせば矩形でマスク塗る          右端のバーに凹みがある領域の時               外接矩形のwidthが指定範囲内               面積が一定px以上               連結成分の外接矩形のtopが一定以下              連結成分の外接矩形のbottomが一定以下              条件満たせば矩形でマスク塗る  マスクを入力画像に反映img mask            表示cv imshow  img   img cv waitKey  ※inrangeの色範囲は キャプチャボードによって色の出方が変わると思うので適宜変更が必要かと思います 長くなりすぎるのでここからは割愛して説明すると 後述するデス状態アイコンを検出し デスの領域等の 非アイコン領域 のマスクを取得し 上記のマスクの形と足し合わせ 画像の上下に接地した矩形を描画します するとうまくアイコンのx y座標を取れるので毎フレームそれを切り抜いています 優勢 劣勢によるスケール変更にも対応しつつアイコンの状態が算出できるようになります   Animation gif  超ざっくりとした説明で恐縮なのですが   詳細なアルゴリズムは複雑で書ききれないのでgitの全体ソースを参考にしてください    デス状態の検出スプラ以前では デスするとプレイヤーのアイコンがパッとバツのアイコンに切り替わりましたが 今作ではバツのアイコンへの切り替わりにエフェクトアニメーションが追加されてしまいました…  Animation gif  まずブキアイコンがグレーと黒のシルエットになったあと  黒 グレー 濃いグレー の色の色で点滅と変形をしながらバツのアイコンになります ご覧の通り色移りするし もバツの大きさも優勢劣勢によるスケールに比例します 単色の記号ならinRangeで簡単に処理できるのでは と思うかもしれませんがそんな事がなく…薄めのグレーの色が SPが貯まってるアイコンの模様を誤検出することがあり 輪郭の外接矩形を画像の高さと一致するものに絞っても誤検出するケースが有りました SP貯まったアイコンの変な模様がぐるぐる回転するあの演出のせいで…ということで ある程度太さを持った だいたい±℃の斜線 という条件に絞り込む必要があります Hough変換による直線検出を思い浮かべましたが あれはあれでしきい値設定やノイズに悩まされそうなのでやめました もう少し簡単な手法として思いついたのがバツアイコンのバッテンの中心のy座標 pxの領域を切り取った上で 切り取った領域の上下に接地する両端の点 である点を取得し 点間の傾き 距離からバツアイコンであることを検出するというロジックです   Animation gif  点間の直線の知識は高校数学で習いましたね あの公式を使います エフェクトアニメーションの途中 バツではなく本の斜め線が表示され 最終的に点は台形になります 上下両端の点の ぞれぞれの線分の傾きを条件にすることでどちらにも対応ができるのです 点t bの傾きが  t bの傾きが の時　斜線エフェクト  naname png  点t bの傾きが  t bの傾きがの時　バツアイコン  daikei png  エフェクトアニメーションの途中 バツではなく本の斜め線が表示され 最終的に点は台形になります 傾きを条件にすることでどちらにも対応ができるのです さらにt tとb bの距離が一定px以上という条件を加えることで 線の太さにしきい値を設けています なお ブキアイコンがグレーと黒のシルエット の状態の検出は inRangeをグレーと黒の回行いマスクを足し合わせた上で 上下に接地かつ上下の接地面それぞれの長さが一定以上というような条件と面積に条件つけて検出してます これも説明が長くなるので詳細はgitのソースのほうを参考にしてください   スペシャルウェポンが貯まってるか検出イカアイコンが光っている状態の検出はヒストグラム比較による手法で実装しました まず対戦開始時に切り取った通常アイコンからHSVのS Vのヒストグラムを取得し あとは毎フレーム切り抜いた現在アイコンのヒストグラムと比較します ヒストグラムが大きく変化した時 ゲーム側で起きている状況は デス 回線落ち した SP貯まった ホコを持ってアイコンが変わった のいずれかです デスの検知は前述の手法で検出しているので デスでは無い ヒストグラム比較値がしきい値を下回るならアイコンが点滅しているとみなし スペシャルが貯まっていることが検出可能です  ヒストグラム比較の実装例  imgのヒストグラムの算出  imgのヒストグラムの算出  ヒストグラムの比較    print  類似している      print  類似していない    動画の解析にdeque 両端キュー が便利って話dequeは デック と呼ぶようです 両端キューとはスタックとキューをあわせたようなもので 直近nフレーム分の 検出結果の真偽値の状態保持 に利用できます gitに上げたコードの方で多用してます 初期化時にサイズを決め appendLeft  でリストの番目に要素を追加し 以降の要素はインデックスをずらしてくれます あぶれた要素は削除されます これがどう役立つかというと具体的には  状態Aが状態Bに切り替わったときになにか処理したい アイコンが生存→デスになったとき読み上げ通知とか   ある状態がnフレーム以上連続したら何かを処理したい  ある状態が検出され そのnフレーム後になにか処理したい等など ifの条件文次第で上記のような処理を書くことができます 認識させる画像のノイズによって一瞬チラっと状態AのことをBと誤認識してしまったとしましょう これを AがBに切り替わった と誤った判定をさせてくない場合 dequeの並びが  B  B  B  A       のように 状態Bがフレーム連続し その直前がAの時 という条件文にすることで雑に対応できるはずです 当然 フレーム単位での状態認識を正確にすることが一番適切な実装ですが  ※ただこれはフレーム落ちを考慮していません キャプボから取得する画像は 漏れ無く取得できるものなのか たまたま負荷が増え処理が遅れたときはフレームを飛ばして取得してしまうことがあるのかそのあたりの仕様は把握してません そこも考慮するならdequeサイズを絞り 条件も少し書き方を変えたほうが良いでしょう   ソースコードとアプリの動かし方前提として公開してるソースでは リサイズを行ってないxでキャプチャされた映像のみに対応してます アイコンを切り抜く座標の値等はそれを基準にしています 既に述べましたが 現状 色覚サポートONの時の黄色 青の背景色のアイコンのみに対応しています スプラトゥーンのゲーム設定でオプション＞その他＞色覚サポートONにしてください テンプレートマッチングに利用するブキアイコンのテンプレート画像は配布していません 実際にゲームの画像から取得したものを利用しているのですが それをgit上で配布することは本記事で扱ってるキャプチャと異なり著作権の引用の範囲から外れるためです アドベントカレンダーの日付に作業が間に合わなかったのですが…ゲーム開始を検知し アイコンを自動で保存するスクリプトを後日配布します 適当にナワバリバトルでも回していれば自動でブキアイコンが採取できますのでそれを命名規則に沿ってリネームして使ってください もう一点 おそらくキャプチャボードによって同じ色に見えてもRGBの値のが微妙に異なることが考えられます 適切な閾値を探るためのスクリプトも後日配布致します 少々お待ち下さい   おわりにいかがでしたでしょうか アルゴリズムの考え方 行列演算やマスク処理 モルフォロジー変換 連結成分分析 テンプレートマッチング ヒストグラムマッチング等一通り触れることが出来   画像処理の古典的手法を学ぶには相当よく出来たアプリだと思います   ついでにスプラ攻略に役立ちますしね自分のアイディアを形にできるプログラミングってやっぱり面白いなあと思いますね   自分はWeb系主戦場の人間ですが やっぱり画像処理のプログラミングが一番楽しい 色んな可能性を秘めてるので   それはそうとスプラは月からチルシーズン開幕によりブキが大量追加されたので 新たな十数個のマッチング用のテンプレート作らねば 結構手間なんですよね キャプチャするために何度も対戦しないといけないので初めてのAdventCalendar参加でした OpenCV界隈の盛り上がりに少しでも貢献できたらなと思います 読んでいただきありがとうございました ,21,2022-11-30
31,31,【高専ロボコン】高専ロボコン2022 沖縄A 撃墜機構のプログラム(トラッキング)解説,画像処理,https://qiita.com/wassy310/items/759a0421c7e540bd5b1a,  高専ロボコン 沖縄A 撃墜機構のプログラム解説   はじめに  こちらでは大会のルールは説明していないため ルール等に関しては ロボコン公式サイト  をご参照ください   こんにちは 私は高専ロボコン 九州沖縄地区大会で 沖縄高専Aチームの撃墜機構 トラッキング の制御を担当していました 残念ながら地区大会で敗退という結果になってしまいましたが 製作者として初めてのロボコンで自分が製作に少しでも関わることができたうえ 他高専さんの素晴らしい技術やアイデアを間近で見ることができ 大満足の初ロボコンでした そんな初ロボコンで 相手の紙飛行機を撃ち落とす    撃墜機構    に用いるトラッキングを任され 頭を抱えながらも担当教員や友人と相談し なんとかプログラムを完成させることができました ソースコードは GitHub  で公開しています 少しでも参考になれば幸いです    note warn後輩への技術継承も兼ねているので 来年以降のロボコンでも使えるような 汎用性の高い箇所 関数やライブラリの使い方など に絞って解説しています    概要画像認識により指定した色の物体 紙飛行機 を識別し 特定の範囲内で検出したらシリアル信号を送信します また PCとカメラの仕様と開発 実行環境は次の通りです   PC    Windows  Pro H    Intel Core i U    GB RAM  Webカメラ    BUFFALO  BSWHDMBK N   シリアル通信受信側マイコン    NUCLEO FZE  開発 実行環境    上記PCにて    Visual Studio Code   使用ライブラリ以下に使用したライブラリを示します   画像 動画処理機能がまとまっている 超有能ライブラリ  画像や動画内の物体の検出 位置の特定 動きの識別などが可能に  機械学習や画像処理において データが大量にほしいときに便利  配列としてデータを管理できる  Pythonとマイコンなどの機器との間でシリアル通信をしたいときに使う  通信の設定 open 送受信 close全てが可能   ソースコード解説     import部Pythonでは  import xxx  で モジュールなどをそのプログラムファイルの中で使えるように呼び出してあげる必要があります 今回のプログラムでは というように書きました  as の前に呼び出した xxx には  yyy という   名前を付ける   ことができます 例えば 今回の import numpy as np では  numpy に np という名前を付けて呼び出していることになります そうすることで 名前の長いモジュールなどを短くし その後のプログラム中でも   短い名前で使うことが可能   になります      main  関数main  関数には処理内容を書いています 本来であれば    関数を複数用意して 処理の流れを明確化する   というのが良いプログラマです  しかし 私のプログラムを見ていただければわかる通り main  関数しかありません       はい ということで このわかりにくいコードを少しずつ解説します Pythonでは 基本的に処理は   プログラムファイルの上から実行   されます しかし 規模が大きくなってきたり 複数の処理があるファイルができる場合は 実質的なプログラムの開始位置というものがわかりにくくなってしまいます この開始位置のことを   エントリポイント   といいますが これがあるとプログラムを   どういう順番で読めば良いかが一目でわかります    そこで Pythonではmain  関数を作成し これをエントリポイントとして使用することが多く用いられています       カメラの定義   main py    行目  の中に入る引数は 以下のようになります     デバイスID      カメラが台接続されている場合 そのカメラが指定される  複数台接続されている場合 カメラの認識順序によってデバイスIDが変わる可能性があるため注意  以上  カメラを複数台接続する場合        と引数を大きくしていけばよい    note info  デバイスIDの確認方法    あまり詳しくわからないです   もしわかる方がいらっしゃいましたらコメントでお願いします   以下のコマンドを実行  vl ctl   list devices  結果  UVC Camera        dev videoまた カメラからの画像 映像の取得には  read   を使用します  カメラオブジェクト read   というように    read  コマンドの直前に自分で決めたオブジェクト名を指定   してあげましょう 今回は  camera   cv VideoCapture   というように記述しているため カメラオブジェクトは camera になります これで どのカメラ映像を取得するかを決めます  台以上接続している場合は カメラオブジェクトも台数分用意してあげましょう これを実行するとどうなるかというと 第戻り値の    frame に カメラから取得した画像 映像データが格納   されます また このデータは各画素毎でRGBの値を所有している   次元配列のデータ   になるため NumPyで   配列データとして扱うことも可能   です ちなみに 第戻り値の    ret は画像の取得ができているか True False を見ることが可能   です 例えば とした場合 画像の取得に成功すれば True  失敗すれば False が返ってきます カメラさえ認識できていれば 大抵はTrueになります うまく認識できていない場合は cv VideoCapture  のデバイスIDが合っているか確認してみましょう       PySerial   main py    行目ser   serial Serial  COM    print ser シリアルポートの設定をしています 構文は以下の通りです どこのポートで どれくらいの速度で通信しますか ということですね    main py    行目ここはif文の中にあるため 条件を満たした場合の処理になっています シリアルデータを送信するためには  writeメソッド を使います  write 送信したいデータ  というかたちで指定してあげましょう また    バイナリデータを送信したい場合は  b データ  というようにbで囲んであげます   一応確認程度に 自分 送信側PC でも受信したデータを読み込めるようにしてあります 受信したデータを読み込むときは  readメソッド を使います  ser readline   でデータを取得し 単純にprint文で表示させています    main py  行目ser close  シリアル通信を終了するために  closeメソッド を使います       色変換  to HSV    main py  行目カラー画像  映像 というのは 基本的にRGBデフォルトでで表されています しかし RGB画像というものは 画像処理をするにあたって少し不便です  下記   HSV色空間のメリット   参照 そこで HSV色空間に変換してあげることで その不便な点を改善しよう というのが目的です HSV色変換は OpenCVを用いてすることが多いです   HSV色空間のメリット  RGBというのは  原色の組み合わせ で成り立っています しかし HSVは 明るさ や 鮮やかさ といった要素で表すため より直感的な調整が可能です       任意のキーを押したときの処理   main py  行目Cキーを押したときの処理をコロン   の後に書いていきます  press  c  to select new color というコメントである程度わかりますが     追跡対象の色を選択する際に Cキーを押下することで選択モードに入る    という処理になります 以下 処理の内容です       関心領域Region Of Interestの略 日本語でいう 関心領域  対象領域 など   画像の一部に対して処理を行うことが可能   です 画像全体を入力として処理をしてしまうと どうしても計算量が膨大になってしまいます ROIを使えば その心配もありませんし コードの読みやすさも段違いです 今回は マウスドラッグで範囲選択をしたかったため OpenCVのselectROI  関数を使用しました  cv selectROI ウィンドウ名  ソース画像  クロスヘア    ウィンドウ名    選択プロセスが表示されるウィンドウの名前  ソース画像    ROIを選択するための画像  クロスヘア    Trueの場合 選択した四角形の中に十字が表示され 中央がわかりやすくなる    特に使わないなら邪魔になってしまうことも少なくないので Falseで良いかも今回のコードを見てみると    main py  行目roi   cv selectROI  Original   image  False    Original というウィンドウで表示   image という画像でROIを選択  クロスヘア無しという情報が書かれていることがわかります       追跡対象の色の調整   main py  行目rr          mergin   main py    行目    行目前項で選択した色の調整をする部分です  rr は調整度 デフォルトは調整無しの      行目でh  s  vそれぞれの最小値 最大値を初期化し   行目で各値をそれぞれ調整できるようにしてあります AIを使っているわけでもないので 微調整は必要になるだろうと思い 実装に至りました       二値化OpenCVで使われるinRange  関数は 二値化を専門とする関数です    note info  二値化とは  対象画像を   白と黒のモノクロ画像に変換   する手法のこと 背景と処理対象の物体をはっきり区別できるため 精度の向上や処理の軽量化が見込めます  cv inRange 多次元配列 画像情報   二値化条件の下限  二値化条件の上限  となっています プログラム中では 次のように記述しています    main py  行目      モルフォロジー変換   main py    行目  モルフォロジー変換とは  主に二値画像を対象とし 画像上に写っている図形に対して作用する処理を指す モルフォロジー変換には様々な種類がありますが 今回は    オープニング処理  クロージング処理    のつに絞った変換をしました オープニング処理は 背景などのノイズ 画像の中のゴミみたいなやつ 小さい点が散在している場合がある を除去するのに有効です 逆にクロージング処理では 前景のノイズ除去をします   オープニング処理  クロージング処理      輪郭抽出OpenCVのfindContours  関数により 二値化された画像中の白または黒の部分の輪郭のデータを取得することができます   この関数で取得できる情報    輪郭を構成している座標群    輪郭の外側 内側の情報   main py  行目構文は以下の通りです  findContours 入力画像  mode  method    mode    輪郭の階層情報の取得方法を指定する   method    輪郭を構成する点の座標の取得方法を指定する また 上記つ mode method に関して 詳しい記事  を見つけたのでご参照ください       輪郭の描画   main py  行目cv drawContours image  cnts     color         thickness  引数の情報をもとに 画像上に輪郭を描写してくれる関数です 構文は以下の通りです  drawContours 入力画像  輪郭  contourIdx  color  thichness    contourIdx    描画する輪郭をindexで指定 マイナスを指定すると   全ての輪郭が描画   される   color    RGBで指定する   thichness    オプションで指定 線の太さを指定できる       長方形の描画   main py  行目概要で 特定の範囲内で検出したらシリアル信号を送信 と書きましたが この    特定の範囲    を決め 描写していたのがこの行です ROIで関心領域を指定した際に 各頂点の定義もしたので それを使っています 構文は次の通り  rectangle 入力画像  長方形の左上頂点座標  長方形の右下頂点座標  color  thickness  shift    thickness    線の太さの指定であることは先ほどと同じ マイナスを指定すると   長方形の中を塗りつぶす   処理になる   shift    各座標において    小数点以下の桁を表すビット数    あまり目に見える変化はないので 省略しても良いかも 省略した場合はとして扱われる       円の描画   main py    行目cv circle image   int x   int y    int radius            cv circle image  center              今回 指定した色のうち 最大面積の中央に円を表示するプログラムになっています 構文はこう  circle 入力画像  円の中心点の座標  円の半径  color  thickness  こちらでも thichnessがマイナスだと円の中が塗りつぶされます       文字の描画座標情報 円の中央座標など を描画してかっこよくしたかったので 文字も入れてみました これしたら一気に撃墜機構っぽくなったので 個人的に一番好きかもしれません    main py     行目構文 putText 入力画像  描画したい文字  長方形の右下頂点の座標  文字のフォント  文字の大きさ  color  thichness  thichnessは指定した値 px で描画されます マイナスを渡すとエラーが出るので注意       撃墜 自弾発射 条件の指定   main py     行目長方形で描画した範囲内に入ったら まずPCのターミナルに    中央座標  半径  GO     を表示します また  でも紹介した通り ここでシリアル通信をしています    おわりに簡単ではありますが 以上が主な処理 機能です わかりにくいところや ここ教えてほしい  というところがあれば コメントでもDMでも口頭でもいつでもどうぞ ,3,2022-11-29
32,32,PyAutoGUI 画像認識と拡大比率の問題,画像処理,https://qiita.com/Suntory_N_Water/items/9f4ca2d700c0e748e3a8,  目的PyAutoGUIを用いて作業を効率よく進めようと試みている最中に発見したことを備忘録として投稿します   経緯同じ作業を繰り返すものがあったため自動化することにした 友人も似たような作業をするので どうせならアプリ化して配ろうとしたところ以下の問題にあたった   PCの画面解像度が異なる場合 正常に画像認識ができるのか またこの問題は現在解決していなく 解決策を模索している    画面解像度が異なると読み込めない 例として以下の画像を掲載します これはPC版のメルカリで商品を出品していると表示されるページの一部です   スクリーンショット     png  今回は画面中心部にある  商品の編集  を自動で押すために 指定した画像の Path に合致する関数を作成  Path には予め切り抜いた画像を用意しました 何回かうまく動作しないことがあったので確認したところ 拡大比率が画像を切り抜いたときとは違うことに気づきました 検証として同様の部分を枚切り抜き動作確認を行います 一見ほぼ同じの画像ですが 上はWebページの拡大比率が に対して 下は で切り抜いています   syouhinnnohensyuu png    test png  関数には拡大比率 のときでしかクリックするように設定していないため 人間の視覚ではほぼ同じものでも読み取れないことが分かりました    対策  Seleniumで自動化する  拡大比率ごとに切り抜くこのあたりでしょうか どちらにせよもう少し勉強していきたい分野なので 今一度調べてみます    備考思いついた瞬間に記事を書いているのであまり深く調べていません こんなん常識だよ と感じる方は是非ご教授お願いいたします ,0,2022-11-28
33,33,「それホントにAI必要？？」現場で活きる画像解析ソリューションの選定方法とは,画像処理,https://qiita.com/michelle0915/items/e90fc99f21c44ffc978d,  動機普段お客様から案件の依頼を受ける際 当然のことながらお客様は弊社にAIによるソリューションを期待されるのですが 社名が社名   ですからね  そのうちのいくつかについては AIによる解決が難しい もしくは そもそもこれAI使わなくてもよくない  という類のものがあったりします AIを使わないというのは AIによる確率論 統計学的な処理でなく ルール ロジック ベースのプログラムによってデータを解析することを指すのですが システム専門家でない一般の方々にとっては区別がつかないであろうことも承知です というわけで 今回の記事では 画像の物体検知というトピックを中心に AIを使ったアプローチとAIを使わないアプローチによって それぞれの結果や精度にどのような違いが生まれるか メリット デメリットなど各観点から比較し 現場でのソリューション選定のヒントを提示することを目的として各種検証を行っています 最後までお楽しみいただければ幸いです   AIと画像処理技術はじめに 用語の大まかな区分について確認しておく必要があります   AI  とは 機械学習に代表される ヒトの知能を模倣することによってデータ解析を可能とする技術の総称であり 最近ではDeep Learningという言葉がAIと同義で扱われることが多いですが 実際はもっと広義の漠然とした概念です 対して  画像処理技術  とは 主に画像データを加工 変換することによって情報を得る技術であり 簡単なものでは ぼかし  マスキング  色相変換  高度なものでは 輪郭抽出  差分検知 などが含まれます     開発やお客様への説明においては AIソリューションと対比する形で画像処理技術という言葉を使うことがあったりしますが 論理的に考えれば 枚の画像をラベルごとに分類したり 物体位置を検出したりするAI技術というのは 画像処理技術の一部と考えることができます そのため 本記事においては  AI VS 画像処理技術 でなく  AIを使った画像解析 VS AIを使わない画像解析 非AI  という表現で話を進めます      オープンソースソフトウェアである OpenCV  により あらゆる画像処理の機能を簡単に利用できます   検証環境  物体検知 AIを使用した場合 画像 動画 の中から物体の位置を推定する物体検知タスクについて AIを使ったアプローチと その結果得られる出力の特徴についてご紹介します 今回使用したAIモデルは   SSD MobileNet V   という 物体検知モデルの中では一般的によく使用されるモデルを採用しています 解析方法は サンプル動画をフレーム毎にAIモデルに入力し 検出結果を反映したフレームを再度動画にするという手順で行います こちらが サンプル動画をAIモデルで処理した結果となります 物体検知AIの出力形式は 物体名 ラベル  物体位置 四角形の頂点座標  確率 信頼度 の配列としてあらわされます 動画全体を通して 人の人物は 以上の信頼度をもって高精度で検出できていることがわかります ノートPCは Laptop というラベルとして検出されていますが 動画の途中で検出できていない箇所もあります 検出されない物体については 信頼度による検出判定ライン 閾値 を下げることによって検出されやすくすることは可能ですが 反対に検出すべきでない物体が誤検出される可能性が高くなります 一般的に閾値を下げるほど 画像内の小さな物体でも検知しやすくなります 今回の検証では 信頼度閾値を に設定してあります 動画途中でヘッドホンを外した際に物体として検出されないのは AIモデルが ヘッドホン を検出するように 学習 されていないからです  ヘッドホン も検出できるようにするには AIモデルを 再学習 する必要がありますが ヘッドホンの画像を大量に収集し 学習データセットを作成した後 AIモデルの学習 転移学習 をプログラミングにより行う必要があるため AIを専門としない一般的な技術者にとっては非常にハードルが高くなります   物体検知 AIを使用しない場合 AIを使わない物体検知の代表例として 今回は  差分検知   特に  背景差分法  と呼ばれる手法について検証します 動画解析の前に あらかじめ背景のみを撮影した画像をプログラムに登録します ある意味AIの学習に近いですが 画像は枚からでも適用可能です  以後 動画のフレーム毎に 背景画像との差分を計算し 差分の大きいピクセルの集合を一つの物体として抽出するというステップで処理を行います 差分の計算方法は RGB値の差やグレースケール値の差を基準とするなどいくつかパターンがあります 以下 基準となる背景画像と差分検知により解析した結果です AIの場合と同様 物体と思われる箇所が四角枠で検出できています 机を物体と認識しないのはもちろん 背景画像に机が含まれていたからですね AIの結果と比較して気が付く点としては AIで検知できなかったヘッドホンが差分検知では正確に検出できていることがわかります このように AI的な学習を伴わない手法は 物体の種類によらず物体を検知することに向いていると言えます さらに 動画の結果だけではわかりにくいですが 左上に出力しているFPS 秒当たりの処理フレーム数 という数値はプログラムの処理速度を表しており 一般的にはAI処理を伴わない画像処理手法のほうが高速に処理できます 動画は通常FPS程度の速度で撮影されるため 差分検知の処理パフォーマンスであれば リアルタイム処理も十分可能でしょう AIの場合 リアルタイム処理にはGPUと呼ばれる高価なリソースが必要となるため 処理速度の観点では非AI手法に大きなアドバンテージがあります 差分検知の欠点としては 人と人が重なった際に一つの物体として認識してしまう点があります AIの場合も 近接した物体を正確に分離して検出することは難しいですが 差分検知の場合と比較するとある程度物体の重なりによる未検知 誤検知は少なくなります   で 結局どっちがいいの 実際現場導入にあたって AIと非AIでどちらが有利かというとケースバイケースということになります 評価基準ごとに対応表を作成しましたので ご参考にしていただければ幸いです      AI   非AI画像処理    検知対象物体   AIに学習させた物体のみ差分検知 動いている物体を検知  色抽出 赤色物体のみ抽出する など    パラメータ調整の難易度   簡単撮影環境 照明 を固定するなど工夫が必要    処理速度   遅い普通AIより速い    導入コスト   高い GPUを導入する場合 Raspberry Pi上でも容易に動作可能    導入スピード   学習済みモデルを使用する場合 早い対象の物体を検知できる学習済みモデルがない場合 再学習に大きな時間と手間がかかる   早い    まとめAIと非AIでは それぞれできること できないことの違いがある他に 処理パフォーマンスやコストの観点でも向き 不向きがあるため 現場導入における経験 特に失敗経験    がないと選定の判断が難しいところとなります 今回の記事で それぞれのソリューションの違いについて具体的なイメージを持っていただけたのではないかと自負しています 今後の問題解決のヒントとして役立てていただければ幸いです 最後まで読んでいただき ありがとうございました ,6,2022-11-28
34,34,【論文読み】Detecting Deepfakes with Self-Blended Images(SBIs)【DeepFake検知新技術】,画像処理,https://qiita.com/keisuke-okb/items/7faba2537bab8916651a,原論文をもとに解説していますが   間違い ご指摘などあればお気軽にご連絡ください     はじめに Deepfake は   AIによって写真や動画に映る顔を他人の顔に置換することでフェイク画像／動画を作る技術  です 近年は  権利やプライバシーなど様々な分野で問題視されており      フェイクかどうか と見極めるAIも近年登場しています   こちらの技術は東大の研究室が開発したものです この記事で引用する図表はすべて原論文が出典です 一部の図は独自に構成したものです    概要  DeepfakeはAIによる生成物だが それを見破るAIも近年活発に研究されている    SBIsは Deepfake画像を疑似的に作成してDeepfake検知AIの学習用データセットを作成する技術      画像認識AIを この独自データセットで追加学習することで 高精度なDeepFake検知AI ができあがる      同一人物の一枚の写真から疑似的なDeepfake画像を作成する ことが本論文の新規性である    実際の顔画像を用意するだけで簡単にデータセットを作成できるうえ 最新の画像認識モデルで学習することで最も高精度にDeepfakeを検出できることが示された  image png     研究背景   note info ポイント 疑似的なDeepfake画像を生成しデータセットを作る技術は従来からあるが 近年の巧妙なDeepfake AIの生成画像を見破ることはできなかった  従来からDeepfake検知AI学習用のデータセットを作る技術がある     ある顔写真の顔の部分を他人の顔に置き換える ことが主流    しかし 最近の巧妙なDeepfake画像に対しては検知精度が上がらないことが問題だった   自分の画像をブレンド  Self Blended して疑似的なFake画像を作る ことでDeepfake検知AIの精度を向上させることに成功した 従来技術のイメージ   image png   SBIsのイメージ   image png     DeepFakeの特徴   note info ポイント DeepFake AIによってある顔写真を他人の顔に置き換えると 一部分で画像の一貫性が失われる 下記参照※   近年の巧妙なDeepFakeにもこのような微小な 合成跡 があり   これを 同じ顔写真を用いて 再現することで DeepFake検知AIの精度向上を実現した  Deepfake画像は 顔の部分を置き換えることでアーティファクト ※合成による不自然な部分 が発生することが知られている    ランドマーク 顔パーツ  境界 色 周波数特性の不一致＊  image png  ＊ 周波数特性の不一致 の技術的な解釈額の部分は髪やほくろが鮮明である＝高周波成分を含む領域顔の部分はぼやけている＝高周波成分を含まない領域  本研究のアプローチ   全体概要   note info ポイント   一枚の顔写真のみを用いてFake画像を作成する  ことで Deepfake画像特有の微小な不一致性を持ち かつ よりFakeと認識しにくいデータセットを作成できる⇒   このデータセットを使って画像認識AIを学習すれば わずかなDeepFake合成跡を検出できるAIができあがる    image png  その処理方法から Self Blended Images SBIs “自身ブレンド画像 と呼んでいます    SBIs生成の方法   note info ポイント 合成前／後の顔写真を生成する Source Target Generator STG  と合成する顔の領域を出力する Mask Generator MG  で構成される  No   処理の流れ    STEP      実際の顔画像をSTGとMGに入力する    STEP      STGは 画像の色味などの変更を加え 加える前と後をペアにしておく Sourceの顔の位置や角度をわずかに移動する    STEP      MGは 元の顔画像の顔の位置を検出し  置き換える顔の領域 のマスクを生成する Stepで移動した位置 角度を反映させる     STEP      TGの出力画像と MGのマスクを利用して わずかに色味や位置 角度がずれた顔画像を合成する    image png     SBIsの例   note info ポイント 一見しただけではFakeと見破れない顔の合成画像が生成できる従来と比べ合成する顔写真の検索が必要ないため SBIsは汎用性が高い  本研究の処理により わずかに顔の位置 色や輝度 シャープネスなどの違いのある合成画像が生成できた    一見しても 顔写真を加工し合成された画像とはわかりにくい    画像に施す変換処理は背景と顔でそれぞれ異なるため SBIsはDeepfake特有の不一致性を再現できている  image png  上段：入力画像 下段：SBIs DeepFakeを再現した画像   実験手法   note info ポイント 従来手法と本研究の提案手法を 複数のDeepfakeデータセットを用いて検出精度を比較  既存の画像認識ネットワーク EfficientNet の学習済みモデルを SBIsを用いてDeepfake検知タスク向けにファインチューニングしたものを利用  実際のDeepfake画像を用いた検知精度をAUC＊で比較   note infoAUCとは値分類タスクにおいて精度を図る指標の一つです Deepfake画像を正しく 偽物 として検出できた割合が高く かつ本物の顔画像を 偽物 として検出してしまった割合が低いほど値が大きくなる評価指標で   の範囲を取ります 定義的には 真陽性率と偽陽性率をプロットしたROC曲線の下側の面積のことを指します Deepfake画像を正しく 偽物 として検出できた割合＝真陽性率 TPR 本物の顔画像を 偽物 として検出してしまった割合＝偽陽性率 FPR   実験結果   note info ポイント 従来の検知技術よりも 多くのDeepfake画像データセットで検知精度の向上に成功  つのうちつのDeepfakeデータセットにおいて 提案手法が最も高い精度を出している  提案手法はDeepfake画像を用いない学習にもかかわらず 多くの従来手法を凌駕している表  従来手法と提案手法の検知精度の比較従来手法のAUCは各論文に示されている値  image png     従来手法と提案手法のモデルの特徴付けの違い①   note info ポイント 従来手法で学習したDeepFake検知AIは 置き換えた顔全般の特徴を重視していたが 提案手法では より詳細な不一致性のある個所を重視していることがわかった  image png  図  各Deepfake画像を入力したとき検出モデルが重視する領域 顕著性マップ 左の列から Deepfake画像 ベースライン 従来手法 モデルの顕著性マップ 提案手法の顕著性マップ   従来手法と提案手法のモデルの特徴付けの違い②   note info ポイント 従来手法では SBIsと本物の顔画像を混同していたが SBIsで学習したモデルは判別できる能力を有しており より正しく検知できる  従来手法のモデルでは 本物の顔画像とSBIsの特徴マップが重なり SBIsをFakeとして検知できない  提案手法では 本物の顔画像の特徴がSBIsやほかのDeepfake画像と離れた領域にあるため 誤検知率を下げることに貢献している  image png  図  従来手法と提案手法の特徴マップの可視化次元削減手法t SNEを用いて次元に圧縮したベクトルを可視化している上：従来手法 下：提案手法　水色：本物の顔画像 茶色：SBIs  技術的な制限   note info ポイント SBIsは ある顔画像の顔部分を置き換えることの不一致性に重点を置いた画像フレーム単位の手法であるため 場合によってうまく検知できない場合がある  動画特有の一時的な不一致性 動きで顔がぼやける等     提案手法は画像単位で学習されたモデルであるため 時間方向の特徴を検出することはできない  画像“全体 を生成するDeepfake画像    実際の写真や映像の顔一部分を置換するのではなく 画像全体を生成するようなAIの生成画像に対してはうまく検知できない  まとめ  SBIsは 顔画像自らを用いて疑似的なDeepfake画像を合成する手法    合成した画像は Deepfake検知AIの学習データセットとして用いることができる  この合成データセットを用いて検知AIを学習することで 様々なDeepfakeの検知精度が向上した  参考文献,29,2022-11-27
35,35,【実装あり】Pythonで顔検出するライブラリをまとめてみた,画像処理,https://qiita.com/keisuke-okb/items/de7845dc695e3b432243,  目的処理  カメラで撮影した写真 ここでは人を想定 から 顔の部分だけを抽出し  他のAI StyleGANなどの画像生成AI等 に応用することを考えています この記事では 以下の種類のライブラリを用いた独自の顔検出プログラムをまとめました    note warnライブラリの導入に関しては様々な記事があるため ここでは割愛させていただきます   利用するライブラリの比較それぞれいずれかを使うことで 写真を顔検出し 検出した顔の部分を切り取って画像に保存することができます CPUを利用することを前提としています   ライブラリ名   検出方法   精度   検出速度   導入コスト   特徴    Dlib   ランドマーク   〇高   △遅＊    △中   利用するランドマーク検出モデルによって顔の細かいパーツ  か所モデル等 を認識可能 顔検出においては文献が多く実装しやすい反面 GPUを利用する際はコンパイルや パッケージの依存関係に注意が必要     OpenCVカスケード分類器   バウンディングボックス   △中    ◎速    ◎低   PythonのOpenCVライブラリを用いて直感的に実装可能 XMLを用いた軽量カスケード分類器のため 検出速度は速い分精度は他と比べると低い     InsightFace    ランドマーク    ◎高    ◎速   △中   ランドマーク検出可能 Dlibに比べ 高解像度のまま高速に検出できる 片目が見えなくなるほど角度の付いた横顔でも検出可能 ピンボケしていても検出できる Dlib同様 パッケージの依存関係には注意      note info  ＊検出速度について  Dlibは 入力画像の解像度が上がるほど顕著に検出速度が遅くなってしまいます 事前に画像をダウンサンプリングしておくことで検出を高速化できます    note warn私の環境で試した際の感覚ですのであくまでご参考になればと思います   顔検出に利用した画像写真素材サイト ぱくたそ の画像を利用しました クロップ機能を使い 入力画像のアスペクト比や解像度はまちまちです 正面を向いている人の検出を想定しているので 横顔の写真は利用していません   image png  眼鏡やサングラス 帽子を身に着けた方 コスプレをしいる方の写真を利用しています   実現する機能  検出した顔の傾きを補正する  どんな環境で どんな人が撮影しても 正面を向いていればなるべく同じ画角になるように写真を正方形に切り取る  顔検出のロジックライブラリ共通で 以下のような流れで顔検出 顔部分の抽出を行います    note infoOpenCVにおいては  顔検出器 と 目検出器 を組み合わせて利用します   それぞれの目のランドマークの重心を算出→目の位置として利用  口のランドマークの重心を算出→口の位置として利用  左右の目の位置の横方向と縦方向の差をそれぞれ w  h とする  写真の中央を原点として 写真を heta だけ回転 アフィン変換       画像座標系で示されたランドマーク座標  x  y  を中心座標系  x   y   に変換       heta に合わせてランドマーク座標  x   y   を回転      回転後のランドマーク座標を画像座標系に変換      画像座標系で示された回転後のランドマーク座標  x   y   を返す  目か所の幅から 切り出しの画像幅を求める crop width   縦方向の位置修正パラメータを求める h adjast   写真の切り抜き実行    Dlib   検出モデルのクラス定義 傾き回転関数定義StyleGANの顔検出の実装 Dlibを利用している部分 を参考にしています    顔検出 切り出し実行dst file：切り出し画像保存先のパス  各パーツの位置の平均を算出  目の傾き補正   顔検出結果※検出できなかった場合黒塗りの画像を出力しています 入力画像  image png  切り出し結果  image png     note info魔女コスプレの方の写真枚をのぞき 傾きを補正してうまく検出できています     OpenCVカスケード分類器   note warn顔の傾きに影響して顔検出のバウンディングボックスが大きく変化してしまったため 目の位置を用いて補正した後再度顔検出を行っています    検出モデルの定義   顔検出 切り出し実行dst file：切り出し画像保存先のパス        初期検出実行         検出されなかった場合の閾値緩和  目の位置を検出することによる傾き補正if len eyes         目を合計個以上誤検出した場合はスキップ    if     tan deg       顔が傾いていると判別する角度の判定          傾き補正後再検出  目の位置を考慮したトリミング調整   顔検出結果※検出できなかった場合黒塗りの画像を出力しています 入力画像  image png  切り出し結果  image png     note warn顔の検出そのものは枚をのぞきできていますが うまく顔の傾きを補正できていないものがいくつかありました     InsightFace   検出モデルの定義 傾き回転関数定義   顔検出 切り出し実行dst file：切り出し画像保存先のパス  各パーツの位置の平均を算出  目の傾き補正   顔検出結果※検出できなかった場合黒塗りの画像を出力しています 入力画像  image png  切り出し結果  image png     note info傾きを補正しつつ すべての写真において顔検出に成功しました    おわりに顔検出ができるつのライブラリを用いて顔検出の比較を行いました 顔検出をしたいと考えている方の参考になれば幸いです ,43,2022-11-27
37,37,OpenCVで画像処理（平滑化）,画像処理,https://qiita.com/yuzu_m/items/364cc5ab262f0d02db99,  はじめにこんにちは  MYJLab Advent Calendar   の日目は私が担当します よろしくお願いします 昨日は  Glen Maki さんの XGBoostでチャンピオンズカップを予想してみたという話  でした こちらの記事も是非ご覧ください   この記事でやりたいことGoogle Colaboratory上でOpenCVの機能を使って画像処理をおこないます 今回は 画像の平滑化をしていきます   やってみる早速ですが はじめていきましょう    環境私は Google Colaboratory を使用しました    準備今回はGoogleDrive内の画像を使用するのでMatplotlibは 画像を表示するために使用します 次に 処理をする画像を指定します このように 処理したい画像を置いているディレクトリのパスとファイル名を繋げたものを記述します IMG FILE    処理したい画像を置いているディレクトリのパス   ファイル名 私は GoogleDriveのMy Drive下にある image というディレクトリの中に置いている bear jpg という画像を処理したいので   content drive My Drive image bear jpg と記述しました そして 下記の方法で画像を読み込みます OpenCVはMatplotlibと色の並びが異なるので cvtColorを用いて色の並びを変換します ここまでの手順で加工前の画像を表示する準備ができました 実際にこちらの記述を追加して画像を表示してみましょう ※plt axis  off  は軸を消去するための命令です実行結果はこのようになります 可愛いですね    本題さて ここからが本題になります ここまでの準備で画像が表示できるようになったので ここからは画像処理をおこなっていきます OpenCVで画像を平滑化する方法は複数あるようですが 今回はcv blurを使用して平滑化します ※数値を大きくするほど平滑化具合が強くなります 準備の時と同様に 画像を表示します    実行結果実行結果はこのようになります 無事に平滑化できたようです   bear png    おわりに今回は OpenCVで画像を平滑化してみました 何か問題などがございましたら ご指摘いただけますと幸いです 宜しくお願い致します 最後までお読みいただき ありがとうございました  MYJLab Advent Calendar   には面白い記事がたくさんあるので 是非ご覧ください   参考,6,2022-11-26
38,38,cudaをdockerで使う方法,画像処理,https://qiita.com/HarunobuEnami/items/de1c15406820ceacff08,本記事ではyoloを使うことを例として cudaを実行できる環境をubuntu上にdockerで構築する方法を説明します． イントロcudaの上で動くアプリケーションはそのバージョンごとに必要とするcudaや依存するパッケージのバージョンがバラバラである．例えば tensorflowには系と系があるが それによって必要とするcudaのバージョンが異なり また yoloが要求するtensorflowのバージョンも異なる．すなわち依存関係が極めて複雑であり 直接OS上にパッケージをインストールして使うのは困難かつ 既存の環境を破壊する恐れがあり危険である．そこで dockerというコンテナ方式の仮想環境を用いることでその作業専用の依存関係を満たした環境を作成し 他の環境に影響を与えないうようにすることが求められる．本記事ではdockerのインストールが完了している環境においてgpgpuをdockerで使う方法を解説する．  前提条件dockerインストールが完了していること． 必要に応じて docker composeもインストールする  nvidia smiを実行してnvidiaのドライバが正しくインストールされているか確認するのように表示されればドライバはインストールされている．されていない場合には先にドライバのインストールを行う．Ubuntuの場合には sudo ubuntu drivers autoinstall コマンドでインストール可能 公式サイト  のやり方通り,1,2022-11-23
39,39,顔面フィルタリングアプリ（あくまで設計だけです。）,画像処理,https://qiita.com/taichin777/items/676af05f1dd1c118e7b0,  わたしの自己紹介社目：テストをする人　万円社目：SE　万円社目：AWSに関わる人　万円   note info社目で大きく  年収  が上がりました 趣味：節約 風呂なし万円 半額弁当生活 資格：AWS SAA TOEIC 点オーバー  はじめに　マッチングアプリについてわたしは昨年結婚をしました 私たち夫婦はマッチングアプリで知り合って結婚まで行きました    note infoマッチングアプリで結婚した人は年の現在は人に人の割合でお見合いや会社で街コンよりも割合が増えている見たいです 私はマッチングアプリのペアーズとwithを使っていました フィルタリングを活用して 表示数を私自身減らしていました フィルタリング機能とは年齢や年収 現在住んでいる場所 子供が欲しいかどうかなどをあらかじめプロフィールで登録している情報から弾くことができます   フィルタリングについてお金持ちや庶民に関わらず時間だけは時間と平等です 例えば 専業主婦 主夫になりたい人間は年収万円以上のパートナーしか表示させないということができます 結婚相手を選ぶ上で何が大事かというアンケートにおいて〜位にルックスが含まれます わたし  ルックスで相手を見てない ってやつが大嫌いです 何故ならば証拠がないからです 日本人が結婚を行う上で仮面舞踏会で知り合って ルックスを知らないまま結婚をしたケースは約年間日本で過ごしていますが ニュースでも聞いたことがありません ルックスを見た上で我々は結婚を決めているからです 年収を知らないで結婚をするケースは十二分にあるでしょう 趣味を知らないで結婚をするケースも当然あると思います が ルックスだけは知らないで結婚するケースは存在しません が 上述しておりますが 国内でもトップのマッチングアプリサービスにおいて顔面フィルタリングはありません 全くないかと言われると微妙ですがグラマー スリム 普通など体型を自己評価するものがありますが グラマーで本当に私が考えているグラマー体型は人に 人しか存在しません グラマーというのはBMIがいくつで 胸のサイズがいくつで という数値化は当然されていないため入力するその人の感性により決められてしまいます   顔面フィルタリングの機能があったらもしもボックスの世界ではありませんが 機能がある場合 どのようなことが起こるかというとぱっちり二重で 髪の毛がロングのストレートでスタイルがいい人しか表記させないということや逆に一重で歯並びが悪い人間を弾くことができるようになります ルックスに点数をつけることはどうかという一部の人間は当然一定数いますが 私から言わせると無しの人間に時間を使うことの方が双方にとって不幸せです 無しだとどちらかが思った時点で恋愛は無しになります    note infoルックスに点数をつけることはどうかという一部の人間は当然一定数いますが 私から言わせると無しの人間に時間を使うことの方が双方にとって不幸せです 無しだとどちらかが思った時点で恋愛は無しになります    顔面フィルタリングの設計について男女共にアンケートを取り 好き嫌いを明確にする 例えば 目の大きさは顔面の割合に対して目がどれだけの大きさを占めているか胸の大きさは横向きの写真を撮ることや正面からの膨らみからある程度わかると思います もちろん目が極端に大きい場合はプラスではなくマイナスに働くこともあるため目や胸の大きさに関して点 点で示すのではなく小さい 大きいという表記にすることでフィルタリングができるかと思います    一番大切なことなぜ設計だけなのかというと わたしには技術力がないということももちろんあるのですが こちらの学習をするためには尋常じゃないほどのAIの学習コストがかかること超人手が必要であること   note info顔面フィルタリングがマッチングアプリに搭載されたとしても活用されない未来しか見えないのであくまでわたしの妄想だけの世界に留めておきます ,0,2022-11-19
40,40,ディレクトリ内にある画像をリサイズするPythonコードを公開するよ！！,画像処理,https://qiita.com/ukyoda/items/8fe5aec9a0a0eac79208,ディレクトリ内にある画像をリサイズしたいとき いつも同じようなコードを作り直している気がするので メモがてらQiitaに投稿しようと思う    必要なもの   使い方下記のどちらかのコマンドを実行するだけ srcdirにある画像をリサイズしてdstdirに出力します    bash  サイズを指定してリサイズ  サイズは  width x height  で指定する   割合でリサイズ   ソースコード,0,2022-11-18
41,41,【文字検出_備忘録】OpenCV X Python エッジ処理画像における物体検出,画像処理,https://qiita.com/Miya_0000/items/edd2c3945d08810e6a9c,続きエッジ処理あとの画像で物体検出をしようと思う 待て待て焦るんじゃない たくさんネット記事が出てくるので 何となくできそうな気はするけどちょっとここで立ち止まって 画像をちゃんと確認したい     エッジ処理後の画素値早速画素値を見てみよう ちゃんと かになってますね この  に落ちる点が特異点な気がするなー なんかそういうのできないかなーと思って この次元行列をX軸方向に一つずらして計算する方法を考えてみました ちょっと文字では言いづらいので頑張って残しますね   元行列 png  本来はかかですが 簡易的にとにしています 元の画素値行列がこれだったとします 物体ってどこのことを言うんだろうって考えた時に こういう検出ができたらいいのでは と思いました   元行列 png  つまり 一番左の列を基準として 右のお隣に増えてるかな 減ってるかなって言うのがこの四角の始点 終点でありそれをもとに一つのグループを作っていけば良いのかなと よって この行列だと 以下の二つの行列を先に作ります 行列 左にゼロをパディングした行列  左にパディング png  行列 右にゼロをパディングした行列  右にぱでイング png  そして 行列ー行列をすると 一番左を基準として右のお隣さんとの比較ができますね この場合だと 計算結果がこんな感じかな   比較結果 png  それで 一番左の列と右の列を無視すると以下のような行列ができますね   比較結果最終 png  ちなみに 元の行列の左から二番目以降にこの値が出ている点を取り出してみると  元画像比較 png  こんな感じになって なんか良くない  語彙力 ここからこのエッジから周りを探索して 同じグループをグルーピングすればいいじゃん と言うことで こんなコードを書いてみました  物体検出処理 ゼロパディングを実施した行列をそれぞれ作るいい感じのファイルができたのでbool行列にでもしておこうかなと思います さて ちょっとここまでで Trueの点プロットでもしてみようかなと思います   失敗 png  逆ですね xとyが逆ですね 見事に どっかでミスってますねーこれは さてさて 妊婦健診もあるし ご飯を作るので もうそろそろやめます 笑,0,2022-11-16
42,42,【文字検出_備忘録】OpenCV X Python エッジ検出処理,画像処理,https://qiita.com/Miya_0000/items/0aac9a555082cdb2b748,続きをやっていきましょう 今日は値化ファイルの画素値出力をした後 エッジ検出処理をやります     画素値出力 値化ファイルの画素値出力としてみると ちゃんとかになっていることがわかりました ヒストグラムみると若干違ったのでちょっと怪しんでましたが まぁ良さそう     エッジ検出Canny法でのエッジ検出をやってみます 参考資料：Canny法について日本語がありがたい 閾値を求めるのが難しそうだったのですが 一旦サンプル通り  でやってみようと思います  Canny法を使ったエッジ検出処理 画像出力すみません 今まで全然画像を見せてなかったんですがいい感じになりました 左がオリジナル画像で 右がエッジ検出後です   Edge検出後 png  ,0,2022-11-15
43,43,【文字検出_備忘録】OpenCV X Python グレースケール画像の２値化,画像処理,https://qiita.com/Miya_0000/items/0c42a93babb3733d5263,こんにちは 続きをやりましょう     グレースケール化の確認グレースケール化までやったのでそれができてるかの確認をします せっかくなので ndarrayの出力も試したいなと思って 以下のコードを書きました 実行結果 print     print     print     print     print     print     print     print     print     print     print     print     print     print     print     print     しっかり入ってますね では早速 これのヒストグラムでも書いてみましょう  ヒストグラム参考：ヒストグラムの出し方参考：plot関数についての描画領域Figureと座標軸Axes出てきました   Histogram png  多分 この一番右の多い群が白色かなぁとか思ったり 大津の値化を使って やってみましょう     値化 大津の値化をって 閾値を求める 値化 画像出力thに閾値 img gray th otsuに値化後の画素値が入ります cv thresholdの引数 threshにはいったんを入れてます これが大津法によって最適化されてthに入ると考えてます Maxはにしときました maxの画素値を入れてもいいですが そこまでする必要はないかなと思いました ,0,2022-11-15
45,45,"LaMa で物体を""無かったことにする"" (画像内の物体をAIで自然に消す)",画像処理,https://qiita.com/RyoWakabayashi/items/ae6264a38897ecea7dc2,   はじめに写真を撮ったら 余計なモノまで写っていた なんてことありますよね   sofa png     机の上のお菓子を消さなきゃ     足元のコードが汚いなぁ そんなときに役立つのが この LaMa です何はともあれ動かしてみましょう今回は Docker で動かします   実行環境  macOS Monterey     Rancher Desktop     Python   CUDA 対応の GPU がないので CPU で実行しています   準備公式リポジトリーをクローンしてきます   bashgit clone ディレクトリーを移動します   bashcd lama   コンテナのビルドDocker ディレクトリーでコンテナをビルドします   bashcd docker  build shcd      モデルのダウンロードファイルをダウンロードするために必要なライブラリをインストールします   bashpip install wldhx yadisk direct最良のモデルファイルをダウンロードし 展開します   bashcurl  L   yadisk direct   o big lama zipunzip big lama zip   テスト用データの準備まずはテスト用データで動作を確認してみましょうテスト用データをダウンロードし 展開します展開すると 下画像のように xxx png と xxx mask png がペアになったファイルが入っています  スクリーンショット       png  xxx png　が元画像で  xxx mask png が元画像から消したいところを示すマスク画像ですxxx mask png の黒い部分が残すところ 白い部分が消すところになります    テスト画像の例マスク画像を透過して元画像に重ねてみたのが下の画像になります看板内の似顔絵や観覧車のいくつかの柱 パラソルや何人かの人間が削除対象として選択されています   テスト画像での削除処理実行私の macOS で実行したとき   Permission Denied  エラーが発生したので 以下のファイルを編集しましたこの状態で以下のコマンドを実行しますoutput ディレクトリーに各画像の処理結果が出力されます見事に対象物が消えました 画像を交互に切り替えてみると このようになります   gif  元あったものが消えて 違和感ないように背景が補完されているのが分かりますね   目的の画像を処理するそれでは 自分の用意した画像を処理してみましょうまずマスク画像を作らないといけないので 元画像を画像編集ソフト レイヤーが使えるもの で開きます※以降の画像は FireAlpaca　で編集した例です  スクリーンショット       png  レイヤーを追加して 不透明度を などの透ける数値にしておきます  スクリーンショット       png  追加した方のレイヤーで画像全体を黒に塗りつぶした後 消したい部分だけを白に塗ります  スクリーンショット       png  いい感じに塗れたら追加したレイヤーの不透明度を にして  xxx mask png として出力します  スクリーンショット       png  以下のコマンドを実行しますoutput ディレクトリーに処理結果が出力されましたお菓子とコードが完全に消えて あたかも最初から存在していなかったかのようです画像を交互に切り替えてみると いかに自然かが分かりますね  sofa gif     まとめGPU があれば refine True を指定してもっと高精度にできるようなので  Google Colab で試してみよう,16,2022-11-14
46,46,【文字検出_備忘録】OpenCV X Python 画像の読み込みとグレースケール化,画像処理,https://qiita.com/Miya_0000/items/3bd18f5312714de64fe5,産休にはいって 出産まであとヶ月切ったので 子供を産まれるまで どこまでできるかチャレンジ 私のスペック 情報工学 学部卒 なのでたかが知れてる  とあるメーカー　設計 開発業務 年目 主にマネジメントしかしてない 子供人 歳 まだ夜泣きもするし 寝るのもへたっぴなので夜はほぼ活動してない いや 多分こんな文字検出なんかね どこでも載ってるんだけど 色々やってみたいので お勉強兼ねて少し 鈍臭くやった備忘録を書きます 鈍臭くっていうのは データの中身をちゃんとみたり どうしたらAPIやライブラリを使用せずに組んでみたりするってこと です   前提条件   開発環境OS macOS Montereyハード iMac    inch  Late  Memory GB   今までやってきたこと 開発環境整備　Python  OpenCVI Numpy含む   Matlabplotlib エディタ SublimeText   やりたいこと 画像から文字検出   考えてるステップStep 画像の読み込みStep 値化を実施Step 輪郭を抽出Step のエッジを検出し 文字を切り抜き  進捗   Step 画像の読み込みまず いろんなライブラリの読み込み 必要なのは cv 画像処理のライブラリ  numpy 行列計算系 行列のプロットに役に立つ  matplotlib 画像処理結果を 画像で教えてくれる だと思う 今のところ  ファイルパス最初グレースケール化で遊んでたので GRAYが入ってマス めんどくさいのでそのまま 続いて 画像の読み込みIMG file pathのファイルに保存してみた せっかくなので中身を見てみよう  e d    b   a c  c  f  ef f   a  cc     a  c   c d          afbfb fbfb fbfb fbfb fbfb fbfb fbfb fbfbうーんなるほど   わからん  どうやら bitの何からしい こういう時は 定義をみましょうね Imreadの定義どうやら cv  Mat型で返してるらしいのでそっちをみる  わかんないので 先に行こう どうやらこれが参考になりました BGR値で返しているらしく 次元配列 一つのセルにつ値が紐づく状態 を numpy ndarrayで出力しているそう なるほど numpu ndarrayとはということで 以下でデータの中身を見るお手伝いをしてみた なるほど つまり y軸 pixel x軸 pixel  dimention 次元 が次元の配列 各要素はUINT型の行列式なのである ちゃんと理解するために ここでちょっと遊んでみようと思う 参考print img         img Y X らしい出力結果 なるほど めちゃめちゃわかったぞ セルめのBGR値が    X  行目 が 以下 これがy軸方向に行続いてるってことだね なるほどね  参考記事ということは 値化するためにはどまずはBGR値だとすこぶるめんどくさいのでグレースケールに落とし込みます グレースケール化は cvtColorでやりました imreadでもできるらしいんで どっちでもいいと思います  グレースケール処理実は これをやったあと    黄色    緑っぽく出てきて ちょっと困惑したんですが とりあえず BGR値から解放されてるっぽいのでいいでしょう 何でかはわからんが いかを試したら グレースケールになりました まぁね 何色かはどうでもいいから ,0,2022-11-14
48,48,OpenCVに入門してみた,画像処理,https://qiita.com/ulxsth/items/5f45ae8e76450c1aa82a, こたつにはいってぬくぬくしている間に画像の下処理が終わってたらよいな  などと思いましたが サンタさんに頼んでも多分来ないので 作ることにしました ふわ っとした知識で書いているので  こんなことができるんだ  程度に見ていただければと思います マサカリ大歓迎です   OpenCVとは  画像処理 画像解析および機械学習等の機能を持つC C   Java Python MATLAB用ライブラリだそう   やること資料作成だったり動画制作の際に行う  白抜き  を自動化します 背景の白を透明色に置き換える操作ですね  枚ならツールで手作業でもよいのですが  枚と数がかさばってくるとかなりストレスですよね こんなものはさっさと自動化してしまいましょう   やってみた   キホンライブラリをインストールして      shellpip install opencv python使いたいファイル内でインポートすればOK ついでにセットでよく使う  NumPy  も入れましょう NumPyに関しては別途インストールする必要はありません 実は セットで使われることを見越してOpenCVのインストール時に一緒についてきてくれます やさしい    仕様  ファイル名を入力すると そのファイルを白抜きする  拡張子は  PNG  に変換する   やってみたまずは画像を読み込んで      pyimg   cv imread imgFileName RGBA方式 透明度を扱える色画像 に変換   ここでちょっとだけ補足 上の二行では変数にファイルを格納してるけど 実際に格納されているのは  画像の各ピクセルが持つデータが格納されたndarray  です     補足：画像の扱い方どうせ数日後に見返したおれが なんやねんそれ    になるので 詳しく補足 これは  NumPyが提供する多次元配列  みたいな感じで Python標準の多次元配列よりもいろいろなことができる って感じだと思う たとえば  xx の多次元配列に             って感じで数値を入れたいなら ってカンジ こんな感じで 多次元配列を手軽に扱えるのが ndarray のよいところ    次に 画像データの扱いについて 画像のデータは各ピクセルごとのRGBA値が ndarray で管理されてて 例えば   pyimg         インデックスはそれぞれx  y  RGBA   なら 画像の    のRの値を参照するで  ということになります     本題にもどるということで続き RGBA変換した画像のうち 白の部分を透過していきます    pyrgbaImg          np where np all img      axis        はい 急に訳がわからなくなりました 順番に整理していきます     obj       こいつについて 一見すると 引数めんどくさいから略したんか  みたいなナリをしていますが れっきとした  NumPyの構文  です 一応こいつは書き換えると   pyobj        となります これでもよくわからん これ 実はPythonの  スライス  という構文になります スライスは簡単に言うと 配列をわざわざforループでまわすのめんどくさいし 短く書いたろ  みたいなカンジの機能です 知らんけど そのなかでも コロン単体のスライスは  配列全体を取り出す  という意味となります つまり この三点リーダは    省略した引数たちがとり得る値の組み合わせ を全部出す   という意味を持ちます 長ったらしく説明しましたが とりあえず rgbaImg          が示す意味は    rgbaImg 内のピクセルすべての透明度に対して代入するで  ってカンジです     np where  右辺もひとつずつさらっていきましょう この場合における np where   は いわば  三項演算子  の役割を持っています 今回の場合    np all    が真偽値を返す  結果が True であれば  透明    False であれば  透明ではない  を返すといった感じですね 単純     np all  さて あとはこいつです こいつなんですが   与えた評価式を評価する  という仕事を持ちます もちろんそれだけではありません この引数に多次元配列を与えた場合 その多次元配列の すべての要素に対して順番に評価 します 今回の場合 img     がそれですね これは img 変数と数値を比較しているわけではなく    img 変数の各要素  と比較している感じですね  axis オプションについても説明しようと思いましたが 今回はとくに重要ではないのと とても分かりやすく解説されているサイトがあったので よければ参考までに覗いてみてください 他力本願     帰還さて 舞い戻ってきました どこまでいったっけ 画像に対する処理は完了したので あとは完成したファイルを  PNG形式で  保存します これでオッケー    全体像今後のことも考え いいカンジに関数化しておきます  白抜きのことを表す英動詞がいまいち見つからなかったので それっぽい hollow にしています かっこいい      imgFileName   input  ファイル名を入力してください 拡張子を含めること      実行今回はいらすとやから画像をお借りしてきました  ラーメンの油をまとめる人 のイラストです 準備ができたらメインファイルを実行して ファイル名を入力すると      shellファイル名を入力してください 拡張子を含めること merge ramen oil man jpg  できた              image png  ん    なんか   すごく 汚いです 丼 穴空いてますし    反省  対象の周囲がうまく白抜きできていない  白を全部抜いてしまうため   イラスト内の白  も抜いてしまう  閾値 白と判定する範囲 が低すぎるこんなところでしょうか ともあれ よい経験になりました      ほかにも 要素に対して型が割り当てられる  PythonのリストはすべてObject型 などなど様々な面で便利だけど 全容はつかみきれなかった；；,0,2022-11-13
49,49,画像の任意の座標を中心とする余白を追加する方法,画像処理,https://qiita.com/yusuke_s_yusuke/items/59861bd35ee5dc907794,  概要画像の任意の座標が中心にくるように余白を追加 Padding する方法を紹介する   実装Google Colabで作成した本記事のコードは  こちら  にあります    Paddingサイズを最小限に位置合わせ画像の任意の座標が中心にくるように最小限のPaddingをする方法です メリットは 最小限のPaddingのため 画像サイズが大きくなりにくいところです   グラフに表示左図の赤点が中心に位置合わせする座標で 右図は中心に位置合わせした後の画像です   コードの簡単な説明  x shift   img shape     coord    で 画像の端の座標と任意の座標の倍のした値を比較します ここで x座標は  x shift  が正であれば 任意の座標は画像サイズの半分よりも小さく 負であれば半分をより大きい座標であると判定できます   x shift  の正負に合わせ 任意の座標が中心にくるように  pad x  を定義します y座標も同様に計算します 簡単のため次元配列で考えます  img  index  は画像のindexとします 座標 i が中心となるようにPaddingします コードに従いとなります これは i 番目の座標の移動した値と等しいので shift  の時に成り立ちます これは i 番目の座標の移動した値と等しいので shift\leq  の時も成り立ちます 従って 任意の i においてこのコードは成り立ちます    出力画像サイズを固定して位置合わせ出力画像が入力サイズによって一意に決まるコードになります こちらは stack overflow  Rotate a D image around specified origin in Python  を参考にしています 参考にした記事では 本記事で対応させると  pad x    img shape     coord    coord     と同じ意味で書かれると思いますが 本記事では中心の座標を一意に決めたいため   pad x    img shape     coord     coord      と書き換えています 参考にした記事では 出力サイズが入力サイズの倍のため  偶数となり中心の座標を座標で表せない  ため      しています 中心の座標を厳密に定義して実装したいかの違いです メリットは コードがシンプルであるのと 複数の同じサイズの画像で中心を位置合わせした場合に画像毎に出力画像が一意に決まるため 管理がしやすいところです    Python 出力画像サイズを固定して位置合わせ  グラフに表示左図の赤点が中心に位置合わせする座標で 右図は中心に位置合わせした後の画像です   コードの簡単な説明x座標の  pad x    img shape     coord    coord      より Paddingする数は img shape     coord     coord      img shape     となり   coord    の値によらず常に一定であることが分かると思います y軸も同様です このコードは一見処理が分かりにくいと思いますが数式による説明で成り立つことを確認できます 簡単のため次元配列で考えます  img  index  は画像のindexとします 座標 i が中心となるようにPaddingします まず  img  index  の左端に n i 個分追加すると となります 次に 右端にも i  個分追加すると となります    math\frac n    nこれは i 番目の座標の移動した値と等しいので任意の i においてこのコードは成り立ちます   補足RGB画像の場合は とすると動きます   まとめ状況に合わせて好きなものを使いましょう   参考資料,1,2022-11-08
50,50,【試行錯誤】「〇〇で歌ってみた」動画の自動生成 その４：静止画の生成,画像処理,https://qiita.com/shimajiroxyz/items/690faf3aeae096c65b17,  概要前回作成した 替え歌情報に基づいて 動画用の静止画を生成する関数を作ります 最終的に以下のようなファイルが得ることを目指します 上から替え歌歌詞を表す画像 替え歌歌詞 元歌詞を並べた画像です   image png  シリーズ一覧：  試行錯誤  〇〇で歌ってみた 動画の自動生成 リンクまとめ    背景最終成果物として 替え歌歌詞 元歌詞 替え歌歌詞に対応する画像をセットで表示した静止画を音楽に合わせて流した動画を想定しています そこで まず静止画を作る関数を実装し 替え歌情報に基づいて実際に使う画像を作成します 以下つのステップを行います   事前準備  静止画生成関数の作成  替え歌情報に基づく静止画生成  事前準備 その で作った替え歌情報のデータフレームを用意し 読み込んでおきます マルタ     春が来た　春が来た　どこに来た Flag of Malta pngイラン     春が来た　春が来た　どこに来た Flag of Iran pngコロンビア     春が来た　春が来た　どこに来た Flag of Colombia pngハンガリー     山に来た　里に来た　野にも来た Flag of Hungary pngまた国旗画像を一つのフォルダにまとめて格納しておきます 今回は   figs 下にあるものとします   静止画生成関数の実装   方針今回は以下のように 上から順に画像 替え歌歌詞 元歌詞を並べた画像をつくります   image png  以下のステップで作成します   単色 黒 の背景画像を用意  上部 の高さ幅に国旗画像をリサイズして貼り付け  その下 の高さ幅に 替え歌歌詞 単語 を貼り付け  下部 の高さ幅に 元歌詞 フレーズ を貼り付け   関数全体関数の最終形を先に載せておきます     定数の定義    背景を定義    オブジェクトをpaste    オブジェクトが背景の上側にバランスよく収まるようにリサイズ    描画領域よりもオブジェクトのほうが横長の場合    描画領域よりもオブジェクトのほうが縦長の場合    替え歌歌詞の描画    オリジナル歌詞の描画    notebookで出力を確認したいとき    pltなしでみたい場合    notebookであれば 戻り値だけをセルの最終行にかけばいい感じに出力してくれる適当な単語で試してみます notebookであれば以下のような出力が得られると思います いい感じです   image png     解説    黒背景の作成以下の記述で指定されたサイズによる黒画像を作成しています ライブラリはPillowを使っています 黒以外の背景にしたい場合は 第引数にRGB値をいれればよいです     国旗画像の貼り付け関数冒頭のobject area hegith ratioで 上部からどの高さまでを画像領域とするかを定義しています また 余白を持って画像を貼り付けられるように 領域内でどの程度まで画像を拡大するかの割当をxxx width height limitで幅 高さについて定義しています    Python    定数の定義以下で領域内にちょうどいい感じに収まるようにリサイズしています 画像が領域よりも横長の場合は 縦いっぱい 逆の場合は横いっぱいに拡大するようにしています またPillowでは画像の左上座標を入力とするのですが センタリングをしたかったので センタリングできるような座標を計算しています 画像のリサイズ センタリングについては 以前  パワポで似たようなこと  を実装したので参考にしています    Python    オブジェクトをpaste    オブジェクトが背景の上側にバランスよく収まるようにリサイズ    描画領域よりもオブジェクトのほうが横長の場合    描画領域よりもオブジェクトのほうが縦長の場合    センタリングした場合の左上座標の計算    画像の貼り付け    替え歌歌詞の貼り付け替え歌歌詞も同様にまず 貼り付ける縦幅を定義します 以下だと 画像領域の下にフィールドサイズ高さの の大きさを替え歌歌詞領域とするようにしています また余白を作るための文字の最大サイズ割合も定義しています 横ははみ出さないような文字数にする想定なのでwidthは定義せず heightだけを定義しています    Python    定数の定義文字を貼り付けるには ImageDraw クラスを使います フォントは各自のOSに合わせて適切なパスを設定してください なお文字サイズを任意の値とするには ImageFont クラスが必要です 今回はフォントサイズは 領域の縦幅をはみ出さない程度に自動計算しています 縦幅しかみていないので あまりに替え歌歌詞が長い場合は横にはみ出る可能性があります その場合はparodytext height limitを小さくするか 長い文字列を使わないようにするなどしてください 表示位置は 文字の場合は draw 関数の anchor 引数でセンタリング指定が可能ですので その機能を使っています    Python      替え歌歌詞の描画    fontを使う場合 anchor引数にて表示位置指定が可能 横も縦もセンタリングなら mm     元歌詞の貼り付け元歌詞の貼り付けは 替え歌歌詞の貼り付けとほぼ同様です 領域の縦幅は 過去に定義したものから計算しています またheight limitは替え歌歌詞よりも小さくしています これは文字数が大きくなる想定のため またなんとなく見栄えを考えたためです    Python    定数の定義    オリジナル歌詞の描画  替え歌情報にも基づく静止画生成最後に  parodyinfo df に基づいて動画に使う画像を生成するコードは以下のとおりです  parodyinfo df にはファイル名しか書いていないので 親ディレクトリ情報は別途定義して 補っています  parodyinfo df に親フォルダまで含めるようにしてもよいのですが ファイル置き場を柔軟に移動させたくなることもあるかと思い 今はこのようにしています これで output figs 下に画像ファイルが生成されます   おわりに今回で動画の材料となる静止画を生成することができました 画像 文字の表示位置 順番 などは関数内部で固定してしまっているので 少し柔軟性に駆ける課題があります 情報 タイプ 画像か文字か  座標を入力として任意の個数受け取れるようにできればもう少し柔軟性は上がると思います 今後の課題とします 次はこの静止画をつなぎ合わせて動画を作ります   参考にさせていただいた記事   OpenCVやPILで画像を表示させる だけ の話     Python  Pillowで文字 テキスト を描画 フォント設定        今回は使っていないが これがあれば 画像ファイル出力を経由しなくても動画を作れそう,0,2022-11-06
51,51,画像処理のリピート（折り返し）処理を条件分岐なしで数式にまとめてみた,画像処理,https://qiita.com/genkaigakuseiprogrammer/items/7f8553ada92abda4bc1b,単純にリピート処理を書きたければ 条件分岐を使って簡単に書くことができます 例えば簡単にC言語で書くと以下のようなプログラムが書けます    val   x yなどの座標値   max   画像の幅や高さ多少最適化してはいるものの やはり条件分岐では処理速度が大幅に落ちてしまいます 特に画像処理のような膨大な計算を要する際に 処理速度が遅いプログラムを書くのはナンセンス そこで数学的にこれらを処理できないかと考えたところ 第一号が完成したので紹介します   そもそもリピート処理とは取得したい画素座標がソース画像の領域からはみ出しているとき ソース画像を折り返して座標を取得するといった処理ですが   言葉で説明してもわかりづらいので画像で説明します 下のようにLenna画像を右にpxずらすと画像領域から飛び出る部分があるので それらを補完する処理をここではリピート処理と呼ぶことにします また 少し今回の処理の仕組みとは異なりますが OpenGLのテクスチャのラッピングについて こちらの記事  で紹介されているので参考にしてみてください   数学的なアプローチ 導出上で紹介した様に 以降 x   軸     横軸方向   を処理対象とします また 画像の横幅を w とします したがって これ以降扱う文字は以下のように定めます  x を w で割ると 商が x が画像何枚分先なのかを示し 余りが画像領域内の x 座標を示します また この時商が負の値だと厄介なので x の絶対値を割ることにします そうすることで x 軸負の向きにも対応できます したがって 商を \alpha  余りを \beta とする式は   math\alpha    x  \div  w       math\beta    x  \bigl  mod w  \bigr  w    としているのは 実際のプログラム上では画素配列が  から始まるので w  番目で割る必要があるのに合わせるためです  数学的に正しいのか怪しいですが とりあえず余りを示すのに合同式の mod を使いました   \beta は  x  を w で割った余りという意味合いで捉えていただければ大丈夫です また  \beta が  になるごと   x  が w で割り切れるとき に区切って商を画像に割り振ると以下のようになります この画像から見てわかる通り 商が偶数の時画像を横方向に反転しています これらより 商が奇数の時は画素座標        \cdots   w     w     w    のように取得していき 逆に商が偶数の時は   w     w     w    \cdots        のように取得していくような操作をする方針が立てられます よって   mathP \alpha     \Bigl  \alpha \bigl mod  \bigr  \Bigr        mathP \alpha  \cdot w関数 P \alpha  は \alpha を  で割った余りを  引き その絶対値をとっています つまり \alpha が偶数の時  になり 反対に \alpha が奇数の時  になります これに対して w  をかけることで上で説明した偶数パターンが完成しました さらに この式に対して奇数パターンを加えていきます 先ほど説明したものを整理すると  \alpha が奇数の時上の式に対して  ずつ足していき 偶数の時は  引いていくような操作をすればよさそうです よって   mathQ \alpha    i  P \alpha     mathQ \alpha  \cdot \beta※ i は虚数単位一目見ても何がしたいのかわかりづらいので解説します   最終的に \alpha が偶数の時 \beta を負にしたい  ので 関数 Q \alpha  は P \alpha  が偶数の時   奇数の時  になる性質を利用できるものがないかと模索したところ虚数単位への累乗がよさそうという結論に至りました  多分他にも様々な導き方はあると思いますが    よって \alpha が偶数の時 i を  乗して   に 奇数の時  乗して  にしたいので  P \alpha  に  をかければ求めたい形になります 以上より式をまとめて   mathP \alpha     \Bigl  \alpha \bigl mod  \bigr  \Bigr        mathQ \alpha    i  P \alpha     mathf x    P\bigl  x  \div  w    \bigr  \cdot w   Q\bigl  x  \div  w    \bigr  \cdot \Bigl  x  \bigl  mod w  \bigr \Bigr 完成しました これこそが条件分岐 場合分け を使わないリピート処理の関数であります     学校の授業中暇な時間にこれらを考えてたことは秘密  ちなみに縦軸にもこの関数を当てはめることで適応できます 具体的には 先ほどまでの w を画像の縦幅の長さ  h など にし  x を y に置き換えるだけです   実際に数式をコードにしてみるさて 本題の関数が完成したところでソースコードを書いてみます 今回はC  言語で書いていきます これにてやりたかったことはすべてできました   結局処理速度は鬼遅かったなんて絶対に言えない   恐らく虚数計算が処理速度を大幅に下げていると予想 今回解説したことが何かのお役に立てれば幸いです また新たに第二号などでき次第記事にしたいと思います ,1,2022-11-06
52,52,【matplotlib】2次元ヒートマップのProjectionを取る方法,画像処理,https://qiita.com/yusuke_s_yusuke/items/12fc3056f88a1579f4d0,  概要次元ヒートマップの縦横比が等しい場合の軸方向のProjectionを取る方法は matplotlib公式  Scatter plot with histograms  で紹介されています しかし 公式そのままでは 縦横比が異なるイメージの場合に下図左側のようになってしまい上手くいかないと思います そこで 本記事では  縦横比が異なる場合のProjectionを取る方法  について紹介します また 応用編では慣性主軸を使って次元ヒートマップのいい感じの軸を選び その方向にProjectionを取る方法について紹介します おまけでは次元空間での表示例も紹介します   図 png    実装コードGoogle Colabで作成したコードは  こちら  にあります   各種インポート   Python 各種インポート実行時の各種バージョン  Name   Version    opencv python          numpy         matplotlib         scipy         使用関数    img  次元配列    fig size  matplotlibの出力時の画像サイズ  n  m のようなつの引数を入れることができないので注意     padding  matplotlibの出力時の隅の余白 全体の占める割合で指定     spacing  matplotlibの出力時のProjectionエリアとヒートマップとの余白 全体の占める割合で指定     注意事項     Google Colabでは出力画像の余白は自動で除かれる仕様のためあまり関係はないが       fig savefig    で保存する時はpadding などにするとプロットエリアも途切れる場合がある     全体の占める割合の引数は 仕様上完全に全体の割合が反映されないので 大体の目安程度に指定する        次元のビンとその値の入った配列からビンまとめ    x  各ビンの座標が入った次元配列    y  各ビンの値が入った次元配列    rebin  ビンをまとめる個数    renorm  rebin数で割ってビンあたりの値に規格化するか       任意の回転軸からの回転    img  次元配列    rebinc    ビンまとめ用の関数  pythonでヒストグラムをビンまとめ rebin する方法  の記事を引用     get gravity coord      次元ヒートマップの重心を求める関数   OpenCV matplotlib 画像の重心と慣性主軸を表示する方法  を参考     get inertia angle      次元ヒートマップの慣性主軸を求める関数   OpenCV matplotlib 画像の重心と慣性主軸を表示する方法  を参考   サンプル画像サンプル画像は何でも良いですが 本記事では 縦  横      の次元画像を使います 現実のデータに近づけたいため ガウシアンノイズも加えています   使い方  projection layout    関数で画像サイズに合わせてグラフエリアを調整できるようにしています   projection layout    の詳細な説明は 補足のprojection layout関数の説明   projection layout関数の説明 をご覧ください コード上で とはじめにaxisを定義して その後は通常のmaptlotlibの書き方で動くと思います    ビンまとめなし    プロットで表示   Python プロットで表示  次元ヒートマップの表示Projectionの目盛ラベルの非表示にしたい方は 補足のProjectionの目盛ラベルの非表示   projectionの目盛ラベルの非表示 をご覧ください     ヒストグラムで表示   Python ヒストグラムで表示  次元ヒートマップの表示   ビンまとめありビンでのヒストグラムを表示します こちらの詳しい説明はを参照ください     プロットで表示   Python プロットで表示  次元ヒートマップの表示  x軸方向のProjectionを表示 ピクセル毎にビンまとめしたものを重ねる   y軸方向のProjectionを表示 ピクセル毎にビンまとめしたものを重ねる     ヒストグラムで表示   Python ヒストグラムで表示  次元ヒートマップの表示  応用編慣性主軸を使って次元ヒートマップの下図のようにいい感じの軸を選び その方向にProjectionを取る方法について紹介します   図 png     慣性主軸点が重心 直線が慣性主軸です 慣性主軸が明るいところに沿うように引かれてほしいですが このようにノイズがあると上手く軸を求めることができません そこで ある値以下をに置換して再度慣性主軸を計算してみます    ある閾値を導入した場合の慣性主軸   Python ある閾値を導入した場合の慣性主軸閾値を導入してに置換することでノイズの影響を抑え 慣性主軸が期待通りのところに引かれたのがわかると思います ここで  重心を回転軸に取り慣性主軸の角度分だけ回転させてProjectionを取ります   次元ヒートマップの表示  x軸方向のProjectionを表示 ピクセル毎にビンまとめしたものを重ねる   y軸方向のProjectionを表示 ピクセル毎にビンまとめしたものを重ねる このように 慣性主軸を利用すると最適な軸に対するProjectionを取ることができます ただ今回のようにノイズがある場合は 閾値を設けるかなど何かしらの方法ノイズを除去する必要があるので注意しましょう   おまけ次元ヒートマップを次元に表示して各軸にProjectionを取ることもできます こちらの詳しい説明はを参照ください  ax view init    を使うと 任意の角度で表示できます   次元ヒートマップから次元ヒートマップへ変換y  x   np mgrid  img shape     img shape     次元ヒートマップ表示  次元ヒートマップから次元ヒートマップへ変換y  x   np mgrid  img shape     img shape     次元ヒートマップ表示  補足  変数とグラフエリアの関係x軸方向について説明します 画像サイズも割合で示す必要があるため どんなサイズの画像が読み込まれてもコード内でとりあえずを超えない  img shape   np sum img shape   と定義しています 全ての位置を割合で表したいので   norm x  で割ってあげることで 各軸毎で割合を規格化でき 割合で位置を表すことができます y軸方向も同様に計算します   各パラメータの見た目  図 png     Projectionの目盛ラベルの非表示  目盛ラベルの非表示  次元ヒートマップの表示  目盛ラベルの非表示 目盛内向き   Python 目盛ラベル非表示＋目盛内向き  次元ヒートマップの表示  まとめ次元ヒートマップのProjectionを取ることで画像情報を別の視点から議論することができるので便利です 本記事は単純に軸方向に足し上げただけですが 画素値あたりの平均に規格化しても良いと思います また 本記事のサンプルのような細長い構造などは 最適な軸を選ぶと構造の厚みなどの特徴を知ることができます 画像の特徴を見るときは おまけのように一旦次元で表示してみるのもおすすめです 散布図のProjectionの記事  散布図のProjectionを取る方法  も書いているので よろしければ合わせてご覧ください   参考資料  関連資料,8,2022-11-04
53,53,【試行錯誤】「〇〇で歌ってみた」動画の自動生成 その２：MusicXMLから歌詞の発話タイミングを取得,画像処理,https://qiita.com/shimajiroxyz/items/6d51f376df7f231e5140,  概要替え歌字幕動画の生成に向けて そのに続き MusicXMLの内容を読み取る試行錯誤をしています 今回は歌詞の発話タイミングを取得してみます シリーズ一覧：  試行錯誤  〇〇で歌ってみた 動画の自動生成 リンクまとめ    背景 〇〇で歌ってみた と呼ばれるジャンルの替え歌動画の自動生成に挑戦しています 最終的に作りたい動画は 元歌詞 替え歌歌詞 替え歌歌詞と対応する適当な画像を 音楽に合わせて表示する静止画のつなぎ合わせのような動画です 例えば下記動画のようなイメージです ときどきあるアニメーションや右下の都道府県画像の更新などは再現しない予定ですが   駅名替え歌  駅名で うっせぇわ   これをするためには歌詞を表示するタイミング タイムスタンプ を知る必要があります 歌詞の発生タイミングは歌声合成システムに入力するMusicXMLに記述されているはずなのでその情報を読み取ってみます   歌詞のタイムスタンプ情報の抽出NEUTRINOに付属しているサンプル  sample musicxml  を眺めて タイムスタンプ情報がどのように読み取れるか考えてみます わからなかったら 公式Document  や わかりやすい解説  を読みます もし間違いに気づいたらご指摘いただけると幸いです 楽曲全体のbpm情報については 最初のmeasureタグに書かれていそうです    xml divisions は音符 休符の最小長さを規定するための数で beat typeの長さをこの数で割った値が最小長さとなります 今回だと後述のようにbeat typeが分音符なので 最小長さは分音符の  つまり 分音符になります  attributes time にbeatの情報があります beatsが拍数 beat typeが拍の長さです つまり拍子 今回だと分の拍子 となります  direction sound の tempo タグにもbpmっぽい情報が書かれていますが 今回は無視します  今回はtempoとper minuteが同じなので虫で良さそうですが 違っていたらどうなるんでしょう  違っているファイルを見つけたときに考えたいと思います また続きを眺めたところ  attributes などの情報はnumber 以降のmeasureには書かれていないようです もし 楽曲の途中でbpmが変わった場合にはその都度書く ということなのでしょうか  わかりませんが 先に進みます 歌詞の情報が初めて出てくる measure number    を見てみます    xml note が音符に対応しており 歌詞がある場合  rest 出ない場合 には note lyric text が存在するようです 拍の長さは duration にかかれており これは前述の divisions で計算した最小長さ 今回は分音符 が何個分かを意味しています divisionつあたりの秒数は bpm divsionsで計算可能です したがって直前のnoteの終了時間を基準として durationを順に計算していくと 各noteの始点と持続時間がわかります Pythonで取得するコードは以下のような感じです       楽曲情報の格納変数 一応デフォルトで適当な値を入れておく      以下 各情報は多くて一つとして取得 本当はつ以上 ないは限らない       division情報があれば更新する      beat情報があれば更新する      bpm情報があれば更新する      休符やテキストがない場合      一応 self以外のrootに対しても使えるようにしておく        楽譜全体の情報をupdate 基本は最初のmeasureにのみ存在するはずだが 念の為毎回チェックする   は            る            が            き            た            は            る            が            き            た            ど            こ            に            き            た            や            ま            に            き            た            さ            と            に            き            た            の            に            も            き            た            は            な            が            さ            く            は            な            が            さ            く            ど            こ            に            さ            く            や            ま            に            さ            く            さ            と            に            さ            く            の            に            も            さ            く         小数点の扱いが若干不安ですが だいたい良さそうです   漢字かな交じり表記との対応付けMusicXMLから取得できるのは歌詞の発音 かな ですが 字幕に表示するときには漢字かな交じりの元歌詞を使うと思うので 元歌詞と発音を対応付けておきます 以下のような元歌詞が記述されたファイルがあるとします 春が来た　春が来た　どこに来た山に来た　里に来た　野にも来た花がさく　花がさく　どこにさく山にさく　里にさく　野にもさく対応付けは以下のステップで実施します   上記歌詞から sudachpy で発音を取得  取得した推定発音とMusicXMLから取得した正解発音を対応付け   発音を取得発音取得は過去に実装した SudachiPyで発音を取得 Python   とほぼ同様ですが 発音のない要素ができると面倒な気がしたので 発音なしのトークン 記号など は適宜 前後の表層形に結合させるようにします      各条件を正規表現で表す    c     ウクスツヌフムユルグズヅブプヴ  ァィェォ    ウ段＋ ァ ィ ェ ォ     c     イキシチニヒミリギジヂビピ  ャュェョ    イ段  イ を除く ＋ ャ ュ ェ ョ     c     テデ  ィュ     テ デ ＋ ャ ィ ュ ョ     c     ァ ヴー    カタカナ文字 長音含む     c     a zA Z     念の為アルファベットも抽出できるように        発音の微修正      if pos in   補助記号    空白     記号の発音はなし         pronunciation             elif surface     は  and pos     助詞    助詞の は は わ になおす        pronunciation    ワ       elif surface     へ  and pos     助詞     助詞の へ は え になおす        pronunciation    エ         要素の追加 join sign Trueの場合 surfaceを微修正してから追加する          発音がないとき            括弧開でなく last start braも空文字で surfacesの長さがでないとき 直前の要素と結合            それ以外のとき 次ループ以降で処理          発音があるとき          無音を足す        春    が    来    た        春         が         来    た             どこ    に    来    た                          ハ    ル      ガ       キ       タ           ハ    ル          ガ           キ       タ               ド    コ      ニ       キ       タ                    対応付けモジュールの作成 過去に実装したAllocatorクラス  をほぼ使いまわします ただし exec メソッドは不要なので作っていません pip install editdistance   Python  編集距離と対応のリストを返す    出力は分割のindexとその分割をした場合の編集距離        特殊ケースの対応        test segmentが最後一つのとき 全部を対応させる        全体の編集距離がゼロなら先頭から順番に対応付けすれば良い        プラスマイナスwindow sizeの幅で最適な対応をみつける          indexを最初の対応の長さで補正    デバッグ 確認用 correspondance 始点終点のindex を文字列のペアになおして見やすくする   対応付けの実行  MusicXMLから発音 ひらがな のタイムスタンプを取得  発音だけをカタカナに直して取得  歌詞の表層形を行 フレーズ 単位で取得  行分割して空の行は削除    各フレーズを単語に分けて発音を取得estimated word moras       単語列のよみ 形態素解析による推測なので間違っている可能性あり   phraseの情報も念の為保持しておくphrase span        各フレーズが何単語めから何単語めまでかを保持する    phraseのスパン情報を保持    単語の格納  単語の単位で推測歌詞発音と正解歌詞発音の対応付け  対応付けの結果をもとに単語ごとの正しい発音 mora を取得  タイムスタンプ 元歌詞の対応をテーブル化ハ   春  は    ル   春  る    ガ   が  が    キ   来  き    タ   た　  た    MusicXMLから取得した歌詞とそのタイムスタンプ 対応する表層形が取得できました   おわりにMusicXMLからタイムスタンプ情報を取得することができました また字幕動画の生成を見据えて 発音 かな 歌詞と漢字かな交じり歌詞の単語レベルの対応も取得しました 次は 画像に字幕を重畳した静止画を作ってみます ,0,2022-11-04
54,54,【試行錯誤】「〇〇で歌ってみた」動画の自動生成 リンクまとめ,画像処理,https://qiita.com/shimajiroxyz/items/5df23a0ca81a3a2d9568,  概要  試行錯誤  〇〇で歌ってみた 動画の自動生成 シリーズの記事一覧です 随時更新していきます   リンクまとめ作成済み その：MusicXMLの歌詞を書き換えて歌声合成   その：MusicXMLから歌詞の発話タイミングを取得   その：動画生成に必要な替え歌情報の整理   その：静止画の生成   その：静止画をつなげて動画化  今後の予定 タイトル等 変更可能性あり その：替え歌歌詞の自動生成その：MIDIからMusicXMLを自動生成その：処理のワンパス  メモ 参考にした／できそうな記事 ,1,2022-11-04
55,55,機械学習に役立つ画像収集ライブラリ「icrawler」を使ってみた,画像処理,https://qiita.com/momiji777/items/1bf228d0d4964f1c6e38,   動画   検証皆さん 機械学習をするにあたり学習データの収集に苦労したことはないでしょうか 今回は 学習するにあたり役に立つライブラリ icrawler を紹介します ライブラリの詳細については github   gist   を確認して頂ければ良いですが 簡単に言えばWebクローラーです 画像サイトのFlickrだけでなく Google Bing Baiduの検索エンジンも利用することができます では 実際にサンプルコードを書いていきます まずは インストールをしましょう pip install icrawlerインストールができましたら コードを書いていきます たった行で書くことができました では 中身を見ていきましょう 見て頂くと確認できると思うのですが 約 ほどノルウェー出身のシンガーソングライターのAURORAが含まれていることができますね もし 機械学習をするのであれば データクレンジングをしていく必要がありますね ,0,2022-10-30
57,57,【FPGA】PYNQで画像処理を簡単に,画像処理,https://qiita.com/Alaska_Panda/items/72aab17dba652a28f416,  はじめにPYNQは Xilinx社製FPGA開発キットやアクセラレーションカード上に実装されたロジックをPythonから利用するためのフレームワークです   本記事では Kria KVとPYNQを利用したFPGAアクセラレーションの入門として 画像を反転させる単純なアプリの実装方法を概説します   今回は画像反転という単純なロジックをFPGA上に実装しますが   ロジックの変更によって様々な処理への応用が可能になると考えています   尚 使用するXilinx社製FPGA開発キットは Kria KV AIスターターキットを用いました   Screenshot from       png  キャプチャの引用元：  対象読者  Pythonで書かれたプログラムをアクセラレーションしたい方  Kria KVなどのXilinx社製FPGA開発キットやアクセラレーションカードの活用を検討している方  目的PYNQの利用によって PythonプログラムをFPGAで容易にアクセラレーションできると感じていただくこと  開発環境  参考文献本記事を執筆するあたり 以下を参考にさせてもらいました ありがとうございます   本文     概要画像反転プログラムのFPGA実装方法を以下の順で紹介します   画像反転プログラムの全体像を把握する   Vitis HLSでFPGAロジックを実装する   Vivadoでbitストリームを生成する   Kria PYNQを用いた画像反転プログラムを実行する      画像反転プログラムの全体像を把握する プログラムの全体像を確認し アクセラレーションすべき部分を把握します 以下は 画像反転のPythonプログラムです プログラムを実行すると 枚の画像が表示されます 右から枚目は元画像  Cifar  にある画像  枚目はグレースケールの画像 枚目は枚目を反転させたものです   cat jpg  今回は プログラムの中盤にある以下の画像反転部分のみをFPGAで処理させることを目指します また 実装簡略化のために   グレースケールの画像を反転させるFPGAロジックとします   つまり 上記の枚目を入力に枚目を出力するFPGAロジックを実装します   入出力の画像サイズは uint型 xの配列になります        Vitis HLSでFPGAロジックを実装する 画像反転させるロジックは以下のようにコーディングしました ポイントは Vitis HLSがコードをRTLに変換する際の指示をするために   pragma  を利用することです  INTERFACE  としては s axiliteとm axiを利用しました s axiliteは制御用のインターフェースで m axiはデータのメモリ転送を実施するためのインターフェースです  LOOP TRIPCOUNT  は ループの反復回数を指定するものです 上記のコードをVitis HLSで高位合成します 詳細な手順はXilinx公式のマニュアルや こちらのブログ  などを参考にして下さい 尚 下図のように   Solution Settingsにてm axi addrのチェックを外すと 不要な制御信号が生成されなくなります     Screenshot from       png       Vivadoでbitストリームを生成する 先ほどVitis HLSで作成したIPは img flipという名前のIPです 以下は 配置配線後のDiagramです   Screenshot from       png  オレンジでハイライトされている配線は m axiインターフェースにつながる配線です 配線配線時の注意点は   Zynq UltraSCALE  MPSoCのIPを追加した段階では S AXI HPC FPDの端子は用意されていないため 下図のようにユーザ側で端子を追加する必要があることです   端子を追加した後に Autconnectの機能を使えば 自動的に必要なIPなどの追加を含めて全体のロジックを組み上げてくれます 配線接続完了後は Create HDL wrapperなどを実施し Vivado上でbitstreamを生成します Vivadoの使い方は Xilinx公式のマニュアルや こちらのブログ  を参考にしてください   Screenshot from       png       Kria PYNQを用いた画像反転プログラムを実行する   Kria KVに Kria PYNQ  をインストールすると Kria KV上にjupyter notebookが立ち上がり 開発マシンからIPアドレスとポート番号 e g       を指定してアクセスできるようになります   これにより 開発マシンからKria KV上で動くPythonプログラムをインタラクティブに開発できるようになります 以下の枚のキャプチャは PYNQを用いてFPGA上の画像反転ロジックを利用したプログラムのキャプチャです プログラムの重要な部分のみを説明します   Screenshot from       png    In    ライブラリのインポート OverlayでbitstreamをFPGAにロードする   In    pynq allocateでFPGAからアクセス可能なCPU側で管理しているメモリ領域 入力画像用 を確保する   In    pynq allocateでFPGAからアクセス可能なCPU側で管理しているメモリ領域 出力画像用 を確保する   In    確保したメモリ領域 入力画像用 の物理アドレスを確認する   In    確保したメモリ領域 出力画像用 の物理アドレスを確認する    これらの物理アドレスをaxislite経由でFPGAに渡すことでFPGAからメモリを参照できるようになります     In    グレースケールの画像を読み込む   Screenshot from       png    In    入力画像を表示する   In    入力画像用に確保したメモリの物理アドレスを渡す先となる 制御インターフェースのアドレスを格納する   In    出力画像用に確保したメモリの物理アドレスを渡す先となる 制御インターフェースのアドレスを格納する   In     先ほどの制御インターフェースのアドレスに 入出力画像用に確保したメモリの物理アドレスを設定する   In    FPGAロジックを起動させる   In    出力画像を表示する   Out  とOut  を比較すると 無事に画像が反転していると分かります   成功です 今回は簡単なロジックをFPGA上に実装しましたが より複雑なロジックを実装することで FPGAならではのアプリ開発も可能になると期待できます      まとめKria KVとPYNQを利用した画像反転アプリの実装方法を概説しました 今後は より高度なロジックをFPGAに実装する予定です    付録読者がjupyter notebookに記載したコードを利用しやすくするために 以下のコマンドでpythonに変換したコードを示しておきます ,4,2022-10-29
59,59,IPM逆透視マッピングよりBEVを得る,画像処理,https://qiita.com/chiba1sonny/items/37a48d5ab3862aad448f,逆透視マッピング IPM で画像のBird s Eye View BEV を得る方法を紹介致します から  BEV Bird s Eye View BEV画像とは 上空から斜めに見下ろしたような形式のものを言います 鳥瞰図 俯瞰図 パノラマ図とも言えます   IPM Inverse Perspective Mapping カメラで撮影された画像は 透視投影なので 本来平行だったものは画像内で交差します IPMでこのような透視投影をBEVに変換できます そのため 逆透視変換とも呼ばれます IPM変換方法として OpenCVの関数を利用します 理論知識について これを参考ください：  src  Source画像の四角形の頂点の座標  dst  生成される画像内の対応する四角形の頂点の座標  M  変換マトリックス  dsize  出力画像のサイズ      ．手順以下の画像を例として使います   つの点を選びます タブレットの頂点にします Matplotlibを利用して 座標を求めます 座標を入手します 結果：   coordinates p      p      p      p        出力画像の長さと高さを求めます 出力画像はソース画像中の選んだつの点からなるので つの点の間の距離を算出します これで  cv warpPerspective  関数のパラメーター  dsize  出力画像のサイズ  の値を決定しました  max w max h です 結果：,0,2022-10-19
61,61,OpenCVを用いたリアルタイムマスク有無判別,画像処理,https://qiita.com/shota_seki/items/d34b2c51cc8ccc3b82b4,  はじめに年あたりから新型コロナウイルスの感染によって 私たちの生活は一変してしまいました その中でもマスクの着用をどうするかの問題が連日ニュースで取り上げられています そこで 今回はOpenCV 深層学習を使ってリアルタイムでマスクをつけているかどうかを判別するシステムの開発に挑戦しました   開発環境  Atom バージョン    手順  マスクなしの人 マスクありの人の画像を収集  顔部分の抽出  モデルの構築 予測精度の評価  カメラからの入力画像をウィンドウに表示   マスクの有無判別結果をウィンドウに表示   マスクなしの人 マスクありの人の画像を収集モデルを構築するにあたって データを用意することが一番初めにしなければならないことです この作業が一番時間がかかりました 特に マスクありの人 の画像を抽出する時の検索ワード選びに試行錯誤しました  マスク という単体のワードでは 商品だけの画像が出てきたり プロレスラーなどが被るマスクが出てきたりとうまくいきませんでした なんとしてでも マスクをつけた人 の画像を収集したいと思い よくニュースで東京都の感染者数が取り上げられていることをヒントに マスクとつけている人の画像は  マスク　東京 で検索することにしました すると 下のようなかなりいい画像を収集することができました    OYTI  jpg  一方で  マスクなしの人 の画像は 集合写真 で検索することで収集しました 確認してみると ラッキーなことに乃木坂の集合写真も含まれていました    jpg  画像を収集するためのモジュールとして  icrawler  を使いました 画像を収集するコード scraping py はこちらです キーワードとして 集合写真  マスク　東京 を指定しています   画像を収集するメソッド  引数は画像を保存するパスpath 検索ワードkeyword 収集する枚数num     検索ワードにkeywordを入れたときに得られる画像をnum枚収集 ファイルの形式はjpegなので ファイル名には必ず拡張子 jpgがつくkeywords   集合写真    マスク 東京     顔部分の抽出先ほど収集した画像の中から顔の部分だけ抽出した画像をモデルに学習させます やることはこんな感じです   スクリーンショット       png  ここで使用するのは   Dlib  というモジュールです 顔部分の抽出をするコード face cut py はこちらです    python Dlibを始める Dlibは画像から顔の検出をするためのツール  顔画像を取得して保存     画像のサイズが大きい時はリサイズする     顔検出  検出した部分の矩形の座標を取得している          x にリサイズ         保存する        out outdir     str fid    jpg         cv imwrite out  im         fid  dlibの中にあるget frontal face detector関数の戻り値は顔を検出したときの座標を示しています   スクリーンショット       png     モデルの構築 予測精度の評価顔部分抽出した画像を使って モデルに学習させます 今回は  tensorflow  でCNN 畳み込みニューラルネットワーク を構築しました CNNに関しては下記のサイトが非常にわかりやすかったです 今回扱う画像はカラーなので 画像のサイズは  タテxヨコxチャンネル数  です マスクなしの場合は マスクありの場合はと判別させます 訓練データはそれぞれ枚ずつ テスト用データは枚ずつでモデルの学習を行いました  マスクありなしの画像の枚数に差があることでうまく認識ができなかったので同じ枚数で行いました モデルの構築 予測精度の評価を行うコード make model py はこちらです  画像形式の指定 CNNモデル構造の定義 入力層：xxch 畳み込み層  xのカーネルを個使う 畳み込み層  xのカーネルを個使う プーリング層  xで区切ってその中の最大値を使う 畳み込み層  xのカーネルを個使う 畳み込み層  xのカーネルを個使う プーリング層  xで区切ってその中の最大値を使う 全結合層   出力層   マスクありorなしの値  モデルのコンパイル 画像データをNumpy形式に変換         画像の読み出し        img cv imread fname          画像サイズを x にリサイズ ディレクトリ内の画像を集める テスト用画像をNumpy形式で得る データの学習 データの評価 モデルの保存 学習の様子を可視化  model add  でCNNのアーキテクチャ 層の種類 数 ユニット数などの詳細 を定義しています   model compile  で損失関数 最適アルゴリズム 評価指標を定義しています ここでCNNがモデルとして使えるようになります   model save  でモデルをファイルとして保存することができます 別のファイルでモデルを読み込んでそのまま使用することが可能になります これを知った時 感動しました 学習の様子を可視化した結果がこちらです たまに精度が急降下するときがありましたが最終的な正解率は約 でした より高い精度を出したい場合はパラメータを調整するというより 画像データの選択が重要だと感じました 前述の内容はうまくいってますが はじめは枚数の指定なしで学習をしていました その結果 マスクなしの画像を極端に多かったことが原因で過学習していた上に マスクをしていても顔の認識すらできませんでした    カメラからの入力画像をウィンドウに表示モデルが完成したので 次に準備としてPC内蔵のカメラに映った様子をウィンドウ表示をしました  下のコマンドはpython版のインストールコマンドです  pip list   grep opencv pythonopencv python                    コードは下記の通りです たったこれだけでPC内蔵のカメラを起動してそこで捉えたものをウィンドウに表示できるのは驚きでした   デフォルトではが内蔵カメラのIDだが Snap Cameraを使用している影響でIDはとなっていた  editorのterminalではなく 単体のterminalから実行するとできるcapture   cv VideoCapture    capture関数からカメラからの読み込みができているか判定     retは読み込みができているかの判定でTrue  False     frameはカメラが捉えた情報    ret  frame   capture read       ウィンドウに表示    cv imshow  Mask Checker  frame      qをキーボードで入力するとカメラ停止 メモリを解放するcapture release   ウィンドウを閉じるcv destroyAllWindows  ただひとつ ハマったことはVideoCapture関数の引数の指定です デフォルトではPC内蔵カメラのIDはとのことなので 引数をにすると動くと思いきや 動きませんでした 私の場合 ZOOMでSnapCameraを使っているので 外部カメラと認識されていたみたいです そのため引数をにすると動かせることができました    マスクの有無判別結果をウィンドウに表示モデルとカメラの準備ができたので いよいよマスクの有無判別です ここで 仕様に関して先に定義しておきます   マスクをしている場合は 緑色の枠で顔部分を囲って OK と表示  マスクをしていない場合は 赤色の枠で顔部分を囲って NO MASK  と表示  判定結果の画像をディレクトリに保存 結果ラベル 保存した学習モデルを読み込む webカメラから入力を開始     カメラの画像を読み込む     画面を縮小表示する    frame cv resize frame           顔検出         顔部分を切り取る        im frame y y  x x         im cv resize im              im im reshape                  予測         枠を描画  マスクない時 v  は赤で強調する         テキストを描画     結果を保存     ウィンドウに画像を出力 カメラを開放capture release   ウィンドウを破棄cv destroyAllWindows      マスクあり   jpg      マスクなし   jpg  部屋の明るさなど 環境によってうまく認識できないときがなくはないですが うまくできていることが確認できました   まとめ普段の生活で目の当たりにすることが多いので やっていて楽しかったです その反面 精度を上げるための画像選びの難しさと重要さを感じることができました Pythonでは処理速度が遅いこともあり ウィンドウに表示された様子にラグが見られました リアルタイム性を重視するには処理速度の速いC  を用いる必要があると感じました    参考資料    OpenCV 接続したカメラから動画を取得しよう     Pythonによる AI 機械学習 深層学習アプリのつくり方     ソースコード ,13,2022-10-15
62,62,PDFに画像を挿入、置換、削除、抽出する方法（Java）,画像処理,https://qiita.com/iceblue/items/ee7f2de066cae10f2c0d,テキストと比較して 画像は通常 意味を伝えやすく PDFドキュメントをより魅力的なものにすることができます 画像はPDFにおいて重要な役割を果たすため   PDFへの画像を挿入 置換 削除 抽出する方法  を知っておくことは すべての開発者にとって真の利益となり得ます この記事では  Free Spire PDF for Java  を使用して PDF内の画像関連の操作を処理する方法を紹介します      PDFに画像を指定した位置に挿入する   pdfに画像を指定した位置に挿入する        PDF内の画像を別の画像に置き換える   pdf内の画像を別の画像に置き換える        PDF内の指定した画像を削除する   pdf内の指定した画像を削除する        PDFから画像を抽出する   pdfから画像を抽出する       依存関係の追加     この方法は 無料のFree Spire PDF for Javaが必要ですので 先にjarファイルをインポートしてください         MavenMaven を使用している場合 プロジェクトの pom xml ファイルに以下のコードを追加することで 簡単にアプリケーションに JAR ファイルをインポートすることができます    XML      公式サイトよりJarファイルをダウンロードまず  Free Spire PDF for Java   の公式サイトよりzipファイルをダウンロードします zipファイルを解凍し libフォルダの下にあるSpire Pdf jarファイルを依存関係としてプロジェクトにインポートしてください    PDFに画像を指定した位置に挿入する  drawImage     メソッドは 指定した幅と高さの画像を PDFページ上の指定した位置に描画するために使用されます したがって 描画の前にあらかじめ画像のサイズを変更する必要はありません     以下は Javaで新しいPDFドキュメントに画像を挿入するための主な手順です       PdfDocument   クラスのオブジェクトを作成します     PdfImage fromFile     メソッドを使用して画像を読み込みます   画像のサイズとページ内の位置を指定します   PDFドキュメントを保存し ます           PdfDocumentクラスのオブジェクトを作成する        PdfDocument doc   new PdfDocument             ページの余白を設定する        doc getPageSettings   setMargins             ページを追加する        PdfPageBase page   doc getPages   add             画像を読み込む          画像の幅と高さをPDFで指定する          画像の描画位置を指定する        double x   f         float y   f           ページに画像を描く        page getCanvas   drawImage image  x  y  width  height            ドキュメントを保存する        doc saveToFile  画像の挿入 pdf   FileFormat PDF       結果のPDFファイル   PDFに画像を指定した位置に挿入する     PDF内の画像を別の画像に置き換える  replaceImage     メソッドを使用すると PDF内の画像を置き換えることができます このメソッドは 置き換えられた新しい画像のサイズを自動的に変更し 周囲にある他の要素の可読性に影響を与えないようにします     以下は その手順です       PdfDocument   クラスのオブジェクトを作成します     PdfImage fromFile     メソッドを使用して画像を読み込みます     PdfDocument replaceImage     メソッドを使用して ドキュメント内の特定の画像を新しい画像に置き換えます   PDFドキュメントを保存します           PdfDocumentクラスのオブジェクトを作成する        PdfDocument doc   new PdfDocument             PDFファイルを読み込む          最初のページを取得する        PdfPageBase page   doc getPages   get             画像を読み込む          ページ内の最初の画像を読み込まれた画像に置き換える        page replaceImage   image            ファイルを保存する        doc saveToFile  画像の置き換え pdf   FileFormat PDF       結果のPDFファイル   PDF内の画像を別の画像に置き換える     PDF内の指定した画像を削除するPDFドキュメント内の画像は   deleteImage     メソッドを使用することで簡単に削除することができます     以下はその手順です       PdfDocument   クラスのオブジェクトを作成します   PDFドキュメントを保存します      結果のPDFファイル   PDF内の指定した画像を削除する     PDFから画像を抽出するPDFページの画像は   extractImages     メソッドを使用して抽出することができます     以下は Free Spire PDF for Javaを使用してPDFドキュメントからすべての画像を抽出する手順です       PdfDocument   クラスのオブジェクトを作成します   ドキュメント内のすべてのページをループし   PdfPageBase extractImages     メソッドを使用して特定のページから画像を抽出します   画像を pngファイルとしてフォルダに保存します           PdfDocumentクラスのインスタンスを作成する        PdfDocument doc   new PdfDocument             PDFドキュメントを読み込む          int型変数を宣言する        int index              全ページをループする              指定されたページから画像を抽出する                  ファイルのパスと名前を指定する                  画像を pngファイルとして保存する                ImageIO write image   PNG   output       抽出された画像   PDFから画像を抽出する      この記事では PDFドキュメント内の画像を扱ういくつかの方法を紹介します PDFドキュメントの取り扱いに関するより詳細な情報は  Spire PDF Forum  をご覧下さい   ,0,2022-10-13
63,63,goで画像の色調変換の処理を行う,画像処理,https://qiita.com/icemint0828/items/86d7823ba1228b7f11b6,  はじめにこんにちは icemintです 今回は自作パッケージの紹介も兼ねて goでの画像の色調変換の処理の方法を紹介します    動作環境   使用パッケージ  画像の色調変換の処理に使用しています   画像ファイルの拡張子の変換にも使用しています    インストール  画像の色調変換の処理	   保存 jpeg  gif形式での保存も可能 色調はグレースケールとセピアの種類が用意されています 後述しますが独自の色調変換を行う事も可能です  fc SaveAs の第二引数では以下の形式で保存するファイルの形式を選択出来ます   画像の色調変換の処理 image取得 	   srcImgを何らかの方法で取得ファイルを介さずにimageを処理したい場合は 上記のように imgedit NewConverter で直接画像の加工が出来ます   パッケージ内の処理の説明 imgedit  パッケージ内の converter go で画像の色調変換の処理を実施しています 引数から色調変換のモデルを取得し   image RGBA に対して 各ピクセルの色情報の変換を行った後に書き込みしています   α値 透過度 がのものは色調変換時に予期せぬ色調になる事があるため 別途標準パッケージで定義されている color AlphaModel にて変換を行っています  imgedit SepiaModel を参考にr g bの値を変化させる事で独自の色調変換の実施が可能となります   おわりに簡単な内容とはなりますが 画像の色調変換の処理を紹介させて頂きました 最後まで読んで頂き ありがとうございました      前の記事 goで画像の反転処理を行う  ,0,2022-10-11
64,64,今さら聞けない画像処理,画像処理,https://qiita.com/taka_yayoi/items/2e652c2796a8738acac8,こちらのイベントで説明した内容の抜粋です ウェビナーで使用したノートブックはこちらにあります スライドはこちら   画像処理とは我々が日常的に目にする  画像  をコンピューターで処理する技術です     画像処理  とは 主にコンピュータを使用して 画像を変形したり 色合いを変えたり 別の画像と合成したり 画像から何らかの情報を取り出す等の処理を指します   そして 近年では コンピュータが画像の内容を理解し 情報の抽出やデータ化を行う  画像解析  の適用も進んでいます   Screen Shot    at    png    画像解析とは  画像  から被写体の形状 色 サイズなどを抽出する技術です     画像データの増加  による画像データ活用の可能性の拡大   機械学習 人工知能技術  の進展により コンピュータを用いて 画像データから人 顔 自動車 部品などを抽出し その形状 色などを判定することで 異常行動の検知 認証 品質チェックを行うという取り組みが増えています 対象は画像だけではなく 動画にも拡大しています   最近ではMidjourneyなどによる画像生成も流行り始めていますが 今回は割愛します   Screen Shot    at    png    画像解析の適用事例もはや画像解析は実験段階ではなく実運用されるレベルになっています   コンバインによって収穫される穀物をカメラで継続的にモニタリング   穀物にダメージが認められた場合には 自動的に機械の制御パラメーターを調整   eコマースサイトに掲載する服の形状を検出し 自動で背景を除去   背景画像を統一することで サイトの一貫性を確保   画像処理技術の進展と新たな効率的な計算ツールの出現により デジタル病理学は研究 診断の両方において中心的な位置を占めるように   癌や感染症を診断し 治療する際の効率 効果を改善   画像解析でできること画像解析によって人間の目 あるいはそれ以上の能力を活用できます   物体検知  画像 映像内の特定の物体を検知することで 個数カウント サイズ測定 不良品検知などをおこないます   パターン検知  X線画像から腫瘍のパターンを検知することで 病状の早期検知につなげます   顔認識  スマートフォンでも広く用いられており 新たな認証手法として確立されています   文字認識  画像上の文字を認識し 翻訳を行うことがごく自然に行われています   類似画像検索  画像の形状などから類似性を算出し 似ている画像を検索します   画像処理ウォークスルーここではコンピュータビジョンのライブラリOpenCVを使用します 以降のウォークスルーで使用したノートブックはこちらにあります   グレースケール変換  カラー画像をグレースケールに変換します   エッジ検出  画像の輪郭を検出します   サイズ変更  画像サイズを変更します こちらのノートブックを使用しました Databricksの環境にあるサンプル画像を使用します   画像の表示   グレースケールカラー画像を白黒の階調で表現する様にします    py Python  imreadの第二引数でグレースケールを指定  画像の表示   エッジ検出輪郭を検出して強調表示します    py Python  エッジ検出  画像の表示   サイズ変更   py Python  画像の高さ幅を指定width height       画像をリサイズ  画像の表示  画像解析ウォークスルーDatabricksにおける画像の取り扱いから画像解析の例を説明します ウェビナーでは以下のユースケースのノートブックをウォークスルーしました   画像データの取り扱いおよび画像のラベリング  画像解析を行う際の基本的な流れを説明します   物体検知  画像 映像内の特定の物体を検知することで 個数カウント サイズ測定 不良品検知などをおこないます   パターン検知  X線画像から腫瘍のパターンを検知することで 病状の早期検知につなげます   類似画像検索  服や靴の画像を入力として 類似する画像を抽出します こちらでは物体検知のノートブックを説明します    note  注意  使用しているImageAIのバージョンの関係からランタイムは   ML  を使用してください    セットアップ   ImageAI  の一部である  事前学習済み  モデルを追加するためにインストールを行います     ファイルパスの設定このノートブックではユーザー名に基づいてファイル格納パスを設定します 以下のパスを使用します     tmp  workshop     検知対象データ 検出結果などを保存  Username の英数字以外を除去し 全て小文字化 Username をファイルパスやデータベース名の一部で使用可能にするため   DBFS上のファイル格納パス  パスをシェル環境変数に設定して シェルコマンドから参照できるようにします    検知対象ファイルのダウンロードシェルで wget を使用して画像ファイルをダウンロードします     学習済みモデルのダウンロードYoloをダウンロードします   ダウンロードされていることを確認します   ユーティリティ関数  displayVid    クラウドストレージ上の動画を表示  displayImg    DBFS クラウドストレージの動画を表示   単一画像からコーヒーの検知YoloVと以下のスターバックスのアイスコーヒーの画像を用いて物体検知を行います   実行パス 分析画像 出力先のパスを指定    物体検知検知結果を確認します    py Pythondetections領域とラベルのセットが得られています   dining tableの確率は 程度ですが 他のcupに関しては  程度の確率となっています 検知結果がオーバーレイされた画像を表示します   result jpeg    まとめ  Screen Shot    at    png  ウェビナーでは これ以外にもデジタル病理学の実例  TensorFlow Similarity  を用いた類似画像の取得 アプリとの連携までをご紹介しました 画像処理よりも画像解析に重きを置いた内容となりましたが これらすべては実験段階の域を超え すでにビジネスで広く活用されている技術となっています 皆様も街中で画像解析が活用されている事例は結構見かけているはずです このような画像解析技術をクイックに試すことができ 実運用段階に容易にこうする仕組みを提供しているDatabricksを活用してみませんか 以下はDatabricksが提供している機能のごく一部です   並列分散処理による画像の高速処理  GPUクラスターを簡単に構成して活用  様々なライブラリのサポート  モデル管理からデプロイまでをカバーするMLOpsの実現DatabricksのセミナーなのでDatabricksの宣伝になってしまいました Databricksでどの様に画像を活用できるのかに興味がある方は 以下の記事もご覧ください    Databricks Community Editionで画像データを分析してみる     画像アプリケーションに対するDatabricksリファレンスソリューション     Databricksにおける画像の取り扱い     Databricksにおけるバイナリーファイルの取り扱い     PyTorchによるeコマースのファッション画像背景の自動除去     eコマース向け類似画像レコメンデーションシステムの構築     Databricksにおける機械学習による病理画像分析の自動化     Databricks機械学習ランタイムを用いた動画における不審な振る舞いの検知     データレイクハウスでコンピュータービジョンアプリケーションを実現する    その他の 今さら聞けない シリーズ,3,2022-10-10
65,65,RepVGGのre-parameterizationをPyTorchで計算して確認してみた,画像処理,https://qiita.com/DM99999/items/7dd6a67c4d8eebb3e772,  はじめにこの推論時と訓練時のアーキテクチャの分離は re parameterizationテクニックを用いることで実現されています  a ec cc bba aa png  各re parameterizationについて以下の順に見ていきます   つのx conv   各結果の加算  → x convへのre parameterization   x conv   BN  → x convへのre parameterization   x conv   BN  → x convへのre parameterization   identity   BN  → x convへのre parameterization     つのx conv   各結果の加算  → x convへのre parameterization↓この部分です  dcaac f a a cdedcf png  つもつも一緒なので  つのx conv   各結果の加算  → x convへのre parameterizationを確認します   全体のコード      入力画像    入力特徴マップの生成C とします      入力画像    つの✕ convに変換    計算結果の確認同じ計算結果になりました     x conv   BN  → x convへのre parameterizationM  特徴マップμ  σ  γ  β  BNのパラメータとすると  cdbc a ec b cfcd png  のようになります↓実装を見たほうが分かりやすいと思います   全体のコード    入力特徴マップの生成値は適当です    BNの定義BNのパラメータも適当です    計算結果の確認誤差はありますがほぼ同じになっています     x conv   BN  → x convへのre parameterization× conv → × convに変換すれば後はと同じです下の画像のオレンジの部分です  de c c be efacc png  x convの中心にx convのフィルタの値をコピーして その他の箇所の重みをにすれば変換できます     identity   BN  → x convへのre parameterizationこれもidentity → ×のidentity convに変換すれば後はと同じですさっきの画像の黄色の部分です×のidentity convについて確認します畳み込みしても特徴マップの値が変わらない畳み込みですidentity convの実装については FrancescoSaverioZuppichini repvgg  py  を参考にさせていただきました   全体のコード  参考,1,2022-10-10
66,66,HSV色空間の値でグラデーションを画像を作る,画像処理,https://qiita.com/hiratake_0108/items/a455bd4f25eca189a9f2,    はじめに色を表現する際 RGBではなくHSVを使って表現することがあり その際 値の範囲をグラデーションで表現したかったので コードを書いてみました 何かの役に立てば幸いです     やったこと画像のサイズ width height と HSVの範囲 lower hsv upper hsv を指定することで その範囲をグラデーションで表現することができます   画像のサイズwidth   height     HSVの範囲を設定  初期化hsv np zeros  height width   np uint   画像のサイズに合わせて値を分割  横軸を色相 H       縦軸を彩度 S  明度 V   ファイルに出力     このまま実行した場合  image png       彩度を固定で緑のグラデーションを作りたい場合     明度を固定で青のグラデーションを作りたい場合,0,2022-10-09
67,67,要素のドラッグ移動、回転、拡大などが簡単に実装できるJavaScriptライブラリ「Moveable」の紹介,画像処理,https://qiita.com/mmgggg/items/25b1e5e467f805c1831c,  はじめに本記事はページ上の要素のドラッグ移動 回転 拡大縮小などを手軽に実装できるJavaScriptライブラリ  Moveable の紹介記事です 日本語の導入記事があまりなく  使ってみた 系の記事の数も少ないので導入の手助けになればと思い執筆しています 初学者のため間違いなどありましたらご指摘いただけますと幸いです   Moveableとは上記でご紹介した通り 要素にさまざまな動きをつけることができるjsライブラリです MoveableGithub→  導入において導入に関しては こちら  の記事と同じ手順で進めていきます またこの記事においては参考記事に We ll use the vanilla JavaScript environment to discuss the features of Moveable とあるように 他のライブラリやフレームワークを導入していない状態を前提として進めます Moveable自体はVueやReactに対応しており それぞれのコンポーネントが用意されていますのでご自身の用途に合わせてお使いください   導入の手順まず今回の導入のためのディレクトリを作りましょう      ruby  コンソールmkdir featuresfeaturesディレクトリに移動したら moveableをインストールします     ruby  コンソール yarn add moveable公式Githubや参考記事には  npm i moveable  でインストールするようにガイドがありますが 私の場合は  npm i  がなぜか機能しなかったためyarnでインストールしています 次にhtml css jsファイルを作ります ファイル名は何でも良いですが ここでは参考記事に則って命名しています     ruby 　コンソールtouch index htmltouch style csstouch index js次に  index html  の中身を書きます     html  index htmlstyle cssを書きます ※参考記事の中ではroot classの指定が   root  になっていますが classなので   root  になります 今の状態でサーバーを起動してページを見てみるとこうなっています    Image from Gyazo  このグレーの四角に色々動きをつけていきます   ドラッグでの移動ドラッグ移動ができるようにindex jsを書いていきます ページはこうなります    Image from Gyazo  何だか動かせそうになりましたね これだけだとまだ動かせないので 動かすためのコードを書いていきます   追記move on  drag      target  transform          target style transform   transform これで四角を動かすことができるようになりました    Image from Gyazo    他の機能をつけてみよう 拡大 縮小 回転 基本的に他の機能を付け足すには    機能名  true  を追記 昨日の動きを指定するコードを追記のstepになります 拡大 縮小機能は  resizable  という機能を使います   追記move on  resize      target  width  height          target style width   width    px    target style height   height    px     Image from Gyazo  なお 画像の縦横比率を保ったまま拡大縮小したい場合は  keepRatio  true  を追記します 回転機能は  rotatable  という機能を使います ※本記事で紹介する以外の回転の動きも用意されています 公式Githubよりご確認ください   追記move on  rotate      target  transform          target style transform   transform   Image from Gyazo    その他特定の機能を消したい場合は コードを消さなくても　  true  を  false  にすれば動かなくなります 本記事では移動 拡大 縮小 回転のみ紹介しましたが Moveableには他にもさまざまな機能があります javascriptだけだとドラッグ移動だけでもマウスの位置を特定して距離を足して  など組むのが大変ですが Moveableなら簡単に実装できます この記事がお役に立てば幸いです ,21,2022-10-06
68,68,【画像処理】Web会議ツールに依存しないバーチャル背景,画像処理,https://qiita.com/spc_ehara/items/1330636fe8a4fa724792,今回はWeb会議ツールに依存しないバーチャル背景を作成してみようと思います 使用するパッケージ名とバージョンは以下の通りです  パッケージ名 バージョン 説明  pyserial   シリアル通信を簡単に実装できるパッケージで PCに接続されているカメラのポートIDを探索 取得するために使用します   mediapipe    人間の顔や身体 表情などの検出を簡単に行うことができるパッケージです  OpenCV    カメラから画像を取得 処理するために使用します   pyvirtualcam    仮想カメラにフレームを送るために使用します  これに加えて 仮想カメラ機能を標準で搭載している OBS Studio  も使用します   データの流れ  構成図 png    実装    オプション引数  解説      指定したカメラから映像を取得      取得した画像から人間を検出      検出された人間以外の部分にバーチャル背景を描画      描画した画像を仮想カメラに出力      任意のWeb会議ツールで仮想カメラを入力に指定試しにteamsのデバイス設定から仮想カメラの映像を確認してみました   結果無事にWebカメラの映像から人間を検出して バーチャル背景を適用することができました また 仮想カメラの映像を取得することでアプリに依存せずにバーチャル背景を作成することが可能になりました ただし 人間の検出精度がバーチャル背景の輪郭精度に大きく影響するため 今回使用したmediapipeはteamsやzoomに標準搭載されているバーチャル背景よりも精度は落ちました   sunshine gif    さいごに今回は  Web会議ツールに依存しないバーチャル背景 について解説しました 目次は以下の記事からご覧になれます ,2,2022-10-03
69,69,【OpenCV/C++】cv::canny()等で得たエッジをブロック化する ,画像処理,https://qiita.com/GouNakano/items/727df26555a5de818e39,  はじめにcv  canny等で得たエッジは 物体の輪郭となる場合がありますが 複数の物体がある場合に エッジをブロック化して 物体毎の大まかな範囲を得て 物体検出の一助になるようにします 同一物体で複数のブロックをまたぐものは無いとすれば 処理する範囲を限定することが出来ます 今回は二値化マスク画像を結果とします   ブロック化手順ブロック化手順はシンプルです エッジが見つかった場合は 行と列がブロックサイズの白の矩形で塗りつぶします 同一ブロック内に ほかのエッジが見つかっても既に塗りつぶされている事にします   実装の手順　今回の実装は以下の様になります   画像の取り込み  カラー画像をグレースケール画像に変換  Gaussian Blurをかける   Cannyなどでエッジを検出する   輪郭を探して位置情報をためておく   エッジのブロック化関数 makeBlockMaskFromEdge を呼び出す   結果を表示する 入力画像はOpenCVのサンプルの basketball png とします   basketball png    ソースコード  エッジからブロック化したマスクを作成する	  グレイスケールとして読み込み	   平滑化を行います．これがないと誤検出が起こりやすくなります．	  canny法でエッジ検出	  輪郭を得る		  面積チェック		  点群に外接する傾いていない矩形を求めます	  エッジからブロック化したマスクを作成する	  結果表示  エッジからブロック化したマスクを作成する	  マスクの作成	  全てのエッジを検索		  エッジ情報取得		  エッジの領域		  ブロックを考慮した新しい左上座標		  ブロックを考慮した新しい幅と高さ		  エッジ画像の切り出し		  マスク画像の切り出し		  エッジを最後まで検索			  エッジを探す			  無い場合は終了			  見つかった座標から処理する範囲をセット			  マスクの更新			  使用エッジの消去			  次の検索位置  動作結果  cv  Canny  によるエッジ化画像しきい値の操作をすると検出されるエッジが変化します   edges png    ブロック化画像一つの物体で一つのブロックになるのが理想です ブロックサイズを変えて見ると 結果が変化します   block png  以上 ,0,2022-09-30
70,70,【画像処理】OpenCVを使ってWebカメラからキャプチャを生成して保存する,画像処理,https://qiita.com/spc_ehara/items/5c3aa5e2ef9592422031,今回はリモート会議等で使用しているWebカメラを入力として OpenCVを使ってキャプチャを生成して保存してみようと思います それぞれのバージョンはPython    OpenCV   になります また 今回の記事の内容はOpenCVの 公式ドキュメント  を参考にしています   out gif    OpenCVでWebカメラ画像を取得OpenCVに実装されているWebカメラから画像を取得する処理は  VideoCapture になります    python VideoCapturecap   cv VideoCapture deviceID  parameters 説明  deviceID WebカメラのデバイスID Webカメラがつであればであることが多い    実装実装はこのようになります     フレームサイズの統一速度の面から取得したフレームのサイズを横px 縦pxに統一しています     カメラ入力停止キーボードのqボタンでカメラからの入力停止ができます     画像キャプチャキーボードのcボタンで画像キャプチャを取得して 現在時刻をファイル名に設定し 画像保存ができます   実行結果cボタンを押したときにキャプチャされた画像がスクリプトと同じ場所に自動で保存されるようになりました また ファイル名は日時を取得しているため 秒間隔でキャプチャをすることでファイル名が被らずに保存することができます   out gif  ただし やはりwhile文でループしているため 分岐の部分でうまくボタンを押せない場合はキャプチャをとれないことがありましたので ここは次回の課題とします   さいごに今回は  OpenCVを使ってWebカメラからキャプチャを生成して保存する について解説しました 目次は以下の記事からご覧になれます ,1,2022-09-29
71,71,複数の画像に一括で透かしを入れる方法,画像処理,https://qiita.com/hailiziyema/items/3f68ba43fab1792dbd62,  複数の画像に透かしを挿入する著作権保護のために画像に透かしを入れるという人も多いようです  一枚ずつ透かしを挿入するのは難しく 時間がかかることもあります  今回は 複数の画像に同時に透かしを挿入する方法を説明します  ご興味のある方はご覧になってみてください    環境と前提条件    デバイスとソフト携帯電話やパソコンで使える画像編集ソフトウェアがたくさんあります  もちろん個人的には パソコンで写真を加工する方が少し便利だと思います  ここでは 画像編集ソフトの PicWish を例にとして説明します     使う手順 パソコンに PicWish  ソフトをインストールして起動します  ダウンロードurl    透かしを入れる というアイコンをクリックします  画像を選択してアップロードします  最大枚の写真をアップロードできます   右側にある編集ツールで文字 画像の透かしを作成して挿入します ,0,2022-09-29
72,72,画像認識技術で空港の飛行機数を数えてツイート投稿する,画像処理,https://qiita.com/barry0518/items/e4be92ac912f0d7f5d8d,休日にやることないから 画像認識を使って羽田空港の飛行機を数えてツイートに投稿するシステムを開発しました Darkflowというオープンソースの物体検出のライブラリを使います Darkflow のgitリンク環境：Windows Anacondatensorflow   Python   OpenCV   まず githubからクーロンしましょうgit clone cd darkflowdarkflow masterフォルダ内にbinフォルダを作成し binフォルダ内にyolo weightsをダウロードしましょう ★少し時間かかります 直接ウェブサイトからダウンロードしてもOKです darkflowをインストールする．★darkflowディレクトリで実行 どれでもいい 今回こちらの羽田空港のYoutubeライブカメラを使います ツイートを利用するためのライブラリをインストールpip install tweepy出力結果の整形するライブラリをインストールpip install pprint実際のソースコード タイムゾーン設定  API情報を記入  ツイートクライアント関数を作成  ツイート送信関数  カメラの起動 cap   cv VideoCapture   ライブカメラ映像  動画のプロパティを取得  動画のプロパティを表示 クラス種別          動画ストリームからフレームを取得             confで精度はある程度調整できます                          飛行機をカウントする                 枠の作成                 ラベルの作成          メッセージを指定        now   datetime datetime now JST         t   now time   strftime   X           print t         message    Python自動送信：現在時刻  t   羽田空港の飛行機数は  str n   機   ライブ映像：         時になったら自動送信          表示          qを押したら終了 実行コード飛行機が認識されていた様子設定された時刻になったらツイートも送信されました ,1,2022-09-29
73,73,画像の切り抜き処理,画像処理,https://qiita.com/hiratake_0108/items/7cabc89814a4555c266c,    はじめに画像認識をする機械学習のモデルを自分で作ってみようと思い その下準備として画像の一部を切り抜いて 学習する画像を作成するスクリプトを書きました 画像が保存されているフォルダを指定するとその配下にあるファイルを一括で加工するようにしています 実施する内容は以下 画像をグレースケールにして色の特徴を消す  画像から学習したい領域を切り抜いて保存する     やったこと C \\XXXXXX\\XXXXXX\\ の下に加工したい画像を置いて スクリプトを実行すると  C \\YYYYYY\\YYYYYY\\ の下に加工後のファイルを出力します パスは実行する環境に合わせて記載してください 私の場合 Y軸は 　X軸は 　の長方形で切り取りたかったので img       　としています  ファイル名を取得         グレースケール処理         画像の切り取り         加工後のファイルを保存※加工前※加工後,0,2022-09-28
75,75,画像処理エンジニア検定エキスパートに合格した勉強法,画像処理,https://qiita.com/Kenty250/items/32f54afb7748c7ae456b,  はじめに画像処理エンジニア検定エキスパートに合格したので 勉強方法などを残しておきます 試験の詳細については主催のCG ARTSのページをご確認いただけたらと思います  画像処理エンジニア検定について    受験理由仕事で顔認証を扱う 既存の顔認証エンジンを使う ことになったのですが 顔認証の仕組みやカメラの選定 パラメータなどが全然わからなかったので その辺りの勉強になるかなと思って受験しました  ちなみに その後転職したので今回勉強したことを活かす場はありませんでした      受験時の私のスペック画像処理というものを全く知らない状態でした   AIとか機械学習もやったことないです 写真の趣味などもないのでカメラの知識もゼロ 数学も学生時代の知識はほとんど残っていません   勉強期間約ヶ月  勉強方法   Udemyの動画視聴私のスペックのところにも書きましたが 前提知識ゼロ というかむしろマイナス  な私がいきなりテキストとか過去問を解くのは厳しいだろうと思い 画像処理の入門に最適そうな動画をUdemyで見つけて視聴することにしました   それがこちらです    画像処理の基礎：フィルタリング パターン認識から撮像過程モデルまで  内容は初学者を対象としていて非常にわかりやすく かつ画像処理エンジニア検定エキスパートの試験範囲をかなり広い範囲でカバーしています   Pythonで実際に動かしながら学べるのもポイントです   トータルで約時間なのでかなり長いですが この動画を見ることでテキストを読み始めた時にあまり抵抗なく読み進めることができました   期間的に余裕があるなら視聴をお勧めします    テキスト動画視聴の後は公式テキストを通しで読みました   xw jpg  難しいところはあまり深く理解しようとせず 試験範囲を一通り把握するイメージで読み進めました   難しい計算式はスルーしましたが 行列だけはネットで調べて復習しました    過去問過去問も公式から出ています   xw    jpg  ベーシックとエキスパートの過去問がそれぞれあります   ベーシックの問題を解いたら結構解けたのですが エキスパートの問題は全く歯が立ちませんでした   ちゃんと採点していませんが 正答率は半分以下だったと思います  合格ラインは割      公式テキスト再読エキスパートの過去問を解いてみて分かったのは 数式に関する問題が一定数出ること ほぼ毎回出る分野がある 値画像処理 周波数フィルタリング 幾何学的変換など ということです   この段階でエキスパートの過去問を解いてもあまり勉強にはならないと思い もう一度公式テキストを読み直すことにしました   一度過去問を解くとテキストを読んでいるときに試験に出そうな箇所がなんとなく分かってきます   一度目には読み飛ばしていた数式ですが 数式の名前や各項の役割 値を大きくしたり小さくすると画像にどのような変化が起きるか など を覚えるようにしました   頻出の分野はちゃんと理解できるまで何度も読み直しました 必ず出る著作権の分野は暗記で済むので問も落とさない気持ちで暗記しました    過去問 再挑戦 この段階でエキスパートの過去問を解いてみたところ 割くらいは安定的に正解するようになっていました   たまに合格ラインの割を超える時もありました   あとはひたすら過去問を解くのみだと思い 試験の直前まで過去問を繰り返し解きました   試験結果試験は全てマークシートなので自己採点でおそらく合格しているだろうというのはわかりました   合格証とともに送られてきた試験結果を見ると 点数が書かれていないのでわかりませんが 若干余裕を持って合格できたのかなといった感じでした   おわりに無事に合格することができてホッとしています   試験といえば普段はIPAの情報処理技術者試験しか受験しないので 合格ラインの割がとても厳しく感じました  IPA試験は割 　　あと 思ったより暗記力が試される試験だったなと感じました   暗記していないと解けない問題が多く 試験範囲も広いので受験勉強はとても疲れました   ただ テキストの内容は大変興味深く 画像処理という技術をとても身近なものに感じるようになりました   スマホで写真や動画を撮ったり PCで資料を作成したりといった何気ない日常の中にも画像処理の技術がたくさん存在していることを知ることができたのは良かったと思っています ,7,2022-09-24
77,77,外部Webサーバーにある画像ファイルをプログラミングレスでリサイズする,画像処理,https://qiita.com/SECUAL_masa/items/b5fa388ed88fd25e7ef0,  はじめに外部のWebサーバー というかIPカメラ がCGIを持っており というURLでスナップ画像が取得できる この時ダウンロードできた画像ファイルの解像度は WQHD x  ちょっと大きめ   変換前 png  このスナップ画像を定期的に取得してVGAで保存したい要件が出てきた 以下の記事でNginxの設定だけでグローバルIPアドレスを取得する方法を記載したが 画像ファイルのリサイズも似たような方法で実現できないだろうか と思い立ち調べてみた   環境お試し環境は以下の通り   環境    Ubuntu    LTS     Nginx        必要なNginxモジュールは libnginx mod http image filter だったが インストール済み になっていたので 標準で入るモジュールなのかもしれない   結論以下の設定だけで実現できた   説明  ポート番号が以外の場合はupstreamを利用する必要がある模様 なら次の proxy pass だけで実現できる と思う    image filter で必要なパラメータ値を指定  crop などもできる模様    URL上記設定 reload後に ブラウザから以下のURLにアクセスすると所望した解像度の画像ファイルが表示される   変換後 png    最後にいやぁ  Nginxってすごいですね ,1,2022-09-21
78,78,画像認識での出退勤管理を「Teachable Machine」を使って、作ってみた。,画像処理,https://qiita.com/kou-n/items/8825a4a0994682f0ed9b,  ご挨拶こんにちは 先日初めて LINE Bot の記事を書き Qiitaデビューを無事果たしました とある小売業で働くものです 今回は  Teachable Machine と Node RED を使った制作に挑戦しました まず 率直に感じたことは 機械学習 ってすごい です これを使えば 日々困っている業務の問題も 解決へのアプローチの幅がぐっと広がるなぁと思いました 早速 その試行錯誤の軌跡を記事にしましたので 読んでいただけたら幸いです  LINE Bot の記事はこちら  制作の意図私の会社では 出退勤時に従業員カードを機械にスキャンすることで 出退勤時刻を記録してます その際 わざわざカードを出して スキャンして   それを入店⇒出勤⇒退勤⇒退店と 計回 毎日     あれ これって ちょっと面倒だなぁ もっと楽に出来ないのか と思いました 機械学習で自分の顔を覚えさせて 出退勤スキャンが出来ればいいなぁと思ったので  Teachable Machine を使って作ってみました  その後 Node REDに連携して 管理もやってしまおう と思ってそれも追加   制作の注意点①今回使用した画像は サンプルです  photoAC写真のフリー素材サイトに登録して入手 ②実際に出退勤スキャンの機械にカメラが付いていて 勤怠システム連携がされ 出退勤の記録が管理できればベストですが 今回は  時間と従業員名を出力し 出退勤管理をスプレッドシートにする というもので制作しました   理想とする完成型出退勤の機械にカメラが付いていて 機械学習させたAIによる顔認識⇒勤怠システムへ連携し 当日の出退勤を記録⇒共有のフォルダ内で同部署の従業員の出退勤を確認できる  今回はPCでのスキャンを想定 そうすることで 結構多いカードのスキャン漏れも防げるし わざわざカードを出してスキャンする手間も省ける また 上司が会議中などで直接退勤の挨拶が出来ない時などに 共有フォルダから上司が自分で部下が帰ったかなどを確認できるというメリットも付与   使用したツールTeachable MachineNode RED  完成図★Teachable Machineでの顔認証★Node REDTeachable Machineノードで顔認証⇒日時を出力⇒Google Sheetsへ出力  完成図 jpg  本来は 従業員名 日時がGoogle Sheetsへ 下に下に足されていくようなものを制作したかった しかし 今回は画像認識 日時出力はデバッグとして表示されたが Google Sheetsへの書き出しまでは至らなかった   制作の過程 プロセス ①Teachable Machineで人物の画像を機械学習させる 従業員画像はサンプル   image png  ②モデルをエクスポートして URLをnote redのTeachable Machineノードに入力  image png  ③changeノードを使って  person 従業員A が出力されるように設定  image png  ④Date TimeFormatterノードを使って 日時が出力されるように設定  image png  ⑤顔認証のあと 氏名 日時がスプレッドシートに出力されるようにする   image png    試行錯誤の失敗例最初はCSV出力を考えていたり 日時がうまく出力されなかったり 試行錯誤ありましたので ここで少し失敗例を一部紹介 ①日時の出力がうまくいかなかった    image png  ②日時と従業員名は出力されたが タイムスタンプとカメラノードが分かれていて 実用的ではない   image png    総括今回制作した 顔認証勤怠管理フロー は 日常業務の中で聞こえてきた何気ない上司の会話がきっかけになりました 上司： あれ 〇〇さんは  部下A： 先ほど帰りましたよ 上司： そっかぁー なら明日でいいか 会議か何かで席を外していた上司が 机に戻ってきた際に言われた内容ですが その時従業員の出退勤をどこか共有で見れたら解決できるのでは そう考えて 顔認証による出退勤スキャン 出退勤情報の共有化を思いつきました  Node REDでの顔認証から 日時と氏名を出力して それをGoogle Sheetsに書き出す という流れは構築できたものの ノードの中身設定が中々うまくいかず 試行錯誤した結果 デバック出力することは出来ましたが Google Sheetsに書き出すというところには至りませんでした 今回学んだ Teachable Machine  Node RED は 本当に一部の基本的なものでしたが もっと知識を深めればさらに高度なものが制作できると思います 今後もできるだけ多くの情報を取り入れていきたいと思います ,7,2022-09-21
79,79,この子は誰？選手の名前を教えてくれるアプリ開発,画像処理,https://qiita.com/mhsc/items/2931f9294608526c7c1e,こんにちは テレビなどを見ていて  この人名前なんだっけ  となることありませんか 超有名人ならGoogleの画像検索で引っかかるかもしれませんが そうでもない 失礼ですね すみません汗 特定の人を知りたい 顔が覚えられない そんな時にちょっと役立つかもしれないアプリを作ったのでご紹介します   母のためのヤンスワ検索アプリ私と母親は数年前からプロ野球のヤクルトスワローズのファンです しかし母は名前を覚えるのが苦手で 中継を見ていてもよく 〇〇番は何て選手  などと聞かれます その都度答えますが 私がいない時でも楽しめるようになんかいい方法ないかなーと思っていたら おもしろそうなものがありました 今回は今注目の ヤクルトスワローズ期待の若手たち ヤングスワローズ 略してヤンスワ のお顔を認識すると名前を教えてくれるアプリ作りに挑戦しました     使用したい画像を用意する現地観戦に行っていいカメラで写真を撮るタイプのファンではないので 今回は画像検索の力を借りました そしてやり方はまるまるこちらを参照しました これによりダウンロードした画像をアップロードして学習させていきます     Teachable Machineでモデルを作成し Webアプリへ Teachable Machine で検索するとやり方を教えてくれるページはいくつかあったので モデル作成の方法はここでは割愛します 今回はスマートフォンで使えるようにしたかったので Javascriptではなく 年月現在iPhoneでは動かず  p jsのスニペットとやらをコピーして  CodePen  のHTMLの部分に貼り付けてアプリ化しました     使いやすいようにカメラの設定を変更プログラミング知識ゼロのため この部分に苦戦しました 通常 端末がなんであれ 内側 自分側 のカメラに設定されているのですが   facingMode    exact   environment     という設定をしてあげると 外側のカメラが動くようになるそうです 実際のコード次に 背面カメラが使えるようになったものの 画像が反転してしまうという問題が発生しました  video と flippedVideo をきちんと設定すればうまくできるのでしょうが 私は色々変えてみたものの 反転させないようにすることができませんでした   時間ある時に再チャレンジしようと思います     作ったモデルを共有するCodePenのURLを共有すれば誰でも使えるようになります 今回はGoogleChromeの機能を使ってQRコードも作りました 一応載せておきます 一応      おわりにいざ 使ってみよう と 選手の顔写真にかざしてみましたが   判定率はいまひとつ というかかなり外してる 涙 Teachable Machineの現状の精度では顔の識別まではできないのかもしれません 母を喜ばせることができず残念 しかし 機械学習モデルなので 勉強させ続ければいつかは正解するかも ということで こちらも時間がある時に再挑戦します そして本当の本当は劇団四季の俳優さんたちを教えてくれるものを作りたかったのですが 画像データが少なく学習が十分にできないこと そもそも私がそんなに名前を知らないことから断念しました 公式で作ってくれたら喜ぶ人結構いそうな気がしますが 以上 身近なちょっとしたことに使えるおもしろそうな技術を紹介しました これからもいろいろなものづくりにチャレンジしていきたいです 読んでいただきありがとうございました ,4,2022-09-21
80,80,【無駄遣いゼロ】海外ショッピングを楽しもう！LINEに商品写真を送って値段をCheck！,画像処理,https://qiita.com/rsaaa269/items/b2e0bae837f093e77660,   開発奮闘記 その前回  タイの魅力を発信する というテーマのもと    週間LINE Bot製作    に挑戦しました 今回はそんなタイお役立ち情報発信のための  開発奮闘記第弾  です タイに来て困ったこと それは買い物中  陳列棚に書かれている商品名がタイ語のみで分かりづらい  こと そもそも値札が無かったり 陳列されている商品と値札に書かれている商品が一致していなかったりして 会計の時に思っていた値段と違うなんていうことが結構な頻度で起こります 旅行者はもちろん タイ在住者であってもとても不便に思います というわけで 今回はこの困りごとを解消すべく    Teachable Machine    を使用した開発に挑戦しました    完成品 写真で判別できれば いちいち店員さんに聞かないでも済むのに そんな思いから LINEで写真を送ったら商品名 英語 と値段を返してくれるLINE Botを作製してみました    手順    使用ツール  　※ こちら  の記事を参考にLINE Developersに登録し LINE公式アカウントの作成 　　 Botにするための設定は済ませておきます まず Node REDのURLを取得します 赤丸の部分にカーソルを合わせるとURLが表示されるので コピーします    note warn  重要ポイント  Node REDはし URLを取得します ローカル環境からURLを取得しても自分のPCからのアクセスしかできず LINE BotとNode REDとの連携ができません 次にLINE DevelopersのWebhook設定のページに戻り コピーしたURLを貼り付けます この際 末尾に  linebot  と追記して保存 Webhookの利用をオンにしましょう わたしはローカル環境のURLをそのまま貼り付けていました  有効なHTTPS URLを入力してください の警告が消えず 何時間も費やすことに… 　今回はPCのカメラを使用してサンプリングをするので それぞれの商品を手元に用意しておきます 商品名と値段を入力し Webcamボタンからサンプリングする商品を〜枚ほど撮影します 全ての商品のサンプリングが終わったらトレーニングボタンを押して 学習させます 今回はタイのセブンイレブンで販売されているつの商品をサンプリングしました  年月日 現在 とても正確に判別しています    note① LINE Botの設定 でも触れましたが Node REDのフロー製作は します LINEで写真を送ったら該当商品の名前と料金が返ってくるフローの作製をしました 各ノードの設定は以下のサイトを参考にしました 手順−のみ コードを貼り付け後に文章の中身を変更しました LINEに写真を送信して動作を確認します こちらも正確に判別しています 以上 実装手順でした Node REDでの実装がうまくいき LINEの動作がイメージ通りにいったときはとても嬉しかったです ただ 完成したは良いものの 実用化は難しいということに気づきました 今回は各商品を自分で選んで購入してからサンプリングや動作確認のための写真撮影をしましたが  支払い前に商品の値段を知る という本来の目的は 店頭での商品の撮影が許されるという環境が少ないため達成されません 写真判定以外の別の案か 個人の撮影にならないような方法を考える必要がありそうです    あとがき今回の挑戦では目的が達成できず反省点が多く残りましたが Teachable Machineを使用してみて AIの様々な可能性を感じることができました  音声判定をつかったら 英語やタイ語で行われる会議の議事録作成が楽になるんじゃないか   目元や顔の表情の画像サンプルを集めて疲労レベルを判定 倒れる前にお知らせする機能をつくれなか   画像判定でデスクを撮影 高評価の人に景品をプレゼントすれば 綺麗なオフィスを維持できるのでは  などなど 日頃の業務や困りごとの解決にも役立てるのではないかと感じました AIに少し触れた程度でまだまだ知識は足りませんが 今回感じたこと 学んだことをわたしひとりに留めず 職場のAIに詳しい人に相談をしたり 同僚と共有をして業務改善に取り組んでいきたいと思います また 今回浮かび上がった他のアイディアをnoteでも少し取り上げてみたので こちらも読んでいただけると嬉しいです ご覧いただき ありがとうございました ,14,2022-09-21
81,81,従業員が車に乗る際に義務化された「アルコールチェック」報告結果を効率的に収集する方法を考える,画像処理,https://qiita.com/ak-ishi/items/231d2c890b0047364aae,こんにちは 今回は いま社内の各部署から寄せられる要望の中から 業務で車に乗る際に実施することが義務付けられた アルコールチェック について考えてみたいと思います  年月日　一部加筆修正しました    ．アルコールチェックの義務化今年月から 安全運転管理者に対し 第三者による目視等により運転者の酒気帯びの有無の確認 およびその記録を年間保存することが義務付けられました  月からアルコール検知器による検査も義務化される予定でしたが こちらは　いったん延期になったようです さて私が勤務する会社でも この件に対応するための取組が始まりましたが 社有車も私有車もレンタカーも利用対象となっており 本人以外の従業員がいつでも目視できる状況にあるわけではないため 非常に困っています 今はMicrosoftのTeamsのチャット機能で 乗車前に顔写真を撮影し投稿する というやり方が採用されていますが これは事前に作成した チーム 内でしか共有できず 社内で数百人いる対象者を一括管理できないのが難点です そんなとき GoogleのAIサービス Teachable Machine で画像認識機能が使えるということを教わったので 今回は 顔画像を撮影して本人確認するアプリを作ってみたいと思います    ．今回使用するツール Teachable Machine　：GoogleのAI 人工知能 の機械学習ツール CodePen　：Web上でHTMLやJavascriptなどのコードが書けるエディタ   ．アルコールチェック報告のイメージ  業務の流れ png  　  事前対応  　   運転者   　　　業務目的で乗車する機会がある従業員は 本システムで利用登録を行う 　　　 従業員IDの登録＋顔画像の撮影 　  運転時  　   運転者   　　　業務目的で乗車する際に スマホ等を使って アルコールチェック報告 画面上で　　　設問に回答 顔画像を撮影し送信する 　   システム   　　　運転者が送信したアルコールチェック報告データを 事前に登録した運転者情報　　 従業員ID 顔画像など を照合して 本人かどうかを判定する 　  事後確認   　   主管部署   　　　定期的に 全社の運転者が実施したアルコールチェック報告と運転計画 実績とを　　　照合し 運転者がもれなくアルコールチェック報告を実施したか アルコールを　　　摂取した状態で運転した形跡がないかを確認する この流れをどこまでできるか チャレンジしてみたいと思います    ．Teachable Machineで顔画像を認識するアプリを作る　Teachable Machineでは 画像 音声 ポーズの種類のプロジェクトが作成　できるそうです 　今回は 画像プロジェクト を選択します   新規 png       ①．対象者の顔画像を認識　まずは 対象者の顔画像を認識させます 　左側の枠のつつを クラス と言いますが 今回は人を登録するという想定で 　つのクラスに人ずつ撮影するか または撮影済みの顔画像をアップロードします 　画像は程度あると認識が正確になるそうです   画像① png  　まず私自身をPCのカメラで撮影してみました 　赤枠の数字は従業員のIDです 　私には仮に  を付与してみました 　同様に番目の従業員には   番目には  を付けました 　 たまたま手元にあった雑誌の写真を使用したため 人物が特定されないよう　　加工しております       ②． トレーニング で各クラスの情報を記憶　次に 各クラスで記憶させた顔画像を区別できるよう AIに トレーニング 　させます   画像② png       ③．実行　では カメラに私の顔を映してみます   結果① png  　うまくいったようです 　雑誌の二人の写真をかざすと それぞれ    と認識されました 　当社の場合 対象者が最大数百人規模となるため 全員の顔画像をずつ撮影する　となると相当大変かと思いますが そこはまた別途考えてみたいと思います    ． CodePen を使って Teachable Machineをウェブアプリに組み込む　今回作成するアプリは 対象者がスマホから起動することを想定しています 　そこで今度は  CodePen というツールを使ってウェブアプリとして展開できる　ようにしてみたいと思います      ①．CodePenを起動し サイドバーの Pen を選択　まずは新たなアプリを作るため  Pen ＞ Pen を選択します   Codepen png       ②．Teachable Machineで作成されたコードスニペットを貼りつける　左から HTML  CSS  JS の枠があります 　今回は HTML の枠に さきほどTeachable Machineでアプリを作成 エクスポート　した際にできたコードスニペットより p js 側を選択してコピーし･･･  エクスポート png  HTML枠に貼り付けます ↓  Codepen① png       ③．実行　しばらく待つと コードの下にカメラ画像が表示されます 　ちなみに実際に実行する場合 コードを表示させないよう Change Viewボタンより　フルページを選択しておきます   Codepen③ png  　私を投影すると   雑誌の女性を投影すると    と 　設定したIDが正しく表示されました      ④．スマホで実行できるようQRコードを作成　フルページ表示に変更したところで このURLからQRコードを作成します   Codepen❺ png       ⑤．実行　では スマホで実行してみます   スマホで実行 png  　スマホカメラが私をとらえると 従業員ID  が表示されました 　とりあえず 第ステップは完成しました    ．今回の反省 と 今後の取組　スマホで顔画像を撮影するアプリは作れましたが そのデータを主管部署が管理する　格納場所に送信し 蓄積するところまで至りませんでした   業務の流れ png  　図の左側の赤枠だけがなんとか完成した形です 　右側の管理者画面を作るにあたり 画像を含むデータをデータベースを作って保管する 　というのは現実的ではないと思い Googleドライブなどのクラウドストレージを活用　することも検討しました 　GoogleのAPIを登録し Teachable Machineで撮影した顔画像と従業員ID情報を渡す　方法を考えてみましたが いまのところ わかっていないところが多く実装できて　いません もう少し詰めてみて またこちらに書いてみたいと思います 　せっかくAIの画像認識機能を活用することができたので シラフのときと酔っぱらった　ときの両方の画像を記憶させておけば 管理者がチェックする前に AIがアルコールを　摂取しているかどうか自動的に区別してくれるかもと期待しましたが 誰も酔っぱらった　時の写真など撮らせてはくれないでしょうね   酔っぱらい png  ,7,2022-09-19
82,82,python、画像の読み込みと表示,画像処理,https://qiita.com/nemoyuya/items/f64c74cccd2a9f2b90a4,  やること  画像の読み込み  画像の表示  ライブラリ  cv   python用のOpenCV  コード     画像の読み込み  は元画像のチャネルのまま読み込むフラグ     画像の表示    cv waitKey     の数字を変えると表示のタイミングが変わる  画像表示ウィンドウを閉じる,0,2022-09-19
83,83,OpenCVとPILでアクリルバッジの型枠画像を作成する,画像処理,https://qiita.com/can110/items/2acddf12feaa3d8b2e21,   概要こんな画像から  side png  出典： フォクすけの画像  こんな画像を作成します   ret png  画像は透過PNGであることが前提です    コード  上下左右に縁取りを追加  輪郭をなめらかに膨張  img    中身が白 背景が黒のグレースケール画像      円形 なめらか に膨張      モード 最頻 フィルターを適用      外接する輪郭を取得して内部を塗りつぶすsize      膨張サイズ  上下左右に縁取りを追加  透過画像から中身が白 背景が黒のグレースケール画像を作成  輪郭画像を作成  元画像を透過情報を維持しつつ合成   コード解説まずは元の画像から 中身が白で背景が黒の値画像を作成します   gray png  次に cv morphologyEx で周囲が円形になるように画像を膨張させます   morph png  ただ そのまま膨張させると凹部分が鋭角のままになってしまいます そこでPILの ModeFilter という最頻値をとるフィルターをかけます   mode png  凹部分もうまくなめらかになりました あとは cv findContours で外接する輪郭を取得して内側を白で塗りつぶします これはドーナツのように内側に穴のあるような画像を考慮しての後処理になります 最後に元の画像と合成して完成です   ret png     その他わりといいかんじにできました 元ネタ： 画像輪郭のなめらか処理の方法  ,1,2022-09-18
84,84,画像のグレースケール化とマスク処理,画像処理,https://qiita.com/hiratake_0108/items/789282e90e5877cef453,    はじめに画像認識をする機械学習のモデルを自分で作ってみようと思い その下準備として学習する画像を加工するスクリプトを書きました 実施する内容は以下 画像をグレースケールにして色の特徴を消す  画像上の画像認識に使用しない領域にマスク処理をする 準備するもの マスクしたい箇所を黒で塗りつぶした画像 mask file jpg     やったこと C \\XXXXXX\\XXXXXX\\ の下に加工したい画像を置いて スクリプトを実行すると  C \\YYYYYY\\YYYYYY\\ の下に加工後のファイルを出力します パスは実行する環境に合わせて記載してください  ファイル名を取得         グレースケール処理         マスク処理         加工後のファイルを保存このように画像を加工して出力されます ※加工前※加工後    つまづいた所ファイルのパスを指定する際に日本語のファイルパスだとOpenCVでこんなエラーが出ます ファイルパスは英語にする必要があります     つまづいた所マスク画像をそのまま使おうとしたら配列のサイズが違ことでエラーになりました 白黒の画像をグレーアウト処理を追加することで解決しました ,0,2022-09-16
85,85,自動運転に使用されるディープラーニングによる車線検出,画像処理,https://qiita.com/paypay-1126/items/c047925e83ef36af3f95,  はじめにこんにちは ぺいぺいです 大学の研究で自動運転がらみの研究をすることになり 自動運転に必要な 車線検出についての備忘録です 間違い等ありましたら 指摘していただければ幸いです また 本記事では特定の技術の解説よりかは全体像の把握に重きを置いていますのでご了承ください     大まかなまとめ  自動運転の画像処理は ディープラーニングのセグメンテーションタスク 回帰タスクにあたる   セグメンテーションタスク      セマンティックセグメンテーションベース    それぞれのピクセルに対する道路か 背景か 人か    かを分類  SCNN  CurveLanes NAS       Row wise classification    画像を列単位で見ていき 列のどの部分に車線があるかを予測  EE LMD  IntRA KD   回帰タスク      Ancherベース    事前知識をもとに 車線分離線がどこにあるかのあたりをつけておき そのあたりからどの程度ずれがあるかを予測する YOLOvやSSDに近い  Line CNN  LaneATT       Parameterベース    車線の点の位置などをモデルで直接回帰するのではなく 形状のパラメータを回帰して 車線の位置を把握する      PllyLaneNet  LSTR   太陽光の反射 車などの障害物によって線が隠される 悪天候により線が見えなくなるなどの多くの困難な点が存在する   上記の条件下でも正しい車線検出を行うには 大域的特徴と局所的特徴が共に必要     自動運転における車線検出  車線は 自動運転を行う際に 必要不可欠な情報のつである 車線の位置が正しく分かれば 自分の位置 進むべき方向 などさまざまな情報を得ることができるため 正しく検出することが必要不可欠である   応用方法を変えれば 車が車線を踏んだ際に警告音を出すなどの運転手のアシスタントとしても使用できる  Adcanced Driver Assistance System   image png      車線検出の困難性近年のディープラーニングを使用したコンピュータビジョン技術の向上により 画像の特徴を効率的に抽出できるようになった しかし 人の命がかかる自動運転に必要な精度をいかなる状況下でも満たすのはかなり難しい 検出が難しい要因を挙げていく      大域的特徴が必要  image png  この画像は 車線を誤って検出してしまった一例である 本来は 赤線の左隣の線がターゲットであるが 間違えて他のレーンの車線を検出している 正解レーンと誤検出レーンは 以下の共通点がある   白くて長い白線である  共に傾きが似ているこれはモデルが小さい範囲しか見ていないために起こるミスである  畳み込み層で言うとカーネルサイズが小さいイメージ      局所的特徴量が必要  image png  この画像は 車線の検出が大雑把すぎた例である この結果を出力したモデルは 広い範囲の特徴量しか使用していないため 車線の細かな予測ができなくなってしまっている 命がかかっている自動運転では このような些細なずれも許されない      障害物による遮り  image png  運転中 車やバイク 歩行者などによって車線が見えなくなってしまうことも多くある このような条件下でも車線を検出するには やはり広い範囲から特徴をとってくる必要がある      太陽光の反射  image png  太陽光が道路に反射することで車線が見えなくなってしまうことも考えられる この場合も広い範囲からの特徴がなければ 反射している部分に車線があるかないかを判断することはできない      環境の変化そのほかにもさまざまな環境による変化が考えられる   国ごとの表記の変化 左車線や 右車線など   天候 時間帯による変化 雨 雪 夜の暗い道など   歩行者の量 車の量や種類    車線検出のためのディープラーニング近年は 車線検出にディープラーニング技術が使用されることが多い  非ディープラーニングの車線検出としてはハフ変換による直線検出などがある  ディープラーニングを使用した車線検出もまた いくつかの手法に分類することができる      Segmentation based  image png  セマンティックセグメンテーションは どのピクセルがどのクラスに属するかを出力するピクセル単位の分類問題である 対応するモデルとしては 以下のものがある   SCNN  SAD  CurveLanes NAS道路が写っているピクセルを正しく出力することができれば 自動運転等に利用することができる また 人や車 障害物を正しく検出できていれば それを避ける運転をすることもできる セマンティックセグメンテーションは 計算時間が大きくなることが多く リアルタイム処理が求められる自動運転では いかに計算時間を減らすかが肝になってくる 上記の画像では 道路を丸ごと検出しているが 以下のようなセグメンテーションも考えられる これも同様のセグメンテーションタスクであるが 車線のみを検出しており 道路や木などを検出していない また 全ての車線は区別されている 左レーン 中央レーン 右レーン  このように同様の車線というクラスでも別のものと区別する手法は正確には インスタンスセグメンテーションと呼ぶ Row wise classificationという 列ごとに画像を見ていき その列の何番目が車線に該当するかを予測する手法もある 該当するモデルとしては 以下のものがある   EE LMD  IntRA KD     Anchor basedアンカー Anchor とは 事前知識を活かして作成する基準点のことである 例えば 道路は画像上部に写っていくことはほとんどないし 事故でも起こらなければ  車線はたいていの場合 手前から斜めで中心に向かっていく これらの事前知識を生かせば 基準とした斜め線から目的の車線までどのくらい移動させれば一致するかどうかをモデルに予測させるように学習させることができる 事前知識を利用している分 からの学習よりもスムーズにいくことが期待できる 有名なYOLOvやSSDっといったモデルは BoundingBoxを出力するためのアンカーを使用しているが 車線検出を行うモデルのアンカーは 少し異なる Ancher basedな手法は以下のものがある   Line CNN  LaneATT  SGNet  UFLD  CondLaneNet  スクリーンショット       png     Keep your Eyes on the Lane  Real time Attention guided Lane Detection  上記のLaneATTというモデルを例に挙げる   あらかじめ斜め線のアンカーを複数用意しておき それぞれのアンカーに対して 複数のθを設定しておく   斜め線をいくつに区切るかも予めパラメータとして与える   モデルが予測するのは アンカーが車線からどのくらい離れているかの値  x とアンカーの何番目の区切りまでが車線に対応するかを表す値  l  車線のクラスはなんなのかを表す p  背景も含む を予測できるように学習させる      Parameter basedParameter basedな手法は 点を回帰するAncher basedとは異なり 車線の曲線をパラメータでモデル化し そのパラメータを回帰することで車線を検出する 以下のモデルが当てはまる   PolyLaneNet  LSTR    終わりに今回は 自動運転に採用されるディープラーニングの画像処理手法を俯瞰的にまとめました 今後 それぞれの手法を細かく確認していこうと思いますので もし更新されたらぜひ見てみてください   参考文献   区間線認識論文サーベイ  ,2,2022-09-16
87,87,関数アートジェネレーター　技術解説,画像処理,https://qiita.com/inoz/items/f4df1111d8ea222dfd78,  はじめに   関数アート   と呼ばれるアートをご存知でしょうか こんなふうに  猫を数式で表現  してみたり        時にはこんなふうに  音楽のPV  そのものを数式で表してしまったりする 数学好きならではの  職人芸  です ところでこの関数アート   自動生成  してしまえそうではないでしょうか ブラウザ上で動作する    関数アートジェネレーター     を制作したので このツールが自動的に関数アートを生成している  仕組みについて解説  します     技育展    に出展しました 開発期間は  正味週間ほど  です   作ったものお手持ちの画像から   関数アート  を  自動的に生成  することができます 検出精度は現状  まずまずの水準         で 単純なイラストなら  いい感じに  関数アートを生成してくれます   写真  でもそこそこの検出精度です   ブラウザ上で動作  するため インストールやダウンロード不要で  誰でもすぐに  利用することができます なお 現時点で生成される関数の種類は   線分      円弧      多項式   の種類です 画像認識のライブラリや 機械学習のライブラリなどは用いず   ゼロからフルスクラッチで開発  しています   フーリエ変換  などを用いれば非自明な形の関数アートが生成できそうですが できる限り  人間が理解しやすい関数アートを生成  するため 今回はフーリエ変換を用いない仕様となっています   現時点では 座標平面はx yがそれぞれ 〜の範囲で固定です また 生成された関数アートの式を修正することはできません これらの機能は そのうち  追加実装  するかもしれません   概観今回製作した  関数アートジェネレーター  は 以下の  つのステップ  で関数アートを生成します         エッジの検出  　画像の  エッジ  を検出します         勾配の計算  　　画像中の各点について   局所的な傾き  を算出します         線分の検出  　　画像に含まれる  線分  を検出します         円弧の検出  　　画像に含まれる  円弧  を検出します         グラフ構造化  　ここまでで近似できなかった点を   グラフ構造化  します         多項式で近似  　グラフ構造化した各連結成分を   多項式で近似  します ここから 各ステップの動作について解説します    ステップ　エッジ検出ステップでは   エッジを検出  して画像を関数表現しやすくします   突貫工事の開発  だったこともあり このステップはかなり  プリミティブなエッジ検出  を実装しています       画像をピクセルずつ  走査  する      右隣のピクセルが存在するとき そのピクセルとの   差   が閾値以上であれば 該当する点を中心に  一定の半径の円  を描く      終端に到達したら終了で定義しています 実際のプログラムでは   平方根の計算は省略  しています この部分を   輝度  を与える式を用いてなどと改良すれば より望ましい結果が得られるかもしれません また   垂直方向の差分  も考慮したり    キャニー法    を用いるように改良したりすれば よりよい結果が得られます とはいえ 関数アートジェネレーターの上では  十分望ましい結果  が得られているため 今回はこれでよしとします    ステップ　勾配の計算エッジを検出できたら 次は検出されたエッジ上のピクセルそれぞれについて 付近の  局所的な傾きを算出  します 傾きを算出することで 次の処理である   線分の検出   と   円弧の検出   の  正確性  を高めることができます なおこの処理は  やや負荷が高い  ため 適当にエッジ上の  ピクセルを間引いて  処理を行なっています 以下の図において   局所的な傾き  を計算したい点が  赤い点     サンプリングする点  が  青枠の点  と  青い点     灰色の部分  が  検出されたエッジ  を表します また 赤い点を \mathrm P   サンプリングする直線の角度を \alpha  サンプリングする各点を  図  に示した要領で \mathrm P   n  P    n    \cdots P  n   P n  とします このとき   局所的な傾き  は以下のステップで算出します       エッジ上の各点を順番に走査する       \mathrm P  を通る傾き an\alpha の直線上の点を一定間隔で  サンプリング  する       サンプリングした各点のうち   エッジ上にあった点の割合   r \alpha を求める この処理を行った結果得られた局所的な傾きを  図示  したのが以下の図です ※処理を分かりやすくするため 検出された勾配の  一部のみを描画  しています ここからさらに   二次元配列を用意  して 得られた各点の傾きを  二次元配列上のデータに移し替え  ます        この二次元配列を  可視化  したものが 本項目の  冒頭  で示した  右側の図  であり これが  ステップのゴール  です   処理時間  が気になりますが 例で示している  キノコの画像  の場合 主要な  データ  は以下の通りです     ピクセル数  ：　 imes   傾きを調査した  ピクセルの個数  ：　  　 縦横ピクセル間隔で調査   サンプリングの  角度に対する分割数  ：    度ずつ直線をずらす   サンプリングの  半径に対する分割数  ：    片方の方向あたり   閾値を超えた傾きの  検知数  ：   これより   計算した回数の概算  は以下の通りです 極端な話 仮に  全ての点がエッジ  だったとしても縦横  ピクセルずつ間引いて傾きを算出していけば   概算計算量は   を超えない  ので 任意の画像について  現実的な時間で処理  できます    ステップ　線分の検出まずは  直線を検出  し あとからその直線の上にある  線分を抽出  するという流れです はじめに   拡張されたHough変換  を用いて直線を検出します 拡張されたHough変換の解説については   別途記事を書きました   Hough変換について知りたい方は   以下の記事  をご覧ください その後   直線の補正  を行います 直線の補正は以下のステップで行います  N l 本の直線が検出されたとき   検出された直線  を l n\  \leq n  N l  とし その式が  l n \  \sin\alpha x  \cos\alpha y \beta   で与えられるとします       検出された直線 l k を一つ選ぶ       \delta \alpha \ \delta \beta を既に調査した範囲と被りが出ないよう  定数倍  して 同様の処理を繰り返す ここまでが 直線検出のステップです その後 検出された  各直線 l k 上の点をサンプリング  し   エッジ上にあるかどうかを調査  します   つ以上連続して  エッジ上にある点の組  を検出し 該当する範囲を   線分あり   と判定します    ステップ　円弧の検出線分の時と同様に まずは  円を検出  し あとからその円の上にある  円弧を抽出  します こちらも    拡張されたHough変換    を用いてまずは  円の中心を抽出  します ステップの段階で各点の  局所的な特徴を一次式で取り出している  ため ここでは  円の半径の情報はわからない  ことに注意が必要です その後   円の半径を決定  します   円の半径  は 次のような流れで決定します  N C 個の円の中心が検出されたとき その中心を \mathrm P n \  \leq n N C  と表します 以下の図において   灰色  は  画像のエッジ     赤い点  は \mathrm P  n を表します   検出された円の中心 \mathrm P  k を一つ選ぶ    r i\delta を一定間隔で一つとる ここで i は  i\leq i R を満たす自然数であり  i R は適当な自然数  \delta は適当な正の実数である    \mathrm P  k を中心とする半径 r の円上から   一定間隔で点をサンプリング  する   サンプリングした各点のうち   エッジ上にあった点の割合   s i を求める   半径 n\delta の円を検出する   ※    t  \ t  \ \cdots t  i R   と順にみていったとき   正負が逆転する点  を探しています  t が連続関数の場合の  微分に相当  すると考えると分かりやすいです ここまでが 円検出のステップです その後 線分検出の場合と同様   検出された各円上の点をサンプリング  し   エッジ上にあるかどうかを調査  します   つ以上連続して  エッジ上にある点の組  を検出して   円弧あり   と判定します この際  x 軸と交差するような円弧を別物と認識してしまわないように注意が必要です 今回の実装では   エッジ上にない円弧上の点  をまず見つけ   そこから一周  して調査しています    ステップ　グラフ構造化このステップでは   多項式で近似するための準備  を行います まず   ステップ  と  ステップ  で検出された部分に相当するベクトル場の大きさを  強制的に  とします   その後   ベクトル場の値がとなっていない点をエッジ通りに結ぶグラフ  を以下の要領で作成します       ステップで傾きが検出されていた点で なおかつまだその付近のベクトル場の大きさが  ではないものを一つ選ぶ       その場の傾き方向の周辺で 半径 r  i\delta の円周上の点に   エッジ上の点  がないかどうかを調べる ここで  r  \ \delta は適当な正の実数  i は  から適当な整数の閾値未満の整数である       エッジ上の点が見つかったらそこを  新たな頂点  とし   点間に連結を生成したあとで同様の探索を行う       エッジ上の点が見つからなかったら  i を  インクリメント  して同様の探索を行う        i が閾値を超えたら  探索を終了  し その頂点は  グラフの端点  とする 上部の図の  ピンク色の円弧  が     で探索をおこなったラインです ピンク色の円弧の間隔が狭いところはグラフの  連結成分  が生成されています この探索を行うと 線分や円弧で取りきれなかった  エッジ上の点を滑らか       に結ぶ連続成分  をいくつかもった  グラフ構造  が得られます ステップの  冒頭に示した右図  をよく見ると 線分と円弧で取り去れなかったベクトル場の付近に   連結成分ごとに色分けされたグラフ  があるのがわかるかと思います    ステップ　多項式で近似最後のステップで 連続成分をそれぞれ  多項式で近似  します  x と y の間の式を得る手順は 以下の通りです       連結成分を一つ選ぶ          確率的勾配降下法    を用いて 連結成分に含まれる  各点の y 座標  に対して   各点の x 座標の累乗  による重回帰を行う            と同様に   各点の x 座標  に対して   各点の y 座標の累乗  による重回帰を行う            と     で得られた近似式のうち   残差の二乗和がより小さい方  をその  連結成分の近似式  として採用する      と     においては 今回は正則化は行わずに  残差の二乗和の最小化  を行なっています 重回帰の次数は    事前に設定した閾値   と    連結成分に含まれる点の個数   のうち小さい方です なお 次数として後者が採用される場合はわざわざ確率的勾配降下法を用いずとも  LU分解  を用いて  連立一次方程式  を解けばよいです しかし   極端なパラメータ  を持つ式が生成されることは  関数アート的には望ましくない  ので 今回は確率的勾配降下法に統一しています      と     で得られた回帰式における残差の二乗和が  いずれも閾値を上回る場合  は 連結成分を  強制的に中央付近で分割  して再び重回帰を行います どちらも望ましくない場合に x と y それぞれを連結成分内の通し番号 t で重回帰するようにすれば パラメータ表示による近似曲線が得られそうです   おわりに関数アートを生成するにあたり 最終的に採用しなかった  いくつかのアプローチ  も考えていましたが なかなかうまくいかないものも多かったです また今回は  時間の制約  上   実装を諦めたアイディア  もかなりあります   技育展  でのフィードバックも活かしつつ さらに  パワーアップ  したものをいつかリリースしたい所存です 最後までお読みくださってありがとうございました     注釈   表示されている数式は猫のかかと部分 緑色の部分 の数式です    かなり曖昧ですが 見た目が不自然ではない ということです    傾きが検出された点それぞれについて その近くに相当する部分の  次元配列の要素にその傾きを影響させる処理を実装しました    数学的な意味ではなく 日常生活での意味における 滑らか です ,4,2022-09-12
88,88,Hough変換と、局所的な傾きを用いたその拡張,画像処理,https://qiita.com/inoz/items/dbc18a7845e81d050886,  はじめに画像から  直線や円を検出  する手法として最もポピュラーなものの一つに  Hough変換  があります    ハフ変換   と読むそうです Hough変換は最も  シンプルで理解しやすい図形検出の手法  であり 現在に至るまで  さまざまな改良  が施されています 初学者向けに   Hough変換の流れ  と 局所的な傾きを用いた  Hough変換の拡張  について解説します   Hough変換の流れ   概要簡単のため ここからはまず  Hough変換によって直線を検出  する手法について解説します 画像は以下のように  エッジ化  されているものとします 例えば上記の右側ような画像から  直線を検出  することがゴールです 上の画像では   ノートの端  と  パソコンの端     タブレットの端  がそれぞれ正常に検出されていることが確認できます    下準備するまずは 直線を扱いやすくするために画像を  座標平面上  に載せます  x \ y の範囲は ここではどちらも   から  までとしています    投票をとる次に   画像のピクセル  を一つ一つ見ていきます エッジ上にないピクセルは  スルー  します あるピクセル \mathrm P   が  エッジ上にあるのを検出  したとしましょう 以下の図の第一象限にある  黄色い点  が \mathrm P   です ピクセル \mathrm P   に相当する  座標  を  x  \ y   とします ここで 検出する  直線の式  をとおきます  y 軸に平行な直線も統一的に処理するための表し方です  anheta\neq のもとでこれを y について解くととなるので  anheta  も含めれば    で  任意の直線の式を網羅  できていることがわかるかと思います さて 今  エッジ上の点   \mathrm P   を見つけ そこに相当する  座標  が  x  \ y   で与えられたのでした このエッジ上の点を通る  直線 l の存在を仮定  すると  l が \mathrm P   を通ることから  l の  式  はと表すことができます これは    heta と r に関する関係式  と見ることができます ここで 以下のような   投票箱   を用意します  R は 設定した座標平面にあわせて適当に設定する  正の実数  です   検出された直線が座標平面内に収まる  という条件から 三角不等式を用いて定めることができます 今回の場合の値は \sqrt   です そして   関係式  の条件  を満たす位置に存在する   投票箱   に票を入れていきます        なおこのとき   三角関数の合成  を用いれば  式は heta r 平面上で  正弦波  を表すことがわかります  \mathrm P   に相当する座標  x  \ y   を通る直線は 下図の  黄色い直線  のように   無数に存在  します ここでの投票は これら  無数の直線のパラメータについて投票  したことに相当します そして ここまで紹介した処理を   エッジ上の各点  について繰り返していきます そうすると   真のパラメータ付近の点  に票が集まります 右上図         真のパラメータ付近から  ずれた点  にも票が集まりますが それは  周囲の方向に分散  します そのため   票が集まっている点に着目  すれば   直線の真のパラメータ heta \ r が正しく求められる  というわけです    円検出への応用直線検出の場合と同様に   Hough変換を用いて円を検出  することもできます 直線の場合と同様に   エッジ上の点   \mathrm P   が検出され それに相当する  座標  が  x  \ y   であるとしましょう 検出する  円の式  をとすれば この  エッジ上の点を通る円 C の式  はすなわちとなります 今度は  次元配列を用意して   式を満たす点  a \ b \ c  について直線の場合と同様に  投票  を取っていけば   真のパラメータ付近に票が集中  し   円を検出  することができます なお この場合パラメータ空間  a b c  内で  式は  円錐面  を表します   次元に限った話ではありませんが このパラメータ空間のことを   Hough空間   と呼ぶそうです    問題点Hough変換は  汎用性の高い手法  ですが   いくつかの問題点  も存在します     計算量が多いHough変換は通常  重い処理  です ピクセルごとに全て投票を取っていると  計算量が膨大  になってしまうため 通常は  投票をとるピクセルを一定間隔で間引く  など何らかの方法で  工夫  を施します 最近では   投票をとるピクセルを確率的に選ぶ  という手法も盛んに研究されています     隣接する直線の検出におけるジレンマ画像によっては 画像のノイズや隣接して存在する直線などによって   複数の投票の山が生成  されることがあります   近くの山同士を強制的に併合  するようにすれば 同じ直線の上に   パラメータがわずかに異なる複数の直線が生成される   という状況を防ぐことができます しかし同時に 実際には  異なる直線上  にあるのに   同じ直線として検出されてしまう   という問題が発生します なお 投票後の直線の併合には   クラスタリング  の手法を活かすことができます     次元数が増えると精度が急激に低下する  Hough変換の致命的な欠点  として   パラメータ数が多い場合には急激に精度が低下する  という点があります 実際の画像には  ノイズ  も多く含まれるため   パラメータ付近の広い範囲に点が分散  するようになり 実用上使い物にならなくなってしまうのです そのための解決策として 次章で紹介する  局所的な傾きを用いる  方法があります   局所的な傾きを用いたHough変換の拡張ここからは 主に  円を検出  するために  Hough変換を拡張した手法  の解説です   独自に考えた手法をベースに解説  しますが 古くから  類似の手法が提案  されているようです        直線の場合はそれほど恩恵がありませんが わかりやすさのためまずは  直線の場合について解説  します    下準備する従来のHough変換の際と同様に まずは  画像に座標平面を設定  します その次に   エッジ上の各ピクセル  に対し その付近の  局所的な傾き  を求めます 局所的な傾きは  エッジ検出の過程で副次的に求められることも多い  ですし   愚直な実装  でも現実的な時間で求まります 今回の画像の場合   局所的な傾きを図示  すると以下のようになります           投票をとる  エッジ上の点   \mathrm P   が検出され 相当する  座標  が  x  \ y   であったとします また その場所の  局所的な傾きを表す角度  が heta  であったとします 下図   ただし    heta  は  \pi  heta \leq\pi  の範囲  で  x 軸から反時計回りの向きに取ります すなわち上図の場合 heta   です ここで   投票箱  を用意します  img src    width      \mathrm P   を通る  直線 l の存在を仮定  すると  l の式はと表すことができます これは heta と r   に関する関係式  と見ることができますが  heta  が  既知である  ことに注意すると これは  heta \ r    平面上におけるある一点    heta  \ x \cosheta  y \sinheta   を表すことがわかります したがって  P  に関する  投票を実施  したあとの  投票箱  は以下のようになります この投票を  エッジ上の全ての点  について実施すると   投票実施後の投票箱  は以下のようになります 先に  局所的な情報  を調べておくことで   無駄な投票を削減  し   精度を向上  させることができます    円検出への応用同様に   円検出へ応用  することも可能です   エッジ上の点 \mathrm P   が検出  され 相当する  座標  が  x  \ y      局所的な傾きを表す角度  が heta  であったとします このとき  \mathrm P   を通り  x 軸となす角度が heta で表される  直線の方程式  はと表されます 一方   円の方程式  をと表すと  C 上の点  x  \ y   における  接線の方程式  はすなわちと表されます     と  の係数を比較  してを満たす    次元空間内の点    a \ b \ c    に投票  すればよいです   ゴツそうな見た目  をしていますが 第二式の右辺にある  y \cosheta  x \sinheta   などは  単なる数値  なので 実際はそれほどややこしくありません なお     次元空間内に投票するのはコストがかかる  ため   次で紹介するような改良  も可能です     円検出における改良  円の半径と中心の座標を同時に発見するのは大変  なので   困難を分割  します すなわち   先に円の中心  を求め   後からその円の半径  を求めるという戦略をとります   円の中心の座標  を求めます エッジ上の点 \mathrm P   における  局所的な傾きと直交する傾き  を持ち   点 \mathrm P   を通る直線   m の式はと表されます ここで   円の中心を求めるための  次元の投票箱  を用意し  m 上にある投票箱に投票していきます   円上の点  から その点における  傾きと直交する向きに引かれた直線  は  必ず円の中心を通る  ので 円上の点からの投票を重ねると  投票箱は以下  のようになります ここで投票が重なった点が   円の中心  です その後   検出された中心とエッジ間の距離を計算  します ここでは    次元の投票  が実施され 多く投票が集まっているところが  その円の半径  となります もちろん 同じ点を中心に持ち半径が異なる  複数の円が検出  される可能性もあります    おわりに  画像検出っておもしろい  ですね   パラメータが多い  場合は 上手くHough空間を定めることで  Hough変換したあとのパラメータをさらにHough変換する   なんてこともできるんでしょうか Hough変換は  応用や改良の余地が大きく   工夫しがいがありました     脚注   ここでは説明のために投票箱を大雑把に区切っています    実際はここまで綺麗になりません また 図においては 黄色い点たちが乗る直線上以外の点からの投票は省略しています    D H  Ballard   Generalizing the Hough Transform toDetect Arbitrary Shapes   PR            図ではエッジの周りにも細かい流れがありますが 本来これは除去した方が良い結果が得られます ,4,2022-09-12
89,89,モノクロ画像をAIでカラー化してくれるPythonツールを試してみた,画像処理,https://qiita.com/minorun365/items/217395d517671a5c27b4,このツイートが目に留まったのがきっかけでした お コマンド行打つだけじゃん 面白そうやってみよう 環境準備 地味に色々なエラーと格闘したので奮闘記を残します   環境前提私の環境は以下でした   M MacBook Air  標準ターミナル zsh   その：環境準備編    Pythonのインストール公式ページからMac OS用の最新Python をダウンロード GUIインストーラーを利用して導入 pipも同梱されています 実行確認   zsh  python   version  pip   version    Zshプロファイルにエイリアス登録プロファイルをテキストエディターで開く   zsh  open    zshrcファイル内に以下を追記する   zshalias python  python alias pip  pip  参考ページ プロファイルを上書き保存後 ターミナルを再起動する バージョン確認コマンドが正しく通るかチェック   zsh  python   version  pip   version    pipのアップグレード   zsh  python  m pip install   upgrade pip  その：依存ライブラリー導入編    依存ライブラリー名の修正 requirements txt  をそのままpipで実行するとエラー多数になるため ライブラリー名を修正  skimage → scikit image  PIL → pillow 参考 直せたら以下を実行    zsh  pip install  r requirements txt  その：アプリ実行編適当なモノクロ画像ファイルを用意し いよいよアプリを実行 するとこんなエラーが発生    zshraise URLError err urllib error URLError  FinderでPythonインストールフォルダーにあった   Applications Python  XX Install Certificates command  というファイルをダブルクリックして実行したら上記解消 以下サイトのおかげです  参考 多分ここまでやればエラーなくアプリ実行できるはず 実行中のダウンロード処理 にそこそこ時間かかりますが 成功するとPythonウィンドウが開いてカラー画像をパターン並べてくれます   スクリーンショット       png    さいごに面白いツールの情報を発信してくださったみやさかしんやさん ありがとうございました m     mみやさかさんのブログ記事は以下です ,49,2022-09-03
90,90,画像認識における Pytorch VS Tensorflow ,画像処理,https://qiita.com/minh33/items/1fe31071e83db62ea988,  概要　よく議論されるTopicで一度は皆が疑問に思う  PytorchかTensorFlowどっちが良いの  について個人的な意見を述べたいと思います PytorchとTensorflowそれぞれに良い所があり 場合によって使い分けましょうという記事が沢山あるので 今回はどちらか断言してみたいと思います 画像認識のタスクを学習し ロボットに搭載する仮定で話します   論文で使われいる割合　画像認識ではPytorchが圧倒的に多いです 感覚的には割くらいかなと 基本的に開発は前に書いた人のコードを元にするので 違うライブラリーだと一から書き直さないといけないので大変です 汗    Third Party Library　Pytorchには Torchvision    Pytorch Lightning    mmlab  があります これらが優秀すぎます 比較ではないのですが これらのライブラリーの便利さを簡単にまとめます 　Torchvisionは様々な画像認識に使える関数があるのですが 中でも学習済みモデルを create feature extractor  という関数を使うとBackboneの部分として再利用出来ます Neckは FPN  をサポートしています という事は残りはHeadだけを自分で書けばモデルが出来上がります 　Pytorch Lightningは学習時に必要な幾つものコーディングを簡略化してくれます 使えるようになるとバグが減り コーディングリビューがしやすくなります 　mmlabは複数のモデルをConfiguration fileから設定できます 現状 mmlab以上にデータ構造が綺麗なPerceptionのライブラリーは見たことがありません mmlabを直接使う事も凄く便利ですし 僕はmmlabのコーディングを参考にして複数モデルかつ複数タスクを行えるPerceptionのライブラリーを作りました   Pythonで学習したモデルをC  で実行した場合の速度 TensorflowはC  で書かれていて それをPybindでPythonをサポートしています 故に実行速度が早いと言われています 一方PytorchはPythonとC  は別々に書かれているので PythonのコードをC  にexportすると多少遅いと言われています これらの理由からTensorflowはC  でのリアルタイム実行に向いていると言われています  しかし 実行速度を早くしたい場合は ONNXやTensorRTのフォーマットに変換して実行します 故にどっちを使っても変換してしまえば 速度は変わらない筈です しかも Pytorch 以前のJIT Compiler 実行時用コンパイルラー がCPUとGPUで同じnncというコンパイラーを使っていましたが  Pytorch はGPUのJIT CompilerがNVIDIAが開発しているNVFuserになった  ことにより 処理速度が向上しました 今後もVersionが上がる毎にサポートが増えたり より速く実行できるようになると開発者は言っていました 　余談ですが 僕はTorchScriptを使っています 理由としてはLibtorchにある関数を使えるので 前処理や後処理の部分をPytorchとほとんど同じようにコーディング出来るからです TensorRTに関しては Torch TensorRT  を使っています これはTensorRTのラッパーなので TorchScriptでもTorch TensorRTでも同じコードで実行できます   結論　結論は研究場合も実用の場合もPytorchが良いと思います 僕がPytorch Userなので完全に贔屓しているような記事になってしまいました 汗  実用の際はライブラリーの選択はかなり重要になるので Tensorflowを使うメリットをかなり調べましたが Pytorchに出来ない事が無いかなという感じです やはり論文で使われたgithubやThird Partyが豊富な事が開発に置いて重要だと思っています 良ければコメントにどちらを使っているかとその理由を書いて頂けると 勉強になりますのでよろしくおねがいします ,11,2022-09-03
91,91,Python：pngをgifに変換する（アニメーション・透過対応）,画像処理,https://qiita.com/fuk101/items/a4ad3b6ab9053d79596c,  やりたいこと  透過に対応したい  apng アニメーションpng ならgifアニメーションに変換したい  環境Pythonのインストールは済んでいる状態とします   pngとgifの違いどちらも画像ファイルの拡張子です 違いについて軽く確認しておきます 	  ビットカラー 色 	  特定のピクセルを透過指定し 背景画像が透けて見えるようにできる	  半透明は表現できない	  種類のタイプが存在する		  PNG ：ビットカラー 色 		  PNG ：ビットカラー 約 万色 		  PNG ：ビットカラー＋ビットのアルファチャンネル	  ビットのアルファチャンネルによって半透明にできる  pngからgifへの変換PythonのPillowというライブラリを使用します Pillowとは 画像ファイルの読み込み 操作 保存の機能を提供するフリーのライブラリです Python Imaging Library 略称PIL の後継であり PILは開発が停止していますが Pillowは年月現在も開発が進められています Pillowリファレンス： Pillow  PIL Fork     documentation  PILのwiki： Python Imaging Library   Wikipedia  もしPillowをインストールしていない場合は コマンドプロンプト上で以下のコマンドを実行し インストールしておいてください    cmd コマンドプロンプトpip install pillow   静止画pngの変換静止画の透過png画像を 透過gif画像に変換するサンプルコードです    python python  パッケージをインポートする  pngを取得する  RGBAに変換する  alphaのマスクを取得する  gifへ変換するために減色する  gifにマスクを貼り付ける  透過gifをエクスポートする    処理内容処理内容について説明します   パッケージをインポートする	   python python	  パッケージをインポートする	PillowパッケージからImageモジュールをインポートします 	インポートするパッケージの名前は  Pillow  ではなく  PIL  なので注意です 	Imageモジュールはファイルから画像を読み込んだり新しい画像を作成できるモジュールです   pngを取得する	   python python	  pngを取得する	指定したパスのファイルを取得します 文字列の前に r をつけることによって  \ のエスケープシーケンスが行われず そのまま文字列として扱われるようになります   RGBAに変換する	   python python	  RGBAに変換する	 img png convert  RGBA    で 画像をRGBAに変換します RGBAとは 透過マスク付きの色を表す形式です 	R：赤 Red 	G：緑 Green 	B：青 Blue 	A：透明度 Alpha 	上記のつの組み合わせで色を表現します 	  getchannel  A    で アルファチャンネルの画像を取得し 変数  alpha  に代入します   alphaのマスクを取得する	   python python	  alphaのマスクを取得する	ビット 色 のアルファチャンネルから 以下をすべてにそれ以外をに変換し 変数  mask  に代入します   gifへ変換するために減色する	   python python	  gifへ変換するために減色する	png画像を色に変換します   gifにマスクを貼り付ける	   python python	  gifにマスクを貼り付ける	色に変換した画像のピクセルにマスクを貼り付けます   透過gifをエクスポートする	   python python	  透過gifをエクスポートする	指定したパスにgifをエクスポートします 	 transparency  は ピクセルを透過色に指定するという意味です    アニメーションpngの変換透過apng アニメーションpng を 透過gifアニメーションに変換するサンプルコードです    python python  パッケージをインポートする  pngを取得する  gifアニメーション格納用変数を定義する  フレームの数だけループする	  RGBAに変換する	frame   frame convert  RGBA  	  RGBAのalpha 透過 を取得する	alpha   frame getchannel  A  	  alphaのマスクを取得する	  gifへ変換するために減色する	  gifにマスクを貼り付ける	  透過後のデータを格納する 参照渡しではなく値渡し   apngの再生回数の情報を取得する  透過gifをエクスポートする    処理内容処理内容について説明します  静止画pngの変換   静止画pngの変換 で説明した内容については省きます   パッケージをインポートする	   python python	  パッケージをインポートする	ImageSequenceモジュールはアニメーションのフレームを処理できるモジュールです   gifアニメーション格納用変数を定義する	   Python Python	  gifアニメーション格納用変数を定義する	アニメーションから枚枚の画像をリストで格納するための変数を定義します   フレームの数だけループする	   Python Python	  フレームの数だけループする	 ImageSequence Iterator img png   で アニメーションのフレームごとの画像をリストで取得します 	取得したフレームごとの画像を枚ずつ変換するために ループします   透過後のデータを格納する 参照渡しではなく値渡し 	   Python Python	  透過後のデータを格納する 参照渡しではなく値渡し 	 imgs gif append    で 透過gifに変換した画像ファイルをリストの末尾に追加します 	リストへは値渡しします    copy    をつけないと参照渡しになります 	参照渡しは変数の値そのものではなく 変数のメモリ番地を渡します ループで同じ変数を使いまわすため 次のループで値が書き換わった場合 すでに代入済みの変数の値も変化してしまうので それを防ぐために値渡しで代入する必要があります   apngの情報を取得する	   Python Python	  apngの再生回数の情報を取得する	アニメーションpngのループ回数を取得します loop は無限ループです   透過gifをエクスポートする	   Python Python	  透過gifをエクスポートする	指定したパスにgifをエクスポートします 	それぞれの引数の内容は以下です 	 save all  ：Trueの場合 画像のすべてのフレームを保存します Falseの場合 最初のフレームのみ保存します 	 append images  ：追加のフレームとして追加する画像のリストを指定します 	 loop  ：gifアニメーションをループする回数を指定します 	 optimize  ：Trueの場合 未使用の色を削除してパレットの圧縮を試みます 	 transparency  ：透過色を適用するピクセルを指定します 	 disposal  ：フレームの処理方法を指定します を指定することで フレームごとに背景色へ戻してから描画します disposalを指定しないと 前フレームの画像が残り続けます    実行ファイルにドロップしたpngの変換コマンドライン引数に渡したpngファイルを gifファイルに変換するサンプルコードです サンプルコードをテキストファイルに貼り付け 拡張子を  py にして保存すれば使用できます おおまかな処理内容は以下です     py にpngをドラッグ ドロップするとpngファイルと同じディレクトリにgifファイルをエクスポートする  静止画とアニメーションの両方対応可能   python python  パッケージをインポートする  受け取ったコマンドライン引数の数だけループする	  パス   拡張子に分解して取得する	root  ext   os path splitext fp 	  もし拡張子がpng以外なら処理を終えて次のループへ	  pngを取得する	  pngがアニメかどうかで分岐する		         pngがアニメのとき         		  gifアニメーション格納用変数を定義する		  フレームの数だけループする			  RGBAに変換する			frame   frame convert  RGBA  			  RGBAのalpha 透過 を取得する			alpha   frame getchannel  A  			  alphaのマスクを取得する			  gifへ変換するために減色する			  gifにマスクを貼り付ける			  透過後のデータを格納する 参照渡しではなく値渡し 		  apngの再生回数の情報を取得する		  透過gifをエクスポートする		         pngが静止画の時         		  RGBAに変換する		  alphaのマスクを取得する		  gifへ変換するために減色する		  gifにマスクを貼り付ける		  透過gifをエクスポートする	  処理終了のメッセージを出す    処理内容処理内容について説明します  静止画pngの変換   静止画pngの変換 と アニメーションpngの変換   アニメーションpngの変換 で説明した内容については省きます   パッケージをインポートする	   python python	  パッケージをインポートする	パッケージをインポートします それぞれのパッケージの特性は以下です 	 sys  ：コマンドライン引数をうけとるため使用します 	 os  ：パスから拡張子と拡張子以外へ分解するために使用します 	 messagebox  ：メッセージボックスを出すために使用します 	 Image  ：画像処理をするために使用します 	 ImageSequence  ：フレーム処理をするために使用します   受け取ったコマンドライン引数の数だけループする	   python python	  受け取ったコマンドライン引数の数だけループする	受け取ったコマンドライン引数の数だけループします 	 sys argv    は自分自身のパスを返すため 以降をループします   パス   拡張子に分解して取得する	   python python	  パス   拡張子に分解して取得する	root  ext   os path splitext fp 	コマンドライン引数へ渡したファイルのパスを 拡張子とそれ以外に分解します 	 C est\filename png  の場合   C est\filename  と   png  に分解します   もし拡張子がpng以外なら処理を終えて次のループを実行する	   python python	  もし拡張子がpng以外なら処理を終えて次のループを実行する	コマンドライン引数へ渡したファイルがpng以外の場合 処理を終了します   pngがアニメーションかどうかで分岐する	   python python	  pngがアニメーションかどうかで分岐する	 img png is animated  は フレームが枚以上 アニメーション のとき Trueを返します 静止画かアニメーションかで処理を分岐します   処理終了のメッセージを出す	   Python Python	  処理終了のメッセージを出す	メッセージボックスを表示します   さいごにapngからgifアニメーションに変換する方法がわからなくて苦労しました Pythonはあまり慣れていないので 書き方のお作法がよくわかっておらず 処理内容の説明も自信ないです 動くものは作れたので満足しています   参考にしたサイト Pythonで画像の減色をする   Python  Pillowで透過png画像を作成するputalpha   note nkmk me   Python  PillowでアニメーションGIFを作成 保存   note nkmk me   python — プログラムでPythonでビデオまたはアニメーションGIFを生成しますか     備忘録  PythonのPILで透過GIFを作る時に後ろの画像を表示させない方法   Qiita  ,2,2022-08-31
93,93,StableDiffusionの出力結果を修正出来るデモサイト,画像処理,https://qiita.com/tanreinama/items/9cde6d43c948e841625b,  StableDiffusionの出力結果を修正できるデモサイトを作りました から 日本語文章→イラスト生成AIのデモンストレーションにジャンプします   image png  文章を入力して 画像を生成します ↓は 湖に浮かぶボート で生成した画像です   image png  さらに下にスクロールすると キャンバスに上書きできるので 気に入らない場所を塗りつぶします JaveScriptが動いていないと思ったときは ページをリロードしてください   image png  消えて無くなりました   image png     これは何かもう想像出来ているでしょうけど Diffusionモデルのinpainting機能を使っています なので 周りに合わせて消すことは得意ですが 別の何かを書き足すのは不得意です 書き足したものの境界線をイイ感じにするには使えそうなので それ用のデモもそのうち作りたいと思います ほかにも ファンタジー風の風景を生成して   image png  いらない通行人だとか よくわからないグチャッとした出力を塗りつぶして   image png  キレイにします   image png  うん このくらい出力できれば ゲームの背景とかの実用用途でも使えるレベルになるんじゃないでしょうか しかし最近のAIは凄いな StableDiffusionはA GPUを個 個×クラスタ 使って学習したとあるし もうスパコン並のリソースがないとついて行けないのでしょうか 個人で色々とモデルを作っている人としては一抹のさみしさも感じる次第です   ,8,2022-08-29
94,94,ffmpeg：動画ファイルから連番画像をつくる,画像処理,https://qiita.com/fuk101/items/cf4db17419ff913cb9b2,  やりたいこと動画ファイルから連番のpngをつくりたい コマンドプロンプトからFFmpegコマンドを実行して 動画から画像ファイルに変換する   環境FFmpeg エフエフエムペグ は動画や音声を変換できるコマンドラインツール クロスプラットフォームなので Unix系やMacでも使用できる 出典： FFmpeg   Wikipedia    ffmpegの準備   ダウンロードHPからFFmpegをダウンロードする   公式サイト   へ行き  Download をクリックする  windowsのファイルを選びダウンロードする  ダウンロードしたzipファイルを解凍し 適当なフォルダに格納する	  unzip png     環境変数にPATHを追加解凍したexeファイルのフルパスを指定すればコマンドを使用できる 例：   C \Program Files\ffmpeg\bin\ffmpeg exe ffmpeg     が いちいちフルパスを入力するのは面倒なので PATHに追加することで ffmpeg とだけ入力すればコマンドを実行できるように環境変数にPATHを追加する    Win マークを押し検索窓に Path などと入力し  システム環境変数の編集 をクリックする   環境変数 をクリックする   〇〇のユーザー環境変数 もしくは システム環境変数 から Path を選択し  編集 をクリックする	システム環境変数とユーザ環境変数の違いは以下 好きな方を選ぶ 	  システム環境変数：PC上のユーザ全員に適応する	  ユーザ環境変数：今ログイン中のユーザのみに適応する   新規 をクリックし 先ほど解凍したファイルのexeファイルが格納されているパスを記入し OK をクリックする  環境設定が完了すれば コマンドプロンプトで ffmpeg と入力すればコマンドが使えるようになる  ffmpegコマンドの実行   ffmpegコマンドの構文   ffmpegを使って動画から連番画像をつくる例コマンドプロンプト上でffmpegコマンドを実行すれば 動画ファイルから連番画像をエクスポートできる 以下は D \sample\ファイル名 mp から秒あたり枚の画像を D \sample\image png からの連番でエクスポートする例    batrem フォルダ名rem ファイル名rem 動画ファイルから連番画像をエクスポート以下オプションの説明 	  入力ファイルのパス	  フレームレート 秒あたりの枚数 を指定値に変換する 指定しない場合は入力ファイルの値を継承これで動画から連番画像がエクスポートされる ,2,2022-08-28
96,96,日大文系卒が学び実装する画像認識,画像処理,https://qiita.com/The_Boys/items/1820f3200853ecf56436,   初めにここ最近ずっとディープラーニングでの画像認識の学習をしておりました その成果物 学習過程で作成したipynbファイルをシェア致します 勿論忘備録 データ格納が主ですが なかなかどうして初心者にも分かりやすい内容になっているのではないかと思います   気軽にダウンロードして実行できる   実装して確認できるのでイメージを掴みやすい 理論は他を当たって頂ければ     至る所に私のメモがあります 是非ご参考に  ダークソウル並みにあります 結果的には実装メインになってしまったので理論は他で補っていだたければと思います    前提知識  pythonの基礎   ディープラーニングの大雑把な概要   機械学習の簡単な経験やイメージ 　その他諸々前提知識は私の気まぐれで更新します  畳み込みとDenceの簡単な違い 共通部分：どちらも同じディープラーニングの手法 こんなやつ Denceが特徴 入力層 中間層 隠れそう  出力層で構成され それぞれがされている 隠れそうがあるから多層なんです 基本で単純なディープラーニング こんな形状 ConvD で もある    pythonmodel add ConvD         activation  relu   ある程度の概要は こちらのcolab  に記載されております 他にもRNNやLSTN等様々なディープラーニング手法がある 参照はこちら これからちょくちょく登場するモデルサマリー どのような意味なのでしょうか    実装早速実装していきましょう 資料の保存方法はこちらマージするのはやめてくださいね     難易度★☆☆☆ ディープラーニング  　によるmnist分類 ⇒ 結構分かりやすくできたと自負しております 最初にイメージを掴んで頂ければ 　例：手書き数字の画像  をAIが判断して予測を返す  これはです    CNNにおける分類 mnist   　ほぼほぼ上と同じですが 理論の補足や学習モデルが異なっております つセットでどうぞ  上二つの資料はこちら   CNNにおける分類 cifer   　もう少し精度の向上が必要     難易度★★☆☆ CNNと転移学習 cifer   VGGというモデルをダウンロードして分類に用います それとモデルの保存と読込  自前のモデル読込  　自分で保存した学習図済みモデルをcolabにアップロードし予測を行っております 使用教材はmnistの数値予測です 　こうする事により学習の手間やモデル構築の手間が軽減されることが期待できます 　 モデルはこちら      難易度★★★☆ CNN cifer の画像増殖  ⇒データが沢山ある場合は問題ないのですが 少ない場合はどうしましょう 限られた資源の中で最大限の効用を発揮するにはあらゆる工夫をしなければなりません 例えば｢画像データが枚しかないならばその枚を水増しして枚に｣  男女識別   ⇒ 同じ分類ですが よりイメージが湧きやすいかと 今までの資料を参照すれば決してできないものではないので是非力試しに 　 Qiita   男女識別の画像増殖  ⇒試してみたけど 思ってた以上に成果は伸びず…　 batch size だとtestデータでのloss：  　testデータでのaccuracy：  　SGD  手書き分類  　こちらもmnistとは基本的な思考は同じですが より複雑にデータを扱っております 興味があればどうぞ  資料はこちら  こちらは調整中  画像増殖②  　　 手書き分類kaggleのCNNのみ     その他メモmnistとcifar 同じディープラーニングで分類なのに 若干前処理が異なりますよね 正規化を求められたり画像の調整を求められたり これは整合性を取るための物なので エラー内容やモデルの種類によって柔軟に対応して頂ければと思います ちゃんとエラーを解消する為の処理 ニューラルネットワークは正規化しなければなりません ディープラーニングのモデル構築の際に使用する精度向上が期待できるハイパーパラメータ 詳しくは こちらの動画の後半    ミニバッチ学習ニューラルネットは データが増えれば増えるほど epoch毎の計算時間が多くなるため 全てのデータを使って 勾配 重み を更新していては 非効率的である そのため 全データから ランダムに任意の個数のデータを抽出し 学習させると 扱うデータ量が減り 計算速度が向上し 限られたリソースで 多くの勾配 重み を更新できる   重み更新法の変更今まで使用してきたSGDは確率的勾配降下法と呼ばれるもので その他にも  Adam  RMSprop  Adagrad  Adamax  Nadam などがある   ドロップアウト一定の確率でランダムにニューロンを無視して学習を進めることで 過学習を押さえ 汎化性能の高いモデルを作ることができます   Batch Normalization：バッチ正規化基本的には 勾配消失 勾配爆発を防ぐための手法であり これまでは 活性化関数の変更 学習係数を下げる DropOut層の追加などで対応してきたが Batch Normalizationは ミニバッチの各出力を正規化させ 学習過程の安定と学習速度の向上を実現した 学習 callback関数 については こちらを参照     終わりに正直数学的な前知識も必須だろし気が乗らないなーとは思っていたのですが やってみると案外楽しく実装できました 大変だったのはshapeをちゃんとそろえる事 沢山のエラーを解決しここまでこぎつけましたが 結局いt版多いのはshpaeの問題でしたので 少しでも学習のお役に立てば幸いです 宜しければ時系列versionもありますのでいかがでしょうか ,0,2022-08-24
97,97,【Tesseract.js】画像から数字を読み取って数独を解くWebアプリを作ってみた,画像処理,https://qiita.com/coelacanth_i/items/4576deeda11e86ee88e3,  image png    image png    はじめに　タイトルの通り 画像から数字を読み取って数独を解くWebアプリを作ってみました 現状では うまくいく場合もあれば そうでない場合もあります 　完全に自己満足なので 詳しい説明はしないつもりです    Tesseract js について　 この記事  を参考にさせていただきました  　ざっくり説明すると  Tesseract js とは 画像から文字を認識するためのライブラリです 　 公式ドキュメント  から CDNをとってくるのが手軽で良いと思います  記事執筆時のバージョンは以下のとおりでした     html  数独を解く　数独を解くアルゴリズムについては  この記事  を読むのがいいと思います  今回は 解ければなんでも良いので 効率よりも分かりやすさを優先しています     javascript sudoku js   数独のクラス       コンストラクタ       表示 canvasに線を引いたり数字を書いたりしている        数独を解く       配列のi行j列をみる        if i           すべて埋まった        for let num     num      num          の数字を入れて成立するか確かめる                   横 縦 ×のブロックに同じ数字がない場合       次の行と列を取得する関数       同じ行に同じ数字がないか        同じ列に同じ数字がないか        同じ×のブロックに同じ数字がないか        配列をコピー　 solve process では 再起処理を行っています ゴールにたどり着いたら  solution を解となる配列に変更します 実際に数独を解きたいときは 次のように記述します    javascript   arrayは問題を配列化したもの 空欄はに設定   画像認識を行う　では 本題の画像認識について説明したいと思います ソースコード どーん    ユーザがファイルを選択したら   myCanvasの画像から数字を読み取る   imageをcanvasにいい感じに表示 後で説明    マスに数字が含まれているか または空欄か  マス内のピクセルの色をみる    画像の読み込み base形式に変換    ファイルのロード   textが正しく吐き出されたと仮定している　恥ずかしながら 今回はじめて Promise というものを知りました コードがすっきりかけて便利ですね   画像の処理について   javascript    画像処理を行っている場所を抜粋    imageのマスの大きさ   canvasのマスの大きさconst d         imageで数字のみを切り取るために領域を 倍に狭める           imageのマスをcanvasのマスに書き込む　最初は ユーザが選択した画像ファイルからそのまま数字を読み取れることができるかを試しました すると 縦線 横線の影響からか うまく数字が読みとれないことが分かりました 　そこで 縦線 横線を除去するために image の各マスの一部 真ん中の部分 を切り取ることにしました すると 今度は数字は読み取れるものの 空白の部分がうまく読み取れないことが分かりました 　そこで 空白の部分に別の文字      を書き込むことにしました すると 今度はうまくいきました  すべてのケースでうまくいくわけではない         image png  　画像処理がうまくいけば 上のような画像が myCanvas 上に生成されます これを Tesseract js に読み取ってもらいます   感想　 Tesseract js は素晴らしいライブラリですが 実際に使ってみると まだまだミスは多いのかなと感じました 使い方が悪い可能性もある  　最初の最初は 各マスを Tesseract js に読み取ってもらっていたのですが それでは時間がかかりすぎるということで 紹介した方法を選択しました ですが 正確さを求めるなら 各マスを読み取ってもらったほうがよさそうかもしれないですね 　各パラメータは試行錯誤して調整したものですが もっと良いパラメータがありそうです 　今回作ったものは実用には耐えなそうですが これからも改善していこうと思います   おまけ 謎の失敗   image png  O  oはまだ理解できるが eはどこから出てきたんだ  ちなみにこのときに得た文字列は ooOOOooeでした 同じ文字を重複して数えてしまっているのでしょうか   追記　読み取る文字を   に限定したところ 数字の読み取りの精度が少し上がりました  このページ  を参考に  main js の recognize 周辺を以下のように変更しました 　正直なところ チンプンカンプンですが の部分で 読み取る文字を   に限定しています ご参考までに ,1,2022-08-22
98,98,CUDAによる画像処理が何十倍も高速化,画像処理,https://qiita.com/rueiwoqpqpwoeiru/items/203ba0b090180bd118e3, カーネル内のループは時間が掛かる　　フィルタ処理を行う場合 CUDAカーネル内で フィルタ係数を二重ループで走査する 　　このとき フィルタサイズが大きいと 許容できないほどの処理時間を要してしまう 　　例えば xの画像にxの線形フィルタ もしくはerodeフィルタ を適用　　したときの処理時間は 約ミリ秒であった    高速フーリエ変換を積極的に活用すべき  　　そこで 線形フィルタを 周波数空間で適用するようにしたら 処理時間が約ミリ秒から　　約ミリ秒に改善した なお 高速フーリエ変換は   xの画像で約 ミリ秒  だった 　　また 周波数空間における画素ごとの乗算は約ミリ秒だった 　　すなわち   フィルタサイズがいくら大きくても 計算時間は一定  である 　　知っている人には当たり前かもしれないが 高速フーリエ変換を使わない手はない  関係無い画素はスキップすべき　　また erodeフィルタなど フィルタ係数がになっている領域の最大値 や最小値 を　　求める処理では フィルタ係数がになっている領域を走査しないようにした 具体的には 　　フィルタ係数がになっている場所の座標を配列に格納し CUDAカーネル内では 　　配列に格納されている座標の画素だけを走査するようにした その結果 処理時間が　　約ミリ秒に改善した  ※ただし フィルタ係数は非常にスパースで  以上が なお 上記方法で実装したコードについては以下の記事で紹介 追記   二値画像に対するモルフォロジー処理は 高速フーリエ変換を用いることで高速化できる   dilateの場合 fftによる畳み込みを行い 閾値処理で二値化する erodeの場合  を反転してから fftによる畳み込みを行い 閾値処理で二値化した後  を反転する ,0,2022-08-20
99,99,シンプルなカラーピッカーを作りました【配布してます】,画像処理,https://qiita.com/ika_kk/items/5105bbf0bb443244cd64,  はじめに最近Flutterを勉強しているのですが UIを作る際に画面上の色を取得したいときがままあります そんなとき 手軽に色情報を取得できるソフトがほしくて作りました もしほしい方がいらっしゃいましたら本記事下部からダウンロードできますのでぜひ 以下どんなアプリかのイメージ  経緯色々な方法で色取得を試していたのですが なかなか自分に合うものが見つからず…… 最初これでやってたけどあまりにもめんどくさい     ペイントD  やりたいことはできるけどやっぱり予備動作が多い優秀なソフト 機能としてはこれ以上ないんじゃないでしょうか ただウインドウサイズがやや大きく たまに邪魔に思うことがあるのが玉に瑕でした 他にも色々ありそうでしたが 勉強がてら自作してみることにしました   このアプリの特徴非常にシンプルです    ①抽出サイズを指定できる今回の目玉機能です ピンポイント画素の色を抽出すると ノイズの影響でわりとばらついてしまうので 抽出範囲を指定できるようにしました つまり抽出範囲を大きくすると 範囲内の平均色の情報を取得するようにしています これにより ノイズの影響をある程度キャンセルできます   説明 png  ただしモニタの領域外は黒色   となっているようで 抽出サイズが大きい状態でディスプレイ端付近を抽出すると少し暗い色になったりします そのときは抽出サイズを絞って対策してください    ②カラーコード RGB HSB グレースケール値を取得できるよく使うであろうカラーコード  RRGGBB とRGB値のほか HSV値とグレースケール変換したときのグレー値を取得します HSVに関しては使うシーンによってほしい値域が異なるので 指定できるようにしています Hは もしくは  SとVは もしくは  使うシーンに合わせてラジオボタンで指定します グレースケール値は自分がよく使う 標準テレビジョン放送規格  にのっとり  R  G  Bとしています    ③その他特徴いつでも色情報が見られたら便利かなと思い 常時画面最前面に表示するようにしました あと 見た目かっこよくしたくてタイトルバーを消してみました 位置は適当な位置をドラッグして調整できます   WPF  オシャレな半透明ウィンドウを実装してみる   まめ   たんたんめん  URL    カスタム png    技術的な話   画面上の画素値取得方法今回のアプリのキモとなる部分です 下記のように画素抽出を実装しました  行目 graphic CopyFromScreen      にてスクリーンの任意の位置 サイズ抽出を実施しています   引数：抽出範囲の左上の座標 現在のカーソル位置から直径  半径だけ引いた箇所を指定   引数：bitmapへ書き込むときの開始座標 bitmapの左上    を指定   引数：抽出範囲のサイズ 直径を指定    C  カーソル周辺のキャプチャ実装部   サーチ直径       現在のカーソル位置周辺の画像取得       変数bitmapには 現在のカーソル周辺をキャプチャした画像が入ります        以降その画像をよしなにして平均値を取得し 画面に表示しています          お気づきの方もいると思いますので白状します さも円の内側の平均値を取っているように謳っていましたが 実際は正方形の内側の平均値を取っています   円内判定がめんどくさかった   優しい嘘ってやつですね おそらく円でも正方形でも結果はたいして変わらないので許してください   理想と現実 png     抽出サイズと処理時間について抽出サイズを大きくすると平均値を取得するのに時間がかかるんじゃないかと思い計測してみました   処理時間相関  サーチ直径 N に対して処理時間は O N   で増大すると予測していましたが 直径pxまではfpsをキープしてくれています px以上になると二次関数的に増大していきますが 実際こんなに大きい半径で使わないので問題ないでしょう  今回は上限pxで実装しているのでなおさら CPU使用率とメモリについてはデバッグモニタで確認しました CPU使用率は目に見える変化なし メモリはpxの場合で最大MB程度でした    コード全容GitHubに上げていますが一応   特別なライブラリは使わず基本ネイティブで書いてます            コンストラクタ             プログラム実行と同時に色取得処理を開始する         public MainWindow              InitializeComponent                  ウインドウ半透明にしたときにドラッグできるようにする               以下参考URL               メインループ            プログラム実行中まわりつづけるループ             常に現在のカーソル周辺の画素値を分析しつづける             タスクとして使用                    現在のカーソル位置周辺の画像取得                   デバッグ用                   画素値 RGB 取得                byte r  g  b                 GetMeanValueFromBitmap bitmap  out r  out g  out b                     HSVへ変換                double h  s  v                 RgbToHsv r  g  b  out h  out s  out v                     グレースケール値                   表示更新                   実際にはHSVも同時に計算して表示する            画像の各チャンネルの平均画素値を取得する             下記参考URL        private void GetMeanValueFromBitmap Bitmap bitmap  out byte r  out byte g  out byte b             BitmapData data   bitmap LockBits                 new System Drawing Rectangle     bitmap Width  bitmap Height                  ImageLockMode ReadWrite                 System Drawing Imaging PixelFormat FormatbppArgb                 リストに画素値を格納していく byte型だと平均が出せないのでdouble型でやる                なんかの拍子に格納されてなかったら怖いので一応条件分岐            Hは  sとvは で取得する         private void RgbToHsv byte r  byte g  byte b  out double h  out double s  out double v             double max   Math Max r  Math Max g  b               double min   Math Min r  Math Min g  b                  色相               彩度               明度            v   max             RGB値とHSV値をもとに画面表示を更新する             HSVに関しては画面の設定をもとに値域を変換する         private void UpdateValue byte r  byte g  byte b  double h  double s  double v  byte gray                サーチ直径の表示色を見えやすい色に設定            brushEllipse Stroke   v                     new SolidColorBrush System Windows Media Color FromArgb                背景が暗い場合は白線                new SolidColorBrush System Windows Media Color FromArgb                   背景が明るい場合は黒線               hsvの値域を調整               値更新            string code       r ToString  X    g ToString  X    b ToString  X                 colorCodeTextBox Text   code             rTextBox Text   r ToString               gTextBox Text   g ToString               bTextBox Text   b ToString               hTextBox Text     int h  ToString               sTextBox Text     int s  ToString               vTextBox Text     int v  ToString               grayTextBox Text   gray ToString               colorRectangle Fill   new System Windows Media SolidColorBrush System Windows Media Color FromRgb r  g  b               マウスホイールでスライダーを制御するイベント             スライダーの値をもとにサーチ半径表示を更新            終了イベント  ダウンロードGitHubにソースとexe ソフト本体 をアップしました   exeダウンロードリンク 自分の環境でしかデバッグしていませんが ネイティブで実装したのでWindows以降なら問題なく使えるはずです ただし何も設定せず書き出したので警告が出ると思いますが 下記方法で実行できます もちろん悪意のあるコードは書いていませんが 一応自己責任にてお願いします  誰かデジタル署名のやりかた教えてください……    警告 png    おわりに本当はもっと多機能にしても良かったかもしれませんが シンプルなものを作る と決めていましたので色々諦めました   カラーコードコピー機能　…　ショートカットとかで実現できそう   カラーパレット提案機能　…　同じ彩度でいい感じの色を提案してくれたり  XY座標表示　…　たまに座標値が欲しくなるときがある これは実装してもよかったかも 結果的に 結構使い勝手のいいカラーピッカーができたと思いますし だいたいの色情報を手軽に見たい という当初の目的は達成できたので良しとします もしよければみなさんお使いください 以上,5,2022-08-18
100,100,vSLAMの動作原理の概要 + 最近の研究事例DROID-SLAM紹介,画像処理,https://qiita.com/iitachi_tdse/items/b89547548cb733e8cf00,    SLAMとはSLAMはSimultaneous Localization and Mappingの略で 自己位置の推定 Localization と 環境のマップ構築 Mapping というつの問題を同時に解くタスクです 元々はロボット制御の文脈で提案された技術ですが 最近では拡張現実や自動運転などへの応用も展開されています 一般にSLAMにはLiDAR Wi Fi カメラなど種々のセンサーの測定結果が利用されますが 特にカメラで撮影した画像の情報を手掛かりとしたSLAMはVisual SLAM vSLAM と呼ばれます ここでは特にvSLAMについて その手法の概要と最近の研究の紹介をしたいと思います     vSLAMの動作原理近年のvSLAMの研究は 他の画像処理タスクと同様に深層学習を用いたものが活発ですが 基本的な考え方は深層学習の登場以前の古典的なvSLAMである程度確立しています そこで まずは古典的なvSLAMがどのような原理で動作しているのかを簡単に説明します 古典的なvSLAMは カメラで撮影された動画のフレーム画像群に対して 各フレーム画像に映っているオブジェクトの位置関係の幾何学的な制約が満たされるようにカメラポーズ カメラの位置と向き と環境の三次元構造を決定する最適化問題の形で定式化されます このような幾何学的な制約からカメラポーズと環境の三次元構造を決定する処理は バンドル調整と呼ばれます この最適化問題の目的関数として何を用いるかによって vSLAMの多くは非直接法 特徴点ベースの方法 または直接法と呼ばれるものに分類されます 非直接法では フレーム画像から検出されたコーナーやエッジなどの特徴点のフレーム画像面上および世界座標上での位置の辻褄が合うようにカメラポーズと特徴点の三次元位置が決定されます 歴史的には最初のSLAMは非直接法によるもので 代表的なものとしてはPTAM  やORB SLAM  などが知られています 非直接法は特徴点だけを利用するため計算負荷が軽く このことはリアルタイム処理を志向する ロボット制御や自動運転では特に vSLAMにとっては有利な点ですが テクスチャレスなオブジェクトの多い環境では十分な数の特徴点の検出ができず 原理的に解けない問題となってしまうことが欠点です 一方 直接法では 特徴点か否かに関わらずフレーム画像面上での全てのピクセルの輝度値が利用されます 代表的なものとしてはDTAM  やLSD SLAM  が知られています 特徴点の検出をベースとしていないのでテクスチャレスなオブジェクトでも問題なく動作することは利点ですが 最適化の計算自体が難しくなることが欠点です またローリングシャッターカメラで撮影された動画に対しては 直接法は適切に動作しないことが知られています  非直接法 直接法の動作原理の説明を図   に示しました    図  png    fig     emsp  emsp  emsp  emsp  emsp  emsp  emsp 図   非直接法 特徴点ベースの手法 の動作原理図 について説明します   N個のカメラポーズで撮影したフレーム群が存在する 図では N    予めカメラ行列は特定しておく   エッジやコーナーなどの特徴点を検出し そのうち j 番目の特徴点の画像面 i 上での座標を z   i j   i   \cdots N  とする   各フレームのカメラポーズ カメラの世界座標での位置と向き に適当な初期値 p   i   i   \cdots N  を設定する   手順で検出した特徴点の世界座標での位置 X   j   に適当な初期値を設定する   カメラ行列とカメラポーズ p   i   i   \cdots N   特徴点の位置 X   j   から 各画像面に特徴点を投影したときの画像面上での座標 z p   i   X   j    が一意に定まる   geometric errorを最小化するようにカメラボーズ 特徴点位置を決定する：以上が図 の説明になります   図  png    fig     emsp  emsp  emsp  emsp  emsp  emsp  emsp  emsp  emsp 図   直接法の動作原理図 について説明します   N個のカメラポーズで撮影したフレーム群が存在する 図では N    予めカメラ行列は特定しておく   特定のフレーム i  図では i  に着目 の特定の画素 j に着目し その画像面上での座標 z   i j   を定める   各フレームのカメラポーズ カメラの世界座標での位置と向き に適当な初期値 p   i   i   \cdots N  を設定する   手順で着目した画素のデプスに適当な初期値 d    j   を設定する   カメラポーズ p   i   i   \cdots N   着目画素の着目フレームにおけるデプス d    j   から 着目画素の世界座標での位置 X p       d    j    が一意に定まる さらにカメラ行列から これを各フレームに投影したときの各画像面上での座標 z    j i   i   \cdots N  が定まる   Photometric errorを最小化するようにカメラポーズ デプスを決定する： l   i   z  は画像面 i 上の座標 z における輝度値以上が図 の説明になります     最近の発展近年のvSLAM研究は 深層学習を用いたものが活発に行われています ここでは 最近のvSLAM研究の一例として NeurIPSで発表されたDROID SLAMを紹介します このアルゴリズムは vSLAMの一連の処理をEnd to Endで実行できるものとなっていて 単眼カメラだけでなく ステレオカメラ RGB Dカメラに対応可能なもので これまでのvSLAMのstate of the artを大きく更新しました DROID SLAMは 直接法と非直接法の折衷的な方法で バンドル調整 カメラポーズとデプスの推定 をアップデートオペレータと呼称されるニューラルネットワークで実現するのが特徴です アップデートオペレータは図 のような構造のリカレントニューラルネットワーク RNN で カメラポーズ G とデプス d を逐次的に更新する推論モデルです アップデートオペレータには 共通するオブジェクトが映ったフレーム対 図ではフレーム i j  が入力されます 一般的にRNNは時系列データやテキストなどの系列データを処理するために利用されることが多いですが ここでは少し変わった使われ方をされています ニュートンラフソン法などの一般的な最適化アルゴリズムでは値の更新を繰り返すことで最適な値に収束していきますが アップデートオペレータではRNNの繰り返し構造でこのような振る舞いを模倣しています つまり 図 で推論が右の方向に進むにしたがってカメラポーズ G とデプス d が少しずつ更新されていき 適当な回数の更新を繰り返したあとに正確な値に収束することを狙った構造になっています   図  png   emsp  emsp  emsp  emsp  emsp  emsp  emsp  emsp  emsp  emsp 図   アップデートオペレータ図 について説明します アップデートオペレータには 共通するオブジェクトが映るフレーム対が入力されます  上図はフレーム i j が入力された場合を示しています  図では奥行きがあるように描かれていて 右端には  imes E   と書かれていることが確認できますが これはフレーム対の数   E   の分だけ同じ処理がおこなわれることを示しています 図の上部の緑の矩形で囲まれた部分は フレーム対の対応するピクセルを特定する処理です DROID SLAMでは geometric errorを用いた非直接法的なアプローチをとりながら 画像全体の情報を用いる直接法的な側面も持っています これにより 最適化が比較的容易な非直接法のメリットと 豊富な情報を扱える直接法のメリットを享受しています 図の下部にあるDBAは フレーム対の対応するピクセル情報から カメラポーズとデプスの推定をおこないます 繰り返し更新されることで最適な値に収束するようなアーキテクチャになっています 以上が図 の説明になります DROID SLAMは 大規模なCGデータセットであるTartanAirを用いて訓練されます TartanAirはvSLAMのために構築されたデータセットで シーケンス 合計万フレーム以上から成り カメラポーズやデプスなどの正解データとともに公開されています DROID SLAMの訓練では その正解データを用いた教師あり学習によってvSLAMとしての能力が獲得されます 論文中では実データでの性能を他手法と比較した結果が記載されています ここでは EuRoCとTUM RGBDのつのデータセットに対する単眼カメラ推定の精度評価の結果をご紹介します これらのデータセットのうち TUM RGBDはモーションブラーや激しい回転運動が含まれていて 単眼カメラによる位置推定が難しいデータセットです 評価に用いている指標は絶対軌道誤差 ATE で 正解の軌道とSLAMで推定した軌道との誤差をメートル単位で算出しています それぞれのデータセットでの性能を他手法と比較したテーブルが図    引用元： DROID SLAM  Deep Visual SLAM for Monocular  Stereo  and RGB D Cameras   です   図  png   emsp  emsp  emsp  emsp  emsp  emsp  emsp  emsp 図   他手法との性能比較 単眼SLAM 図 について説明します EuRoCの各シーケンス MH  MH       V に対して 絶対軌道誤差 ATE をメートル単位で記載されています DVO DSOで値が    “となっている箇所は 当該論文中で記載がなく値が不明であるものです ORB SLAM SVO ORB SLAMで値がXとなっているものは 算出に失敗しているものです 深層学習ベースの手法 DeepFactors   DVO DSO は比較的ATEが大きく精度は今一つですが 失敗しているシーケンスはなく 頑健性が高いことが分かります 一方で最適化ベースの手法 ORB SLAM   ORB SLAM は比較的ATEが小さく精度は高いですが 失敗しているシーケンス Xのシーケンス が散見され 頑健性に課題があることが分かります それと比べると最下段のDROID SLAMでは 精度と頑健性の両面で優れていることが確認できます 以上が図 の説明になります   図  png   emsp  emsp  emsp  emsp  emsp  emsp  emsp  emsp 図   他手法との性能比較 単眼SLAM 図 について説明します TUM RGBDの各シーケンス   desk       xyz に対して 絶対軌道誤差 ATE をメートル単位で記載されています ORB SLAM ORB SLAMで値がXとなっているものは 算出に失敗しているものです 最適化ベースの手法 ORB SLAM ORB SLAM は それぞれ半分以上のシーケンスで算出に失敗しています 一方で深層学習ベースの手法 DeepTAM   DeepFactors は全シーケンスで算出に成功していますが 精度は今一つ 最も精度の高いDeepTAMでも平均  cmのATE です それと比べると最下段のDROID SLAMでは ATE＝  cmと高い精度で なおかつ頑健であることが確認できます 以上が図 の説明になります 最適化ベースのvSLAMでは 幾何学的制約から導出される最適化問題が不良設定問題となって原理的に解けなくなり 自己位置の推定が不能となる問題があります 一方で深層学習ベースのvSLAMでは 原理的に幾何学的制約から自己位置が決定できない場合でも 学習データから得られた知識を用いて推論自体は実行することが可能です この意味で 一般に深層学習ベースの手法は頑健性の点で最適化ベースの手法よりも優れています ところが 深層学習ベースの手法では学習したデータに対する過剰適合が課題となっていて 精度の面では最適化ベースの手法の方が優れていることが多いです この傾向は図   の結果からも確認できますが DROID SLAMは頑健性と精度の両面で多手法より優れていることが確認できます その他 論文中ではステレオカメラやRGB Dカメラを用いたパフォーマンス評価も記載されています 詳細は論文をご確認ください     まとめ今回はvSLAMの動作原理と 最近のvSLAMの研究事例としてDROID SLAMを紹介しました 従来の深層学習ベースのvSLAMでは 学習していないデータに対する汎化性能が課題となっていましたが DROID SLAMはこれを克服したものとなっています DROID SLAMである程度vSLAMは成熟した感がありますが 今後どのような発展を辿るか注目です   参考文献  代表的なｖSLAMのサーベイ論文  RAFT DROID SLAMと同著者のオプティカルフロー推定NN の論文,19,2022-08-18
102,102,CLAM(Clustering-constrained Attention Multiple Instance Learning)のメインの処理を整理してみた,画像処理,https://qiita.com/epocaipath/items/d5df7c9944296708a4b0,   概要CLAMは年にハーバード大学医学部ブリガム アンド ウイメンズ病院 病理部のMing Y Lu氏らが発表した スライド画像から特徴量を抽出し スライド単位のラベルを用いて弱教師あり学習でクラス分類を行うスクリプトです スライド画像とラベルを用いてモデルに学習させることで パッチ単位でラベルの特徴が強い箇所を推測することができます スライド単位のラベルを持つ画像に対して スライドに含まれる多くの情報を活かすことでより効率的に学習が行えるという考え方のようです    イメージ図  基本的にはLinuxを前提としていますが フォルダパスを一部編集することでWindowsでも使用できます 概要と実際の使用した結果についてはこちらの記事をご参照いただけましたら幸いです   本記事では CLAMにおけるモデル学習時の処理について コードや出力を元に確認できたことを記載いたします CLAMを使ってみた方や調べてみたい方にとって 少しでも参考になればと思います      CLAMの実行ファイルCLAMではおおよそ以下の順で処理を進めます 各WSIをパッチに分割する処理 二値化  組織の輪郭情報作成  パッチ情報作成  WSI単位でパッチ情報を含む hファイル等を出力するコード 各スライドをパッチ単位でresnet ディープラーニングのモデル に通して特徴量を抽出し  ptファイル 特徴量 および hファイル 特徴量 パッチ座標 に保存する処理 このステップで画像から低次元の特徴量を抽出することで 後の学習において学習時間の大幅削減や計算コストの抑制ができるとのことです クロスバリデーション用のfold数に合わせて 各スライドをtrain val testに分けたcsvファイル等を保存する処理      ．main py  本記事の対象 extract features fp pyで出力した特徴量データを用いて アテンションネットワークを通してモデルの学習を行い パラメータ概要 fold情報 fold毎のモデルファイル  pt  予測結果  pkl 等を保存する処理      ．eval py学習したモデルの検証を行う処理 スライド単位のラベルの予測結果等を出力します ．で学習したモデルとスライド画像から パッチ単位でラベルの特徴の強さを示すヒートマップを作成する処理 全体的にはおおよそ以下のような流れになります 今回は 上記   main py   のアテンションネットワークについて見ていきたいと思います main pyの処理ではモデルがMILとCLAMに分かれていますが MILは比較のためのベースラインとして実装されているため 本記事では特にCLAMについて記載いたします    CLAMの繰り返し図main pyでは全体としては以下のような繰り返し処理が行われています    note info   フォールド数：   交差検証のフォールド数  各フォールドにおける学習  検証  テストの振り分けは    create splits seq py   にてcsvで出力 本記事では 上記塗りつぶし部分で示された学習部分の主な箇所を説明したいと思います また 厳密には引数等を変更するなどにより多くの分岐が見られますが 基本的にデフォルト設定を前提として記載しております    CLAMの構造CLAMには 大きくクラス分類を行うCLAM SBクラスと クラス分類を行うCLAM MBクラスのつがあります クラス分類には腫瘍vs正常とサブタイピングがあります 大まかなネットワークとしては以下のような構造になっております 主要なクラス メソッド 関数を表示していますが 全てのクラスやメソッド 変数までは記載しておりません    モデル引数各モデルのネットワーク説明の前に モデルのインスタンスを作成する際の引数について整理します モデルへの引数には辞書形式のmodel dictと 変数instance loss fnが用いられています model dictにはコマンド実行時の引数などに応じて以下のような値が入ります    python インスタンス作成時の引数 k sample   args B              アテンションスコアを元に使用するパッチ数   default                                        ※Noneでもce同様クロスエントロピー誤差が用いられる   note info その他gateという引数もありますが 基本的にTrueが選択されるようになっています  モデルサイズは  small          と   big          がありますが 本記事ではデフォルトのsmallのケースを記載します 上記を引数として CLAM SB又はCLAM MBクラスのインスタンスを作成します 次に CLAM SBに特徴量データhを通す際の処理を見ていきます CLAMでは パッチ毎の特徴量をアテンションネットワークに通し パッチ単位の分類とスライド単位の分類を行い それぞれの分類に対してロスを計算しています そのため ここでも以下つに分けて見ていきます   ①アテンションネットワーク    ②パッチ単位学習部分    ③スライド単位学習部分        ① アテンションネットワーク  入力データ  メモ  h パッチ数 x 次元の特徴量   出力データ  メモ  h パッチ数 x 次元の特徴量  A raw  A  パッチ数 x 次元のアテンションスコア   処理：  データローダーからスライド単位で特徴量データhが入ってきます バッチ数  入力データhは最初の全結合層を通り その後 Attn Net Gatedのネットワークを通してアテンションスコアAが出力されます 最初の全結合層を通った後をh とすると attention aではh が全結合層を通り次元   次元となった後にtanh関数で各値が  に変換されます 結果 特徴量の値が大きい程に近くなると思われます attention bではh が全結合層を通り次元   次元となった後にsigmoid関数で各値が に変換されます こちらも 特徴量の値が大きいほどに近くなると思われます attention cではattention aとattention bのアダマール積 要素積 が求められ 全結合層を通ることでアテンションスコアとしてパッチ毎につの値が出力されます コマンドの引数でdrop out Trueとした場合は 図の位置に のdrop out層が配置されます 最初の全結合層を通ったhと attention cを通ったアテンションスコアAが返り値として出力されます   hは model clam pyのAttn Net Gatedクラスではx表記 Aは後にsoftmax関数を通した値との区別のためA rawとされます    note info本記事のアテンションネットワークとは構造は異なりますが 画像領域で用いられるアテンションについてこちらの記事が参考になりました     深層学習 図で理解するAttention機構        ② パッチ単位学習部分    note warnここでの入力 出力データは部分的な処理としてみたものですので コード上は関数やメソッドの返り値として出力されるものではなくつの変数として見ていただけましたら幸いです   入力データ  メモ  A raw  A  パッチ数 x 次元のアテンションスコア   出力データ  メモ   処理：  A rawがsoft max関数を通してAに変換されます この時点で 各パッチがアテンションスコアをつ持っている状態となっています  アテンションスコア：全パッチの値の合計がとなる値 ここで アテンションスコア上位 k 個 下位 k 個のパッチのインデックスを元に k x 個のパッチについて特徴量 h  ①の出力値 を抽出しall instancesとします 次に all instancesをクラス毎に異なる全結合層iに通します CLAM SBでは スライドのクラスがnormal tissueの場合はi  で全結合層を通して各パッチの値がつのlogitsとなります クラスがtumor tissueの場合はi で別の全結合層を通して同じく各パッチ値がつのlogitsとなります 各クラスで同形状の異なるネットワークにて別々に重みを学習する形になります   パッチ  パッチ  … パッチ  x k   また all targetsがk x 個のパッチのラベルとして用意されます 前半 k個が  後半 k個がからなるテンソルで 正解のインデックスと考えられます アテンションスコア上位k個については パッチ毎のつの値の内 インデックスの方が大きければロスが小さくなります 逆に下位k個については インデックスの方が大きければロスが小さくなります   こういった手法により アテンションスコアが高いほどスライドの特徴を良く表し 逆にスコアが低いほどスライドの特徴を表さない方向に学習をしていくものと思われます   例えば 正常 normal tissue スライドの場合は 正常の特徴を表すパッチほどアテンションスコアが高い方向へ学習していると考えられます ロスの計算とは別に logitsからパッチ毎の値の内 値が大きい方のインデックスを記録したテンソルをall predsとします 腫瘍vs正常ではなくサブタイピングの場合 癌のサブタイプが互いに排他的であることを仮定として 追加で以下の処理が行われます  図内紫色部分   Ⓐ アテンションスコア上位k個のパッチの特徴量をスライドラベルと異なるクラスの全結合層に通します  logits     Ⓑ 要素の値がのk個の項目を持つテンソルp targetsと損失関数でロス instance loss を求めます     Ⓒ logits 値 の大きい方のインデックスをp predsとします   例えば サブタイプ に分類されるスライドについて アテンションスコアの高いパッチが サブタイプ の特徴を示した場合 同時に存在するはずのない サブタイプ について陽性の判断をしたということで  偽陽性 としてロスに加算する といった考え方のようです CLAM SB 正常 vs 腫瘍 の場合は つのスライドに正常部分と腫瘍部分が共存するため 上記処理は行われていません サブタイピングの場合 total inst lossには別クラスの全結合に通した際のinstance lossも追加されているため クラス数 全結合層の数 で平均されます       ③ スライド単位学習部分    note warnここでの入力 出力データは部分的な処理としてみたものですので コード上は関数やメソッドの返り値として出力されるものではなくつの変数として見ていただけましたら幸いです   入力データ  メモ  A パッチ数 x 次元のアテンションスコア  h パッチ数 x 次元の特徴量   出力データ  メモ  logits スライド単位  Aとhの行列積を全結合層に通して値となったテンソル  Y prob logitsをsoft max関数に通して 合計になるように調整したテンソル  Y hat logitの値の内大きい方のインデックスを取った予測値   処理：  Aを重みとしてhとの行列積を取ることで 個の要素を持つテンソルMを出力します Mは全結合層を通り つの値を持つテンソルlogitsとなります ②のlogitsと同じ変数名ですが こちらはスライド単位の値となります 次に logitsをsoftmax関数を通すことで正規化し 値の合計がとなるY probを出力します また logitsの値が大きい方のインデックスを取ったテンソルをY hatとします CLAM SB全体としては ① ③からCLAM SB全体の主な入力データと出力データとしては 以下のようになると思われます   入力データ  メモ  h パッチ数 x 次元の特徴量   出力データ  メモ  A raw パッチ数 x 次元のアテンションスコア  logits スライド単位  Aとhの行列積を全結合層に通して値となったテンソル  Y Prob logitsをsoft max関数に通して 合計になるように調整したテンソル  Y hat logitの値の内大きい方のインデックスを取った予測値 次にCLAM MBについて順に見ていきます       ① アテンションネットワーク  入力データ  メモ  h パッチ数 x 次元の特徴量   出力データ  メモ  h パッチ数 x 次元の特徴量  A raw  A  パッチ数 x 次元のアテンションスコア 処理：クラスがつに分かれる以外はCLAM SBと同じような処理になります データローダーからスライド単位で特徴量データが入ってきます バッチ数  入力データhは最初の全結合層を通り その後 Attn Net Gatedのネットワークを通してアテンションスコアAが出力されます 最初の全結合層を通った後をh とすると attention aではh が全結合層を通り次元   次元となった後にtanh関数で各値が  に変換されます 結果 特徴量の値が大きい程に近くなると思われます attention bではh が全結合層を通り次元   次元となった後にsigmoid関数で各値が に変換されます こちらも 特徴量の値が大きいほどに近くなると思われます attention cではattention aとattention bのアダマール積 要素積 が求められ 全結合層を通ることでアテンションスコアとしてパッチ毎につの値 各クラスにつの値 が出力されます コマンドの引数でdrop out Trueとした場合は 図の位置に のdrop out層が配置されます 最初の全結合層を通った後のhと attention cを通ったアテンションスコアAが返り値として出力されます   hは model clam pyのAttn Net Gatedクラスではx表記 Aは後にsoftmax関数を通した値との区別のためA rawとされます    note info本記事のアテンションネットワークとは構造は異なりますが 画像領域で用いられるアテンションについてこちらの記事が参考になりました     深層学習 図で理解するAttention機構        ② パッチ単位学習部分    note warnここでの入力 出力データは部分的な処理としてみたものですので コード上は関数やメソッドの返り値として出力されるものではなくつの変数として見ていただけましたら幸いです   入力データ  メモ  A raw  A  パッチ数 x 次元のアテンションスコア   出力データ  メモ   処理：  まずA rawがsoft max関数を通してAに変換されます この時点で 各パッチがチャンネル クラス分 を持ち 各チャンネルがアテンションスコアをつずつ持っている状態となっています  アテンションスコア：各チャンネルにおいて 全パッチの値の合計が ここで 各チャンネル i について以下の処理が進みます アテンションスコア上位 k 個 下位 k 個のパッチのインデックスを元に k x 個のパッチについて特徴量 h  ①の出力値 を抽出しall instancesとします 次に all instancesをクラス毎に異なる全結合層iに通します CLAM MBでは スライドのクラスがsubtype の場合はi  で全結合層を通して各パッチの値がつのlogitsとなります クラスがsubtype の場合はi で別の全結合層を通して同じく各パッチ値がつのlogitsとなります 各クラスで同形状の異なるネットワークにて別々に重みを学習する形になります   パッチ  パッチ  … パッチ  x k   また all targetsがk x 個のパッチのラベルとして用意されます 前半 k個が  後半 k個がからなるテンソルで 正解のインデックスと考えられます アテンションスコア上位k個については パッチ毎のつの値の内 インデックスの方が大きければロスが小さくなります 逆に下位k個については インデックスの方が大きければロスが小さくなります こういった手法により アテンションスコアが高いほどスライドの特徴を良く表し 逆にスコアが低いほどスライドの特徴を表さない方向に学習をしていくものと思われます 例えば 正常 normal tissue スライドの場合は 正常の特徴を表すパッチほどアテンションスコアが高い方向へ学習していると考えられます ロスの計算とは別に logitsからパッチ毎の値の内 値が大きい方のインデックスを記録したテンソルをall predsとします また CLAM MBは腫瘍のサブタイピングを前提とするため サブタイプが互いに排他的であると仮定して以下の追加処理が行われます  図内紫色部分   Ⓐ アテンションスコア上位k個のパッチの特徴量を異なるクラスの全結合層に通します  logits     Ⓑ 要素の値がのk個の項目を持つテンソルp targetsと損失関数でロス instance loss を求めます     Ⓒ logits 値 の大きい方のインデックスをp predsとします   例えば サブタイプ に分類されるスライドについて アテンションスコアの高いパッチが サブタイプ の特徴を示した場合 同時に存在するはずのない サブタイプ について陽性の判断をしたということで  偽陽性 としてロスに加算する といった考え方のようです total inst lossには別クラスの全結合に通した際のinstance lossも追加されているため 後にクラス数 全結合層の数 で平均されます       ③ スライド単位学習部分    note warnここでの入力 出力データは部分的な処理としてみたものですので コード上は関数やメソッドの返り値として出力されるものではなくつの変数として見ていただけましたら幸いです   入力データ  メモ  A パッチ数 x チャンネルのアテンションスコア  h パッチ数 x 次元の特徴量   出力データ  メモ  logits スライド単位  Aとhの行列積を全結合層に通してch x  値となったテンソル  Y prob logitsをsoft max関数に通して 合計になるように調整したテンソル  Y hat logitの値の内 最大値のインデックスを取った予測値   処理：  Aを重みとしてhとの行列積を取ることで ch x 個の要素を持つテンソルMを出力します Mは各ch毎につの全結合層を通り つの値を持つテンソルlogitsとなります つまり M  が クラス subtype  の全結合層 を通りつの値を出力 M  が クラス subtype  の全結合層 を通りつの値を出力 M  が クラス subtype  の全結合層 を通りつの値を出力しており logitsは  logits                  のような値となります ②のlogitsと同じ変数名ですが こちらはスライド単位の値となります 次に logitsをsoftmax関数を通すことで正規化し 値の合計がとなるY probを出力します また logitsの最大値のインデックスを取ったテンソルをY hatとします CLAM MB全体としては ① ③から次に以下の処理へとつながります CLAM MB全体の主な入力データと出力データは以下の様になります   入力データ  メモ  h パッチ数 x 次元の特徴量   出力データ  メモ  A raw パッチ数 x 次元のアテンションスコア  logits スライド単位  Aとhの行列積を全結合層に通してch x  値となったテンソル  Y Prob logitsをsoft max関数に通して 合計になるように調整したテンソル  Y hat logitの値の内 最大値のインデックスを取った予測値    CLAM SB MB出力 パラメータ更新次に CLAM SB及びCLAM MBからの出力 パラメータ更新を見てみたいと思います おおよそ以下のようになっています 図の④ ⑥について 順に確認してみます       ④ スライド単位で正解数の記録  入力データ  メモ  Y hat 予測インデックス  label 正解ラベル   処理：  スライド単位の予測インデックスY hat スライドラベルlabelを比較し       ⑤ パッチ単位で正解数の記録  入力データ  メモ   処理：  パッチ単位の予測インデックスと正解ラベルを比較し       ⑥ ロスの計算  入力データ  メモ  logits スライド単位の予測値  label スライド単位の正解ラベル   出力データ  メモ  loss スライド単位のロス  total loss パッチ単位ロスとスライド単位ロスを合わせた最終的なロス   処理：  logitsとlabelがロス関数 default クロスエントロピー誤差 に通されlossを出力します    ロスとパラメータ更新について  スライド単位  ネットワークの出力がラベル値と一致するほどロスが小さくなると思われます   パッチ単位  アテンションスコアの高いパッチがスライドのクラスの特徴を示し アテンションスコアが低いパッチとの特徴が明確に分かれるほどロスが小さくなると思われます CLAM MBの場合は偽陽性の観点が加わり アテンションスコアの高いパッチが別のクラスの特徴を示すとロスが大きくなります   全体のロス  デフォルトでは スライド単位のロスを割 パッチ単位のロスを割として全体のロス値が計算されます   パラメータ更新     モデルの学習についてCLAMでは 上記の形でロスの計算とパラメータの更新を繰り返すことで モデルがスライドとラベルからパッチ単位で注目すべき箇所を予測できるような形に学習していくものと思われます    おわりに本記事の内容は以上となります 分からないところ等ありましたら お気軽にご質問をいただけましたら幸いです 作成者：平尾参考文献URL,2,2022-08-17
103,103,数行のC++コードでCUDAの画像処理を行う,画像処理,https://qiita.com/rueiwoqpqpwoeiru/items/69a931e4042a4f9d4434,CUDA関数が使いづらいので 簡単に使える部品を作った コード全体は以下の通り　　 利用にはlibtiffが必要 libtiffをソースからビルドする方法は以下に書いた 　　 ビルドしたバイナリを引数無しで実行すると サンプルコードが実行され 　　　入力画像 test img tif に対して以下の各種処理を適用した画像が出力される　　 tifのαチャンネルを利用するため 画像の表示にはphotoshop等が必要 数行のC  コードとは ※以下のサンプルコードはmain cuから呼ばれる 　　 画像変形 拡縮 回転 射影変換 →　Warp hの行目　　 フィルタ 畳込み いまはGaussianのみ  メジアン 　　　morphology 膨張 収縮 Opening Closing  →　Filter hの行目　　 高速フーリエ変換 FFT を用いた畳込み いまはGaussianのみ →　Fft hの行目　　 基本的な演算 四則演算 絶対値 べき乗 閾値 クリップ  型変換　　　→　Basic hの行目　　 統計量算出 平均値 標準偏差 最大値 最小値 →　Stat hの行目 共通する設計思想を Warp h を例に説明　　 入力画像と出力画像の型を テンプレート引数として与える 行目 　　 画像は多チャンネルで保持　　　　 行目では入力の倍サイズの出力領域をチャンネル確保　　 画像処理パラメータも多チャンネルで保持　　　　 行目ではチャンネル確保 ※出力のチャンネル数と異なっていても構わない 　　　　 行目から行目では チャンネルごとにパラメータを設定　　　　 例 行目では チャンネルに 射影変換のパラメータ 四隅の座標 を設定　　 実行時は 入力画像のチャンネル 出力画像のチャンネル パラメータのチャンネルを指定　　　　 例 行目では 入力がチャンネル 出力がチャンネル パラメータがチャンネル　　 出力画像やパラメータへのアクセスは 行目のget out   行目のget param  　　　　 処理後の画像を 次の処理の入力として与える例は Basic hの 行目を参照　　　　 デバグのために 行目のimwriteや 行目のparam to csvも備える　　 使用するクラスは Warped img のみ 行目 　　 拡縮：　横の倍率と 縦の倍率を指定し チャンネルも指定  行目 　　 回転：　回転の中心座標と 回転角を指定し チャンネルも指定  行目 　　 射影変換：　変換前の四隅座標を左上から右回りで指定し チャンネルも指定 行目  Filter hの使い方　　 使用するクラスは Filtered img のみ 行目 行目 行目 　　　　 その上にあるクラスは Filtered img クラスの部品で 単体でも使える　　　　 新しい機能を作る際は Conv img クラスをコピペして修正するのがお勧め　　 出力領域を確保する際に conv median dilate erode open closeを引数として　　　与えることで 実行時の動作が変わる 行目 行目 行目 　　 フィルタ：　フィルタサイズと フィルタ数を指定 行目 　　　　　　　　　Gaussianのσと チャンネルを指定  行目 　　 メジアン：　フィルタ数を指定 行目 　　　　　　　　　フィルタサイズを指定  行目 　　　　　　　　　※行目はミスだが正常に動く get out  を介しても画像出力ができる 　　　　　　　　　※NPP関数を使用しており 初回は遅い　　 morphology：　フィルタサイズと フィルタ数を指定 行目 　　　　　　　　　　構造的要素 値がの矩形 のサイズを指定  行目 　　 使い方は前述のFilter hのフィルタとほぼ同じで ほぼ同じ結果が得られる　　 高速化効果は フィルタサイズが大きいときに絶大　　　　 xの画像に xのフィルタを適用した際 Filter hではミリ秒　　　　　Fft hでは ミリ秒であった Basic hの使い方　　 使用するクラスは Cast imgと Op imgと Op img ※これらは動作確認が一部未実施 　　　　 入力画像と出力画像の型は テンプレート引数で指定　　　　 Cast imgだけ例外的に 全チャンネルを同時に処理　　　　　 Filter hの行目と行目は非効率だが 処理時間が短いため問題ない 　　 Op img 行目 は 枚の画像の 和 差 要素ごとの積　　　　 演算の種類は do proc関数の引数 mode で指定 行目 　　 Op img 行目 は 枚の画像と スカラー値との演算 など 　　　　 演算の種類や 入力値は do proc関数の引数で指定 行目 　　　　 add sub mulは 定数の加算 減算 乗算 引数で指定するスカラー値はつ 　　　　 abs pow divは 各画像の絶対値 べき乗 逆数 スカラー値は指定しない 　　　　 thは閾値処理 引数で指定するスカラー値はつ ※出力の型によって出力値が変わる　　　　 clipはクリッピング 引数で指定するスカラー値はつ 　　 行目では 元画像 in img を 倍した画像と  倍した画像を足し合わせるため 　　　結果が格納されるop imgのチャンネルは 元画像 in img と同じになるはず Stat hの使い方　　 使用するクラスは Op statのみ　　※NPP関数を使用しており 初回は遅い その他　　 上記コードにおけるメモリ管理については以下に記載　　 CUDAカーネルをC  クラスのメンバ関数に実装する方法は以下に記載　　 高速化の工夫については以下に記載なお 上記コードは CUDAを個人的に学ぶために作成 ご自由に使ってください ,0,2022-08-15
105,105,写真から背景を削除する 5 つの方法,画像処理,https://qiita.com/ShotaKazama/items/b05be4dde842340abd20,  前提：この記事は 写真から背景を削除したい人に適しています 背景除去とは 写真の背景を除去し 人物を残すことを指します 写真の背景を削除する機能は 今日では  時間もかかる複雑な操作ではありません     オンラインの背景除去ツールを使用します 現在 インターネット上には 便利な操作を提供する多くの画像除去背景ツールがあります 背景除去ページの  つを開いて画像をアップロードし AI が 背景を除去する  のを待つだけです 迅速かつ簡単で 不正確なエッジを手動で調整できます もちろん 無料ではありません     パワーポイントを使用して背景を削除します PowerPoint に画像を挿入し  ツールバー  を選択します その後 ワンクリックで戻ります     これは予想外です 背景除去効果はあまり正確ではありません    Python コードを使用して背景を削除します    方法  paddlehub という Python ライブラリがあります インポートできます 次に コードを記述します  モデルをロードする  切り出す画像のディレクトリを指定  背景を削除する    方法   バックグラウンド リムーバー コードを含む Python パブリック ライブラリがあります このライブラリをインポートできます ソース：ライブラリをインストールします     pip install backgroundremover   デバッグ  ダウンロードしたライブラリを簡単に見つけられるディレクトリにインストールします Window：c  Windows user  unet unet pthMac  Users lyc  unet unet pthLinux  root  unet unet pth次に 必要に応じてコードを実行します    方法  remove bg 公式 Web サイトには バックグラウンド削除 API があります この API を取得できます APIのキー値を取得したら pythonプログラムを書き始めましょう ライブラリをインポート api keys    上記で取得したキー値 ,5,2022-08-10
107,107,Rustで画像処理実践編 図形情報の取得,画像処理,https://qiita.com/numekudi/items/066ab3a0e18a18bf201d,今回は唐突に ブロブの最大内接円 が欲しくなったのでコードを書いてみました 意外とパッケージの画像処理システムに搭載されていないんですよね を参考にさせて頂きました 画像もお借りします   要件与えられた二値画像の図形からできるだけ大きな円を切り抜きたいので その中心位置と半径を得ることが目的です   解法距離変換    最大値と最大位置を取得距離変換はブロブ内画素の値を輪郭までの距離に変更します ブロブの内側に向けて画素値が高くなっていくイメージですね ユークリッド距離で変換すれば 最大値が輪郭までの距離 位置が中心を示します   実装src pngを読み込み 円を描画してからhighguiで表示します        値化       距離変換       受け取るパラメータの初期化       外輪郭からの最大値 最大値位置の取得       カラーにしてから青で円を描画適当にマウスでグルグルした画像を読ませたらいい感じでした   image png  C言語における NULL を Option の None バリアントで表現するのがRustらしくていいですね NULL安全の言語は素晴らしいです それにしてもRustAnalyzerは優秀ですね 引数名と型推論の表示が最強すぎる C並の速度の言語がこんな楽にかけていいんですかね   みんなも使おう 宣伝 ,0,2022-07-31
108,108,【PyTorch】Vision Transformer (ViT) を自作しながら解説する,画像処理,https://qiita.com/zisui-sukitarou/items/d990a9630ff2c7f4abf2,  はじめに   前提としていること以下のことは この記事では解説しません   基本的なニューラルネット系のモデルの仕組みについて例えば 以下のコードが理解できるくらいの方を想定しています    この記事のゴールこの記事で実装 解説していくモデルの全体像は以下の図です この記事のゴールは このモデルについて理解し PyTorch を用いて実装することです   overview png     この記事の大まかな流れ以下のような流れで説明していきます   ViT を使ってみる  Transformer とは Vision Transformer  ViT      Transformer を画像認識に応用したもの なので ViT について説明する前に Transformer について簡単に説明します Transformer とは 年に  Attention Is All You Need   という論文の中で発表された深層学習モデルです  英語 rarr フランス語 のような自然言語の翻訳に使用され それまでの RNN を用いた手法等に比べて 精度 学習コストの両面でより良い性能が出ました Transformer の概要図を以下に示します 大きな縦長のブロックのようなものがつ並んでいて 左の方の先から右の方の真ん中あたりに矢印が伸びているのが分かると思います この構造は Encoder Decoder モデル と言い ざっくり説明すると 例えばフランス語の文章を英語の文章に翻訳する場合 左のブロック Encoder でフランス語から 意味 にエンコードし 右のブロック Decoder で 意味 から英語にデコードする といった感じで使われます 今回紹介する ViT モデルは この左側の部分 すなわち  エンコーダ部分のみをほぼそのまま利用  したモデルとなっています  枚目の画像の右側の図と見比べると かなり似ています   以降では ViT の理解に必要なエンコーダの部分についてしか解説しないので Transformer の全体像について知りたい方は こちらの記事  などを参照してください    ViT とはそんな Transformer のエンコーダの部分を画像認識に応用したものが ViT です     特徴  SoTA を上回る精度  畳み込みを行わないモデル  それまでの SoTA の約 \frac     倍の計算コスト    モデル概要画像が入力されてから 認識結果が出力されるまでの流れをざっくりと説明すると   画像がパッチに分割されて  各パッチがベクトルに変換されて  その先頭に  class  トークンを付加したものに位置エンコーディングが加算されて  それらが Transformer Encoder によって処理されて  その出力の番目のベクトルが MLP Head で処理されて最終的にクラスの出力が得られます では 具体的なアルゴリズムと実装について 各ステップ毎に 次の章で見ていきます   ViT を作るまず最初に 実装の全体像を眺めます コードの量が増えてしまうので ViT 以外の class の実装は一旦隠しておきます class ViT の forward 内を見てもらうと 入力画像 img に対して 上で説明した処理を行なっているのが分かると思います       後ほど解説      後ほど解説      後ほど解説      後ほど解説      後ほど解説      後ほど解説      後ほど解説              dim  int    各パッチのベクトルが変換されたベクトルの長さ 参考     式 D             パッチに分割            各パッチをベクトルに変換             class  トークン付加   位置エンコーディング             出力の番目のベクトルを MLP Head で処理次に 各処理について詳しく説明していきます  ※注意：本実装では dropout は入れていません      変数 変数   コード中変数名   意味等     P    patch size    パッチのサイズ 縦の長さ および 横の長さ     C    channels    チャンネル数 RGB 画像の場合  C       D    dim    パッチベクトル変換後のベクトルの長さ       パッチに分割まず 入力画像をパッチに分割するステップについて解説します とは言いつつもそんなに難しいことはしていなくて 一枚の画像を複数枚のパッチ 上の例だと枚 に切り分けて 左上から横に並べていくだけです 注意点としては 元の画像は   C  H  W   の次元配列だったのに対し 切り分けた後のパッチのベクトルは  C\cdot P   の次元配列になっているということです 従って コード中の x のサイズはというように変形されます      各パッチをベクトルに変換              dim  int    パッチが変換されたベクトルの次元   スクリーンショット       png  次に 各パッチのベクトルを別サイズのベクトルに変換するステップについて解説します 以下の文字を用いて説明します また 各パッチのベクトルの長さ コード中 patch dim  は  C\cdot P   となります 実装方法としては   C\cdot P   imes D  の行列  \mathbf E   を用いて  \mathbf x  k p \mathbf E   といった具合に変換します  \mathbf E   はコード中では nn Linear の部分で この行列自体も学習可能なパラメータです       class  トークン付加   位置エンコーディング              dim  int    パッチが変換されたベクトルの次元          バッチサイズを抽出           class  トークン付加          位置エンコーディング加算  スクリーンショット       png     各パッチをベクトルに変換 によって作られた  N  コード中 n patches  個のパッチのベクトル達の先頭に  class  トークンを付加します これは学習可能なパラメータで Transformer Encoder によって処理された後の  class  トークンに対応する部分 正確にはそれを nn Linear dim  n classes  で処理したもの が 予測結果を返してくれます この時点で x のサイズは   B    N \color red        D   となりました   その後 位置エンコーディングを行います 後ほど説明する Transformer Encoder では 入力トークンの位置情報を把握することができないため 位置情報をあらかじめ付加する必要があります 実装としては   N  imes D  の行列  \mathbf E   pos   を加算します これは 学習可能なパラメータです       のまとめここまでの処理をまとめます まず最初に 画像を  N  個のパッチに分割しました パッチ  つのベクトル  \mathbf x  p  の長さは  C\cdot P   で これが  N  個並んだ   \mathbf x   p  \mathbf x   p  \cdots   \mathbf x  N p   という形になっています 最後に   class  トークンを付加してから位置エンコーディングを行い 最終的に Transformer Encoder への入力  \mathbf z    は以下のようになります               dim  int    各パッチのベクトルが変換されたベクトルの長さ 参考     式 D   残差接続 上の画像の  \oplus   以下では それぞれの要素について見ていきます     残差接続残差接続は ViT 以外でも広く使われる残差学習のためのパーツです これによって 層を深くした場合に発生する  劣化問題  収束の遅さが解決されます  詳細を知りたい方は  こちらの記事  や こちらの論文  を参照するか  ResNet で検索して見てください      Layer Normalizationこれも ViT 以外にも広く使われる一般的な仕組みで 実装も nn LayerNorm で簡単に実装できるので 説明は割愛します  詳細を知りたい方は  こちらの記事  や こちらの論文  を参照するなどしてください                dim  int    パッチのベクトルが変換されたベクトルの長さ この記事では Multi Head Attention の お気持ち 的な部分は説明せず 実装に焦点を当てて説明するので そもそも Multi Head Attention が何なのか分からないという方は 先に こちらの動画の    を観ることをお勧めします   変数   コード中変数名   意味等   その後 各  i          h   について  \mathbf head  i  を以下の式で計算します               dim  int    パッチのベクトルが変換されたベクトルの長さここはあまり解説することはなく コードの通りです   スクリーンショット       png  Transformer Encoder で処理された後の  class  トークンに対応する部分を MLP Head で処理します 具体的には 最初に Layer Norm で処理し その後 クラスの数の長さのベクトルに線形で変換します   ViT を使ってみるtorchvision datasets CIFAR で学習 検証してみました  こちらの ipynb   を上からポチポチしてもらえばできるかと思います 結果としては 過学習気味ではあるものの dropout を入れてないせいだと思う  一応ちゃんと学習が進んではいるようでした   おわりに間違い等ありましたら 指摘していただけると助かります また pip install で使える 公式の   実装  もご参照ください ,72,2022-07-31
109,109,一番簡単（で一番適当な）なVision Transformer実装例,画像処理,https://qiita.com/nknknaoto/items/615e8057db0a45d7b1be,   初めに ICLRにてViTのポスター発表  ありましたね 　なので遅ればせながらViTの解説とその実装をします 色々実装例を見たところスクラッチから書いてる例かViT専用のライブラリを使ってる例しか見当たりませんでした やっぱりプログラムは少数の有名ライブラリを上手く使って書くものだと思うんですよね というわけで以下のレギュレーションに則りプログラムを書こうということです    pytorch以外禁止   pytorchのライブラリで代用できる処理は代用するあ 組んだコードは以下です    ViTの特徴実装に入る前にViTの特徴について話したいと思います 特徴としては何よりトランスフォーマーを使っていることでしょうか トランスフォーマーはもともと自然言語処理とかのシーケンスに使われていた技術でした 当たり前ながらトランスフォーマーには畳み込みは使いません しかしこれが画期的な点になってきます ViTが出る前は畳み込みを使わない手はないくらい畳み込みの時代でした それなのにViTは畳み込みを使わずに既存の記録を多数塗り替えてしまいました 界隈がひっくり返る大発見ですよほんとに しかもトランスフォーマーは畳み込み処理より計算コストが安いおまけつき 数年経った今でもViTの仕組みを応用した論文出てきてます 例えば Attention機構をmlpにした奴  とか mlpじゃなくて単なるプーリング層にしたろ  とかが個人的に面白い論文でした それだけ画期的な技術だったということですね    ViTの構造 こちら  のFigureがわかりやすいです ViTは自然言語処理としてのトランスフォーマーを意識しています 実際ViT作者も既存のトランスフォーマーを壊したくないみたいなことを言ってました  we wanted the model to be  exactly Transformer  but on image patches なのでBERTを学んでからViTをやったほうがわかりやすいと思います BERT未履修でも何とかなるようにはするつもりですが   ViTにはつの段階があります    画像パッチ化処理   CLSトークン埋め込み   ポジション埋め込み   トランスフォーマー入力   mlp入力これらつについて 実装しながら説明したいと思います    実装してみよう上述のつのステップを順に実装してみましょう まずViTのクラスから importは以降使うクラスを呼び出してます     画像のパッチ化処理トランスフォーマーはそもそも自然言語処理用のモデルでした つまり言語っぽい扱いをしてあげなきゃいけません やってることは以下つ    画像を任意の数に当分   それぞれを単語とみなすこれで無理やりトランスフォーマーに入力している感じです 今回cifar xxの画像 を使おうと思ってるのでxxの画像に分割しましょうか patchifyでグリッド状に画像を分けて flattenで画像を潰しました  以下行追加imageWH   patchWH   splitRow imageWH  patchWH        splitCol imageWH  patchWH         以上     以下行追加     以上潰した画像はまだ生の値で 特徴量ではありません とりあえず全結合層に入れてトランスフォーマーへねじ込めるようにしましょう    python vit py 省略patchWH    以下行追加 以上         省略    CLSトークン埋め込みここはBERTやってない人がつまずきやすいので少し長めに説明しますCLSとはクラスの略です 画像全体の特徴量を集める役割を持たせます するとCLSの出力のみを使って画像分類ができるようになりました やったね 実装方法としては学習可能な変数をCLSトークンとして定義します このトークンをベクトル化されたパッチ列の先頭に配置します    python vit py 省略         省略         以下行追加         以上特徴量を集めるとか大層な気がしますがこれは逆で集まるのは必然的なんですよね ViTの最終段階 以下mlpとかmlpヘッダー への入力はCLSトークンのみです するとViT全体がCLSトークンに特徴が集まるように学習してしまうのは自明ですね ここは重要な点ですが どのような入力にもトランスフォーマー第層のCLSトークンは一定です  トランスフォーマー最終層のCLSトークンに特徴量が一番集まるように第層のCLSトークン 定数 を決める と言えばわかりやすいでしょうか      余談先ほど CLSトークンのみをmlpへ入力しているので CLSに特徴が集まる と言いました これは逆に CLSトークン以外をmlpへ入力にすればCLSはいらない ということでもあります というわけでCLSトークンが気持ち悪い諸兄への朗報です CLSを使わなくてもViTはViT足りえます ViT作者もCLSはViTにそこまで関係しないと明言していますね   略  Great question  It is not really important  However  we wanted the model to be  exactly Transformer  but on image patches   so we kept this design from Transformer  where a token is always used できるだけNLP用のトランスフォーマーをそのまま使いたいため CLSを採用しているみたいです CLSを使わない方式は 余談   余談 で紹介しています     ポジション埋め込みトランスフォーマーは入力順という概念がありません 不便ですがどの入力が画像のどこに対応するか明示する必要があります 元論文を見ると次元の学習可能な変数を加算でいいらしいですね    python vit py 省略          省略        self positionEmbedding   nn Parameter torch zeros   patchTotal     embedVectorLen    追加    def patchify img           省略    def forward self x            省略次元の正弦関数とか高度なの使ったみたいですけど言うて結果は良くならなかったみたいですね     トランスフォーマー入力ここは腐るほど説明されていると思うのですぐ終えます pre norm GELU関数を採用していることに注意    python vit py 省略 以下行追加 以上          省略        self positionEmbedding   nn Parameter torch zeros   patchTotal     embedVectorLen           以下行追加         以上    def patchify img           省略    def forward self x            省略次元出力のmlpを作ってトランスフォーマー最終層のCLSトークンを入力します    python vit py 省略          省略        self transformerEncoder   TransformerEncoder encoderLayer layers         self mlpHead nn Linear embedVectorLen    追加    def patchify img           省略    def forward self x            省略     余談 余談   余談 で言いましたCLSを使わない手法について検討します まずCLSトークンが存在しない世界線のViTを召喚します  省略         省略    def forward self x            省略正味これだけでも何とかなりはします mlpの入力を色々変えてみても良いですね 例えば     省略    def forward self x            省略とかでもよろしいかと あとはself mlpHeadを単純な全結合じゃなくてLSTMとかに変更も面白そうです    できた  というわけで全体像ぽん   テストcifarでやってみます 比較対象として適当に作ったCNNも学習させます   image png    image png  CNNもViTも特別な操作をしていないのですぐに頭打ちになっちゃいますね とはいえやっぱりCNNのほうがいい結果を出します それもそのはずデータを食わせば食わせるだけ精度が上がるのがトランスフォーマーです SOTAに届くには億単位のデータが必用です 我々には手の届かない世界ですね   ということでどんでん返しになっちゃいますが 大人しく既存のライブラリと事前学習データを使うことをお勧めします    終わりにやっぱり自分で書いてみると勉強になりますね なんか腑抜けた感じに終わりましたが ViTへの理解の助けになったら幸いです    参考サイト,10,2022-07-30
110,110,私の戦闘力は53万です。販促物の強さを判定するスカウター,画像処理,https://qiita.com/akao-dx/items/57ce64e4a79b56e1af40,  販促物の強さ判定私はスーパーマーケットに勤務しております その中でよくある課題が…   販促物の優先順位がわからない   近所のスーパーを思い出していただくとわかりやすいかと思いますが 売場では 今日は〇〇の日 引き  △△デーポイント倍 などお客様の購買を促すために様々ポスターが取り付けてあります そのようなポスターやPOPのことを販促物と呼んでおります そんな販促物は当然様々な種類があります ただ限られた売場内で取り付けるため 販促物によって優先順位を決めて取り付けなければなりません   例えば  下画像の左の販促物と右の販促物の場合やはり割引はお客様に与えるインパクトが強いので    日の日曜日    になった際は左の販促物を優先的に多く取り付けます   販促物サンプル png    完成品強さといば   戦闘力    販促物の強さを戦闘力と表現しています LINEで販促物の画像や写真を送るとその戦闘力を答えてくれます   使用ツールHerokuとNode REDの初期設定はこちらの記事を参照してください  herokuに作る無料Node Redサーバ Postgresも付いてくるよ   ※無料で利用する為  の設定のみで大丈夫です またLINE DevelopersとTeachable Machineについてもそれぞれ下記リンクを参考にしてください   完成までの流れ  機械学習の様子です 画像データはなるべくいろいろなものを学習させます 販促物の元画像データだけでなく写真など学習させる画像が多いほうが正確に判定してくれます   機械学習モデル PNG    Node REDで作成したフローノードをダブルクリックで表示される画面で 下記のように設定   httpin PNG  ※URLは任意の文字で良いです 最初に を入れてください ノードをダブルクリックで表示される画面で 下記のように設定   function PNG  コードはこちらノードをダブルクリックで表示される画面で 下記のように設定 認証を使用  チェックを入れるトークン  LINE Developersの Messaging API設定 より チャネルアクセストークン の文字列をコピペします 出力形式  バイナリバッファノードをダブルクリックで表示される画面で 下記のように設定   teachable PNG  URL  Teachable Machineでアップロードした際のURLをコピペします Image  チェックを入れるノードをダブルクリックで表示される画面で 下記のように設定   function PNG  コードはこちら   Reply ノードの配置  ノードをダブルクリックで表示される画面で 下記のように設定   LINEReply PNG  Secret  LINE Developersの チャネル基本設定 より チャネルシークレット の文字列をコピペします    image ノードの配置  これはあってもなくても大丈夫です 最後にデプロイを忘れずに そこへ貼付するURL以下の通りにします Node REDページのアドレスバーにあるURLをコピー com の後ろは削除 代わりにhttp inノードのURL欄に入力した 以降を追加します そのURLをLINE DevelopersのWebhook設定にある Webhook URLへコピペします   ※ Webhookの利用 を必ずONにするのを忘れずに   以上完成です   あとがきTeachable Machineというものがあると知っておもしろそうだと思って作ってみましたが販促物の画像データ集めが意外に大変でした…最初は販促物の元データのみ つまり一つの販促物につき一枚の画像で登録していたら全く判別してくれず サイズ違いの画像を探したり 印刷したものを写真で撮ったりして学習データを増やさなければなりませんでした それでも学習データが足りないのか誤判定や判定できないことがたまにあるため今後もちまちま画像集めをしていきたいと思います 今後もし会社で実用化するためには 今ある販促物を網羅して登録 それぞれについてたくさんの画像を機械学習させることができるかがカギになると思います   まぁそもそも販促企画の種類をもっと減らしてくれればスカウターも必要無いんですけどね…  以上最後まで読んでいただきありがとうございます 少しでも何かの参考になれたら幸いです 今後もいろいろと挑戦して発信できたらと思います よろしくお願いします ,9,2022-07-29
111,111,動作環境以外で収集した画像を学習データとして判別対象を認識する,画像処理,https://qiita.com/shimamotosan/items/1a0c6c73b1e60ae5db43,  やりたいこと以下のテキストでは AIを用いた画像認識により対象物を分類し ロボットアームにより仕分けを行っています    DOBOT Magician AIｘ画像認識ｘロボットアーム制御  上記テキストでは 仕分けを行う環境上で判別する対象物の学習データの収集を行っていますが AIによる画像識別では 実動作環境以外で収集した画像データを学習に含めることもあると思います 今回は スマホのカメラで撮影した実動作環境ではない画像を学習データとし 判別が可能かを検証しました    note前提条件  以下の記事で修正したプログラムを使用し カメラ画像全体を学習済モデルに入れて 判別結果を出力する形式を取っています        カメラ画像全体から判別対象を認識する    対象物を置く位置 ＝DOBOTが対象物を拾いに行く座標 は固定    ※ 切り出しをしない 領域を取得しない ので対処物のカメラ座標が取得できないため  できたもの   判別対象今回は対象物として 種類の板ガムを使用しました 学習データとして 以下のような画像を各種枚程度集めました    動作問題なく判別は出来ていますが やや確率が低めです 学習データを増やすことで改善できるかもしれません   プログラム撮影した画像の拡張子がjpgだったため jpg画像も学習データとして読み込めるように以下の部分を追加しました  元々のプログラムではpng画像のみ ,0,2022-07-29
112,112,カメラ画像全体から判別対象を認識する,画像処理,https://qiita.com/shimamotosan/items/d3a06e834d332b914973,  やりたいこと以下のテキストでは AIを用いた画像認識により対象物を分類し ロボットアームにより仕分けを行っています    DOBOT Magician AIｘ画像認識ｘロボットアーム制御  対象物を判別する手順として OpenCVを用いてカメラ画像中から対象物を切り出し それを学習データとしたり 学習したモデルに入れて判別を行っています そのため カメラ画像中から対象物を上手く認識し 切り取ることができるように下敷きを用意したり 細かいカメラの調整等が必要になります 今回は カメラ画像から対象物を切り出さずに カメラ画像枚を学習データとし 対象物の分類ができるようにプログラムを修正してみたいと思います     note前提条件  修正するプログラムは テキスト記載のサンプルプログラム  対象物を置く位置 ＝DOBOTが対象物を拾いに行く座標 は固定    ※ 切り出しをしない 領域を取得しない ので対処物のカメラ座標が取得できないため  できたもの   判別対象今回 判別対象は xのレゴブロックをつ重ねたものとしました 重ね方によって正常か否かを判別するように以下のような学習データを集めました   正常な形    正常ではない形     カメラの調整カメラ画像全体を学習データとするため 判別対象が画像の中に占める割合が大きくなるように 判別対象が大きく映るように カメラの位置は低めに設置しました    動作問題なく判別することができました 学習データに含まれていない形も 正常ではない として判別しました   プログラム   修正後  教師データの作成       省略           加工なし画像を表示する        cv imshow  Raw Frame   frame           キー入力をms待つ        k   cv waitKey             ESC   キーを押す          プログラムを終了する           C キーを押す          WEBカメラのゲイン値 露出の値を調整する         画像の保存             S キーを押す         そのまま切り取って画像を保存する              リストに格納された矩形を長辺に合わせてサイズ調整する              サイズ調整した正方形を画像 png データで保存する           A キーを押す          補正を加えた画像を保存する              取得した矩形を長辺に合わせてサイズ調整する              サイズ調整した正方形に補正を加えて保存する           R キーを押す          画像を回転させた上に補正を加えた画像を保存する              取得した矩形を長辺に合わせてサイズ調整する              画像の中心位置              画像サイズの取得 横  縦               リストに格納された長方形を画像 png データで保存              回転 °  °  °  ° して 変換処理した画像を保存                  回転変換行列の算出                  アフィン変換      キャプチャをリリースして ウィンドウをすべて閉じる 省略   学習済みモデルを使って仕分ける       省略           加工なし画像を表示する          リサイズする          画像の前処理             分類する             ラベルを表示する              P キーが押されたときの処理                   回転を考慮した外接矩形を表示              確率を表示          描画した画像を表示        cv imshow  Edited Frame   edframe           キー入力をms待つ        k   cv waitKey             ESC   キーを押す          プログラムを終了する           C キーを押す          WEBカメラのゲイン値 露出の値を調整する           P キーを押す          各ラベルの確率を画面上に表示する／再度押すと消える           H キーを押す          DOBOTをホームポジションに移動させる 位置リセット            S キーを押す          最後に取得した矩形とその結果を元にDOBOTでピックアップする      終了処理         DOBOTの終了処理    dc finalize        キャプチャをリリースして ウィンドウをすべて閉じる,0,2022-07-29
113,113,Pythonが嫌いなのでC++版のPytorchで画像認識をやってみる,画像処理,https://qiita.com/pipimaru1/items/326bd9f52148e6794d6c,  Pythonが嫌い  なのでC  版のPytorchで画像認識をやってみる Pythonなんて所詮スクリプト プログラミング言語とは言えない 実行環境作るの面倒だし 型定義は無いし ポインタは使えないし バイナリ作れないし 遅いし 文法は奇妙だし とにかく昭和のおじさんはコイツがキライなのだ とはいうものの C  を触るのは年ぶり 当時はガンガン使えたけど 今の言語仕様はチンプンカンプン Visual Studioも初心者 cmakeて何  make exeは何となく覚えているけど Pythonも完全に理解しているわけじゃない 途中で挫折するかも ダラダラ書いていきます メモ代わりに書いていくので 文章の整理は後   準備とりあえずVSを最新に更新する   image png  再起動が要るのか  image png     CUDAとCUDNNを入れておくcuda cudnn 使うPytorchに合わせて選んでおく必要がある 私はcuda とcuda  を選択 GPUドライバは最新にしておけばOK と思う 最初に書いておくけどCUDAやCUDNNのパスをシステム環境変数を設定しておかないとVSの拡張機能がちゃんと機能しないと思う 手動でインストールしている人は要注意 ふつうにインストーラで入れておくのが無難   LibtorchpytorchのC  版はlibtorchと言うらしい    Pytorchのサイトを読んでみる C   というのはなんか見覚えがある C  コンパラのオプションにあったような 記事の下の方に何か書いてある リンク先を開く  image png  配布セットが有りますよ てことらしい 読み進む 面倒なのでGoogle翻訳 なにやら魅力的なコメントが  image png     VisualStudio拡張機能何だろう  とりあえず ビデオ  を見てみよう  なんか良さそうじゃありませんか   image png     VisualStudio拡張機能を入れてみる  image png  ダウンロードした このファイルをダブルクリックすればいいらしい   image png  この段階で意味が有るのか分からないが一応ウィルスチェックかけてから起動  それにしても開発キットにウィルス紛れ込まされたらどうにもならんな   image png  installを押す 何か始まった 時間がかかるようだ   image png  分経っても終わらない  End Tasks を押してみる 上手くいった  気持ち悪いなあ   image png     LibTorchのダウンロードLibTorchは別途ダウンロードが要るようだ ReleaseとDebugを両方ダウンロードして展開   image png  大きなファイルだこと   image png  チュートリアルと同じように そのままのフォルダ名で展開   image png    VSを起動ツールが揃ったようなのでVSを起動  image png   新しいプロジェクトを作成 を選択  image png  おお  確かに下の方にTorchProjectがある 　選択して次へ   image png  適当なフォルダを選択 開発用の領域になるのかな  注   パスにスペースが含まれると実行時に必要ファイル収集に失敗してにコケる スペースの無いパスを指定しよう あるあるだね    image png  libtorchの在るところを入れろ ということらしい   image png  さっき展開したフォルダを入れる なんかいろいろ自動認識しているようだ 頼もしい   image png  何か作ってる   image png  お  サンプルコード付きでプロジェクトが出来た   image png  Fを押してみる コンパイルしているようだ   image png  動いたーーー  GPUを使って行列の演算をしたようだ   image png  exeファイルと必要なDLLも集められている  pdbてのはデバッグに要るのかな   image png    感想Cuda とCUDNNは既にインストールしてあったので思いのほかサクサク行った コイツラがちゃんと入れてないとここがつまづきポイントになると思う   追記このプロジェクト構成スクリプト でプロジェクトを作るとき プロジェクト作業エリアのパス名にスペースがあると実行時にエラーになる 実行に必要なdllを集めるときにxcopyを使っているらしい 今日はここまで つづく,19,2022-07-26
114,114,全くの初心者が機械学習で雲の画像診断に挑戦したら,画像処理,https://qiita.com/kimiho-kori/items/209f8b56b333ba965b60,   はじめにはじめまして Qiita初投稿 エンジニア職を目指しAidemyで勉強中です 初めてのAIアプリを作ってみての自分なりの経験をまとめます 先に結果を言いますと非常にできの悪いアプリになったため 温かい目で感想 アドバイスなどあればよろしくお願いします    概要何のアプリを作ろうか何も考えてなく ぼーっと空を見ていてふと雲の画像識別を作ろうと考えました まず雲の種類は種類あるので それぞれの画像がどの雲に該当するかを判断するモデルを作る    流れ  まずは雲の写真を収集 種類枚前後   写真からそれぞれの種類の雲を分類  機械学習で雲の種類を学習させたモデルを作成  結果と反省 モデルの精度向上を目指して  まとめ言語はPython 環境はGoogle Colaboratoryを使用しました      まずは雲の写真を収集 種類枚前後 種類の雲は  巻積雲   巻層雲   巻雲   高積雲   高層雲   乱層雲   積乱雲   積雲   層積雲   層雲  という風に分かれています    巻積雲     巻積雲    巻層雲     巻層雲    巻雲     巻雲    高積雲    高積雲    高層雲   高層雲    乱層雲     乱層雲    積乱雲     積乱雲    積雲     積雲    層積雲     層積雲    層雲     層雲 画像を集める際にicrawlerという便利なPythonのパッケージを使いました icrawlerとはGoogle Bing Baidu Flickrなどの画像検索サービスから画像をダウンロードしてくれる便利なPythonのパッケージです使い方は下記のサイトを参考にしました 種類の雲をそれぞれスクレイピングしフォルダに格納しました      写真からそれぞれの種類の雲を分類画像を集めたものをチェックします まず関係ない画像やイラストもとってきてたのでそれ削除し スクレイピングしたものと違う雲の画像も多かったので正しいフォルダに画像を写す また つの画像に複数の種類の雲が写っていた場合は特定の種類の雲のみ切り取る ほぼ全ての画像に複数の種類の雲が写っていたので この作業だけでも一苦労でした      機械学習で雲の種類を学習させたモデルを作成インターネットから取得した画像は cvを用いて×にリサイズしてます 　まず最初にvggモデルを使えば それなりの精度のものが作れると思いモデルの作成を始めました VGGというのは  ImageNet と呼ばれる大規模画像データセットで学習された層からなるCNNモデルです ここに全結合層を再度学習させてみました しかし正答率が 強にしかならず 層を増やしてもあまり変わらず 転移学習を使わずにやってみるとこちらではおおよそ の正答率になったのでこちらでモデルを作ることにしました    正解率   訓練データは回あたりから右肩上がりだが 検証データに対してはずっと横ばい      反省 モデルの精度向上を目指して機械学習では データ集めや前処理が大切と言われている通り 作業の中でいちばん時間を割くことになりました 特に雲の場合枚枚千差万別で枚の画像の中に複数の雲が混じっていることなど当たり前で あまり良く撮れてない画像などもあり 目で見てもどの種類か判別に苦労しました また色や形など 半透明やしろ〜黒に近いグレー で見分けるにしても 朝方や夕方 日が落ちる直前などの画像は見た目でもほぼ判別不可能だったので学習する画像から排除したりして正答率が若干上がりました こちらのサイトでもわかるように変種や部分的な特徴もだいぶ被っているので振り分けるのに苦労しました 例えば  積雲 の一種の雄大積雲と 無毛の 積乱雲  いわゆる雷雲 の違いは雷があるかどうかだけなので画像で判別しようがないと思い 雄大積雲は積乱雲の方に入れました 一般の人にはどちらも入道雲と呼ばれている  その他も少しの大きさの違いや厚み 雲の高さなどで分かれるのでかなり大変でした 改良点ばかりなので まず考えられるのは 画像を増やす 集めたり 左右反転 水平移動 高いところの雲は回転させてもいいかも  空は 高層 中層 低層 に分かれているのでそれぞれの層別にモデルを作る  他の転移学習も試してみる 　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　など   まとめ自分がこの数カ月で学んだことはわずかだと思います これからの課題は山ほどありますが とりあえず何かを作って行ったり 実践することが上達の近道だと思いますので 色々なアプリを作ったり kaggleやsignateへの挑戦 数学的 統計的な知識などを積極的に学んでいこうと思います 長く内容も薄いものになりましたが ここまで読んでくださりありがとうございました ,1,2022-07-26
115,115,オンライン会議いつも遅れる人をTeachable Machineで素早くつかまえてみた！,画像処理,https://qiita.com/natsupariri/items/9773c3f3238b6ebaa488,   出欠確認作業 これを楽に しかも早くしたい   私はこの確認作業で いつも時間に追われながら 一覧表を上に下にスクロールしています   人事部ではないけど 労務管理しています オンライン会議が普通になってはや数年 みなさんは 出欠確認はどのようにしていますでしょうか きっと私と同じように 入室していない人をあの  小さな画面で一人ずつ確認して 来ていない人に電話  をしているのではないでしょうか 電話をかけるにも まずは確認作業が必要ですよね デジタル化になっても  出欠確認はいつまでもアナログ     　時間がかかって困っています これって どうにかならないの と思い  画像判別  に挑戦しました 今回は いつも遅れる  しんのすけさん  を いち早くつかまえてみることにしました おや 上司の方は名前を見つけれず しんのすけさんに電話をかけたようです   画像学習③ jpg  いつもよく遅れるしんのすけさん今日は一番のりで間に合ったみたいですが すぐに名前を見つけてもらえなかったみたいですね    しんのすけさんの名誉のためにも 早く来た日は素早く確認できるようにしてみましょう    note今回の登場人物は フィクションですので 温かい目で見てください 笑   今回使用したもの  学習と検証比較対象として 今回はオレンジを学習しました   画像学習 jpg  検証すると 顔のアップや一部でも認識OKです   画像学習④ jpg  検証では しんのすけさんの認識OKです しんのすけさん あと少しです 待っててくださいね    ▽     noteここまでは 比較的順調に進めます 次に   Makeと連携させ LINEに結果を送ることができるかを検証  します   Makeでの設定  です LINEには   発見  検証結果     　というメッセージが送られてくるようにしました   画像学習⑤ jpg  比較対象のオレンジで検証をしました   画像学習⑦ jpg  　    note warn LINEとの連携はすぐにできましたが メッセージが届く時に　   検証結果の名前   　が来ず 　となって名前が出ず ここがかなり苦労しました   実装してみます それでは   LINEにメッセージが届くか  を検証します はい しっかりと結果が届き   しんのすけさんを素早くつかまえることに成功      ´艸｀ これで素早く見つけることができるようになりましたが 今後は早く入室してくださいね   まとめ今回は いつも遅れる一人を素早くつかまえるために ターゲットを絞りましたが 将来的には   参加者全員をあらかじめ学習しておき スプレッドシートに作成した名簿に入力して集計する   というところまでできたらいいなと考えています 夢は膨らむばかりです 画面をスクロールしながらの出欠確認は時間との闘いです 次の会議はこれを使い まずしんのすけさんを真っ先につかまえます 最後までお読みいただき ありがとうございました ,16,2022-07-25
116,116,EC2+Flask+AWS Rekognitionで画像処理を行う,画像処理,https://qiita.com/Yuma953/items/d2cb35afb0a56103f762,  画像に映っている人物の年齢と人数を取得する   AWS Rekognitionとは  AIを活用した画像認識サービス  APIのように簡単に利用可能  画像から様々な解析が可能   本記事では以下の方法については省略します  ecインスタンスの起動  nginx uwsgi flaskでのWEBサーバの構築   IAMロールを作成する  IAMのダッシュボードから ロール を選択   ロールを作成 を選択  以下の項目を選択して 次へ を選択      信頼されたエンティティタイプ AWSのサービス       一般的なユースケース EC   許可ポリシーで AmazonRekognitionFullAccess と検索しチェックを付けて 次へ を選択  任意のロール名を付けて ロールを作成 を選択   ECインスタンスにロールをアタッチする  ECダッシュボードから インスタンス を選択  AWS Rekognitionを使いたいインスタンスにチェックを付ける  アクションのセキュリティから IAMロールを変更 を選択  作成したIAMロールを選択する   flaskからAWSRekognitionに問い合わせる  EC上でbotoをインストールするpip install boto  年齢と人数情報を取得するプログラム      年齢取得            age   age      解析結果の平均を取得      人数取得  引用,0,2022-07-24
117,117,AI OCRが搭載された帳票電子化ツール AI JIMY Paperbotを使ってみた,画像処理,https://qiita.com/flskh/items/ebec1e7b4ce06ba2609f,  AI OCRについて調査    AI OCRとはOCRは 画像データのテキスト部分を認識し 文字データに変換する認識機能のことを言います 具体的には 紙文書をスキャナーで読み込み 書かれている文字を認識してデジタル化する技術です 人間は 紙に書かれている文字を無意識に理解しますが コンピューターは自動的に読み取ることが出来ません そのため 紙に書かれた文字をデジタルデータとして活用するには 一度 人間が読み取って文字に変換する必要があります これがデータ入力と呼ばれるものです しかし 単に文字を入力するという作業は非常に効率が悪く時間がかかります この作業を人間の代わりに行ってくれるのがOCRです AI OCRは OCRにAI技術を加えたものです AI技術を組み合わせることで 手書き文字も読めるようになりました     AI OCRの現状近年 中小企業では人手不足が深刻化しています そこでAI OCRを使用することでデータ入力の人件費を抑え 作業の効率を上げることが出来ます しかし 現状業績億未満の中小企業でのRPAの導入率は  そのうち AI OCRを導入しているのは と低い数字になっています その原因はコストがかかってしまう点であると考えます 従来の製品では文字を読み取る際に 読み取る項目ごとにコストがかかってしまい 項目数が多いほど料金が高くなります また AI OCRのみでは文字をデータ化することはできてもExcel等のアプリケーションに入力はできません データの入力までを行おうと思うと別途RPAツールや仕分けツールが必要になってしまいます   ツール選択背景出力したデータを他のアプリケーションに入力する際に別のツールが不要で つのツールで完結するものがないか探していました また 読み取り項目ごとに料金が加算されていくと コストがかかってしまう点が気になるポイントでした そこで見つけたのがAI JIMY Paperbotというツールでした  AI JIMY Paperbot公式サイト    AI JIMY PaperbotについてAI JIMY Paperbotはスキャンから文字認識 データ入力まで紙のデジタル化に必要な機能がすべて搭載されているとの事でした AI OCRのデメリットであったコスト面も 読み取る項目ごとではなく 帳票一枚単位のためコストを抑えることが出来ます また スキャンからデータ入力までの工程をつのツールで完結することができ 従来のAI OCR RPAツール 仕分けツールを組み合わせて使用するよりランニングコストも分の以下で運用可能です さらに AI JIMY Paperbotではリアルタイム処理やカメラと連動してスキャンをすることができます 既存製品のRPAとAI OCRを組み合わせた場合と AI JIMY Paperbotとの違いを表にまとめてみました      既存製品の組み合わせ   AI JIMY Paperbot    ツール数   RPA ＋仕分けツール   AI OCR   統合型    RPA   むずかしい   比較的簡単    リアルタイム処理   不向き   可能    スキャン機能   なし   カメラと連動可能    コスト   利用代金と導入開発費用   利用代金のみ      AI JIMY Paperbotの作成手順AI JIMY Paperbotのワークフローの作成手順は以下の通りです   取り込み スキャン   仕分け  文字認識  データ出力  RPA データ入力 それぞれの手順の詳しい説明はAI JIMY Paperbotの ナレッジベース  で確認することができました     AI JIMY Paperbotの認識率実際にAI JIMY Paperbotを使用して認識率を検証してみました その結果 印字された文字は  手書きの文字は という結果になりました 認識率は ではありませんが これはどの製品を使用しても になるのは難しいと感じました 例えば  いち やl エル の違いは前後の文脈から判断しなければなりません 手書き文字であるとなおさら区別が難しいです AI JIMY Paperbotでは認識結果を確認 修正できる設定があるため 読み取りを完全にAI OCR任せにするのではなく 人間が結果を確認すると良いと思います 人間が確認を行うと 自動化ではないと思うかもしれませんが 今までは手書き伝票を目で読みながら入力していました それに比べると 作業が認識結果の確認 修正だけになるため 大幅に業務の効率化を図れると思います    使ってみた感想    良かった点  ワークフローが簡単に作成できる 各項目の設定画面がわかりやすく プログラムを触ったことのない人でも作成することができると思いました RPAの部分もシンプルな作りで 組み立てるだけでシナリオが作成できます   データの入力可能 他の製品では別途ツールを導入しなければ入力を行えませんが AI JIMY Paperbotでは入力まで一つのツールで行うことができます RPAを使用することで条件式等も指定できるため 特定のデータだけを入力することも可能です   コスト削減 従来の製品に比べて項目ごとに課金するシステムではないため 読み取りたい項目が多い場合でもコストを抑えることができます また 別途ツールを導入する必要が無く 運用コストも従来の製品の分の以下にすることが可能です     良くなかった点 RPA機能というより転記機能でした CSVファイルを出力できるので 複雑な処理はPowerAutomateなどを使うのが良いと思います     使う際の注意点  チェックボックスモードの読み取り範囲 チェックボックスにチェックが入っているかの認識を行う際 チェックボックスの枠が読み取り範囲内に入ってしまうと結果がすべてtrueになってしまうため チェックボックスモードでの範囲設定は読み取り画像を拡大して設定するなどして枠が入らないように注意が必要です   認識結果の修正設定 認識結果をAI OCRにすべて任せるのではなく人間が確認 適宜修正を行うことでより確実にデータを読み取ることができます そのため 認識結果の修正を利用するに設定すると良いと思います    まとめAI JIMY Paperbotを使用することで紙文書のデータ化だけでなく データの入力まで行ってくれるため業務の効率化が図れると思いました 他の製品で入力まで行おうと思うと別途ツールが必要であったり コストがかかってしまったりとなかなか導入が難しいと感じました ですが AI JIMY Paperbotではつのツールでデータの入力まで完了する上 コストも抑えることができるため是非導入したいと思いました ,1,2022-07-22
118,118,DIS: Highly Accurate Dichotomous Image Segmentation やーる（Windows10、Python3.9）,画像処理,https://qiita.com/SatoshiGachiFujimoto/items/d6b150ae30126fd5e038,  はじめに画像セグメンテーションのDISやっていきまーす  開発環境  Windows PC  Python    Anaconda  導入． DIS  をクローンしますconda activate pygit clone cd DIS IS Net．requirements txtについては後で書くかもしれない U Netが動いていれば動くかも 行目行目hypar  mode      train hypar  mode      valid ．your dataset imにテストデータを置きます行目行目  実行U Netのテストデータを持ってきました 枚目の処理でGPUが足りないと怒られました  a 入力画像お疲れ様でした   参考文献,2,2022-07-21
119,119,【画像処理】PythonでOpenCVを使った色空間の変換,画像処理,https://qiita.com/spc_ehara/items/5c35d3c9900ad5e18e5c,PythonでOpenCVを使った色空間の変換今回は PythonでOpenCVを使った色空間の変換 を学んでいきます それぞれのバージョンはPython    OpenCV   になります また 今回の記事の内容はOpenCVの 公式ドキュメント  を参考にしています   OpenCVで色空間の変換処理OpenCVではcvtColorという色空間の変換処理を行う関数が実装されています 引数に変換前後の色空間を指定することで 様々な色空間への変換を行うことができます    python cvtColorcv cvtColor src  code   dst   dstCn    Parameters 説明  src 符号なしのビット or ビットまたはビット浮動小数点数 CV F の画像  code 色空間変換コード  dst srcと同じサイズと深さの画像  dstCn 出力画像のチャンネル数 の場合は元画像より自動的に算出     各チャンネルの扱いOpenCVでは各チャンネルをBGRの順で保持し 値は以下のつの型のどれかで表されます   型   範囲  ただし 最近は色空間の変換時にbitの画像がサポートされていない場合が多いため 実質CV UまたはCV Fのみとなります    変換時の注意点    非線形色空間への変換公式のドキュメントでは RGBからLuv等の非線形の色空間に変換する際は 正しい結果を得るためにcvtColorで変換する前に適切な値の範囲に正規化を自身で行う必要があると記載があります ただし 非線形色空間への変換時にbitまたはbitの画像が入力された場合は変換前に自動で適切な正規化が内部で行われるため 以下で紹介するように一部でも情報が失われると困る場合を除き 特に事前の正規化等は必要なさそうです     bit画像の変換通常 bit画像での変換では一部の情報が失われます もし 全範囲の色を必要とする場合や読み込み時に画像を変換して元に戻すような処理をする場合は bit画像を使用するほうが良いでしょう     アルファチャンネルを追加する変換変換によってアルファチャンネルが追加される場合 その値は対応するチャンネル範囲の最大値に設定されます CV Uの場合は CV Uの場合は CV Fの場合はとなります    色空間変換コードの種類についてOpenCVでは様々な色空間への変換ができるように以上の変換コードが実装されています ここではすべて紹介できないため 実装で使用する一部の変換コードとそれに対応する逆変換コードのみを以下に記載します   説明   色空間変換コード   色空間逆変換コード  OpenCVのすべての色空間変換コードは こちら  から確認できます また 変換に使用している数式に関しては こちら  から確認できます     実装実際の実装はこのようになります    入力画像   出力画像 BGR    入力画像  からグレースケールに変換するコード cv COLOR BGRGRAY を使用して グレースケール画像    出力画像  を取得することができました さらに 各変換コードを使用した結果を各チャンネルごとに以下に記載します     出力画像     出力画像     出力画像     出力画像      出力画像     出力画像     出力画像      出力画像     出力画像     出力画像      出力画像     出力画像     出力画像      出力画像     出力画像     出力画像      出力画像     出力画像     出力画像      出力画像     出力画像     出力画像      出力画像     出力画像     出力画像    色空間変換は可逆 不可逆 検証してみた色空間の変換後に逆変換を行い 変換前と逆変換後の画像を比較することで検証しました 画像同士の比較には numpyのarray equal  を使用し 差異があった場合はnumpyのisclose  を使用して その割合を算出しています bit画像とbit画像それぞれで検証に使用したスクリプトと結果をまとめた表を以下に記載します     bit画像の場合  色空間変換コード  色空間逆変換コード   差異の有無   異なるピクセルの割合   異なる値の平均幅         bit画像の場合  色空間変換コード  色空間逆変換コード   差異の有無   異なるピクセルの割合   異なる値の平均幅      異なる値の平均幅     bit bitどちらもBGRにアルファチャンネルを追加する変換のみ 変換前後の差異がありませんでした 既存のチャンネルに対しての操作が一切ないために差異が発生しなかったと考えられます それ以外の変換に関しては 少なからず差異が発生しているため 不可逆の変換だと言えます ただし bitのときとbitのときでは明らかに差異の大きさが異なりました 結果として 色空間の変換をする際にはbit bitどちらも基本的に不可逆だと考えて画像を扱うべきですが 多少の誤差を許容できる場合はbit画像での逆変換を行うことで誤差を最小限に抑えることが可能です   さいごに今回は  PythonでOpenCVを使った色空間の変換 について解説しました それでは引き続きよろしくお願いいたします 目次は以下の記事からご覧になれます ,15,2022-07-15
120,120,Vision Transformer(ViT)を転移学習で使う方法【Pytorch】,画像処理,https://qiita.com/coticoticotty/items/bd735219b84999aeeb6f,少ない画像で認識精度を高めたいときに  転移学習 は非常に有効な方法です ネットで検索したり 書籍を見てみるとCNNベースのVGGが画像認識系の転移学習でよく使われています ただ CNN系よりもTransformer系のモデルを使った方が認識精度は高くなることがあります そこで 今回の記事では Vision Transformer 以下ViT で転移学習する方法を書いていきたいと思います    note infoPython環境windowsanacondaPython     画像認識のモデルの詰まったライブラリ timm を使うVision Transformerを使う方法の一つ目は  timm というライブラリを使用するやり方 timmはpipでインストールしましょう   num classesは識別する画像のクラス数に合わせてください   重みを更新するパラメータを選択する  最終層だけでOKたったこれだけです 簡単ですね ちなみに Vision Transformerの中でも   vit base patch  というものを使っています ほかにもいくつか種類がありますが 細かい違いはよく分かっていません 点注意があります 後続の処理で学習を行う際に image sizeはで設定しましょう    torchvisionの学習済みモデルを使う最初にtimmを使ったのは pytorchにVision Transformerがないと思ったからでした 普通にあります pytorchのインストールの仕方は 公式ドキュメント  を見てください   全ての層のパラメータを訓練不可に  最終層を入れ替え デフォルトで訓練可能      の部分は学習するデータのクラス数に合わせるmodel heads     nn Linear    PytorchでもたったこれだけでOKです ちなみに   どちらの場合も cpuの場合学習にかなり時間がかかります    VGGとの比較最後におまけとして 画像のクラス分類でVGGで学習したときと ViTで学習したときの差を紹介します     VGG  VGGTransferd png  損失関数が全然収束しません     ViT  ViTTransfered png  損失関数がきれいに収束します 正解率もかなり高いです ただちょっと過学習が起きているような気も    参考,31,2022-07-14
121,121,簡単！学習済みモデルで物体検出を試す！,画像処理,https://qiita.com/KamikawaTakato/items/67abef07cd713b0b5c5f,こんにちは kamikawaです今回はpytorchの学習済みモデルで物体検出をする方法を解説します実行環境はGoogle Colaboratory  Colab です    この記事の対象読者  画像AIを試してみたい   物体検出を試してみたい    物体検出画像認識分野 CV分野 において注目を集める物体検出を試していきます 物体検出は画像の　    どこに    　    何が    　あるかを調べるタスクと言えます よくAIを紹介しているニュースなどで 映っている車や人を四角い枠で囲んでいる画像を見たことがあると思います そのイメージです    今回使用するモデル今回は物体検出の代表的なネットワークである  Mask R CNN　   正確なモデル セグメンテーションもできます を使用しますこれらのモデルを自分で学習させると時間がかかりますが pytorchのtorchvisionで学習済みモデルが使用できるので今回はそれを用いて物体検知を試していきます torchvisionが提供している訓練済みのモデルは 公式サイト  から確認できます    コード個人的にcvをよく使っているのでcvを使っています 主に描画の処理の部分  そのためやや冗長なコードになっています 実は PILだけで完結することもできます    python 画像中の物体検出 必要なライブラリのimport Colabには標準でインストールされているのでinstallの必要はなしimport cv  画像処理用ライブラリ 画像を読み込む PILの画像配列にすればモデルに入力しやすいのでPILの配列に変換 モデルをダウンロード Mask R CNNを試したい場合は 一行下のコードのコメントアウト   を外す いよいよ推論     推論に使うデバイスを選択 GPUを使用する場合は　torch device  cuda     device   torch device  cpu     transform   transforms Compose  transforms ToTensor      PILをtensorに変換にするためのインスタンスを用意  inputs   transform image   PIL→tensor  inputs   inputs unsqueeze   to device  デバイスに入力  model eval    モデルを推論モードに切り替え     confidence モデルがその推論にどのくらい自信があるか が 以上だったら   結果を表示  plt axis  off   グラフの目盛りが入るのを防ぐ  plt show    結果の画像表示させる   結果実際に実行した結果を紹介します  Unknown  png  船や車などが四角で囲まれているのがわかると思いますまた 先頭にクラス名のリストを追加し 以下略線で囲まれた部分のコードを追加すると   python これより上は略                      下のコードを追加                                                        検出した物体のクラスとコンフィデンスが表示されるようになります  Unknown  png     コード全文 画像中の物体検出 必要なライブラリのimport Colabには標準でインストールされているのでinstallの必要はなしimport cv  画像処理用ライブラリ 画像を読み込む PILの画像配列にすればモデルに入力しやすいのでPILの配列に変換 モデルをダウンロード Mask R CNNを試したい場合は 一行下のコードのコメントアウト   を外す いよいよ推論     推論に使うデバイスを選択 GPUを使用する場合は　torch device  cuda     device   torch device  cpu     transform   transforms Compose  transforms ToTensor      PILをtensorに変換にするためのインスタンスを用意  inputs   transform image   PIL→tensor  inputs   inputs unsqueeze   to device  デバイスに入力  model eval    モデルを推論モードに切り替え   セッションがクラッシュした場合ランタイムのタイプをGPUにしてみましょう 画像ファイルはもう一度アップロードし直さなければなりません GPUランタイムへの切り替え方は こちらの記事  を参考にしてくださいコードの   pythondevice   torch device  cpu   を   pythondevice   torch device  cuda  に変更してください   最後に最後まで記事をご覧いただきありがとうございました実行環境はColabratoryですので環境に合わせてコードは変更してください ファイルのパス等  Colabの使い方  はこちらの記事を参考にしてください,5,2022-07-14
122,122,【物体検出2022】YOLO最新版のYOLOv7を試してみる　〜デモから学習まで〜,画像処理,https://qiita.com/hkwsdgea_ttt2/items/ba352f6e5ef6032b5dc9,  はじめに物体検出でお馴染みのYOLOシリーズの最新版 YOLOv について 動かしながら試していきます YOLOvは年月に公開された最新バージョンであり 速度と精度の面で限界を押し広げています Google colabで簡単に最新の物体検出モデルを実装することができますので ぜひ最後までご覧ください   YOLOvとはYOLOvは年月に公開された最新バージョンであり FPSからFPSの範囲で速度と精度の両方ですべての既知のオブジェクト検出器を上回り 速度と精度の面で限界を押し広げています これまでのYOLOR YOLOX Scaled YOLOv YOLOv  DETR Deformable DETR DINO scale R ViT Adapter Bなどと比較しても速度と精度における他の多くのオブジェクト検出器を上回る結果を出しています 特にYOLOv Eオブジェクト検出器  FPS V   AP は トランスフォーマーベースの検出器SWIN LカスケードマスクR CNN   FPS A   AP よりも速度が  精度が が上回っており 畳み込みベースの検出器ConvNeXt XLカスケードマスクR CNN   FPS A   AP に対しても 速度が  精度が  上回る性能を誇ります リアルタイム物体検出器に対して パラメータを効果的に利用できる 拡張 及び 複合スケーリング 手法により リアルタイム物体検出器のパラメータを約  計算量を約 削減することができるとしています  引用： YOLOvのベンチマーク結果は以下となっています 詳細は以下のリンクよりご覧ください URL：コード：なお ライセンスは GNU General Public License v  となっています   環境構築早速YOLOvを使って動かしていきましょう ここからはGoogle colabを使用して実装していきます まずは ランタイムのタイプを変更 → ハードウェアアクセラレータ をGPUに変更して GPUが使えるようにしましょう GPUの設定が終わったら Googleドライブをマウントします 公式よりcloneしてきます    ruby python git clone 次に必要なライブラリをインポートします    ruby python cd yolov pip install  r requirements txt以上で準備完了です YOLOvを使えるようになりました   学習済みモデルを使った推論デモここからはサンプル画像で推論デモを試してみましょう まずは学習済みモデルを取得します 冒頭紹介した通り いくつかの学習済みモデルがあります ここでは最も精度が高い yolov ee pt を使うことにします 下記の通り実行してモデルをダウンロードしましょう    ruby python wget 次に推論を実施します サンプル画像として yolov inference images horses jpg が用意されています このサンプル画像を指定しましょう    ruby python python detect py   source inference images horses jpg   weights yolov ee pt   conf     img size    device 実行すると yolov runs detect exp のフォルダ内に結果が保存されます 中身を確認してみると 馬が高精度で検出できていることがわかります 簡単に物体検出を実装することができました   任意の画像で試してみる次に推論デモを任意の画像でも試してみましょう 画像を用意したら先ほどの horses jpg がある yolov inference images  に画像を保存しましょう 今回はYOLOシリーズではおなじみの画像を使用してみます 保存したら先ほど同様に推論デモを実行します    source の引数をアップロードした画像 inference images dog jpg に変えましょう    ruby python python detect py   source inference images dog jpg   weights yolov ee pt   conf     img size    device 実行すると  yolov runs detect exp に結果が保存されます   dog    jpg    任意の動画で試してみる動画でも試してみましょう  yolov inference images  に動画を保存して 同様に推論デモを実行します    ruby python python detect py   source inference images road mp   weights yolov ee pt   conf     img size    device 処理が終わると結果が動画として出力されます YouTubeにまとめましたので ぜひご覧ください 任意の動画で推論デモを簡単に実行することができました   学習データセットを用意すれば 学習も簡単に実行できます 今回はオープンデータであるマスク着用判定のセットを活用して モデルを作成してみます 学習後には各指標も出力できます   results png  作成したモデルを使ってテストをしてみます   mask    jpg  精度良く判定することができました こちらも動画にしました   まとめ最後までご覧いただきありがとうございました 年月に公開されたYOLOシリーズの最新バージョンである YOLOv について 動かしながら試してみました 精度 推論速度ともに向上しており 非常に使いやすいものになっています 物体検出の活用の幅がさらに広がりそうですね ブログではYOLOvによる学習及び推論の方法をまとめていますので ぜひ合わせてご覧ください ,43,2022-07-09
123,123,【YOLO】YOLO v4の改良点を簡単にまとめてみた【物体検出アルゴリズム】,画像処理,https://qiita.com/kindamu24005/items/9ad336354bbc2dfd14ce,   はじめに 上の記事で  YOLO v  の仕組みと  YOLO v  と  YOLO v  の改良点ついて簡単に理解しました  今回は次のバージョンである  YOLO v  に加えられた  改良点  を簡単にまとめてみたいと思います    YOLO v    vリポジトリ     vと比べて物体検出の 精度 は     vと比べて物体検出の 速さ は同等です      GPUが一つあれば 誰でも訓練し リアルタイムでの推論が可能 になりました      vとvの 精度と速度 比較の図縦軸：上に行くほど物体検出の精度が高い横軸：左に行くほど物体検出の速度が速い  yolovのdiff  jpeg     前提知識 YOLOなどの  物体検出器  はで構成されています   vdiff  jpeg     YOLO vを例  にしてみると としています     ◎YOLO vはバックボーン ネック ヘッドのそれぞれで使われるモデルの組み合わせを色々実験することで 物体検出器を良くしました   vdiff  jpeg     YOLO vの構造の組み合わせ バックボーン ネック ヘッド  YOLO v      バックボーン CSPDarknet   になっています        CSPNetって何   vdiff  jpeg    バックボーン になっています  PANではが加えられています  この  新たに加えられた部分  によって FPNよりもさまざまなスケールの物体検出が検出できるようになります        SPP Spatial Pyramid Pooling って何  ネックの部分ではこの  PAN  と  SPP  を両方使って検出精度を高めました      ヘッド YOLO v        ヘッドとしてのYOLO vって何   vdiff  jpeg   詳しくは   の記事を参照してください      オプション の変更が行われました  その他に速さと精度の向上のために   細かい部分  でが付け加えられています  論文中ではその  細かい部分でのオプション  をといったYOLO vオリジナルの言葉で表現されています        フリービー Freebies って何    note  vに加えられたフリービーの種類   複数のアンカーを同一の正解データに適用する Cosine annealingスケジューラー 最適なハイパーパラメータ等       スペシャル Specials って何    note  vに加えられたスペシャルの種類   Mish活性化関数 Cross stage partial connections  CSP  Multi input weighted residual connections  MiWRC  SPPブロック SAMブロック等 上記に書いたがvに加えられました  つまり ということです   YOLOシリーズ各バージョンの簡単な違いについてのまとめ  参考,2,2022-07-08
124,124,雀魂の画面から画像認識で対戦情報を持ってくる（Vol. 5）,画像処理,https://qiita.com/xenepic/items/4d083a37d1cb02d07aac, ≪前の記事    前回までのあらすじ鳴き牌の認識をクリア これでとりあえず雀魂の画面から 門前牌と鳴き牌を認識することができた   今回やること  リファクタリング  ツモ牌の認識 位置調整   牌の認識情報を画像で表示  捨て牌 河 の認識でお送りします   リファクタリング牌の認識がだんだんと形になってきたのは良いのですが まぁーソースコードが汚い 趣味なのでそれでもいいんですが 開発を進める前に一度整理しておかないと手のつけられないスパゲッティになることは目に見えているので 前例あり ここらで一回リファクタリングしておきたいと思います 将来的に公開するならその方がいいよね     リファクタリングとはプログラムの挙動は変えずに プログラム内部の仕様を変更すること プログラムの可読性やメンテナンスの容易性などを上げるために行われる 主な変更点は次の点です   変数名 関数名の変更 スネークケースへ   docstringを記載  処理の関数化    変数名 関数名の変更 スネークケースへ 今までjavascriptを触ることが多かったので 変数名や関数名はキャメルケースを使うことが多かったのですが pythonの公式はスネークケースの利用を推奨しているようです   関数や変数の名前関数の名前は小文字のみにすべきです また 読みやすくするために 必要に応じて単語をアンダースコアで区切るべきです 変数の名前についても 関数と同じ規約に従います  pep ja 日本語版   より    docstringを記載javadocみたいなもんですね プログラムの用途 引数 返り値などを記載します     一枚の牌の画像の種類を認識する        認識したい牌の画像イメージ        牌全種類の画像イメージ        Trueにすると templateがimageのどこにマッチしたかを示す画像を表示する            牌の認識結果 sなら一索 paなら赤五筒など          acc    float            テンプレートマッチングの一致率      内容省略docstringについては 下記の記事が非常に分かりやすかったです    Python 可読性を上げるための docstringの書き方を学ぶ NumPyスタイル     処理の関数化これは今更ですね いい感じになるように処理を分けました     結果      雀魂のブラウザをキャプチャ  雀魂 で検索すると牌譜屋などもヒットするためこのワード         sys exit  雀魂の画面をキャプチャ出来ませんでした        点棒と歯車の画像を読み込み      雀魂のメイン画面を切り取り      牌表画像を読み込み      自分の牌情報を認識随分見やすくなりました 関数内容はここでは省略しますが crop XXX系は画像の中から一部を切り抜く関数recog XXX系は牌の種類を認識する関数ですこれで多少は人様に見せられるソースになったかしら   ツモ牌の認識 位置調整 今までツモ牌の認識位置は下記画像の位置で固定していたのですが   image png  鳴きが入ると 当然ながらツモ牌の位置はズレていきます   image png  なのでそれを修正します     雀魂本体の画像からツモ牌部分の画像を切り取る        雀魂のメイン画像イメージ        面前牌の情報         pai    認識した牌の種類         acc    認識精度        指定されている場合 ツモ牌の画像をその名前で保存する        ツモ牌部分の画像イメージ cv形式 面前牌の情報を与えて そのの数に応じて切り取り位置をズラす仕様にしました   牌の認識情報を画像で表示完全に趣味ですが 面前牌 ツモ牌 鳴き牌の認識結果を見やすく表示する関数を作りました       面前牌を画像化する          牌の認識精度が下がるかunkwnonになったら終わり          画像の読み込み          画像に精度を挿入          リストに追加      ツモ牌画像読み込み      鳴き牌を画像化する          精度 以下かつupper horizonでは無い牌が来たら終わり          精度 以下かつupper horizonの牌が来たらスキップ          画像の読み込み          画像に精度を挿入          牌の向きがupper horizonなら 一つ前の画像の上に連結させる              リストに追加この関数にこれまでの認識結果を入れるとこんな感じの画像を生成できます どの牌が何 の精度で何の牌と認識できているかが一目で分かりますね ちなみに文字入れに関しては 長くなるので別の記事にまとめました    python OpenCV 画像への文字入れの 位置指定 と 縁取り を少しだけ便利にする関数作った  捨て牌 河 の認識いよいよ牌認識も大詰めです まぁいうて河も自分の前に規則正しく並んでるわけでして 多少台形補正する必要はありますが ×等分すればパリっと認識できそうです と思っていたのですが     sutehai png      なんか牌の置き方雑じゃね  そうです 雀魂先生はリアリティを追求するため 捨て牌は微妙に揺らして置いていたのです いやーんここに来てまさかの真打ち登場 まぁそこまで派手な揺れじゃないので 精度に問題がなければ良いのですが   とりあえずやってみましょう     雀魂メイン画面から 捨て牌を認識する        雀魂メイン画面のイメージ cv形式         牌全種類の画像イメージ cv形式         Trueにすると templateがimageのどこにマッチしたかを示す画像を表示する            牌の種類         acc    float            認識精度      切り出す捨て牌の大きさ      右隣 下の牌との距離      捨て牌情報用リスト      × に区切って認識する              捨て牌枚部分を切り出し              テンプレートマッチングに合うようにリサイズ              牌の種類を認識              リストに追加やってることは単純です 二重ループで順繰りに画像を切り抜いていって それぞれパターンマッチングに掛けているだけ 問題はその精度ですが   おぉー 惜しい 右上のsをsと間違えてますね 他の局でもやってみましょう またもや惜しい 右上端のpを中と勘違いしてます ん sとsは分かるけど pと中 もしかして右上の牌の認識がそもそもまずい  と思ってもう一局試してみたところ 重大な見落としに気が付きました     リーチ牌横向くやん    あと右上のpは認識合ってますね 代わりに上段のpとmが それぞれpとmになってますが 他の牌も精度が かそれ以下のものも多いので ちょっと精度としては物足りないんですよね 泣き牌や捨て牌専用の牌表画像を作っても良いかも知れません ではまずリーチ牌への対応を   といきたい所ですが 記事が長くなってしまったので 一旦次回に持越しです   まとめ今回は  リファクタリング  ツモ牌の認識 位置調整   牌の認識情報を画像で表示  捨て牌 河 の認識を行いました が 捨て牌認識の所で横向きリーチ牌の存在を忘れていたことに気付きましたというわけで次回は  捨て牌の認識 横向きリーチ牌への対応   鳴き牌 捨て牌の認識精度向上をやっていきたいと思います 相手の牌の認識は 精度をもう少し上げてからですね それでは皆様 良い週末を ,2,2022-07-08
126,126,雀魂の画面から画像認識で対戦情報を持ってくる（Vol. 4）,画像処理,https://qiita.com/xenepic/items/786f110eb5be6ccf246d, ≪前の記事  　　　　　　　　　　　　　　　　　　　　　　　　　　　　　 次の記事≫    前回までのあらすじopenCVのパターンマッチングを用いて ブラウザのスクリーンショットから雀魂本体の画像の切り出し 更には麻雀牌の識別に成功した   今回やること  牌表画像の修正  赤ドラ牌の識別  鳴いた牌の識別でお送りします   本文   牌表画像の修正前回のプログラムをテストしていると何回かうまく牌を認識できないことがあり それがpやmなど 数牌のに偏っていたんですよね 萬子は漢字の認識なので分からなくもないけど なんでpが と悩んでいたのですが 牌表画像を修正することでうまく認識できました   paiList png  お分かり頂けただろうか の隣に少し他の画像の切れ端を追加しました 多分手牌を個に切り抜く時に出来た僅かなズレが 牌表画像の端っこで悪さしてたんでしょうね    赤ドラ牌の識別openCVのテンプレートマッチングでは画像をグレースケール化するため そのままでは赤ドラの判別が難しいです ただ 牌の種類が分かってしまえば それが赤ドラかどうか判別する方法は色々考えられそうです   赤ドラ png  最も直感的なのは 牌の中に含まれる赤色画素の数ですね    Python OpenCV 赤 緑 青色の検出 HSV色空間 こちらの記事を参考に 牌画像に含まれる赤色のみを抽出して その画素数をカウントしました    py  赤色画素の個数をカウントする      HSV色空間に変換      赤色のHSVの値域      赤色のHSVの値域      赤色領域のマスク ：赤色 ：赤色以外       萬   筒   索    赤ドラ           ノーマル          萬子は 萬 の字が赤く大きいため 他より赤い画素が多いですね それでもぐらいを閾値にすれば判別できそうです こんな感じでなんとかなりました    鳴いた牌の識別    コレが予想以上に曲者です 鳴き牌識別の難点は次の通り  牌が傾いている  縦向きの牌と横向きの牌が混在している  暗槓を含めると 横向きの牌の個数も一定ではないので 通常の手牌のように切り抜いて等分して識別 といった手法は取れません   naki png  なので手順としては  牌の傾きを修正する  右から順に縦横の向きを判定しながら識別していくといったものが考えられます     牌の傾きを修正するこの傾きを修正するためには 射影変換 いわゆる台形補正を施します すなわち   naki png  上の画像を  naki png  下のように変形することで 牌の傾きを補正します こういった変換については こちらのサイトにめちゃくちゃわかりやすく載っているので 詳細を知りたい方はぜひ   Python  OpenCVで幾何変換 アフィン変換 射影変換など 簡単に説明すると 射影変換では画像を表す配列に×の変換行列を掛けることで 四角形を任意の四角形へと変形できます なので 上のようないい感じに変形ができる変換行列をまず見つける必要がありますが openCV先生はとても優秀なので 変換行列を一瞬で生成してくれる関数を用意してくれています    py  雀魂メイン画面の画像を読み込みjantamaMainImage   cv imread    jantamaMainImage png    泣き牌の部分だけ切り抜き  移動点を指定して射影変換用の変換行列を作成 src pts と dst pts は それぞれ変形前と変形後の四角形の頂点の座標です cv getPerspectiveTransform関数で合計つの座標から変換行列を算出します    py  射影変換で鳴き牌画像を台形補正cv warpPerspective関数と先ほど作った変換行列matを用いて 射影変換します 実際にやってみましょう  変換前  変換後 先程まで斜めっていた牌がまっすぐになりました 傾いて見えるような気もしますが おそらく錯視です これで後は順番に切り抜いていくだけですね     牌を切り出す麻雀で鳴いた牌は 鳴いた相手が分かるように一つだけ牌を横向きにします 上家からなら左 対面なら真ん中 下家なら右の牌を横向きします 大明槓の場合も同じルールです 暗槓の場合は つのうち外側の牌を裏側にして全て縦に置きます 加槓の場合は横向きにした牌の上にツモった牌を横にして重ねます 何が言いたいか 鳴き牌は縦横バラバラで しかもいくつ来るかも分からんってことです なので 右から順番に縦向き横向き両方抜き出し パターンマッチングの精度が高い方を採用することにします    py recogNakihaiImage  鳴き牌の種類を識別する関数def recogNakihaiImage img  paiListImage  direction  accLimit      showMatchingImage   False        横縦それぞれの場合の牌画像切り出しライン移動ピクセル数      横向き牌の場合は縦向きになるよう回転      パターンマッチングと合うようにリサイズ    img   cv resize img  dsize              パターンマッチングで牌の種類を識別    paiInfo   recogPaiImage img  paiListImage  accLimit  showMatchingImage       向き情報を追加    paiInfo append direction       カットラインの移動ピクセル数情報を追加  雀魂メイン画面の画像を読み込みjantamaMainImage   cv imread    jantamaMainImage png    泣き牌の部分だけ切り抜き  移動点を指定して射影変換用の変換行列を作成  射影変換で鳴き牌画像を台形補正  縦横そろぞれの向きの牌を認識した時 ベースラインを何ピクセル移動させるか  画像切り出しのベースライン 右端の座標       横向きの牌として識別      縦向きの牌として識別      精度の高い方を採用      牌の向きが横向きだった場合は 加槓牌があるか調べる      切り出しのベースラインを移動recogPaiImageは テンプレートマッチングで第一引数で与えられた画像がなんの牌か識別する自作関数で 識別結果と精度をリストで返します   jantamaMainImage png  上記の画像で動かしてみた結果がこちら ちゃんと認識できてますね 加槓がある場合も試してみます 良いですね 加槓された西もきちんと認識できています 暗槓はどうなるでしょうか   jantamaMainImage png  牌 unknown  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き horizon牌 unknown  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き vertical牌 m  精度    向き vertical牌 m  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き horizon牌 unknown  精度    向き horizonこれはひどい おそらくsがドラでピッカピカに光っており それで精度が著しく下がったのでしょうね 更に鳴き牌認識の場合は最初の牌の認識に失敗すると 以降の切り出し位置がズレるため 全ての牌の認識がバラバラになります sが光ってないタイミングで再度実行してみました   jantamaMainImage png  牌 s  精度    向き vertical牌 s  精度    向き horizon牌 s  精度    向き vertical牌 s  精度    向き horizon牌 s  精度    向き vertical牌 s  精度    向き vertical牌 unknown  精度    向き vertical牌 m  精度    向き vertical牌 m  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き vertical牌 unknown  精度    向き horizon今度はうまく認識できました 裏側牌を 裏側牌 として牌表画像に追加しても良いのですが 白との混同が起こりそうなので一旦見送っています ドラが光るのは仕方ないので その当たりはjavascriptの方のプログラムでうまく処理したいですね   まとめ難所である鳴き牌の識別でしたが 不安定ながらなんとかクリアできました いやーよかったよかった ピクセルを数えたり変形行列を微調整したりと 細かい作業ばかりでだいぶ疲れました 次回は  鳴いた時のツモ牌の位置の修正  捨て牌 河 の識別  相手の牌の識別あたりに手を付けたいと思っています あとソースの量も増えてきたので ここらで一回整理もしておきたいんですよね それでは    追記ソースは完成したらGitで公開しようと思ってますが 未完成の段階でも公開した方が良い ,19,2022-07-05
127,127,【車載動画予測×深層学習(GAN)】PyTorchで実装する動画予測モデルPart3,画像処理,https://qiita.com/satolab/items/31cbd38fd99d9a58b9a0,  概要自動運転が実用化に近づく中 ドライブレコーダ等の車載カメラから取得できる動画像の活用は 今日ますます重要なタスクになっていると思います 当記事では   GANを用いた予測モデル  を構築し 実際どこまでできるのか検証します 今回は Part  の結果を踏まえてモデルの改善をし その性能を評価したいと思います PartではCNNとGRUを用いたシンプルなモデル PartではConv LSTMを用いたモデルで予測を実施してみましたが その予測結果はこんあものか  という感じでした そこで今回は GAN 敵対的学習 の枠組みを導入することでモデルの予測品質向上を狙いたいと思います 以下に動画予測のシステム全体像を示します   図形    jpg  詳細に関しては Part  を確認いただければと思います また 実装は こちら  で公開しています ※当記事のGANの実装の公開はしばしお待ちください   動画予測モデル今回実装するモデルを以下に示します．  図形    jpg  前回との大きな違いは敵対的学習 GAN の枠組みを導入した点です そのため前回のConv LSTMベースのGeneratorのほかに CNNベースのDiscriminatorを追加しています 簡単にGANの説明をします GANはGenerator 生成器 とDiscriminator 識別器 が 相互に学習する手法で 画像の生成タスクで驚異的な成果を上げています 直近ではGANの枠組みを利用した DALL E OpenAI公式サイトより引用   が登場しています 今回は Generatorから生成 予測 された未来の画像に対して Discriminatorによる判別を取り入れることでGANの枠組みによる学習を実現します ここで DiscriminatorはPatch GANの枠組みで学習させるためCNNの中間出力を用いて損失を計算します ※Patch GANの詳細は pixpixの論文  をご確認ください   モデルの学習 検証上で定義したモデルを学習します データセットは前回と同様にウェブ上に一般に公開されているものを用います 学習はepoch程度行いました GANではチューニングが重要ですが 類似モデルの論文の値を踏襲しています 具体的には以下の通りとなります   バッチサイズ   Generator学習率 e   Discriminator学習率 e   画像サイズ ×  シーケンス長T   N   データ総数：約 枚以下 学習済みモデルの予測結果となります      正解データ　結果は前回より少々改善された印象です GANの導入により 画像全体の鮮明さ シャープさ が出ており その優位性が確認できます 細かな部分を確認すると Patrのモデル Conv LSTM のぼやけている部分がよい正確に描写できています  電柱 ビルの窓 看板等 一方でGANの影響からか 粗さ ノイズ の発生も見受けられ 改善の余地はありそうです   考察 まとめ今回はGANを用いた動画予測モデルを構築し 車載カメラ画像の予測を行いました 予測画像の品質は前回と比較して改善されました 一方でまだ予測品質面での改善の余地があります 予測タスクをサーベイしたところ  PredNet  の結果があまりにも美しく 泥臭くPart で努力した結果はなんなのか となってしまいました   思いつきで実装して遊んでいたのでサーベイ不足でした   次回はPrednetの検証 課題の確認等をしようと思います 最後までご覧いただきありがとうございました ,9,2022-07-03
129,129,goで画像の反転処理を行う,画像処理,https://qiita.com/icemint0828/items/32aacfebed52be4fa40d,  はじめにこんにちは icemintです 今回は自作パッケージの紹介も兼ねて goでの画像の反転処理の方法を紹介します    動作環境   使用パッケージ画像の反転処理に使用しています 画像ファイルの拡張子の変換にも使用しています    インストール  画像の反転処理	   反転方向の指定	   反転	fc Reverse isHorizon 	   保存 jpeg  gif形式での保存も可能 水平方向の反転をしたい場合は  isHorizon   true を指定し 垂直方向の反転をしたい場合は  isHorizon   false を指定します  fc SaveAs の第二引数では以下の形式で保存するファイルの形式を選択出来ます   画像の反転処理 image取得 	   反転方向の指定	   srcImgを何らかの方法で取得	   反転ファイルを介さずにimageを処理したい場合は 上記のように imgedit NewConverter で直接画像の加工が出来ます   パッケージ内の処理の説明 imgedit  パッケージ内の converter go で反転の処理を実施しています 元の画像からサイズを取得し   image RGBA に対して 各ピクセルの色情報の書き込みしています   書き込みをする際に水平方向に反転する場合は 元画像の srcSize X   x  y のピクセル情報を参照し 垂直方向に反転する場合は 元画像の x  srcSize Y   y のピクセル情報を参照しています   おわりに簡単な内容とはなりますが 画像の反転処理を紹介させて頂きました 最後まで読んで頂き ありがとうございました      前の記事 goで画像のトリム処理を行う       次の記事 goで画像の色調変換の処理を行う  ,0,2022-07-02
131,131,夏らしい画像で合成練習してみた,画像処理,https://qiita.com/dem_kk/items/4003bb0b8cd25b03ca51,   はじめに連日暑くて困っています 暑くてやる気が出ない 暑さへの対処は暑さでどうにかしようと考え 何か夏らしき画像を重ね合わせて スーパー夏っぽい画像が作れないか試してみました    コード夏らしいもので思いついたのがすいかだったので 今回はすいかに対象を絞りました スクレイピングで画像を取得して 足し合わせています    python googleからキーワード： すいか で画像をスクレイピング 画像のリサイズとRGB化 pngで拡張子統一  画像を合成する 平均化完成したのが下の画像 実の赤い部分が何となく見えるぐらいですね 夏 というよりは アート的な想像力を豊かにする画像ができました   mean png     まとめスクレイピングができるようになった すいかの画像を足し合わせたら アート作品になった   実はそこまですいか好きではない      参照,1,2022-06-29
132,132,雀魂の画面から画像認識で対戦情報を持ってくる（Vol. 3）,画像処理,https://qiita.com/xenepic/items/4aa94bece50364002497, ≪前の記事  　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　 次の記事≫    前回までのあらすじpythonのpillowを使って画面のスクショを持ってこようとするも マルチスクリーンへの対応に四苦八苦 wingui等色々触るも結局もとのpillowのパラメータを変更することで解決 雀魂画面のスクリーンショットを持ってくることに成功   今回やることいよいよこのシリーズの本題の一つ 画像認識で雀魂の画面から牌姿情報を持ってくることに着手します 具体的には  image png  この画面から   pypaiList     m    m    m    p    s    s    s    s    s    s    j    j    j    s  のような配列を生成することを目指します   手順  ウィンドウのスクリーンショットから雀魂の画面だけを切り出す  雀魂の画面から手牌の部分だけを切り出す  手牌の画像を 牌一つ一つに分けて切り出す  牌が何なのかを識別する  個 ＋ツモ牌 分繰り返し 手牌の牌姿情報を出力すると こんな感じです    ウィンドウのスクリーンショットから雀魂の画面だけを切り出す前回のコードで切り出した画像は ブラウザ本体の部分だったり 雀魂画面の外に黒い枠がついていたりします 黒い枠は雀魂本体の画面とブラウザの大きさのアスペクト比を調整するためについているものなので ブラウザの大きさによって上下に付いていたり左右に付いていたりします この部分はユーザの環境によってそれぞれ違うので そこを排除しようというのが ここでやりたいことになります それさえ排除してしまえば 雀魂の部分から牌姿情報を取り出すのは  簡単  やりやすそうですしね その手法ですが 例によってvol でも紹介したこちらの記事の手法を真似させていただきます   ネット麻雀 雀魂 をopenCVと機械学習で自動化した話   Qiita雀魂のメイン画面の大きさを誤差を抑えて推定するために 画面上のなるべく離れた二点をテンプレートマッチングでマッチさせます 上記記事では左上の点棒マークと右下の電波状況マークをマッチさせていましたが 大会の観戦画面などでは電波状況マークが表示されないため 今回は右上の歯車マークをマッチさせます 上下間の距離が近いのでちょっと心配   image png  歯車マーク うっすら透明になってるから背景の卓の画像が変わったら嫌な予感がしますが     OpenCV でテンプレートマッチングするテンプレートマッチングは上記サイトのコードをそのまま拝借してテストします テンプレート画像は次のつ  template PNG  類似度 でマッチングさせると 次のようになります うまくマッチングできてますね   私の彼女探しもこれくらいキレイにマッチングできればいいのに  ここでマッチした座標を元に 雀魂の本体画面の座標のtop  bottom  left  rightを算出します 数値は後で色々修正するかも知れませんが とりあえずこれで座標を算出した後    pyimg top bottom  left right で雀魂の画面部分だけを切り抜きできます これでテストしてみると いい感じに抜き出せてる模様 やったぜ と思ったの束の間 雀魂のブラウザを縮小してテストすると動きません それもそのはず 実はテンプレートマッチングは拡大縮小 回転にはめっぽう弱く 少し違うだけで類似度が大きく下がってしまう性質があります 今回は拡大縮小にだけ対応させれば良いので 例えば大きさの異なるテンプレート画像をいくつも用意しておいて それらをすべてマッチングさせると言った  原始的な  方法もあります が 一旦ここは特徴量マッチングを試してみましょう     特徴量マッチングとは比較する画像からまず特徴点の抽出を行い 全ての特徴点同士の距離を元に類似度を算出するマッチング方法です 特徴点としては 角や輝度の勾配などがよく用いられます 特徴量マッチングでは テンプレート画像の拡大縮小 回転に対応できるメリットがあります デメリットとしては 計算時間でしょうか テンプレートマッチングと比べて 特徴点の算出という工程を挟むので そこに時間がかかります まぁどれくらい掛かるかまだ分からないので とりあえず試してみましょう   Python OpenCVを利用したマッチング処理こちらの記事のからサンプルコードをお借りして動かしてみたのですが 結果としては全然上手く動きませんでした 歯車の画像の特徴量マッチング結果が以下となります   image png  うーん これはひどい 特徴量とは  knn近傍法のマッチ等も試してみましたが 特に改善は見られませんでした 特徴量マッチングは難しそうかなぁ というわけで   原始的な  堅実なアプローチを試しましょう cvのriseze関数を使って マッチするまでテンプレート画像を から順番に小さくしていきます         print  テンプレート画像の個数が足りません         キャプチャ画像の読み込み   グレースケール化        print  キャプチャ画像の読み込みエラーです         テンプレート画像の読み込み        print  テンプレート画像の読み込みエラーです         テンプレート画像のサイズを から ずつ小さくしてマッチング          点棒と歯車 つのテンプレートのマッチング位置を求める              テンプレート画像を縮小してグレースケール化              キャプチャ画像に対して テンプレート画像との類似度を算出する              類似度の高い部分を検出する              点棒と歯車 どちらかでもマッチしなければ次のテンプレートサイズへ              マッチング座標の中で最頻値を求める          点棒と歯車 両方とも座標が求まっていればマッチ終了        print  マッチが見つかりませんでした            マッチしたテンプレートの位置から 雀魂の画面の端の位置を算出するソースはこんな感じになりました 点棒と歯車の上下の位置が近いのでbottomの位置とかズレそうだなーと思ってたんですが   crop png  雀魂のブラウザを拡大縮小してもちゃんと本体画面だけ抜き出せておりました    雀魂の画面から手牌の部分だけを切り出す切り出すだけです 後で牌の種類を識別させる時のテンプレートマッチングをやりやすくするために 画像のサイズは一定にしてあります   myhand png    mytsumo png  手牌とツモがキレイに抜き出せてますね 最大化ではなく小さくしたブラウザのサイズでも試してみます   myhand png    mytsumo png  拡大しているせいで多少ぼやけていたり 手牌の左に若干空間があったりしますが まぁ許容範囲でしょう    手牌の画像を 牌一つ一つに分けて切り出す特に説明は不要ですね ピクセル間隔で切り分けてます   myhand png    myhand png  手牌の最初と最後の牌 最後の牌は少しズレがありますが まぁこれも許容範囲でしょう 後々影響が出れば修正します    牌が何なのかを識別するさぁ問題はここからですね まずは雀魂で使われている牌の画像種の画像が必要になります   paiList png  これを作るために CPU相手にひたすら一通を作り続けました 正確には赤ドラを含めた種ですが テンプレートマッチングでと赤を区別するのは難しいため ごちゃまぜにしています   a   a  テンプレートマッチングは最初にグレースケール化してしまうためと赤を色で区別することが出来ず また雀魂のドラ牌は 光る ため ある程度マッチ精度を低くしておく必要があるため 赤の点の有無を区別するまで精度を上げることが出来ないため 余談ですが 雀魂の牌で完全の上下対称なのは 白とpだけだそうです pやsも よーく見ると微妙に対称ではありません 後は持ってきた牌の画像とこの牌表の画像でテンプレートマッチングを行い 求まった座標から牌の種類を特定するだけです    pydef recogPaiImage paiImage  paiListImagePath        雀牌表画像の読み込み   グレースケール化        print  雀牌表画像の読み込みエラーです         識別する雀牌画像のグレースケール化      キャプチャ画像に対して テンプレート画像との類似度を算出する      類似度の高い部分を検出する      マッチング座標の中で最頻値座標を求める      座標を元に牌の種類を識別する   個 ＋ツモ牌 分繰り返し 手牌の牌姿情報を出力する今まで作った関数を順番に動かします   sample png  上の画像を読み込んで 出力は以下の通り project  python templateMatch py  m    m    m    m    p    p    p    p    p    p    s    s    s    p  うん ちゃんと認識できてますね   ブラウザのサイズを変えてもう一度     sample png  project  python templateMatch py  m    p    p    p    p    s    s    s    s    s    j    j    j    s  問題なさそう   まとめ随分長くなってしまいましたが 画像から手牌を認識して持ってくることが出来ました ただ このプログラムだと赤ドラを認識できないんですよね これはまた次回の課題としたいと思います 他にも   鳴いた場合の牌  自分以外のプレイヤーの手牌  プレイヤー名  点棒 本場  点数状況  自風  ドラ等の認識も 次回以降順を追って対応していきたいと思います それでは   参考  numpyで最頻値を求める 次の記事≫  ,3,2022-06-29
133,133,Laravel 画像のプレビュー表示,画像処理,https://qiita.com/homifu/items/e759c0dde2d3c99a9bb3,  概要前回  Laravel UI登録画面で画像保存処理を実装   からの続きです 画像を選んだときにプレビューを表示させる機能です   スクリーンショット       png    目次   コントローラーの編集   Chapter   コントローラーの編集  登録画面 View の編集                             アイコン画像プレビュー表示                                  アイコン画像       JS読み込み部分の追記〜   min jsをJS記載部分に追加する        アイコン画像プレビュー処理       画像が選択される度に この中の処理が走る        icon   on  change   function  ev              このFileReaderが画像を読み込む上で大切           ファイル名を取得           画像が読み込まれた時の動作を記述今回はこれで以上です 画像ファイルを選択したときにプレビューが表示されます ,1,2022-06-26
134,134,【YOLOv5】で指文字判定を試す！！【物体検出アルゴリズム】,画像処理,https://qiita.com/kindamu24005/items/7ad0514f44d1f245b596,   はじめに 上の記事で   YOLOの学習済みモデル  を使用しました    note infoさらに  こちらの方の記事  に影響されて   自分でYOLOの指文字を判定する学習モデルを作ってみます          指文字判定AIを作る  yolo実装  jpeg   こんなイメージで     ◎やることの流れ データセットの準備 ラベリングを行う 学習させて物体検出モデルを作る 作ったモデルでyoloを実行     データセットの準備 Windowsカメラアプリの連写機能を使って指文字の 撮影するに設定しました  こんな感じで 今回は写真を撮りました       ラベリング を行う       ラベリングって何        ラベリングツールのダウンロードと使い方    というソフトを使います   をダウンロードします  解凍したwindows v  内のdataフォルダに入ってる 今回はをテキストファイル predefind class に書きます   change class png   のアプリを開きます  開いたら左下のにします   labelling change format png   左上の  Open Dir  で画像が入っているフォルダを開きます  認識したい部分を四角で囲います    labellmgのボックス  から  何として認識したいのか  を選びOKを選びます  左の  Save  を押します   labelling png   保存されたラベル  は画像と同じフォルダ  に入ってます      ◎保存されたラベルの中身   text こんな感じの  テキストファイル  で保存されています  それぞれの数字の意味は  とを表しています    jpg     note warn今回の例での  クラスの種類  と  クラスの値  の対応関係  クラスの  種類    クラスの  値                                              学習させて物体検出モデルを作る       トレーニング train フォルダ 検証 Valid フォルダを作る yolovフォルダ内のdataフォルダの中にの名前でフォルダを作ります   create train valid png         trainフォルダとvalidフォルダの中にそれぞれimagesフォルダとlabelsフォルダを作る trainフォルダの中にの名前でフォルダを作ります   create images labels in the training png   validフォルダの中にもの名前でフォルダを作ります   create images labels in the valid png         撮った写真と保存したラベルをトレーニング用と検証用に分ける に入れます    jpg    imagesfolda png    labelfolda png         yamlファイルを作る クラスの数nc   クラスの名前names    a   i   u   e   o   上記のコードを  メモ帳等  で書き という名前で保存してください  data yamlをに入れてください   yamlfile png         Wand aiで学習の過程を可視化する   note info  Wand ai  とはモデルの学習過程を  可視化してくれるwebサイト  です      ◎使い方      学習実行前に     yolov C ¥Users¥ユーザー名¥yolovpip install wandb 学習実行前にターミナルで上のコマンドを打ち インストールします       学習実行後に 学習実行後に上の状態で停止するので キーボードの  を打ちます  その後  を押して プロジェクトを作ります   wandailogin png   上のページの  赤四角の部分をコピー  します  コピーしたものを上の   to quit    の後に  貼り付けます    貼り付けたら学習がスタートします        学習の実行     yolov C ¥Users¥ユーザー名¥yolovpython train py　  data data yaml   batch size    epochs 　  name　handsign 上記のコマンドを実行します      〇コマンドの意味  コマンド   意味                                              train pyを実行します        学習データの情報を記載した  yamlファイル  の位置を書きます  今回はyolovフォルダの中なので   ファイル名だけでOK            バッチサイズ  を    に設定します     を    に設定します        作ったモデルでyoloを実行       学習されたモデルを確認  check model png   学習が終わるとという物が出来上がっています   move bestpt to olov png   そのうちの中に入れます        分析したい画像を用意  prep images png   適当に指文字の写真を撮って  imageフォルダの中  に入れます  busとzidaneは元から入っている画像です         実行           yolov C ¥Users¥ユーザー名¥yolovpython detect py   source data images    weights best pt 上記のコマンドを打ちます        結果 結果は  detectフォルダ内  に  expフォルダ  が自動で作られて  その中に入っています    間違い    正解  正解  正解     あってるのもあるけど なんか普通に間違ってるのがある      note warn 単純に  データ不足  が原因だと思われます    角度  とか  背景  とか  大きさ  とかもっと  バラエティ豊富なデータを大量に集める  ことができれば改善できるかもしれないです        学習過程の可視化  acc png   学習回数を重ねるごとに精度がに近づいています   loss visualization png   学習回数を重ねるごとに損失がに近づいています    まとめ   とにかくデータの収集とラベリングが大変すぎました             収集したデータは  自分の手だけ  だから と思います  つまり  私の手なら間違いなく検出はできる  と思いますが   他の人の手だとうまく検出されない可能性があ理想です    どんな手でも判別できるようになるのは   全世界の人の手をデータとして集める        連合学習 データの収集不足 データの偏りがなくせそうな技術,1,2022-06-24
135,135,goで画像のトリム処理を行う,画像処理,https://qiita.com/icemint0828/items/884dce09413bb8d2570d,  はじめにこんにちは icemintです 今回は自作パッケージの紹介も兼ねて goでの画像のトリム処理の方法を紹介します    動作環境   使用パッケージ画像のトリム処理に使用しています 画像ファイルの拡張子の変換にも使用しています    インストール  画像のトリム処理	   切り取り開始位置の指定 px 	   サイズの指定 px 	   トリム	fc Trim left  top  width  height 	   保存 jpeg  gif形式での保存も可能 切り取り開始位置は画像の左上の位置を指定して下さい 元画像の範囲外を選択すると 正常にトリム処理が出来ません サイズの指定は切り出したい画像のサイズを指定して下さい  fc SaveAs の第二引数では以下の形式で保存するファイルの形式を選択出来ます   画像のトリム処理 image取得 	   切り取り開始位置の指定 px 	   サイズの指定 px 	   srcImgを何らかの方法で取得	   トリムファイルを介さずにimageを処理したい場合は 上記のように imgedit NewConverter で直接画像の加工が出来ます   パッケージ内の処理の説明 imgedit  パッケージ内の converter go でトリム処理を実施しています 引数からサイズを取得し   image RGBA に対して 各ピクセルの色情報の書き込みしています 書き込みをする際に元画像の x left    y top を参照することで左方向 上方向の切り抜きを実施し その位置からの width    height を取ることで右方向 下方向の切り抜きを実施しています   おわりに簡単な内容とはなりますが 画像のトリム処理を紹介させて頂きました 最後まで読んで頂き ありがとうございました      前の記事 goで画像のリサイズ処理を行う       次の記事 goで画像の反転処理を行う  ,0,2022-06-24
136,136,雀魂の画面から画像認識で対戦情報を持ってくる（Vol.0）,画像処理,https://qiita.com/xenepic/items/f25a97735a90546d1451,  はじめに配信で雀魂の大会などを開催してると 今の点数状況などをチームの点数等と合わせてかっこよく表示したり 有効待ち牌を表示したりできたらかっこいいのになぁと思うことが多々ありました しかし 雀魂がそういう類のAPIを公開しているという情報は見つかりませんでした  牌譜屋  では対戦状況を取得しているし ソースコード  も公開しているのでAPI的な感じでデータを持ってくることは可能なのだろうけど なんかちょっとグレーゾーンっぽいのと タイムラグがありそうだったので 今回は画像認識で取得してみようと思いました ただ そっちの知識はほぼ皆無なのでめちゃくちゃ手探りの軌跡をまとめたものになります 突っ込みながらご参考までにどうぞ   目標目標としては 次のつの情報を雀魂の画面から持ってくること   自分の牌姿 観戦モードなら相手の牌姿も   捨て牌  点数状況  風 本場  対戦者名  最終目標イメージとしては  janntama PNG  この画面から  janntama PNG  こんな感じの情報を生成できるようになること  点数とか間違ってたらすんません   開発環境使用言語ですが 最終的にはOBSで点数などを表示したいと思っているので HTMLに組み込みやすいnode jsで開発していこうと思います まぁ 一番使い慣れてるからという理由も大きいですがｗ  構想さて 何から初めていいか分からなかったのでまずは 雀魂　画像解析 とかで検索 ポチッ 一番最初にヒットしたのがこちらの記事でした   ネット麻雀 雀魂 をopenCVと機械学習で自動化した話   Qiita openCV というライブラリのテンプレートマッチングという機能を使って 画像から牌姿情報を得ることに成功しています えーすご 私のやりたいことそのまんまじゃん  一応openCVとテンプレートマッチングについて軽く触れておくと    openCVとは  あのインテル様が開発した 画像や動画に関する処理をまとめたライブラリ   なんと無料で使うことができる さすいんてる    テンプレートマッチングとは  画像認識技術の一つで 画像Aの中から画像Bと似てる部分を探し当てることができる    つまり 雀魂の画面の中から一索の画像を検索したりすることができる すげーこれは勝ったわ ガハハと思ったのも束の間 こちらの記事 Pythonで書かれておりました まぁ画像処理とかPython強いもんな   ただ調べてみるとopenCVは別にPython専用のライブラリではない模様 じゃあjavascriptでもopenCV使えるんでね  と思って検索して見つけたのがこちら   OpenCV jsをJavaScript Node jsで使ってみたどうやらnode jsでもopenCVが使えるらしいので こちらの記事のサンプルを動かしながら次のような手順で進めていこうと決めました   雀魂の画面のスクリーンショットを持ってくる  OpenCVのテンプレートマッチングでスクショから牌姿情報を持ってくる  ただしつ目の記事で テンプレートマッチングを使用した場合判定に 秒ほどかかると合ったので 処理時間によってはテンプレートマッチングではなく別の手法を検討します   実装     スクリーンショットの取得 node js スクリーンショット 等で検索すると  Puppeteer という単語がめちゃくちゃヒットします 読み方は パペティア だそうです 読めねー    Puppeteerとは  Google先生が作ったNode js用のライブラリ   同Google先生作のWebブラウザ Chrome に備わっているヘッドレスブラウザ機能を利用したライブラリで   Webページの操作や入出力処理 ページの画面キャプチャなどができる     ヘッドレスブラウザとは  ブラウザを開かないブラウザ は    要するにプログラム中の操作のみでブラウザに関する様々な処理ができるため   Webページのテストの自動化等で使用される このPuppeteerライブラリを使うと   ヘッドレスブラウザを起動する  ヘッドレスブラウザで目的のURLに遷移する  そのページのスクショを撮るという感じでプログラムが動くのですが 私が欲しいのは今 自分が開いて麻雀を打っている画面のキャプチャなので Puppeteerだとそれは出来ないんですよね 実際   Node jsライブラリのPuppeteerでスクショしてみた   Qiitaを参考に雀魂のスクショを撮ってみましたが 真っ黒な画像が得られただけでした  grabzit は有料なので見送ります で 続く解決策ですが 現在見つかっておりません   笑ただ teratilで質問してみたところめちゃくちゃ親切な回答者様が色々教えてくださったので こちらをもとに進めていきたいと思います   Node jsで雀魂の画面キャプチャを保存する方法   teratail次回に続く    のか   追記 続きました： 次回  ,8,2022-06-22
137,137,goで画像のリサイズ処理を行う,画像処理,https://qiita.com/icemint0828/items/3c854d8f0361abc36b55,  はじめにこんにちは icemintです 今回は自作パッケージの紹介も兼ねて goでの画像のリサイズ処理の方法を紹介します    動作環境   使用パッケージ画像のリサイズ処理に使用しています 画像ファイルの拡張子の変換にも使用しています    インストール  画像のリサイズ処理 サイズ指定 	   サイズの指定 px 	   リサイズ	fc Resize width  height 	   保存 jpeg  gif形式での保存も可能 比率は保持されないため 指定した width と height の画像が出力されます  fc SaveAs の第二引数では以下の形式で保存するファイルの形式を選択出来ます   画像のリサイズ処理 比率指定 	   比率の指定	   リサイズ	fc ResizeRatio ratio 	   保存 jpeg  gif形式での保存も可能  ratio は以上も指定出来ますが 処理に時間がかかるかもしれません   画像のリサイズ処理 image取得 	   比率の指定	   srcImgを何らかの方法で取得	   リサイズファイルを介さずにimageを処理したい場合は 上記のように imgedit NewConverter で直接画像の加工が出来ます   パッケージ内の処理の説明 imgedit  パッケージ内の converter go でリサイズ処理を実施しています 引数からサイズを取得し   image RGBA に対して 各ピクセルの色情報の書き込みしています   書き込みをする前に元画像と書き込み先の画像のサイズの比率を取得しています 元画像が px  px  リサイズ後の画像のサイズが px  px の場合 比率は      となります 例えば  px  px の地点のピクセルの色情報の書き込みをする際には 元画像の px  px の地点のピクセルの色情報を参照しています ぴったりとした比率にならない場合は math Roundで丸めていますので 少し歪なリサイズ処理が行われるかもしれません ratioを元にサイズを取得し   image RGBA に対して 各ピクセルの色情報の書き込みしています  また 書き込みする際のピクセルの色情報の参照もratioから比率を取得して使用しています   おわりに簡単な内容とはなりますが 画像のリサイズ処理を紹介させて頂きました 最後まで読んで頂き ありがとうございました      次の記事 goで画像のトリム処理を行う  ,0,2022-06-22
139,139,7 | Linear Spaceワークフロー,画像処理,https://qiita.com/UWATechnology/items/1d9993b8e7e87b38f170,   Gamma SpaceとLinear SpaceGamma Correctionが存在するため レンダリングでの計算に少し問題があります たとえば sRGB画像の非線形入力があり 操作が線形空間で実行されると 最終的な出力結果が目的の結果から外れます 実世界の物理法則は線形空間で計算されるため Shaderでの計算も線形空間で計算されます たとえば 前の曲線によると 実際の物理的な空間の光の強度が ですが 人間の目はそれを の明るさとして認識します  に従って計算することと に従って計算することによって得られる結果は 当然異なります   image png   Unityエンジンのさまざまなスペースの照明表示したがって 現実の世界の色をより現実的かつ正確に復元するために より良い制作プロセスは 入力 計算 および出力を線形空間に統合させることです 次にGamma Correctionが実行され 最後にディスプレイに表示されます まず 入力データが線形空間に配置されていることを確認する必要があります sRGBテクスチャの場合 Remove Gamma Correction  の累乗の操作を実行 を実行してLinear Spaceに戻す必要があります 次に Shaderを介して計算を実行します この部分の計算は線形空間にあります 次に 結果に対してGamma Correctionが実行され 最終的にディスプレイから出力されます これにより 現実の世界に近い結果が得られます グレースケール値を例として取り上げます Shader計算後の出力結果が入力結果の 倍であると仮定します Gamma Spaceの下にある場合 Shaderで計算された値は        であり ディスプレイに表示されます：        Gamma Spaceでのレンダリング結果が暗いことがわかります Linear Spaceでのレンダリング結果は少し明るくなります 図 の結果に示されているように ガンマスペースが選択されている場合：Unityは入力と出力結果に対して何の処理も行いません 図 に示すようなRemove Gamma CorrectionとGamma Correctionは発生しません 必要に応じて 手動で実装する必要があります Linear Spaceが選択されている場合：sRGBテクスチャの場合 Unityはテクスチャサンプリングの前に自動的にRemove Gamma Correctionを実行します Linearテクスチャにはそのようなステップはありません 出力する前に Unityは自動的にGamma Correctionを実行してからディスプレイに出力します したがって テクスチャの場合 Unityにテクスチャのタイプと色空間を教える必要があります   image png    image png  いくつかの特別なテクスチャタイプは Normal Map Light Mapなど デフォルトでLinear Spaceで行います Defaultタイプのテクスチャには sRGBオプションがあります チェックされている場合は ガンマ 空間にあることを意味し チェックされていない場合は Linear Spaceにあることを意味します さらに Linear Spaceでは Shader Labでの色入力はデフォルトでsRGB色になり Remove Gamma Correctionが自動的に実行されます 頂点カラー属性データについては 手動でRemove Gamma Correction処理を実行する必要があります 場合によっては Float変数でRemove Gamma Correctionもする必要があります その時 ShaderLabで Gamma プレフィックスを使用する必要があります モバイルサポート：Androidでは LinearはOpenGL ES  およびAndroid 以降でのみサポートされ iOSではMetalのみがサポートされます ハードウェアデバイスの開発に伴い より現実的な効果を追求するために ますます多くのプロジェクトがLinear Spaceの下で作業することを選択しています   Unrealの場合  image png   Unrealデフォルトの照明表示 Unrealの場合 原理は同じです 関連するTextureのDetailsページにsRGBオプションがあります   image png  Unityと同様に 画像のストレージカラースペースを表します 同様に 画像サンプリングを実行するときは 対応するサンプリングタイプを選択する必要があります Colorタイプ サンプラーがsRGB色空間で画像をサンプリングすることを示します   image png  Linear Colorタイプ サンプラーがLinear Spaceで画像をサンプリングすることを示します 異なる色空間では 効果が異なります 平行光が追加されます    image png  sRGBがチェックされていない場合 つまりこの画像がLinear Spaceの画像として扱われている場合 結果はより明るくなることがわかります  Unityと比較すると Unrealの設定はより便利であり Unrealは色入力に対応するスペースを設定することもできます   image png     まとめ今日 レンダリングの要件がますます高まっており よりリアリズムのある物理ベースのレンダリング PBR の出現により より正確な計算結果を得るために Linear Spaceでのレンダリング計算が徐々に多くのチームに採用されています  UnityとUnrealのつの主要なエンジンもLinear Spaceワークフローをサポートしています 開発者は Linearワークフローを理解し データの入力 計算 および出力をどのように変更する必要があるかを理解する必要があります この方法でのみ データ計算の精度を保証できます 関連する知識については 以下をお読みください  HDRと色彩管理   HDRと色彩管理 光 色 色度図    HDRと色彩管理 色空間    HDRと色彩管理 SDRとHDR    HDRと色彩管理 HDR標準とACES    HDRと色彩管理 ゲームにおけるHDR  実際のプロジェクトでは Gamma SpaceとLinear Spaceの開発経験について 次の記事を参考してください  ガンマから線形色空間へのUnityプロジェクトの経験共有  中国語注意 推奨： UnityのGamma Correctionと線形ワークフローについて話す  中国語注意 UWA Technologyは モバイル VRなど様々なゲーム開発者向け   パフォーマンス分析  と  最適化ソリューション  及び  コンサルティングサービス  を提供している会社でございます 今なら UWA GOTローカルツールが  日間に無償試用  できます  よければ ぜひ UWA公式サイト：UWA GOT OnlineレポートDemo：UWA公式ブログ：,1,2022-06-17
140,140,6 | Gamma Spaceの起源,画像処理,https://qiita.com/UWATechnology/items/41ea3ddd4362a2b6d3f3,   前書き本節に触れる後処理効果は色の調整に関連しています 特定の後処理効果を勉強する前に 色の最終出力表示に関連する基本的な知識を理解する必要があります それでは Gamma Correctionの起源 プロジェクト開発におけるColor Spaceの特定のアプリケーション UnityおよびUnrealエンジンの知識など Gamma SpaceとLinear Spaceについて学習します 知識学習に入る前に 概念を明確にする必要があります 今までGamma Spaceに関する議論は 限られた階調予算に基づいています 具体的な内容については 後ほど詳しく説明します    CRTディスプレイ理論それほど遠くない過去には 当時の主流のディスプレイはCRT Cathode ray tube ディスプレイでした そのフルネームは よく見られる大きくて重たい物のブラウン管 陰極線管 ディスプレイです   image png  CRTディスプレイの場合：電圧を倍にすると 明るさは倍になりません 出力の輝度と電圧は線形ではなく 輝度の増加は電圧の増加の 乗にほぼ等しいため  はCRTディスプレイのガンマ値と呼ばれ CRTディスプレイの色はGamma スペースにあります   image png  明るさと電圧の関係電圧が直線的に変化する場合 現実の世界と比較して CRTディスプレイの明るさは暗い領域でゆっくりと変化するため 暗いデータの範囲が広くなり 全体的な画像の色が暗くなります   image png  そして 私たちの目標は 現実世界と同様の明るさを見ることができることです CRTディスプレイはGamma  空間にあるため ディスプレイに入力する画像の明るさは Gamma  の影響を平均化除去する必要があります この操作はガンマ補正 Gamma Correction と呼ばれ 実際には 乗の操作にほぼ等しいため 色はガンマ 空間にあります   image png  Gamma Correctionこれにより 実空間の色に似た色に復元できます   image png     人間の目の階調知覚の理論一方 現実の世界では 光の強度がからに増加すると 明るさはそれに比例して増加します しかし 人間の目に知覚される明るさが直線的ではありません この点は ウェーバー フェクナーの法則で示されています 外部刺激に対する人々の知覚は直線的ではなく 感覚量の増加は物理量より遅れ 物理量は等比数列で増加し 心理量は等差数列で増加します つまり 人間の目で知覚される明るさがレベル増加すると 光の強度は実際には数倍に増加します   image png  この現象に簡単な数学的シミュレーションをすると 人間の心理における明るさの知覚は Gamma  空間曲線に似ています   image png  人間は闇の知覚に対してより敏感であることがわかります たとえば 同じ部屋で 暗闇の中でライトを点灯することと 個のライトをオンにした後で別のライトをオンにすることとは まったく異なる明るさの変化を人々に感じさせることができます 現実の世界では 光の強度の増加量は同じですが…今日の主流の画像はチャネルあたりビットであるため 各チャネルにはの階調があり 階調数は比較的少なくなります 人間は闇の知覚に対してより敏感であるため より暗い明るさを保存するためにより多くの階調が必要です 図の対応関係から 実空間での光の強度は と仮定し 人間の目は の明るさとして感じ このデータを番目の階調 中間の階調 とします 人間の目で知覚される明るさについては 階調数を均等に分割しますが 実空間では 〜 の光強度間隔で割り当てられる階調数が多くなります このように階調が割り当てられます 実空間での強度が の明るさは 人間の目が の明るさとして認識し 人間の目が認識した明るさをデータとして記録します 人間の目で と認識した明るさをディスプレイに送信して表示します CRTディスプレイの場合  の電圧はGamma  で調整されます Gamma  は 人間の視覚のニーズを満たす の明るさを示しています これは Gamma色空間の生成に関する主流の見解でもあります すばらしい偶然の一致であり CRTディスプレイのエラーは人間の目の知覚と一致します 実際 人間の目の明るさの知覚曲線の変換は Gamma 空間曲線ほど単純ではなく より複雑である可能性があります この偶然の一致により 人間目の明るさの知覚曲線をシミュレートするために比較的単純で実行可能な方法が抽出された可能性があります したがって 現在の表示電圧と明るさが基本的に線形であっても このルールに従い 表示Gamma値を設定すると より多くの暗い領域の明るさが表示できるようになり 人間の目に知覚された暗い領域の階調数は 明るい領域の階調数に等しくなります ただし 技術の進歩により いつか各チャンネルがビットになり 各チャンネルでの階調が可能になります 階調数が非常に多いため 表示できる明るさは十分なので Gammaが必要なくなります 保存された結果は対応する明るさとして直接表示できます これは この記事の冒頭で述べた前提です 階調の予算は限られており 明るさの範囲を確保するために適切な割り当てが必要です    sRGBCRTディスプレイと一致させ 変換せずにチャネルあたりビットのディスプレイに画像を表示できるようにするために HPとMicrosoftは年にStandard Red Blue Green色空間 sRGB色空間 を設計しました 今のほとんどの画像ファイルはこの色空間にあります sRGB色空間は Gamma が配置されている空間に対応します これは 物理空間の色に対してガンマ補正を実行するのと同じです 物理世界の色は Gamma  が配置されている空間に保存されます ディスプレイGamma で調整した後 実際のデバイスに表示される結果は 実空間の色と一致します    まとめこのセクションでは Gamma Spaceの起源に関するつの主流の見解を紹介し 対応する計算方法とその背後にある実際的な重要性を理解しました 次のセクションでは それが私たちの仕事にどのように影響するか そしてそれを適切に適用する方法について話します UWA Technologyは モバイル VRなど様々なゲーム開発者向け   パフォーマンス分析  と  最適化ソリューション  及び  コンサルティングサービス  を提供している会社でございます 今なら UWA GOTローカルツールが  日間に無償試用  できます  よければ ぜひ UWA公式サイト：UWA GOT OnlineレポートDemo：UWA公式ブログ：,0,2022-06-17
141,141,5 | Radial Blur,画像処理,https://qiita.com/UWATechnology/items/19f105b6c2ce231cf1ec,   基本知識Radial Blurは 中心から外側に放射状に広がるストライプ状のぼかしとして現れる一般的な視覚効果です   image png  レースゲームやアクションの特殊効果で 高速モーションの視覚効果やカメラを突然ズームインしたときの衝撃的な効果を強調するためによく使用されます   image png  Need for Speedにおけるラジアルブラー効果Radial Blurの基本原理は他のぼかし効果と同じです 周囲のピクセルと元のピクセルの色の値が一緒になってピクセルの色に影響を与え ぼかし効果を実現します Radial Blurの効果は 中心から外側に放射状に広がる形状であるため 選択したサンプリングポイントは 中心点とピクセルポイントを結ぶ延長線上に配置する必要があります   image png  図に示すように 赤は中心点 青は現在処理中のピクセル 緑はサンプリングポイント 赤の矢印の方向は中心点から現在のピクセルまでの延長線の方向です ピクセルが中心点から離れるほど ぼやけます したがって サンプリングポイント間の距離は大きくなります 他のぼかし効果と同様に サンプルポイントが多いほどぼかし効果は良くなりますが コストが増加します    Unityの実装上記の原理によると UnityでBuild inパイプラインを使用して Radial Blur効果を実装します まず 画像の中心を中心点として設定し サンプリングステップサイズを計算します このように ステップサイズ Vector は中心点からピクセルまでの距離に正の相関があり 中心点から遠いピクセルはよりぼやけます 変数 BlurRadiusを追加して サンプリングステップサイズを調整し ぼかしの強さを制御します サンプリング 色の重ね合わせ 平均値を求めることで計算を行い または必要に応じて異なる重みを選択することもできます 変数 SampleCountを設定して サンプリングポイントの数を制御します サンプリングされるピクセルが多いほど ぼかし効果はよくなりますが コストも大きくなります 効果は図のようになります   image png  動的に変数 BlurRadiusを調整し 効果は次のとおりです   rb gif     最適化Radial Blurは 前述のBlur最適化と同じ方法で最適化されます  DrawCallを追加する代わりに ダウンサンプリングによって解像度を低下させ それによって画像サンプリング操作が減少し 最適化の目的を達成します UWA Technologyは モバイル VRなど様々なゲーム開発者向け   パフォーマンス分析  と  最適化ソリューション  及び  コンサルティングサービス  を提供している会社でございます 今なら UWA GOTローカルツールが  日間に無償試用  できます  よければ ぜひ UWA公式サイト：UWA GOT OnlineレポートDemo：UWA公式ブログ：,1,2022-06-17
142,142,4 | Silhouette Rendering,画像処理,https://qiita.com/UWATechnology/items/a0915d51782d2a002208,   基礎知識シルエットレンダリングは アウトライン Outline とも呼ばれる一般的な視覚効果で 非写実的レンダリングでよく見られます Borderlandsシリーズのようなコミックスタイルの強いゲームでは 多くのアウトラインレンダリングが使用されます   image png  Borderlandsシリーズのスクリーンショット一般的な方法のつは ジオメトリ空間で シーンが正常にレンダリングされた後 輪郭を描く必要のあるジオメトリを再レンダリングすることです ジオメトリは 最初にその頂点位置を法線方向に沿って移動することによって拡大されます 次に 拡大されたジオメトリの背面のみを残して 正方向の面をカリングし アウトライン効果を形成します 効果は図のようになります   image png  ジオメトリ空間に基づいた方法は 本節で検討しません 画面スペースに基づく別の後処理スキームがあり キーリンクはエッジ検出 Edge Detection です エッジ検出の原理は エッジ検出演算子を使用して画像に対して畳み込み演算を実行することです 一般的に使用されるエッジ検出演算子はSobel演算子で 水平方向と垂直方向の両方の畳み込みカーネルが含まれます   image png  色 深度 その他の情報など エッジにある隣接するピクセル間で特定の属性に明らかな違いがあると見なすことができます  Sobel演算子を使用して画像を畳み込むと 隣接するピクセル間のこれらの属性の差を取得できます これは勾配 gradient と呼ばれ エッジ部分の勾配値は比較的大きくなります ピクセルの場合 水平方向と垂直方向にそれぞれ畳み込み演算を実行して つの方向の勾配値GxとGyを取得し それによって全体的な勾配値を取得します：   png  フィルタリングするしきい値を設定し エッジに配置したピクセルを保留し それらに色を付けてアウトライン効果を形成します たとえば 色の変化が少ない次元オブジェクトの場合 アウトラインの描画には深度情報が使用され 効果は次のようになります   image png     Unityの実装上記のアルゴリズムに従って UnityでBuild inパイプラインを使用してアウトライン効果を実装し 静止画像を選んで色のプロパティの違いに応じて処理します まず Sobel演算子を実装します half SobelUV       half     half    half    					half     half    half    					half      half     half       half SobelX               half SobelY               演算子に従って画像をサンプリングし fixedタイプのカラー値を取得します RGBAというつのチャネルが含まれているため いくつかの重みを設定して明るさの値を計算することができます たとえば 平均値の計算を選択します 明るさの値と演算子に従って勾配を計算します 変数edgeの値がに近いほど 境界と見なされます 次に 描画します 輪郭のみを描画できます 効果は図のようです   image png  元の画像をブレンドしてアウトラインすることもできます 効果は図のようになります   image png  元の画像は次のとおりです   image png  アウトラインの色を調整して 効果は図のようになります   image png  UWA Technologyは モバイル VRなど様々なゲーム開発者向け   パフォーマンス分析  と  最適化ソリューション  及び  コンサルティングサービス  を提供している会社でございます 今なら UWA GOTローカルツールが  日間に無償試用  できます  よければ ぜひ UWA公式サイト：UWA GOT OnlineレポートDemo：UWA公式ブログ：,0,2022-06-17
143,143,3 | Lens Flare —— Streak,画像処理,https://qiita.com/UWATechnology/items/11c931b89b753ff95eac,   基礎知識撮影際 強い光源からの光が多くのレンズで生成されたレンズ群を通過する際に反射や散乱が発生し 他の入射光と同じ方向を保っていない光がフレアを生成することがあります    jpg   右上隅の明るい光により 画像に目立つハローが生じます もともと技術的な欠陥で画像が歪んでいたのですが 意外と特殊効果が出て立体感が増し 雰囲気を盛り上げることができました 写真の世界では いくつかの効果を生み出すために特別なフィルターが作られています 同様に これらの効果はゲームでシミュレートされ 画質を向上させ 雰囲気を高めます 次の章では レンズフレアによって生成されるいくつかの効果を紹介し それを実装します このセクションでは Lens Flare効果の一つ Streak効果を紹介します    png  図の中心に 顕著な長いフレアがあります撮影界にはStreak Filtersという特別なフィルターがあります ストリークフィルターは 輝点を中心に一連の平行線を放射し 放射効果をもたらします    jpg   写真のまぶしさによるStreak効果 ゲーム業界で Streakはよく見られた効果として 輝点のハイライトを表示して雰囲気を引き立たせます    png   Mass Effect のLens Flare Streak効果 このセクションでは Dual Blurアルゴリズムのアイデアに基づいた比較的単純な方法を使用してそれを実現します Dual Blurアルゴリズムでは ブラー効果は ダウンサンプリングで画像を縮むことと アップサンプリングで画像を拡大することを繰り返してぼかし効果を取得します このように 周囲のピクセルも当ピクセルの色の一部を取得したようになって 色のぼかし効果も実現されています この考え方に従って 一方向にアップ ダウンサンプリングを繰り返すことを選択できます    Unityの実装    アップおよびダウンサンプリングまず アップサンプリングとダウンサンプリングのプロセスを実装するには ハイライトポイントを一方向に伸ばす必要があるため 選択したサンプリングポイントは一方向にのみ配置する必要があります ダウンサンプリングを実行する場合 サンプリング範囲を適切に拡大すると 縮小された画像の色のピクセル数ができるだけ多くなり 重みの制御により 明るさの減衰がより自然になります アップサンプリングを実行する場合は 数回デバッグしてサンプリングポイントを適度な範囲内で維持し 色が暗すぎないようにします     しきい値を適用してハイライトをフィルタリングする円形の発光点の場合 望ましい効果は 円の中心を通過する水平ビームが最も高い輝度と最も長い長さを持ち 垂直方向に沿って減少することです Y軸に沿って周囲のピクセルをサンプリングすることを選択します これにより エッジピクセルがブレンドによってこぼれる明るさが少なくなります     重畳同様のアイデアを使用して複数のピクセルをサンプリングし それらを混合してピクセルの色を取得すると 遷移がスムーズになります 自己照明球を使用し 鏡面反射光を適用して 効果を取得します    png  平行光の強度を上げて 効果を得ます    png  興味のある方は参考してください：HDRPバージョンのStreak効果 ​​Deck Unity HDRP LensFlares    png   このプロジェクトの効果 URPバージョンのStreak効果     gif   このプロジェクトの効果    まとめStreak効果の実現から 画像ぼかしアルゴリズムのアイデアの美しさを徐々に理解することができます 色をぼかし 自然な遷移を形成します  いくつかのトリックと相まって 多くの効果を達成することができます UWA Technologyは モバイル VRなど様々なゲーム開発者向け   パフォーマンス分析  と  最適化ソリューション  及び  コンサルティングサービス  を提供している会社でございます 今なら UWA GOTローカルツールが  日間に無償試用  できます  よければ ぜひ UWA公式サイト：UWA GOT OnlineレポートDemo：UWA公式ブログ：,1,2022-06-17
144,144,2 | Depth Of Field（被写界深度）,画像処理,https://qiita.com/UWATechnology/items/2742e5726f3b8a01796d,参考：CatlikeCoding Depth Of Field：   基礎知識被写界深度は ゲームでよく使われている画像後処理効果のつです これは 撮影の基本的な概念に由来し カメラレンズまたは他の撮像装置の前が鮮明な画像を取得できる際に測定された被写体の前後の距離の範囲を指します 被写界深度内のオブジェクトは鮮明に画像化され 被写界深度外のオブジェクトはぼやけます   DepthOfField png  出典：ピンホールカメラの場合 記録されたオブジェクトのポイントごとにつの光線だけが小さな穴を通過して記録されます これの利点は 画像が常に鮮明であるということですが 一本の光の明るさが低すぎるため 画像を鮮明にするのに十分な光を蓄積するために撮影するときは長時間露光が必要です 露出中に被写体が動くと モーションブラーが発生します 高速撮影を行い 露光時間を短縮するために レンズイメージングが使用されます これにより 複数の光線を同時に記録できますが 焦点距離にない点の画像の形状は 点ではなく 錯乱円 Circle of Confusion と呼ばれる円になります 感光性要素と人間の目の解像力の影響を受けて 錯乱円のサイズが一定のサイズよりも小さい場合 それははっきりと見えます 逆に 錯乱円のサイズが一定のサイズよりも大きい場合 ぼやけて見えます    jpg     jpg   枚の写真のピント位置が異なり 各オブジェクトのシャープネスが異なっています    png   Crysisの被写界深度効果 人間の観察には被写界深度という現象はなく 見たいものに焦点を当てていきます しかし 被写界深度で作成された画像は 画像の鮮明な部分に注意を向けさせ 注目を集めて重要なポイントを強調する手段です 次に この効果をシミュレートし 焦点を当てたい部分を決定して 被写界深度内のオブジェクトがシャープになり 被写界深度外のオブジェクトがぼやけるようにします    実装    CoC Circle of Confusion 値を計算する物理方法で計算することができます    png  知っておくべき変数は次のとおりです   Ape 開口径  f 焦点距離  F 焦点距離と呼ばれ 焦点面に完全に焦点を合わせることができるポイントからカメラまでの距離  P 現在観測されているオブジェクトのポイントの距離 焦点面から鏡面までの距離と焦点距離の関係と相まって 正確な関係の計算はより複雑になります 実際 錯乱円の大きさと観測点からカメラまでの距離との関係を考えると F   Pの場合 錯乱円の大きさはであり 差が大きいと考えられます 差が大きいほど 錯乱円の直径が最大の錯乱円の直径まで大きくなり続けます この特徴によれば 比較的単純な数学的モデルを構築することができて近似シミュレーションを行います 例：SIGGRAPHカンファレンスでは Epicチームのトピック ALife of a Bokeh で 被写界深度の実践について説明しました その中で 錯乱円のサイズのシミュレーションに関しては Unrealエンジンのソリューションを図に示します    png   SIGGRAPH A Life of a Bokeh  関連する変数とパラメーターの意味は次のとおりです Pは 焦点を合わせたオブジェクトからカメラまでの距離を表します Zは 現在描画されているオブジェクトからカメラまでの距離を表しますMaxBgdCocは 最大後被写界深度の錯乱円のサイズを表します これは  開口径 焦点距離   焦点距離 焦点距離 として計算されます このように構築された数学モデルは 錯乱円の変化する傾向をシミュレートします 焦点から離れるほど 錯乱円の直径は大きくなり 徐々に最大値になります カスタム関連変数：Shaderでは数学モデルに従って 対応するピクセルのCoCの値を計算します 一時的なRTを使用してCOC値を保存します 簡単なシーンを作成し 深度の関係が次の図に示されます    png  CoC値の概略図は次のとおりです   image  png      Bokeh Filter次に ボケフィルターを作成します   image  png   ウィキペディアから 開口部の構造が円に近いため 画像は被写界深度内になく 明るい領域の形状は円に似ています 前のぼかしコースのアイデアによると このような効果を作成するために 畳み込み演算子が使用され サンプリングポイントの範囲は円に近くなります 次の図は さまざまなサンプリング周波数が異なる円形演算子の形状を示しています：  image  png   SIGGRAPH A Life of a Bokeh  Unityのポストエフェクトスタックvにサンプリング範囲は記載されています 参考してください 円演算子を使用して畳み込みします その効果は次の図のようになります   image  png  オフセットを計算する場合 演算子のサンプリング範囲は単位円で これに最大の錯乱円の直径を掛けて 通常のサイズを取得することに注意してください 効果は次の図のようになります   image  png  処理後 明るい部分に丸い輝点が見られ 要件を満たしていることがわかります ただし 輝点領域は非常にシャープでトランジションがないため トランジションをより自然にするために別のぼかし処理を追加します Dual Blurでのダウンサンプリング操作を使用できます 効果は図のようです   image  png  トランジションがもっと自然になったことがわかります ただし 画像がすべてぼやけている必要はありません 被写界深度外のオブジェクトのみがぼやけになり 被写界深度内のオブジェクトは鮮明になります     CoCを適用する以前に計算したCoCの値をフィルターに適用する必要があります まず 前の手順でぼかし処理を実行するとき 画像に回のダウンサンプリングをしたが 同様に Coc値を保存するRTにも回ダウンサンプリングを行います ここでは Coc値を格納するRTのつのピクセルの平均値をダウンサンプリングされたRTに割り当てることを選択します その結果 ボケぼかしが実行されたときにサンプリングされたテクスチャでは ピクセルのCoC値がAlphaチャネルに保存されます 以前にBokeh Filterを実行するとき すべてのサンプリングポイントが中心点に影響を与えました 実際 CoC値はピクセルの影響範囲を表します  CoC値がサンプリングポイントから中心点までの距離よりも小さい場合は サンプリングポイントが中心点に影響を与えていないことを意味し この時点ではポイントをサンプリングしません   image  png  たとえば 上の図の青い部分の錯乱円の半径は サンプリングポイントから中心点までの距離よりも小さく 中心点のピクセルカラーには影響しません ボケ関連のシェーダーコードの変更について まず Blurをコメントアウトし 絞りの直径を調整して 効果を確認します   image  png  緑の立方体に焦点を合わせると より離れているつのパーティクルシステムのボケがより顕著であることがわかります   image  png    image  png  焦点が合っている部分のボケが殆どありません   image  png     畳み込み実際には 錯乱円の直径はセンサー要素のピクセルサイズよりも小さく オブジェクトは正常に焦点が合っています センサー要素のピクセルサイズを参照する変数を設定します CoC値がそれ以下の場合は 鮮明な元の画像が表示に使用され CoC値がそれより大きい場合は ボケ画像と元の画像の混合値が使用されます 遷移のスムーズさを保証するために Lerp関数を選んで混色を実行します シーンを調整して効果を得ます   image  png      前景と背景を分離するこのアルゴリズムを使用すると 次の状況が発生します   image  png  緑の正方形に焦点を合わせると 前の白い球形の粒子にボケがなくなります これは CoC値が各ピクセルの深度値を使用して計算されるためです 実際 正しい効果は 前面の白い球形の例がボケ状態になり 背面の緑色の正方形をブロックすることです したがって 前景と背景を分割して別々に計算する必要があります CoCの正と負を判断し 前景と背景は異なる変数に格納されてボケ計算を行います 達成したい効果は次のとおりです フォーカスされたオブジェクトの前に前景がある場合 前景はボケ効果を使用して フォーカスされたオブジェクトをブロックします したがって 内挿する場合 前景の重みはより大きく 前景があることを証明し 前景のボケ効果を使用します ソース画像と混合するときに 前景がある場合は 前景のボケ効果も使用します それで 混合モードの重みがアルファ値として保存されます ソース画像と混合する場合 まず背景の深度値に従って補間を実行します これは オブジェクトが後方にあるほど より明白なボケ効果があることを示しています 次に 前景の混合モードに従って補間を実行します これは ピクセルの周囲が前景のボケ効果の影響を受けていることを示しています 前景のボケ効果は 重みに従って表現する必要があります 効果は図のようになります   image  png    image  png     まとめ被写界深度の学習に関しては この章ではいくつかの基本的な実現効果について説明します 被写界深度効果を非常に細かくしたい場合は 多くのスムーズな遷移とパラメータ調整が必要です 研究を継続することに関心のある方は SIGGRAPHカンファレンスでEpicチームによるトピック ALifeofaBokeh を見ることができます  PPT CatlikeCodingによって実現される被写界深度効果と keijiro  によって実現される被写界深度効果：  GIF    gif   kinoBokehプロジェクトによって達成された被写界深度効果 このような効果は Mobile側に適用したパフォーマンスの問題 およびそれを解決する方法は  学堂のコース 中国語注意   を読むことができます UWA Technologyは モバイル VRなど様々なゲーム開発者向け   パフォーマンス分析  と  最適化ソリューション  及び  コンサルティングサービス  を提供している会社でございます 今なら UWA GOTローカルツールが  日間に無償試用  できます  よければ ぜひ UWA公式サイト：UWA GOT OnlineレポートDemo：UWA公式ブログ：,4,2022-06-17
145,145,【YOLO】の仕組みを簡単にまとめてみた【物体検出アルゴリズム】,画像処理,https://qiita.com/kindamu24005/items/efd53c7511a40ddac636,   物体検出って何     物体検出の簡単な流れ  画像を入力　⇒　物体検出モデルが画像を分析　⇒　画像を出力     jpg   分析したい画像をに入れると 画像に何が何処に映ってるかを勝手に調べてくれて出してくれます     物体検出モデルって何  画像を受け取ったモデルは といった感じで調べてくれます  このモデルにはいろいろな種類があって といった感じに日々研究されていて 改良が進んでいます    YOLOって何  自動運転などでは ドライブレコーダーからどれだけ早く何が何処に映っているかを知る必要があるので よりリアルタイムに近い検出ができるようにモデルを改良しました     YOLOの仕組み    jpg    ①　 画像をS×S個の小さな正方形に分割する    ② 　バウンディングボックスの要素 つ を求める    ② 　一つ一つの小さな正方形が何を映しているかを判断する    ③　 ② と② を組み合わせて結果を出す      ①：画像を画像をS×S個の小さな正方形に分割する  yyyy jpg   入力された正方形の画像をさらに  小さなS×S個の正方形  に分割します  この小さな正方形を論文内では   グリッドセル grid cell    と呼んでます     ② ：バウンディングボックスの要素 つ を求める     ◎バウンディングボックスって何    jpg   このバウンディングボックスは  あらかじめ設定  されています      ◎一つのバウンディングボックスは個の特徴を持つ   jpg   ：バウンディングボックスの中心のx座標 ：バウンディングボックスの中心のy座標 ：バウンディングボックスの横幅 ：バウンディングボックスの縦幅 ：信頼度     ◎信頼度って何  までの間の数字です    jpg    を表しています   を表しています      ◎全部の小さな正方形に対してバウンディングボックスを描いた結果  c jpg      ② ：一つ一つの小さな正方形が何を映しているかを判断する   jpg   各小さな正方形がC種類のクラス中で どのクラスに所属している可能性が高いかを判断します      ◎全部の小さな正方形を判断し 色分けした結果  final jpg   全ての小さな正方形が何かしらの種類のクラスに所属します     ③：② と② を組み合わせて結果を出す  final jpg   たくさんのバウンディングボックスから適切なものを選ぶために という方法を使います      ◎NMS Non maximum suppression って何      ◎NMS Non maximum suppression の流れ 各クラスごとに を選びます  その枠と他の枠のを調べて一定以上の割合で重なっている枠を消します  までの間の数字です    jpg    を意味します   を意味します      ◎NMS Non maximum suppression を行った後   jpg    YOLOシリーズの違いを簡単にまとめてみた  YOLOvを実際に使ってみた①   連合学習 YOLOが利用されている次世代の技術  参考文献Yolo公式サイト,28,2022-06-17
146,146,1 | Real-Time Glow & Bloom,画像処理,https://qiita.com/UWATechnology/items/6ea9c25da8da151e2ca8,   前書き先々週 画像ぼかし処理のアルゴリズムの効率を探りながら コアアイデアを深く理解しました これらのアイデアは多くの後処理効果に適用します それでは いくつかの一般的な画面後処理効果を実装する方法を学びましょう    基礎知識Bloom Glow 特殊効果は ゲームでよく使われる画面の後処理特殊効果の一つで ハイライトオブジェクトに投光照明 floodlight 効果を与え 光影の表現力を向上させる機能を持ちます Bloomは通常 より良い結果を得るためにHDRおよびToneMappingとペアになっています まず HDRとToneMappingとは何かを理解する必要があります 電子機器は表示範囲が限られているため RGBのつのチャネルにとって 各チャネルはビットで 合計     種類の色を表示することができます この範囲は低ダイナミックレンジ Low Dynamic Range と呼ばれます この範囲は経験的な輝度の範囲であり 人間の目に害を及ぼすことはありません それに対して この範囲を超える色空間は ハイダイナミックレンジ High Dynamic Range と呼ばれます 実生活での輝度には範囲制限はありません より多くの輝度を表現するには RGBの各チャンネルをビット以上に拡張します 演算が完了したら 出力時にToneMapping トーンマッピング を介してHDR空間の色彩情報をLDR空間に変換して モニターに表示させる必要があります 実生活で起こったBloomは人間の水晶体の散乱に由来します 例えば 明るい光を浴びるとき 見えにくくなります 従って Bloomでハイライトブルームの効果を表すことでこの現象をシミュレートします HDR LDR およびToneMappingの詳細については  HDRと色彩管理 シリーズ 以下のリンク を参照してください    光 色 色度図     色空間     ゲームにおけるHDR  一般的なBloom効果は 畳み込みによって画像のハイライトされた部分をぼかし 元の画像に重ね合わせることによって作成されます  画面後処理効果シリーズの画像ぼけアルゴリズム  以下のリンク では さまざまな画像ぼかしアルゴリズムを紹介し 最も効率的なDual Blur方法を使用してぼかし処理を行うことにしました まず しきい値を設定し 画像をサンプリングします しきい値を超える画像内のピクセルがハイライトされた部分として決定します ハイライトされた部分は色を保持し 残りはに戻し RTに保存します 次に ハイライトされた部分をぼかして 元の画像に重ね合わせます 効果は図のようになります   image png  元の画像は次のとおりです   image png  同様に  ライトセーバー 効果の実現を模倣します   image png     まとめBloom効果は ゲームで使用される最も一般的な画面後処理効果だと言え 実装も簡単です これからは ぼかし処理で自然に滑らかに色をぼかし 色を重ねることで輝度を向上させることができるようになります ブルーム効果のパフォーマンスに影響を与える最大の要因は ぼかし処理を行うコストです Dual Blurアルゴリズムを選んで処理することをお勧めします UWA Technologyは モバイル VRなど様々なゲーム開発者向け   パフォーマンス分析  と  最適化ソリューション  及び  コンサルティングサービス  を提供している会社でございます 今なら UWA GOTローカルツールが  日間に無償試用  できます  よければ ぜひ UWA公式サイト：UWA GOT OnlineレポートDemo：UWA公式ブログ：,1,2022-06-15
147,147,ロボットアームで重なり合っている対象物を移動させる,画像処理,https://qiita.com/shimamotosan/items/5d8890ade413db114e99,  やりたいこと以下のテキストでは AIを用いた画像認識により対象物を分類し ロボットアームにより仕分けを行っています ただ 判別したい対象物が触れ合っていたり重なり合っているとつの塊として認識したり その上で大きいサイズとして無視されたりしてしまい 上手く対象物を判別できません 以下の記事で触れ合っている状態でも個々に認識できるようにプログラムを修正しました    触れ合っている対象物を個々に認識して ロボットアームで移動させる  ただ 対象物が重なり合っている状態だと個々に分けることが出来ずにつの塊として認識してしまうことがありました 本記事では 対象物が重なり合っている状態でも個々に分けて認識できるように修正したいと思います   できたもの左の画像が領域を抽出しているもので 右の画像がそれをもとに矩形として切り出しているものです 取得した領域の面積を取得し その大きさで対象物 円玉 として正しく検知出来ているであろう範囲に収まっているか否かを判別しています 左側のフレームで緑色の領域は円玉の大きさの範囲内にあるもの 赤色の領域は範囲内より大きいもの 黄色の領域は範囲内より小さいものになっています 重なり合っている状態でも検知することができていますが 重なり方や重なり具合によってはつの塊として認識しています 完全に重なり合っているものを個々に認識するためには 奥行きを取得するためにセンサーを追加したり カメラを台にしたりなどが必要そうなので 今回は重なっているであろう領域 赤色の領域と黄色の領域 は 崩す動作を行うようにしました   参考情報以下のサイトを参考にプログラムを作成いたしました プログラムの詳細は以下もご確認ください   追加でインストールしたライブラリテキスト記載のライブラリから追加でインストールしたライブラリは以下のとおりです   重なり合っている対象物を認識する以下の手順で重なり合っている対象物の領域を抽出しています   カメラからフレーム取得  グレースケール画像に変換  値 バイナリ 画像に変換          加工なし画像を表示する        cv imshow  Raw Frame   frame           グレースケールに変換              サイズに縮小          グレースケール画像を表示する          値化              サイズに縮小          値化画像を表示する  背景までの距離を取得する  局所的な極大値を求める  極大値のラベリング            重なり合っているオブジェクトを分けて検知する          背景までの距離を取得する          局所的な極大値を求める          極大値のラベリング              マーク作成  輪郭の抽出   python              輪郭              輪郭の領域を計算               area   cv contourArea contour               ノイズ 小さすぎる領域 と全体の輪郭 大きすぎる領域 を除外                 上記に当てはまらないが領域が小さいものと大きいものをチェック  プログラム 前回の記事  の修正も行っている状態での変更である旨 ご注意ください   教師データの作成            加工なし画像を表示する        cv imshow  Raw Frame   frame           グレースケールに変換              サイズに縮小          グレースケール画像を表示する          値化              サイズに縮小          値化画像を表示する            重なり合っているオブジェクトを分けて検知する              マーク作成              輪郭              中心を描画              輪郭の描画              輪郭の領域を計算               area   cv contourArea contour               ノイズ 小さすぎる領域 と全体の輪郭 大きすぎる領域 を除外                 フレーム画像から対象物を切り出す                 回転を考慮した外接矩形を取得する              回転行列を取得する              切り出す              輪郭に外接する長方形を取得する            x  y  width  height   cv boundingRect contour               輪郭に外接する長方形を描画する              長方形の各頂点を描画する              輪郭データを浮動小数点型の配列に格納              中心を描画              情報を描画          描画した画像を表示          キー入力をms待つ        k   cv waitKey             ESC   キーを押す          プログラムを終了する           C キーを押す          WEBカメラのゲイン値 露出の値を調整する         画像の保存             S キーを押す         そのまま切り取って画像を保存する                  リストに格納された矩形を長辺に合わせてサイズ調整する                  サイズ調整した正方形を画像 png データで保存する           A キーを押す          補正を加えた画像を保存する                  取得した矩形を長辺に合わせてサイズ調整する                  サイズ調整した正方形に補正を加えて保存する           R キーを押す          画像を回転させた上に補正を加えた画像を保存する                  取得した矩形を長辺に合わせてサイズ調整する                  画像の中心位置                  画像サイズの取得 横  縦                   リストに格納された長方形を画像 png データで保存                  回転 °  °  °  ° して 変換処理した画像を保存                      回転変換行列の算出                      アフィン変換           G キーを押す          最後に取得した矩形とその結果を元にDOBOTでピックアップする      キャプチャをリリースして ウィンドウをすべて閉じる    cap release      cv destroyAllWindows  崩す動作を追加するために   dobotClassifier py   に以下のメソッドを追加してください     指定座標で崩す動作を行う      オブジェクトの真上に移動      オブジェクトを取れる位置まで移動し オブジェクトを取る,1,2022-06-10
148,148,【C#】色分けして樹形図を描く（Bitmap）,画像処理,https://qiita.com/nkojima/items/16bf35980a20266619d6,  はじめに年ほど前に   C  樹形図を描く Bitmap    という記事を書きましたが この時は単色の樹形図を描くだけのプログラムでした 別の記事の  C  ネコのイラストを描く Bitmap   では様々な色を使って描画したので 今回は 樹形図の幹や枝部分を茶色 葉の部分を緑色に色分けして描画する というのにチャレンジしてみました   幹 枝と葉の峻別  今回は 樹形図の回の枝分かれ のうち スタートから回までを幹 枝として茶色で描画して 残りを葉として緑色で描画する というルールにしました   作成したコード  基本的なコードは   C  樹形図を描く Bitmap   に掲載されているコードと同じです   今回修正したのは 幹 枝と葉を区別する部分 で ColoredBranchオブジェクトを生成する際に 枝生成の残りの繰り返し回数 を使って 幹 枝と葉を区別するフラグ をセットしました     実際にBitmapオブジェクトに描画する際に このフラグを参照して色付けを行いました             樹形図を描画するキャンバス         private Bitmap bmp             キャンバスサイズ         private int canvasSize             コンストラクタ         public ColoredTreeDrawer int canvasSize                this bmp   new Bitmap canvasSize  canvasSize              this canvasSize   canvasSize             樹形図を描画する         public void Draw string filePath                樹形図の 枝 を作る                末端付近の枝 ＝葉 だけ緑色にして それ以外は茶色で描画する                画像をPNG形式で保存する             bmp Save filePath  System Drawing Imaging ImageFormat Png              樹形図の枝を生成する                出力時の色をコントロールするため 末端付近の枝 ＝葉 であるかのフラグをセットする             角度の 度 をラジアンにして返す          枝 を表すクラス             枝の開始点        public Point StartPoint   get               枝の終了点        public Point EndPoint   get               この枝が葉 末端付近 であるか         public bool IsLeaf   get               コンストラクタ        public ColoredBranch Point startPoint  Point endPoint  bool isLeaf             StartPoint   startPoint             EndPoint   endPoint             IsLeaf   isLeaf   実行用のコード  生成された画像  幹 枝と葉が色分けされているので 葉の部分だけは木の雰囲気が出てきました   一方で幹 枝 葉の線の太さが同じなので 全体としてどうしても不自然な感じになってしまいました     C  ネコのイラストを描く Bitmap   ,2,2022-06-07
150,150,会社から置き忘れ傘を無くしたかった。Teachable Machineを使用した置き傘判定！,画像処理,https://qiita.com/imurasho-ichi/items/d0a8aaca91c850d34be4,　こんにちは 自分の発想から色々と開発していますが 今回はTeachable Machineを使って傘を判別する事をやりたかったのですが 開発途中でなかなか上手くいかず 自分が試みたプランのみを披露します もっと応用したかったのですが   それはまたの機会にし最後に理由を書きたいと思います    なぜこれを考えたか　人間誰しも忘れ物をしがちですが 特に日本の傘の廃棄量は世界で群を抜いているようです 　以前の職業はコンビニのSV スーパーバイザー をしており 巡回でお店に伺うとあの店頭に置いてある傘立てがいつも気になっていました それは決まって雨上がり コンビニに入る前はちゃんと傘立てに置くのに買い物を済ませる事に集中しもう傘を忘れて帰っていきます もちろん忘れて行かれた傘たちはご主人が帰ってくるのを待っていますが 帰ってきません お店の人は所持者不明の傘はわざわざお金を払って事業用産業廃棄物として捨てます なぜなら置き傘がバックヤード 商品やコンテナ置き場 を傘が席巻するからです    途中までの完成品はこちら   準備したもの■Teachable Machine■LINE■enebular   いむぴょん家の傘でまずは実験  スクリーンショット     png  ■enebularを使ってLINEと連携しました ●LINE DevelopersenebularからURLを取得    note warn警告enebularの場合URLが分で変更になり 継続して使用する場合はおすすめ出来ません   スクリーンショット     png  ※体験談：こいつのおかげで全然繋がらなく苦労させられました ■http in LINEBot   スクリーンショット     png  ■ function LINEメッセージ解析   スクリーンショット     png  ■http request  スクリーンショット     png  ■Teachable Machine こちらはノード追加が必要   スクリーンショット     png  ■function 結果取得 ※上のfunctionノードと同じものを追加  スクリーンショット     png  ■LINE ReplayMessage これもノードの追加が必要   スクリーンショット     png  作りは完成    本来やりたかった事　この後はまた別の記事に書こうと思いますが 最近ではゲリラ豪雨など凄いのですが 数時間で雨が止むことも多いです 出勤時は雨が降っていて 退勤時は雨が止んでいる事はまれではありません そんなこんなで今回やり残した課題は自分の傘を登録し それがどこにあるかを判別したり ゆくゆくは社員全員を巻き込み全員の傘を登録し置き傘誰のだをやりたかったです また次回も出来る事を記事にしますね ,2,2022-06-05
152,152,初心者が３日で「ドローンによる物流積載率把握」（画像認識）を作ってみた,画像処理,https://qiita.com/andy981039/items/f004ef7a238f2dc124a1,   この記事の対象の方と完成品      　この記事の対象の方  トラックや長台車の積載率向上にお悩みの方  ドローンが大好きで非エンジニアから情報システム部へ異動された初心者の方  ドローンは物を運ぶだけじゃないと思っている方  画像認識などを活用しDXに取り組んでいる方   あれっ 物流業界の問題っていちいち大規模開発の必要っていらないよね  と思った方私も新米ですがアイデアを形にしようと思いノーコード開発へ挑戦しました よろしくお願いいたします      完成品 注意 人が多い営業中の現場でドローンを飛ばすのが難しかったため 実際の現場での計測 安全な場所で現場の実物写真を使用した 定点およびドローンでの計測二回に分け計測いたしました  学習画像は現場から調達しています  定点計測 積載率計測 ＊計測結果は積載率 を計測  第二回目課題 定点  gif         使用したツールNode Red Teachable Machine DJI社製ドローン Tello Boost Combo   Node Red説明記事 Teachable Machine説明記事Python  ANACONDA NAVIGATOR Jyupyter 及びドローンを飛ばすためのSDK Tello py  Pythonとは ANACODAやJyupyterとは tello py説明 SDKって何 ＊所要時間　各ユーザー登録 制作まで 合計H程度 この記事書いてる時間や調整にかかった時間は除く        この記事の狙いと構成この記事はブラックボックスになるケースが多いトラック 台車 の積載率を画像認識を利用することにより 見える化 へ挑戦しています ですので ドローンのセッティング Pythonを使用したプログラミング 現場での計測 機器設定二つの話が必要に応じて交錯します   　このコンテナ あとなんぼ積めるんや         あとどれぐらい積めるか わからない  コンテナ jpg  物流業界は 陸  海  空 それぞれが違う基準で動いているうえに団塊の世代に代表される大ベテラン勢が 創意と工夫 で戦後から営々と日本の繁栄を支えてきました 　しかし 令和に入り引退される方が多くなる中で残されたのは 昭和の時代の仕組みや仕事 でした 　結果  現場にいなければ貨物積載率が把握できない  あとどれだけ積めるかわからないから 完全に満荷になるまでトラックを出さない といった働いている人への負荷を前提としたシーンが散見され  非効率 が常態化しています 　この辺をどうにかできないかなぁ と思い自分で積載率を判定できる仕組みを作ることに挑戦しました    要件定義  画像認識 ドローン導入 ユーザーテスト       要件定義まず何がKPIなのか 実態をインタビューするところから始まりました  積載率 といっても大型トラックから貨物用コンテナ 運搬用台車まで様々です これを 作る人の独断と偏見 進めてしまうとろくなことになりません 結果  実務では長台車の積載効率を気にしている  店舗向けへ小分けされた瞬間が台車が重要である  人手不足でいちいちチェックすることはできない 見に行ってくれる仕組みが欲しいといったことがわかりました よって 押さえる数値は 長台車の積載率 方法論は ドローン等を使用した積載率判定 となりました      －　ドローン実装さて いよいよドローンの実装です そして 私が勢いで買って未開封のままだったDJI社製ドローン Tello Boost Combo が一年ぶりに開封される時が来ました そして ドローンからの画像に用があるのでPythonというプログラムを使ってパソコンから操作できるように設定開始 実際の設定画面そして 見よ この雄姿   事務所を飛ぶドローン   職場を飛ぶドローンプチャ PNG  他の人も物珍しさに集まってきます が                     ここで問題が発生ドローンからのライブストリーミング設定がどうしてうまくいきません そして できたとしても 積載率判定 する機械学習へ同期することが技術的に厳しいことが判明 プランBとしてドローンが送ってきたライブ動画をいったんスマホで受けて積載率判定へかける方法へ変更     －　現場での学習と数値計測同時に現場での学習素材収集や機械学習構築も進めていました Node Red画面  image png  Teachable Machine画面  image png       － 測定結果 これって測定成功ですよね  現場に同席したIさんの言葉 人が沢山いる現場で撮影や実際にドローンを飛ばすことができないので PCのカメラを使って測定を開始 実際にこちらで振り分けた積載率に近い数値へ収まっています 今回の目的は 積載率の判定 だから最低限の目的は達成できたー  翌日　紙を使った再測定でも似たような数値が出ました   image png  月日追記　そしてドローンからの画像はこんな感じ 保存した測定プログラムが呼び出せなくなってしまったので実際の撮影動画からのキャプチャ   ドローンからの台車映像 PNG     まとめ最期は無事に終了できましたが 途中は期限内に終わりそうになくひやひやしました この記事が初心者でも ここまでできる と全国の物流関係者へ届けば幸いです ,4,2022-06-05
153,153,あなたの笑顔は死んだ魚の目をしている！LINEで撮影した笑顔をTeachable Machineで判定　毒舌マナー講師Bot爆誕！,画像処理,https://qiita.com/katourina0120/items/f457a1f30918894872dc,   オンライン研修だと笑顔や挨拶の指導が難しい…みなさんこんにちは 今回も右も左もよくわからないプログラム初心者の採用担当が接客業の現場の問題をプログラミングで解決していきたいと思います 私の通常業務は採用なのですが 毎年月のこの時期だけは教育担当として新入社員オリエンテーションのお手伝いをさせていただいております 年からコロナのため 集合研修からオンライン研修へと変更になったのですが そのせいで格段に難しくなったのが 笑顔 や 挨拶 の指導です あちこちにちらばる各拠点をZOOMでつないで研修をしているのですが か所に名程度いるので ビデオをチェックしても誰が誰なのかわからない状態です だって あの子もどの子もみんな同じようなスーツでみんな同じような髪型なんだもの… こんな状況では お辞儀の角度とか 背筋を伸ばしてねとか 笑顔を忘れないで とか細かい指導ができません  オンライン研修になってから まともに挨拶できる新入社員が減ったわ… という各店舗の教育担当のお姉さま方のお小言に すみません すみません… と頭を下げるばかりでした    毒舌マナー講師Bot作ってみたそこで今回私が作ったのがこちらのLINE Botになります 素敵な笑顔をLINEでパシャっと撮影して送ると 先生が あなたはお陽さまみたいな素敵な笑顔をしているわね と褒めてくれます 不機嫌な顔を送ると あなたは死んだ魚の目をしているわね  とけなしてくれます 新入社員諸君には是非自宅にてこのLINE Botで笑顔の練習をしてきてほしいと思います    毒舌マナー講師Botの作り方     用意するもの sun with face ノリの良い同僚 名 多ければ多いほど良い      参考にさせていただいたサイト     作成手順      Teachable Machineの設定        Teachable Machineでサンプルを取る まずは下記よりTeachable Machineのページに飛んでください  get started ボタンを押して  image project をクリックすると下記のような画面になります   新ティーちゃぶる JPG  今回はclassにお陽さまみたいな笑顔 classに死んだ魚のような目と入力し それぞれのデータを取っていきます ②ウェブカメラで撮影し ③トレーニングボタンを押しましょう 一人のデータサンプルのみ取って 一人の笑顔にしか反応しないマシンを作ってもしょうがないので 今回 名の  ノリの良い  同僚のみなさまにサンプルデータ取得にご協力いただきました    note infoたくさんの人に協力してもらう際は いったんサンプルをダウンロードして 後から厳選写真をアップロードしたほうが判定精度が上がります ④のつの   ボタンからサンプルのダウンロードができます 連続して写真を撮影していると 笑っちゃいけないと思っているのに薄ら笑いを浮かべてしまう場合があります ノイズを丁寧に取りのぞく地道な作業が必要です         学習モデルをエクスポートして URLをメモしておく トレーニングが終わったらエクスポートボタンを押すと学習モデルをエクスポートできます 下記URLをしっかりメモしておきましょう   URLをメモ JPG        LINE Botの設定   note warn を参考にしてご自身でLINE Botの作成をお願いいたします この後 で  チャンネルシークレット  と  チャンネルアクセストークン  の入力が必要か箇所がありますのでどこかに必ずメモをしておきましょう         LINE BotのWebhookのURLを設定する LINE BotとNode REDを接続します 自分のNode REDのURLをコピーして 末尾に linebotと追記して下記のように張り付けします   ウェブフック JPG     note alertNode REDのURLをコピーするときは comまで  comより後ろの英語やら数字やらは削除してください 末尾に linebotとつけるのを忘れずに これでLINE側の設定は完了です        Node RED でフローを作成するまずは全体のフロー図をお見せします   node red JPG          teachable machine ノードとLINEのmessaging api ノードを追加する まず 右上のハンバーガーラインから パレットの管理 をクリックします  ノードを追加 タブをクリックして  node red contrib teachable machine と node red contrib line messaging api の二つを追加してください   node  JPG          http in ノードを設定する まず最初にネットワークカテゴリにある http in ノードをドラッグ ドロップしてください 画像を見てオレンジ色の四角の箇所 か所設定をします   http JPG          function ノードを設定する 機能カテゴリにある function ノードをドラッグ ドロップします こちらのノードの設定は  下記の記事の にコードが貼ってありますので リンクから飛んでご参照ください           http request ノードを設定する 続いてネットワークカテゴリにある http request ノードをドラッグ ドロップします 設定は下記の図の通りです   リクエストの設定 JPG  URLコピー用　 　        teachable machine ノードを設定する 続いてteachable machine ノードをドラッグ ドロップして下記の通り設定ください   teachable machine JPG          function ノードの設定機能カテゴリにあるファンクションノードをドロップ ドラッグしてください こちらのノードもコードの設定が必要となります   下記リンクに飛んでいただき  をご参照ください   テキスト部分はご自身の好きな文言に変更しましょう         LINEのreplay messageの設定ここまで来たらあともう少しです… LINEのreplay messageノードをドラッグ ドロップして下記の通り設定ください   リプライ JPG  最後に順番にワイヤーでつなぎましょう    フィードバックと最後にこれで毒舌マナー講師ロボットの完成です ちゃんと動くか動作確認してみてください できあがってから何名かと一緒に作動を確認してみたのですが…… frowning 抜群の笑顔をしたつもりなのに 死んだ魚の目判定される frowning 顔の角度で微妙なことになるという残念なお声をいただきました 調べてみたのですが とあるサイトに撮影時の背景は 白い壁 推奨です と書いてありました 今回会社の自席で何にも考えずにみんなとキャッキャしながら軽く撮影しただけだったので このような結果となってしまったのでしょう  point up 多分実用化するならものすごい量のサンプルが本当は必要なんじゃない というお声もいただき その通りだなと思いました 毒舌マナー講師が独り立ちするのはまだまだ早そうです…… 最後に一番大事なことを言っておきます   心の底からの ありがとうございます  が出たときの笑顔にかなうもの無し   これ 真理  ,18,2022-06-05
154,154,"デプス画像から三次元点群を構築の高速化(Python,numpy)",画像処理,https://qiita.com/KEROLL5/items/28a1438a4425dd83277e, MathJax Hub Config   texjax    inlineMath                 displayMath                   \\    \\       デプス画像から三次元点群を構築の高速化 Python numpy     この記事について初めまして KEROと申します ロボット工学や画像工学などこと研究してます 外人ですか もっと日本語で技術のことを話しできるため 日本語で技術blogを書き始めた今日はPyhonでデプス画像から三次元点群を構築のことついて話します 画像は一枚だけで場合なら コード例が見つけやすい例えば この記事  もしくは opend中の関数  以前Pythonとnumpyでこんな計算行って 点群構築のコード書きましたが 計算めちゃくちゃ遅い 自分の目的はロボットのナビゲーションので 連続の画像をリアルタイムで処理なければならない本来は遅くの理由としては 計算量が多いもしくはPython自体は遅いと思うから C  で書き直し予定ですが その前計算アルゴリズムを見直しため もう一回Pythonとnumpyで書き直して結果としては処理速度が大幅上げて Pyhonでもリアルタイム処理できましたテストとしては       サイズのデプス画像を三次元点群に構築    回ループして 処理時間は    秒アルゴリズム更新するため 現在の実行時間は秒   コードまずコードを上げて その下で説明補足しますテスト用例   説明Pythonで高速計算したいならできる限りPythonで計算 ループしなく numpyで完成してそのため 計算手順はできる限り行列式計算でいく    初期化まず カメラ座標系  から画像座標系で変換方程式は  MommyTalk png  説明のため 以下の形で略書きします中では P は画像座標系の座標 すなわちpixelの位置　u v C は カメラ内部パラメータ   D がカメラ座標系の座標 すなわち点群座標 今回計算の目的だから計算のため 先ずは C  カメラパラメータ行列式を導入こちらはのパラメータ行列式は自分ロボット用いたカメラをcalibrationした結果さらに 処理画像のサイズを調整するため  resize scaleパラメータを用意しました画像サイズが調整する場合 カメラパラメータも合わせて調整が必要 P 　pixel座標行列式を初期化   pythonself pixel   np array       reshape    今回は逆変換 つまり上の方程式では　 D 　は求めている答え C の逆行列を計算もう一回この方程式を見直しすると同じカメラの同じサイズの画像が連続入力する場合 P と C は変わらない Zだけが変化しますすなわち  QQ图片 png  ここで一気に画像すべてのpixelの K を計算します結果保存するため 先ずはデプス画像と同じサイズ ひとつpixel中では　ｘ大きさがあるの行列を作り np nditer   用いてトラバーサル 画像すべてのpixelの K を計算して 結果をpcd list中で対応した位置に保存します                 今の座標対して P  pixel行列更新                  K の計算                 結果を保存          これでは初期化の準備が完了    変換計算先色々準備した結果 本番の変換計算はとても簡単先ずはdepth画像を導入 サイズ調整初期化時計算の K 行列を取る 元行列の数値を変換させないため copy  が必要つまりではこんな形で計算する  MommyTalk png  そうすると一気にデプスから点群の変化を完成します残るのは形を調整して 通用の形になるだけ    Extra 左手座標系 右手座標系カメラ座標系は左手座標系であるので もしROSなど右手座標系ツールに入力するため 左手系と右手系の変換は必要そのため 初期化関数に対しては以下コードを追加する   最後そうのように 計算やループはできるだけnumpyを任せすれば 連続計算の場合計算速度はめちゃくちゃ速くなる今一番遅くなる部分は初期化時 K を計算する時np nditer 用いてトラバーサルするちょっと速くなったけど 本質としてはまだPythonでループしているからテスト用例回の処理で秒の時間では 約秒は初期化 実際にデプス画像を処理する時間は十秒だけ本番なら初期化の時間が無視できるし さらにデプス画像サイズをresizeして小さくなるため合格線は一秒フレーム対して今の計算速度を十分と思います     　アルゴリズム更新実行時間をテスト  image png  結果の一致性をテスト,8,2022-06-05
156,156,画像認識におけるOffline Data AugmentationをPytorchで実装してみる,画像処理,https://qiita.com/morinota/items/177c8eee5168afbc5ccc,  はじめに最終的にたどり着いた実装方法としては 非常にシンプルなのですが 以下の通りです   手順  元の画像データをDatasetで読み込む  手順  Data augmentationを実行  手順  それを再度ディレクトリにExport  手順   元画像＋ augmentatedされた画像 を再度Datasetで読み込んでモデリングに使用するもし 他にもっと良い実装方法を知ってる  という方がいましたら ぜひコメントいただければ嬉しいです：   そもそも実装したい Offline Data Augmentation とは Data AugumentationとOffline   Onlineに関して 現時点での自分の理解を軽くまとめます 間違ってたら優しめに教えていただけると嬉しいです   Data Augmentation   学習用のデータに対して 何らかの 変換 を施すことでデータを水増しす方法論 特に画像データに対しては 画像の 反転 や 回転 の 変換 が施される Augumentationを実行するタイミングによって Offline AugumentationとOnline Augumentationに分類でき それぞれ目的が異なる   Offline Data Augmentation      元画像に対して学習前にData Augmentationを適用し 単純に画像の枚数を増やす手法     タイミング：学習前 事前に     目的：学習データ数の水増し   Online Data Augmentation      モデル学習時に ミニバッチ毎にDataLoaderからモデルにデータを流し込む際に Data Augmentationを適用する手法     タイミング：ミニバッチ毎にデータをモデルに入力する直前     目的：汎化性能の向上       学習データ数自体は同じ       同じミニバッチでも 各Epoch毎に 入力される画像が変わり得る 各画像に対するData augmentation適用の有無を確率的に決定する事で    いざ実装前述した通り 最終的にたどり着いた実装方法としては 以下の非常にシンプルなものでした   手順  Data Augmentation用のDatasetを定義する   手順  Data augmentation用のtransformsを用意   手順  元の画像データをDatasetで読み込み Data Augmentation適用  手順  それを再度ディレクトリにExport  手順   元画像＋ augmentatedされた画像 を再度Datasetで読み込んでモデリングに使用する   前提条件   手順  Data Augmentation用のDatasetを定義する まず 以下がData Augmentation用のDatasetのソースコードになります Datasetクラスの定義に必要な   init          getitem          len     に加えて 各ラベル毎のディレクトリから画像を吸い上げる為に  find classes       make dataset   の二つのメソッドを定義しています 後者の二つのメソッドは torchvision datasets ImageFolderクラスの定義を参考にしています というかほぼほぼ写経です 各メソッドの概要としては    find classes   ：コンストラクタで渡されたrootディレクトリから 各クラスディレクトリを探し 画像分類における各クラスラベルを取得する    make dataset   ：オリジナルの画像のファイルパス一覧と対応するクラスラベルを取得する      getitem      index のサンプルが要求されたときに返す処理を実装     len      Datasetのサンプルの長さを返す   唯一アレンジした点  としては あくまで  オリジナルの画像 Data Augmentationで水増しされた画像ではない のみを各クラスディレクトリから読み込む  為に   make dataset   内で  正規表現を使って読み込む画像を選別  しています            コンストラクタで渡されたrootディレクトリから 各クラスディレクトリを探し 画像分類における各クラスラベルを取得するinnor関数            指定したディレクトリ内の画像ファイルのパス一覧を取得するinnor関数                      オリジナルの画像だけ取ってくる            index のサンプルが要求されたときに返す処理を実装          入力側の画像データを配列で読み込み実際に関数を使用する際は 適用したいData Augmentation手法名をListに格納しておいて ループ処理で関数を回していく事を想定しています こんな風に            random affine      ランダムにアフィン変換を行う          color jitter      ランダムに明るさ コントラスト 彩度 色相を変化させる          random resized crop      ランダムに切り抜いた後にリサイズを行う なお 定義した get transform for data augmentation   関数のソースコードは以下です  if文ばかりでかっこよくはない気がします       手順  元の画像データをDatasetで読み込み Data Augmentation適用   手順  それを再度ディレクトリにExport実際にData Augmentationを適用する際は以下のようになります          random affine      ランダムにアフィン変換を行う          color jitter      ランダムに明るさ コントラスト 彩度 色相を変化させる          random resized crop      ランダムに切り抜いた後にリサイズを行う image dir    各クラスディレクトリが入ったrootディレクトリのパス 出力する各画像のファイル名は オリジナルのファイル名  augumentated type  jpg となっています    手順   元画像＋ augmentatedされた画像 を再度Datasetで読み込んでモデリングに使用する   おまけの処理 ストレージの圧迫を防ぐ為 Data Augmentationされた画像のみ削除する この処理は不要かもしれませんが 一応 作成してみました 私の場合は  Offline Data Augmentation＝＞モデル訓練 の後に 下記の関数を実行して Augmentationされていないオリジナルの画像のみが最終的にディレクトリに残るようにしています：  単にストレージの圧迫を防ぐ為です       pathを一つずつ見ていって オリジナル以外を削除                  オリジナルの画像だけは残す  おわりにまだ画像データの扱いに慣れていませんが 今後研究活動でも適用してきたい技術分野なので 色々と試していきたいと思います 最後までお読みいただき 本当にありがとうございました：   参考,3,2022-06-04
157,157,[画像処理] 画像に枠線を付ける,画像処理,https://qiita.com/kotai2003/items/e9570402215a84de5d4c,  はじめに画像に枠線を付ける方法を整理します   やりたいこと下記のように単純に画像に枠線を付けることです   image png  用途ですが 画像の中の物体の輪郭を取るとき 白い枠線があると上手く行くケースがあります   方式Numpyのスライスを使います 例えば       サイズの画像があって 一番下の部分に幅pxの白いラインを書きたいとします すると下記のようなコードになります     pythonimg                    チャンネルごとに値を入れることをお勧めします この方式だとカラーの枠線を入れることが可能です OpenCVで画像を開いた場合 チャンネル順序が B G R になっていることには注意してください     pythonimg                          このような処理を辺に繰り返し行います 引数として 下記の二つを指定します band width   枠線の幅 単位はピクセルcolor    B G R  範囲   整数　  全体コード簡単ですね コードを掲示します  結果,0,2022-06-04
159,159,画像処理で遊べるデモサイト「IP World」,画像処理,https://qiita.com/jw-automation/items/db9994c8f15525db7f2d,   はじめに画像処理 Image Processing で手軽に遊べるデモサイトをFlaskで作りました 画像処理の基本的なアルゴリズムを実装しているので 画像処理を今まで触った事がないという人は 良かったら触ってみて下さい スマホでも遊べます ↓スマホでの表示■デモサイト■環境Python  AWS Lightsail   Flask  Docker   デモ一覧デモの一覧は 以下の通りです    タイトル 内容   Binarization 画像の二値化 大津法  スライダーによる閾値変更    Edge Detection フィルタ処理によるエッジ検出   Dilate Erode 領域の膨張 縮小処理   Open Close オープニング クロージング処理   Thinning 細線化処理   Labeling オブジェクトのラベリング処理   High Contrast ヒストグラム平坦化による高コントラスト化   Denoise メディアンフィルタによるノイズ除去   Unsharp Masking 先鋭化処理   Area Fitting 領域に対する矩形のフィッティング処理 以下 個別に簡単なデモで紹介して行きます      Binarizationこちらは 画像の二値化処理です いわずとしれた大津法を初期値として 閾値を までスライダーで動かせるようにしています パターン程画像を用意しているので スライダーで閾値を動かして色々と遊んでみて下さい      Edge Detectionこちらは 微分フィルタを利用したエッジ検出処理です 画像内のエッジ成分 輝度変化の大きい箇所 を抽出します フィルタ 処理は以下の種類で 各ボタンで切り替えができます   Prewitt h：Prewittフィルタ 水平方向   Prewitt v：Prewittフィルタ 垂直方向   Sobel h：Sobelフィルタ 水平方向    Sobel h：Sobelフィルタ 垂直方向   Laplacian：Laplacianフィルタ 次微分   Canny：Canny法最初のつは一次微分フィルタであるため 輝度勾配がなだらかに抽出されていますが 二次微分フィルタのLaplacianでは 細かいエッジ成分が抽出されているのがわかると思います 一方で細かいノイズ成分も強調される  Canny法はフィルタではなく 細線化と閾値判定を含むエッジ検出用の処理なので 他のフィルタ処理に比べて エッジ成分が明確に抽出されている事がわかります      Dilate Erodeこちらは二値化画像における 領域の膨張 Dilate   縮小 Erode 処理です 大津法で元画像を二値化した画像に対し  Dilation を押すと黒いピクセルの領域が膨張し  Erosion を押せば領域が縮小して行きます ボタンの押下に応じて処理が走るので 連続して押したり 交互に押したりと 画像がどのように変化するのか試してみて頂ければと思います      Open Closeこちらも処理自体は膨張 縮小処理ですが 膨張と縮小を複数回繰り返す事で穴埋めやノイズ除去を目的としたOpening  Closing処理です   Opening：複数回の縮小処理 ⇒ 複数回の膨張処理  Closing：複数回の膨張処理 ⇒ 複数回の縮小処理Openingは先に縮小処理をするので ノイズや小さい領域を除去する効果があります 一方Closingは 膨張処理を先に実施するため 領域内の穴を埋めるような動きをします 処理の繰り返しは それぞれ回ずつに設定しています      Thinningこちらは二値画像の細線化処理です 先ほどまでの縮小処理は 画素の外側を削って行く処理でしたが こちらは 領域の中心の画素分だけを残すように線を細くする処理になります アルファベットのようなケースでは アルファベットの形状を構成する骨格の要素を抽出できている事がわかると思います      Labelingこちらは二値化画像において 連続した領域にラベルを付けて行くラベリング処理になります ピクセルが連続した領域に同一のラベルを振るため 独立したオブジェクトの個数を抽出する事ができます 今回は元画像にそのままラベリング処理を適用していますが 膨張 縮小処理と組み合わせ ノイズや穴を埋めた上でこのラベリング処理を施す事で より綺麗にラベルを振ることができます      High Contrastこちらはヒストグラム平坦化を使った高コントラスト化処理になります 元画像は低輝度領域に画素値が集中しており コントラストが低い状態のため RGB空間で画素値が一定に分布するように ヒストグラムが平坦になるように 処理し コントラストを上げています RGBそれぞれでヒストグラム平坦化をかけると全体の色味が崩れるため RGB空間全体でヒストグラム平坦化をかけています パッと見では暗くて何も見えない画像でも 実はこれだけの情報を持っている 綺麗に再構成できる というのは面白いですね 写真を撮った時に やたらと暗くなってしまったからと言って 諦めなくても良いかもしれません      Denoiseこちらはメディアンフィルタを使ったノイズ除去処理です 各画素値を 対象の画素を含む周辺画素の中央値 メディアン で置き換える事によって ごま塩型のノイズを除去する事ができます シンプルな処理でノイズが除去できる一方 全画素を周辺画素の中央値で置き換えてしまうため 全体的に少しぼやけた画像になってしまっているのも見て取れるかと思います      Unsharp Maskingこちらは画像のボヤけた輪郭を強調する先鋭化処理 アンシャープマスク処理 です 元画像を平滑化した画像と元画像の差分要素を求め この差分を元画像に加算する事で 輝度変化のある箇所を強調する処理になります 先鋭化処理によって ぼやけた元画像がくっきりとした画像になっている事がわかると思います      Area Fittingこちらは二値画像の各領域に対するフィッティング処理です 矩形や円をフィッティングさせる事で 領域の大まかなサイズを抽出する事ができます フィッティングは以下の種類です   Rect：外接矩形 回転なし   Rotate：外接矩形 回転あり   Circle：外接円  Ellipse：楕円フィッティングそれぞれ対象領域に合わせて 矩形や円がフィッティングされている事がわかるかと思います ざっくりのサイズであれば外結矩形 対象が種や細胞のような形状の場合は 楕円フィッティングが良さそうですね    まとめいかがだったでしょうか ちょっとした復習と備忘も兼ねて作ってみましたが 画像処理に触って楽しんで頂けたのであれば幸いです 機械学習の労力の割がデータ収集と前処理と言われるように 画像においても前処理観点での画像処理は重要な上 そもそも 機械学習を使わず画像処理だけで落とせればベストではあるので 画像処理も高いレベルで使いこなせるようになって行きたいですね ■参考  ホテル暮らしはクラウドである    AIの実業務適用に必須なHITLという考え方と HITLを加速させるAI×RPA    RPAの推進に必須なRPAOpsという考え方    VBAが組める人ならRPAは簡単に作れるという罠  個人ブログ  キャリア フレームワークなど  ,69,2022-05-29
160,160,Python+画像認識AIでデジカメ写真を採点したい,画像処理,https://qiita.com/EH-PX/items/e8a8a9f2fe50a8ab895f,   目的で これまでに撮り貯めたデジカメ写真のExifを一覧化した ここから  何年のでもいいから月日の写真 とか  このレンズ＋このカメラで撮った写真 といった条件下での いい写真を探し出したい このため画像認識AIを用いて 写真の採点をするAIを学習したい 前述のExif情報を貯め込んだMySQLデータベースから特定の条件の写真群を抽出し これをAIで採点し ベストな写真を選択したい    環境Windows  bitAnaconda  Python     bit    torch    cu   torchvision    cuNVIDIA GeForece RTX   CUDA     CUDNN       AIの学習まずまず良いと思う写真を枚ほど集めた 苦行だわ そんなにないわ これを  data train  good というフォルダに格納した 次に ゴミのような写真を枚ほど集めてきた 掃いて捨てるほどあるわ さらに ゴミって程ではないが 良いとも言えない写真を枚ほど 各写真をどちらのクラスに分類するかの線引きが難しいが その曖昧さが最終的なAI採点において 点でも点でもない中間スコアを醸し出すのだと思う 迷うような写真は点近辺でいいから どちらに分類してもいいのだ だから学習においてLossが思ったほど下がらなくても それでいいのだと思う TorchvisionのImageNet学習済みAIをベースに使ったので 画像サイズは短辺ピクセルで十分 だが 後々何か細工をするかもしれないと思ったので 今回は長辺ピクセルのデータを準備した なお これは以下のコードには何ら影響しない optimizerはSAMを使った コードは以下のgitより使わせていただいた 学習時のコードは以下 上記のsam pyを同じフォルダに置く Torchvisionの学習済みResNet をベースに 最後のFC層を分類に付け替えて 全レイヤーを再学習する       ResNet の最終FC層を 分類のFCに付け替える      model nameのファイルが既に存在すれば そこから学習再開      GPUが使えれば使う      データセットの準備      最適化手法と最適化指標    criterion   nn CrossEntropyLoss      optimizer   SAM net parameters    torch optim SGD  lr lr  momentum momentum       log準備 追記モード      学習ループ              学習再開時のエポック目は学習しない              val lossが改善した時だけ重みを保存する              フォルダ名を決め打ちしているので注意GPUで時間ほど学習ののち ちょっと早めだが打ち切った   image png     AIの推論テストフォルダ内の写真  test data   jpg について AI判定する       学習したモデルのロード      GPUが使えれば使う          ソフトマックス後のclass の値を出力学習には用いていない適当なデータでテストする から始まるのはゴミまたはいまいちな写真 から始まるのはそこそこ良いと思う写真 まずまず いい感じに採点してくれるんじゃなかろうか    条件にマッチする中から いい写真を選ぶ以下のように MySQLに写真の撮影情報が保管されている という前提 ここからsqlで絞り込んだ写真をAIで採点し スコアの高い写真を選ぶこととする   image png  余談だが 上のようにExcel bit からODBCで接続するにはbit版のMySQL ODBCドライバが必要 下のようにPython bit のpyodbcからODBCで接続するにはbit版のMySQL ODBCドライバが必要だった 両方入れた   学習したモデルのロード  GPUが使えれば使う  〇月×日で検索  Scoreで降順ソート気になるTop は               ScoreDSCF JPG   DSCF JPG   DSCF JPG   DSCF JPG    DSCF JPG   DSCF JPG   DSCF JPG   DSCF JPG   DSCF JPG   DSCF JPG   AI氏のおすすめはこれ↓のようだ 奈良だなあ 悪くはないけどこれが点か  位は連番で どれもほぼ同じような写真だった   image png  ちなみにこの条件下で 私が自分で選んだ写真↓は点だった かろうじて合格   image png  気になる一番ゴミな写真は    うん 確かに これは点だわ   image png  全枚の点数分布をみるとこんな感じ 全体的には点数は甘めかな   image png  この点数をDBに書き込むことで 同じ写真については次回以降はAI判定が不要になる みたいな仕組みも簡単にできると思うが 今回は省略    まとめなど   自分でデジカメで撮った写真を 良い写真   良くない写真 に主観で分けて それを画像認識AIで学習させた    DBに保管したExif情報を用いて 特定の条件で写真を絞り込んで その条件下でのベストと思われる写真をAIに提案させた    AI採点の精度はまあまあ ゴミ写真は正しく判定できそうだが ベストな写真についてはAIと主観が一致するかは 学習方法や学習データ次第だろう    なお ×ピクセルに縮小してからAI判定しているので この解像度ではぶれている ボケているなどが正確には分からないだろうとは思う PatchGANのDみたいに 画像をN × Nに分割して判定して それを合成したものを最終判定値とするとかもありかもしれない    学習時 画像を変形したり 色を変えたりといった一般的なData Augmentationを行っているが  良い写真 というのはDA後も 良い写真 なのだろうか    自分の写真を採点してみたに上げた写真を採点 絶対値はともかく 相対評価としては 割がた納得いくかな 点未満の写真は 苦し紛れに投稿しているのが見透かされているわ ,2,2022-05-29
161,161,DyBNN論文読み,画像処理,https://qiita.com/Etsuwo/items/6ef176bf0660eb0a1259,  はじめにDyBNNの論文を読んだ要約です 自分用にまとめたものなので 理解の漏れなどありましてもご容赦下さい また より詳細な内容を知りたい場合は元の論文をご覧ください     DyBNNの概要  論文名  Dynamic Binary Neural Network by learning channel wise thresholds  著者  Jiehua Zhang  Zhuo Su  Yanghe Feng  Xin Lu  Matti Pietikäinen  Li Liu  公開  年月日  URL  入力によって閾値とシフトパラメータを動的に生成できるDySign関数とDyPReLU関数を用いて ReActNetのRSign関数とRPReLU関数を置き換えたBNN モデルの表現能力が向上し 精度が向上した   ReActNet  URL  ReActNetとは BNNが入力の特徴分布に敏感であることに着目して Sign関数の閾値とPReLU関数のシフトパラメータを学習可能なRSign関数とRPReLU関数を使用したBNN RSign関数およびRPReLU関数は以下の式で表される     問題点Rsign関数は事前に学習された静的な閾値を使用している しかし入力によっては最適な閾値が異なり 静的な閾値では特徴を捉えることが困難である場合がある 下記の画像の場合 左から番目がRSignを使用して特徴検出を行った例であり  b と c の画像の特徴をうまく捉えることができていないことがわかる   図 png     DyBNN入力の特徴分布によって閾値を動的に生成可能なDySign関数とシフトパラメータを動的に生成可能なDyPReLU関数を使用したBNN ReActNetで静的な閾値を使用していたために 特徴情報の損失を起こす問題を改善した DySign関数とDyPReLU関数の式自体はそれぞれRSign関数とRPReLU関数と同様である RSign関数やRPReLU関数では閾値やシフトパラメータを学習により決定していたが DyBNNでは以下の式で表される    mathα   f X    FC  \frac C   ×C  FC  C×\frac C     GAP X   ここで αは閾値またはシフトパラメータ FCは全結合 GAPはGlobal Average Poolingである     評価以下のデータセットと実装で評価 項目 内容  データセット ILSVRC ImageNet classification dataset  学習方法 段階に分けて学習  最適化方法 Adam  線形学習率減衰  初期学習率 e   バッチサイズ      結果mobileNetVベースのDyBNNはTop精度  を達成 これはmobileNetVベースのReActNetと比較して  の精度向上   スクリーンショット       png    まとめBNNにおいて 入力に対して動的に生成可能な閾値やシフトパラメータを用いる手法は有効である ,1,2022-05-24
163,163,画像のトリミングとモノクロ化(Kindle for PCのスクショを撮る2),画像処理,https://qiita.com/dengax/items/5eb93560112da0a99131,  前文のつづきです   前回から二ヶ月ちょっと時間がかかりすぎですね スクリプト自体はかなり早くに出来ていたのですが 譚ページで撮ったときに見開きページをどうするか 見開きで撮ったときに単ページに分割するか その辺どうしようかと思っていたり スクリーンショットの部分をもう少し改良したかったりしたかったので そちらその辺で色々調べていたら放置になっていましたあと あんまり書くことがなくて     ソースコードtrim margins               サイズ自動設定のときのマージン 左 右 上 下   使い方gray check margins グレイスケールを判定する時に検出しない上下左右ピクセルtrim margins トリミングを判定する時に検出しない上下左右ピクセルを書き換えて 実行すれば 指定したディレクトリにある全ての画像に対してトリミングと白黒判定をします コミックかどうかを判定する関数自体は作りましたが 実際には使っていません 小説だと変な結果になるので 使用するのはコミックだけにしてください  スクリプトについて    opencvのimread imwriteでは日本語ファイル名を上手く扱えない前回はこのことをすっかり忘れていて カレントディレクトリを変更して強引に対策しましたが 今回はここをそのまま利用しました 個人的にはcatchする必要は無いか raise eでそのまま例外出す方がいいと思うのですが そのままにしてあります     ndarrayはできる限りforを使わないndarrayは条件式でも範囲を指定できるので こういう部分もできる限りndarrayの機能を使うのが良いです    Python標準のanyとnumpyのanyはちょっと違うPython標準のany allは関数ですが ndarrayではメソッドです ちょっとわかりにくいですね,0,2022-05-23
164,164,公式ロゴの画像変更(.eps→.png)とリサイズで死にそうになった話。,画像処理,https://qiita.com/naiveprince0507/items/3fc19a675d8d3492898f,自社の会社ロゴをリサイズして 外部サイトにアップする必要性が生じたのですが 死にそうになったので 備忘録を兼ねて記載します 自社のマーケティング部門からは提供されたロゴは  epsと言う拡張子でした まず  epsって何だ  と言うことですが 私も知らなかったので まずその調査から始めました 下記サイトに詳しいのですが 要は JPEGやPNGよりも高品質な画像ファイルとの事です デザイナーなら使い慣れているらしいが それ以外の人には馴染みのない拡張子の様ですねえ   一方で 自社ロゴを貼り付ける外部サイトは  epsをサポートしません つまり jpeg peg pngみたいな 普通の拡張子の画像 しかサポートしないので まず  epsを変換しないといけません  今回は pngに変換することにしました  従って 手順としては下記です ロゴをリサイズしてアップする手順に関して ① epsファイルを  pngファイルに変更して ② 更に その pngファイルをリサイズして 外部サイトにアップする こんなのすぐ出来るだろうと思ったけど 甘かったですなあ   ①は あっさり見つかりました このサイトは ネット上のサービスなので アプリを自分のPCにインストールする必要はありません ②のアプリの選定に難航しました ネットでは色々なソフトが紹介されていますが どれもキチンと動作しません 〜個のアプリをチェックして 下記がベストと断定します 笑まず 操作が簡単 そして リサイズがキチンとできます これも ネット上のサービスなので アプリを自分のPCにインストールする必要はありません そんな訳で  epsファイル変換問題は解決しました 各種検証で 〜時間掛かりました 皆様のお役に立てば ,0,2022-05-21
165,165,Teachable Machineを使って間違えやすい野菜を見極めてもらった,画像処理,https://qiita.com/kmt890/items/98f81902503619eb91c4,  間違えやすい野菜をTeachable Machineに見極めてもらいたい　私は小売業でネット販売の運営を仕事としております 今回はTeachable Machineを使って ネットスーパーでお客さまごとに仕分けする際に間違えやすい野菜のパターンのトップを見極めてもらうプログラム作成に挑戦しました    なぜこの野菜に見極めに必要なのか　ネットスーパーの新人が間違えやすい野菜をシステムに確認して答えてもらえると大きなサポートとなります 出荷時間が決められている中 ネットスーパーのお客さまのご注文ごとの商品仕分けで間違えやすいのが  番目にらと小ねぎ  番目サニーレタスとグリーンリーフ  番目じゃがいも だんしゃく とじゃがいも きたあかり です 売場からお客さまがご注文した商品を集める時は売場のPOPがついているのでほぼ間違えませんが お客さまごとに商品を仕分けする時は作業場所の広さの関係上 POPはついていない場所にまとめて置くことと 袋に商品名が書いていない場合もあります 作業場所では売場と違い 商品を見極めるヒントが減るので見間違えが発生します いずれの商品もJANコードが産地で違ったり バラ売りということでシステム対応が難しい商品群のため 画像認識の力を使って間違えを減らしたいと思います     Teachable Machineでの制作にら 小ねぎ サニーレタス クリーンリーフ じゃがいも だんしゃく  じゃがいも きたあかり をTeachable Machineに画像記憶させます それぞれの野菜に応じて特徴となる色合い 形状の部分を念入りに記憶させました 例としてサニーレタス グリーンリーフの画像を掲載しました 見分けのポイントは葉っぱ色の違いですが 商品は袋に入って濡れており見えずらくなっており見間違えそうになります サニーレタスの画像  サニーレタス jpeg  グリーンリーフの画像  グリーンリーフ jpg  Teachable Machineで作成した画像小ねぎをパソコンカメラに向けると小ねぎという正しい判定結果で表示が出ています   野菜判定 JPG     CodePenでの制作スマートフォンで動くようにCodePenを使ってTeachable Machineで作ったコードをHTMLにしてからURLをつくりました スマートフォンで動かした結果は  〇 じゃがいも きたあかり  〇 グリーンリーフ △ 小ねぎの結果となりました      筆者の後記今回はTeachable MachineとCodePenを使って ネットスーパーで間違えやすいにら 小ねぎ サニーレタス クリーンリーフ じゃがいも だんしゃく  じゃがいも きたあかり の野菜を記憶して間違わないように判定してもらうプログラムを作成しました 小ねぎ にらはスマートフォンのカメラを使った段階で正確性が下がりました 正確性を高めるには覚えさせる人が野菜の特徴を理解して見極めるポイントを記憶させないと正確性が上がらないとを感じました 今後は商品群 仕様用途を広げて新人のサポートになるように活用していきたいと思います ,1,2022-05-19
166,166,顔照合用の顔画像データの作り方,画像処理,https://qiita.com/nonbiri15/items/fb3c9219b03a6015dd76,  なぜ 自前の顔画像データなのか  あなたの利用目的で そのライブラリが十分な顔照合性能をもっているかどうかを評価するには   実際に使って評価する  ことだ   その評価を 再現性のあるものにするには 画像データを照合用のデータセットとして 利用可能になっていることだ   さらに 顔照合を利用するシステムを作っていると 数年おきに顔照合ライブラリの見直しをする必要にせまられだろう 機械学習に関わるライブラリやその土台となるデバイスはまだまだ 進化の途中であるからだ   そのため 自前で作った顔画像のデータセットは 数年後にも出番がやってくる そういうわけで 自前のデータセットの作り方について述べる   想定するユースケースは  想定するユースケースでの顔の大きさ 画像サイズ中の比率   カメラの設置高さ 視線と同じ高さか 見下ろし画像か   顔の向きの多様性   照明条件   被験者の人数の規模こういった条件の違いが ユースケースの中での顔照合ライブラリの性能に影響してくる パスポートでの顔照合の精度が高いものが そのままあなたのユースケースでの精度になるわけではない    自前のデータセットを整備する目的を明確にする   被写体の年齢層の分布はどうするのか   撮影時の被写体は 協力行動がある ＝照合のために人がカメラを覗き込むなど ときの画像か 協力行動なしの画像か   被写体の顔を撮影するカメラは 天井にあるのか 目線高さにあるのか   安定した照明条件が得られるのか 照明条件は変動が大きいのか  顔の一部が隠れるのをどの程度まで許容するか       目的によって顔の隠れ具合の許容幅が違ってくる − 被験者の人数の規模は ：N照合をする際の精度に影響してくる 人数が増えてくると他人の空似が増えてくるから 運用目的によって 必要な評価用の人数の規模が変わってくる 顔だけの情報でビル内のコンビニでの支払いをしようとするならば どれくらい間違いを減らすかが重要になってくるだろう  ビル内のコンビニを利用する人の数に制限があったとしても ユースケースによって許容できるものは変わってくる 犯罪捜査のための顔照合の場合には 上位何件の中にその人物が見つかればよいという運用もある    既存ライブラリを利用して 評価用の顔データセットを作る   顔が写っている一連の画像の中から 既存の顔検出ライブラリを使って 顔検出結果の顔切り出し画像を集める   顔の未検出画像を集めて それらに対しては 別の顔検出ライブラリを使って追加の顔検出を実行する   その後の  未検出画像に対しては 手作業で顔の切り出し  を実施する   自動検出と手作業での顔検出とによって 顔を切り出せば 全数手作業で顔検出の正解データを用意するよりは格段に楽になる     照合用顔データは 切り出し画像を使うのが実際的  原画像を そのままの解像度で保存すると 横の幅の くらいだと倍以上ディスクスペースを消費してしまう   切り出し画像を使うことで 同じディスクスペースで倍の顔を扱える   照合について考えるときには 誤検出とか 元画像全域からの検出時間などについて考える必要はない   だから 配布されている顔データベースも マージン付きの顔の切り抜き画像であるのが大半だ   ここでは 切り抜き画像での顔データベースを作る方法について述べる     例： 被写体になっている人物が少人数に限られている動画からの切り出しこの場合だと 少ない人数の範囲で画像を各人物ごとにつのフォルダにグルーピングしていくのが簡単になる 顔照合の評価を 顔向きの変動や照明条件の変動 被験者の表情の変動などに対して実施したいのであれば そのような変化のある動画を利用し 顔画像の自動での切り出しをすることだ     おすすめしない例：若い女性アイドルグループが多数のメンバーが同時に写っている動画からの切り出し 正解データを作る際の手間が増えてしまう また間違ったラベリング ＝名前を間違って付ける してしまう可能性が増えてしまう そのためデータを用意する際の作業効率が落ちてしまう 若い女性でアイドルグループに属する人の顔は 平均顔に近い顔つきの人が多くなるので 人による顔照合でもだれがだれかわかりにくくなりやすい どうしてもそのグループのメンバーでデータを作りたいときは 少人数だけ写っている動画を選んで切り出すことをおすすめする      そのグループについて一人ひとりの名前を間違えずに言える人はその限りではない  愛があれば見分けられる 人はその限りではない      照合用の顔画像のデータセットのディレクトリ構造顔照合用を評価するためのデータセットのための構造は フォルダ名がその人物の名前になるようにしておくのがわかりやすい それぞれの画像は 中央にその人物の顔が写っているように一定のマージンをつけて保存する         │   ├──  jpg        │   ├──  jpg        │   ├──  jpg        │   └──  jpg        │   ├──  jpg        │   ├──  jpg        │   ├──  jpg        │   └──  jpg        │   ├──  jpg        │   ├──  jpg        │   ├──  jpg        │   └──  jpg   valid フォルダには独立性の高いデータをvalidデータには 撮影環境を変えたデータを用いよう 同じ日にち同じ照明環境の撮影にすると test  trainのデータと似ているので 当然顔照合での類似度が高くなってしまうので 本当に顔照合に成功しているのかどうかが判明しなくなってしまう そのことを防ぐために 独立性の高い撮影をしよう    人物名を示すラベルはフォルダ名にここで重要なことは 人物名を示すラベルはフォルダ名で指定すること そうすることで フォルダを開くことで その中にあるのが同一人物の顔画像だけであるのかを確認しやすい 　ファイル名には 必ずしも人物名を含める必要はない ただ 一連の画像の中でユニークなファイル名になるのがよいと考えている 　ツールでの扱いやすさのためには 半角スペースやファイルシステムやシェルで特別な意味をもつような文字を含めないようにする     ファイル名に対象の人物名を含むファイル名にしない理由 biden biden jpg などとしない理由 顔データセットを作る途中では 切り出し元の静止画 動画に関連付けて切り出し画像を管理したい そうすれば 訓練用の画像と検証用の画像に 同じ動画から切り出していないかが確認できる 対象の人物名はフォルダにだけ含めていれば フォルダ名こそが対象の人物名であることが明確になる     整備中の顔画像のラベル名 ＝フォルダ名 は一貫性のある名称を付ける   整備中の顔画像のラベル名 ＝フォルダ名 は person などといった無機的なものにしない 間違った分類がされていても気づきにくくなるからだ     ディレクトリ構造を標準化する利点  ツールを作る際の想定するディレクトリ構造 フォルダ ファイルの命名規則が一貫性を想定できる   そのため 顔につけたラベルが フォルダ名 ファイル名 ファイル名に対応したテキストファイル 一覧のデータのcsvファイルの中などのどれにあるのかと悩まなくてすむ 常にフォルダ名に含めるというのが一貫していれば とても扱いやすい   さらに 異なる撮影データに対して 特定の人物に対するラベルの一貫性が保たれていれば データセットをマージして使うのがとても簡単になる   だから 顔画像のデータセットを作るときには 一貫性のある規則で標準化するのがよい     似すぎている画像を減らす   同一人物の同じ環境で同じ顔向きの画像は 当然ながら似ている   そういった画像で顔照合を実施すれば train とtestにほぼ同じ画像を含んでいるので 顔照合の成功率があがって当然だ   似すぎている画像はwebで静止画を検出して画像を収集した際にも起こりがちだ   類似度が所定の値以上の極端に似ている画像を取り除くツールを作成しよう       スクリプト例      アノテーションを実施するのかどうか  今回のデータセットを作る理由を思い出そう   しなくてもすむアノテーションはしないこと    昔の顔データセットの場合  顔の検出枠の正解データ  両目の位置  顔ランドマークの位置 両目の位置 鼻の位置 口の両端の位置など こういったものがテキストファイルに記載してあることが多かった    今の状況  顔自体の検出率が高くなっているので 顔検出枠について そこまで評価で気にする必要が無くなってきている       横向きの顔画像での顔の正解枠について だれもが合意するような枠は存在しない       深層学習の枠は もはや正方形ではない       顔の検出枠の評価をしようとするのでないかぎり 正解枠をアノテーションする必要性がない   両目の中心位置      深層学習の顔検出ライブラリでは 顔ランドマークの位置を求めるものが増えていて しかもその位置の精度が上がっているライブラリがある       両目の中心位置を手入力する意味がない   顔向きによる画像の分類は 労力が多すぎる       顔の向きを条件をそろえて撮影することは 労力が多すぎる       良いライブラリだと 顔の向きのyaw  pitch  roll が算出される       その値をまずは信用して十分なこともある     作成した顔照合データセットは 社内 部署内で共有化する   ディスクが死ぬ PCが壊れるのはよくあることです   肖像権の懸念が残るので 画像を公表は避ける   あなたがかかわらなくても 社内でそのデータが活用されることが大事   あなたは いずれその部署を離れる   社内 部署内で共有する際には そのデータの制限事項に留意すること    顔照合のライブラリは継続的に改良が進んでいます   顔照合のライブラリを実装するターゲットが進化しています       CPU：マルチコアを最大限に利用するライブラリを用いた実装      GPU：tensorflowなどCudaを利用するライブラリの他にも TensorRTを用いたライブラリも出ています       Intel Movidius Neural Compute Stickを使った実装      FPGAを使った実装こういったライブラリの実装の進化により 演算速度 消費電力当たりの演算速度などが従来よりも向上してきています   顔照合ライブラリのマスク顔対応が進んだ      新型コロナウイルス感染症 COVID‑ の流行以降 顔照合ライブラリでのマスク顔対応が進展しました       マスク顔を顔検出できる マスクをしているかどうかを判定できる マスクをしていてもだれか判定できる       その性能は それぞれの実装のモデルに強く依存しています       複数のライブラリを比較するには 実際に共通のデータセットで評価するのがいいでしょう    顔照合ライブラリは 数年ごとに見直しを迫られることになる   OSのLTS Long Time Service の終了  使用している言語のそのバージョンの終了  顔照合ライブラリとともに使用している各種ライブラリのバージョンの変更   参考 非商用目的なら利用できる顔データセット     企業で利用できる商用顔データセット例：,1,2022-05-17
167,167,スーザン・バリー「視覚はよみがえる　３次元のクオリア」,画像処理,https://qiita.com/nonbiri15/items/8a862b22e5e35da40db0, スーザン バリー 視覚はよみがえる　次元のクオリア      他の人は どのように物を見ているのかを知らない 誰もが同じように物を見ていると思い込んでいる 色覚などは   二色型色覚    以前 色盲 と呼ばれたような色覚 のような違いがあることは理屈では知っている 二色性色覚の場合には 区別のつきにくい色があるので 配色を選ぶ際には 二色性色覚の人にとっても区別のつきやすい色を選択するのがよい    配色のバリアフリー   カラーユニバーサルデザイン推奨配色セットガイドブック  また   斜視  という症状があることを知っている しかし 斜視の症状の人は 斜視での経験しか持たない 斜視でない人は 斜視の経験はないので 斜視の人はきっとこのように物を見ているのだろうと推測するしかない    斜視を克服して立体視を獲得した著者この本は 長いこと斜視だった人物が 歳のときに発達検眼医の集中的な訓練を受けて 立体視を獲得した経験を語っているものである 斜視の人は 協調的に動かせていない両眼の視野が重ならないように 一方の目を極端にそらして その目からの入力を抑制する習慣になっているそうだ 眼位を手術によって矯正するだけでは 立体的に知覚することはできない 著者のスーザン バリーさん  Susan R  Barry  は 視能療法を受けて 立体視の能力を身についている この著者の場合には 視能療法を受けて 立体視の能力を身につけただけではなく 自らも神経生物学者として 既存の画像認識の研究ふまえて 自らの経験とあわせてこの本を執筆している  ヒューベル  と ウィーセル  のネコの視覚に関する研究もその中で述べられている    視能療法視能療法の一つは ブロックひもを使った視覚の訓練だ ビーズを通したひもの一方を鼻先におき ひもの端を遠方に伸ばす そうすると両眼の映像が重なって ひもが本あるように見える  わたしはこの ブロックひも の使い方を学んだおかげで 必要なフィードバックが得られ つの目がどこに向けられているのかを把握して同じ空間に同時に焦点を合わせられるようになった   p     画像認識の視点で興味を覚えた部分 運動視差による奥行き感覚が向上したことに驚いて 図書館でこの話題に関する文献をさらに読んでみた そして わたしたちに立体視力を奥行き感覚もたらすのとまさに同じ神経細胞および脳回路が 運動視差による奥行き感覚をもたらしている可能性があることを知った つまり 立体的に物が見えるようになったおかげで 動きによる奥行き感覚が高まったかもしれないのだ  p   思ったとおり 立体視で奥行きを見る能力と動きの情報から物体の構造を予測する能力には関連があることがわかった  p  以下の内容は 上記の本を離れて 奥行き推定についてのコンピュータビジョンの内容をです    機械学習での奥行きの推測に関しての理解    単眼画像からの奥行きの推定  単眼画像からも奥行きの推定は可能だ しかし 単眼画像から奥行きを推定するための学習では 奥行きの真値を与えて学習することを前提としている   しかしながら 奥行きの真値は ロボットへのビデオ入力そのままからは得ることができない      単眼画像から奥行きを推定できる理由についての私の理解  面の法線の向きによって 光のあたり方 陰のでき方は変わってくる   光のあたり方 陰のでき方を元に 面の法線の向きを推定できる   法線の向きの情報を積算することで 手前側から奥行き方向への奥行き方向成分の積算値が推測できる   その結果 奥行きが推定される 実際はどうなんだろう 参考例    ステレオ画像からの奥行きの推定  コンピュータビジョンの分野では 画像をステレオ平行化したうえで 視差を求めて 奥行きを推定する手法がある   このため この手法で求めた奥行きを 単眼学習への奥行き推定への学習データにすることが可能だ   視差に基づいて奥行きを算出する手法の大半は 対応点が見つからない領域では 奥行きの欠損値をもつ   ステレオ画像からの奥行きと単眼画像からの奥行きの推定とを組み合わせることで 対応点がない領域に対しても奥行きの推定ができないだろうか あるいは 既に解決した実装はないだろうか     深層学習を利用したstereo画像からのdepthの推定従来のマシンビジョンのstereo画像処理では対応点が見つからない画素についてのdepthが欠損値になってしまっていた しかし 深層学習を利用したstereo画像からのdepthの推定では 欠損値を生じさせないでdepthの推定ができる 実装例：    ロボットにとって自らの動きがある場合には 運動からの構造 structure from the motion ができないだろうかこれらは全て 奥行きの推定という点で共通しているものであり 別個に実装が存在するのではなくて 共通の枠組みになった方が 生物の実現している立体視に近づくのではないかと考えている ,1,2022-05-12
168,168,HTML Canvasのインターフェイスのまとめ #1,画像処理,https://qiita.com/simonritchie/items/36e34bf409b6bd10f23a,HTMLのCanvasを触れ始めたので備忘録として記事に残しておきます   前置きフロントエンドとかを専門 生業 としているわけではないので 知識や理解が浅い点などはご容赦ください   本記事で触れることCanvasのfillやlineToなどの各描画のインターフェイスについて触れていきます 画像処理やアニメーション テキスト関係などは今回は触れません また 記事にしていて描画関係だけでもかなり長くなったので全ては触れずに記事を分割したりしようと思います 本記事はつ目の記事に該当します    HTML Canvasって何   Canvasの特徴   HTMLとJavaScriptを使って描画処理や画像変換などを行える機能です   SVGとかでは描画領域が巨大になっても負荷になりにくい 一方で非常に細かい描画で負荷になりやすい 一方でCanvasは描画の範囲が負荷になり 細かい描画とかでは負荷になりにくい特徴を持ちます つまり描画サイズはそこまで大きくないものの要素が膨大な場合のグラフ描画とかに SVGよりも 向いています   その他ピクセル単位で描画データの加工なども行えるため フォトショップで昔からあるようなフィルターのような制御も行うことができます ※SVGに関しては以前以下の記事などでいくつか記事にしているので必要に応じてそちらをご確認ください   本記事で使うもの  HTMLと若干のCSS  JavaScript TypeScriptやライブラリなどは今回触れません ※CSSとJavaScriptは手間なので今回は直接HTML内で記述していきます ※普段Phthonなどばかり書いていて設定の切り替えが手間なのでインデントはスペースつではなくつのまま進めていきます   Canvas最初の一歩ごくごくシンプルな例でCanvasを使い始めてみます    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     aaff          context font    px Arial          context fillText  Hello World        上記のHTMLをブラウザで開いてみると以下のようにテキストが表示されることが確認できます   image png  Chromeなどのブラウザ上で テキストの上で右クリックしてみると通常のコンテキストメニューではなく 名前を付けて画像を保存 といったように画像ファイルのようにCanvas用のコンテキストメニューになっていることを確認できます   image png  それぞれの記述をつつ見ていきます    htmlまずは上記のHTMLタグ部分ですが このタグがCanvas要素となります 幅や高さはCSSではなくHTMLタグに直接指定する方が推奨されるようです 微妙にCSSなどで設定した場合と挙動が変わります     js        let canvas   document getElementById  canvas   続いて上記のjsでCanvas要素を取得しています これはCanvasとか関係無く一般的なjsの記述なので説明は割愛します    js        let context   canvas getContext  d   js上で取得したCanvas要素はgetContextメソッドを持っています このメソッドで描画制御など諸々を制御するためのコンテキストのインスタンスを取得することができます 引数に対象のコンテキストの種類を指定します Dのものなどもあるようですが今回はDのみ触れるためdと引数に指定しています    js        context fillStyle     aaff          context font    px Arial  上記の部分では描画のスタイル設定を行っています fillStyleは塗りの色設定 fontでは文字サイズとフォントの種類を指定しています このようにcontextのインスタンスでは各描画の設定を行うことができます 描画設定だけでなく 後述するように実際の描画もcontextインスタンスを用いて行います     js        context fillText  Hello World        最後に上記のようにfillTextメソッドで文字を描画しています 第一引数に描画する文字列 第二引数にX座標 第三引数にY座標といったように指定します fillTextなどのテキスト関係の詳しい説明は他の記事で触れていこうと思います    各描画設定   描画設定の引き継ぎについて各描画設定は基本的に同じCanvasのコンテキストのインスタンスであれば引き継がれます 例えば一度塗りの色を設定して次に四角を描画 その次に円を描画   とすれば四角も円も同じ塗りの色となります    fillStyle設定fillStyle属性では塗りの色を設定することができます   aaff といったような進数の設定や blue といった色の文字列  rgba          といったようなRGBA表記などCSSのように色々指定できます    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context fillStyle    red          context fillRect                 context fillStyle    rgba                   context fillRect           image png     グラデーション設定 createLinearGradientとaddColorStopメソッド ※Canvasの円形のグラデーションは結構設定と挙動が独特   と感じたのと個人的な利用機会は少なそうな気がしているため線形のグラデーションのみ触れていきます グラデーション設定は単純な単色の塗りの設定よりもコードが複雑になります グラデーションを設定するには以下のような手順が必要になります   createLinearGradient メソッドでグラデーション設定用の CanvasGradient クラスのインスタンスを作成します   作成した CanvasGradient クラスのインスタンスの addColorStop メソッドでグラデーションの色の変化点を設定できるのでそれを複数回呼んで色と位置を設定します   作成した CanvasGradient クラスのインスタンスをコンテキストの fillStyle 属性に設定します   任意の図形の描画などを行います createLinearGradient メソッドの第一引数はグラデーションの開始位置のX座標 第二引数はグラデーション開始位置のY座標 第三引数はグラデーション終了位置のX座標 第四引数はグラデーション終了位置のY座標となります 後に続く描画処理と座標が一致していないと想定したグラデーションの描画になりません addColorStop メソッドでは第一引数にはグラデーションの位置の比率値      第二引数にグラデーションの色の文字列を指定します 第一引数の比率は でグラデーション開始位置  でグラデーションの中央位置  でグラデーションの終了位置となります 以下の例ではX座標が の位置でシアン   af  からマゼンタ   fa  へのグラデーションを addColorStop メソッドで設定しています    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           let gradient   context createLinearGradient                 gradient addColorStop      af           gradient addColorStop      fa           context fillStyle   gradient        context fillRect           image png  注意点としてグラデーションの座標設定と実際の描画の座標がずれていると想定したグラデーションになりません たとえば以下のようにグラデーションの開始X座標が 終了X座標がとなっている一方で四角の描写はXの開始座標が   となっているとごく一部のグラデーションしか四角に反映されません    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           let gradient   context createLinearGradient                 gradient addColorStop      af           gradient addColorStop      fa           context fillStyle   gradient        context fillRect           image png     パターンによる塗り設定※動画の要素もパターンに指定できるようですが個人的にひとまずは使うことは無さそうに思えたので動画関係はスキップします     画像でパターンを指定する場合まずは画像のパターンを塗りに設定するケースに触れていきます サンプルとして以下の画像を kikubishi s jpg という名前でローカルに配置している想定で使用していきます ※以下のフリー素材を使用させていただいています 画像のパターンを塗りに設定するには以下の手順で設定する形となります   対象の画像の読み込み設定を行います   画像の読み込みが終わったらコンテキストの createPattern メソッドでパターンのインスタンスを作成します   作成したパターンのインスタンスを fillStyle 属性に指定します   任意の描画処理を行います createPattern メソッドの第一引数には画像のインスタンス 第二引数には繰り返し設定の文字列の指定が必要です 第二引数の繰り返し設定は以下のような値を設定できます   repeat  縦と横方向両方に対して繰り返しのパターンを設定します   repeat x  横方向のみ繰り返しのパターンを設定します   repeat y  縦方向のみ繰り返しのパターンを設定します   no repeat  繰り返しを行わず画像を回のみ塗りに設定します 画像のパターンで四角を描画することができました 以下にコードを補足します まずは画像の読み込みですが JavaScript上ではImageクラスで行うことができます srcで画像のパスを指定し onloadで関数を指定することで画像読み込み時のイベントを設定することができます パターン関係の制御は画像の読み込みが終わってから実行する必要があるためパターン関係の制御はそのイベントの関数内で実施しています 後はその関数内で createPattern メソッドでパターンのインスタンスを生成し そのパターンを fillStyle 属性に設定して四角を描画しています 上記の例では createPattern の第二引数に repeat と指定しているため縦横両方の方向にパターンが繰り返し描画されます     他のCanvasでパターンを指定する場合画像 Imageクラス などの代わりに別のCanvasを指定することもできます その場合には createPattern メソッドの第一引数に画像の代わりに他のCanvasのインスタンスを指定するだけで対応することができます 以下の例では patternCanvas という変数名で別のパターン用のCanvasを追加しています 幅高さのサイズで Canvas内で余白サイズで四角を描画しています  fillRect           また パターンのCanvas自体は表示しなくて良いので display  none  のCSSを指定して非表示にしています 結果的に patternCanvas の四角を繰り返したパターンでCanvasの塗りが設定されます   image png     globalAlpha設定globalAlpha属性はCanvas全体の透明度を設定できます   透明    不透明 の間で設定します 個々の描画ではなくCanvas全体の透明度が変わる点に注意が必要です    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context globalAlpha             context fillStyle     af          context fillRect                 context fillStyle     fa          context fillRect           image png     globalCompositeOperation設定globalCompositeOperation 属性はCanvasで複数の描画を行った際の既存の描画との合成方法を変更します Photoshopなどで言えば描画モード Blend mode に近いイメージです  context globalCompositeOperation    対象の設定  といったように文字列で指定します 様々な設定があるのでつつ後の節で触れていきます デフォルトでは source over の設定となります globalCompositeOperation の設定を省略した場合も source over 相当の挙動になります  これは後から描画したものが上に来る形となります 以下のコード例では最初にシアンの四角を描画し その後に一部が重なる形でマゼンタの四角を描画しています  source over としてそのまま後のマゼンタの四角が上に乗っかっていることが確認できます    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context fillStyle     fa          context fillRect           image png  以降の節での各設定は上記のコードのつの四角をベースに進めていきます     source in 設定source in設定は描画前のCanvas内の描画とsource in設定後に追加する描画間で重なり合う部分のみが描画されます 重ならない部分は描画されなくなります    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context globalCompositeOperation    source in          context fillStyle     fa          context fillRect         上記のコードではサイズのシアンとマゼンタのつの四角を描画していますが 重なり合う真ん中の一部の部分のみ残っていることが確認できます また 重なっている箇所は後から描画するマゼンタの四角の方が上に来るため結果的にシアンの四角はまったく描画されない状態になっています   image png  また  context globalCompositeOperation    source in   の指定は最初のシアンの四角の描画が終わって且つマゼンタの四角を描画する前の位置で指定している点にも留意してください シアンの四角の描画前などに設定してしまうとシアンの四角描画時にはCanvas上の描画は空になっているため描画が重なり合う部分は皆無で結果的に全て透明となり さらにその後に続くマゼンタの四角の描画時でもCanvas上の描画は空になっているので結果的に何も描画されなくなってしまいます     source out 設定source outはsource inと逆の挙動をします つまり描画前のCanvasの現在の描画と新たな描画で重なる部分が透明となり 重ならない部分のみが描画されます また 描画前の既存の描画に関しては透明になります 以下のコード例では中央の重なっている部分は透明となり そして既存の描画に関しても透明になるため結果的に後から描画したマゼンタの四角の右下の方の部分のみ描画されます    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context globalCompositeOperation    source out          context fillStyle     fa          context fillRect           image png      source atop 設定source atop設定は既存の描画と設定後の新たな描画で重なる部分の描画が残ります また 既存の元の描画部分も残ります 以下のコード例では中央部分の一部重なる部分はマゼンタの四角が残り そして既存の元の描画も残るためシアンの四角部分も残っています    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context globalCompositeOperation    source atop          context fillStyle     fa          context fillRect           image png      destination over 設定destination over設定はsource over設定 後から描画したものが上に来る設定 と逆の挙動をします つまり先に描画したものが上にきます 以下のコード例ではシアンの四角を先に描画した一方で後から描画したマゼンタの四角の上に来ていることが確認できます    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context globalCompositeOperation    destination over          context fillStyle     fa          context fillRect           image png      destination in 設定destination in設定はsource in設定 重なり合う部分のみを描画し 後から追加したものが上に来る と同じように重なり合う部分のみを描画します ただしこちらは先に追加したものが上にきます 以下のコード例ではつの四角で重なり合う中央部分のみが残り そして先に描画したシアンの方の四角が上に来る形になることが確認できます    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context globalCompositeOperation    destination in          context fillStyle     fa          context fillRect           image png      destination out 設定destination out設定は重なり合わない部分のみ描画し 且つsource outとは逆に先に描画した方のみが残ります 後から描画した方は残りません 以下のコード例では先に描画したシアンの四角のみが残り 且つ重なりあっていない領域のみ残っていることを確認できます    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context globalCompositeOperation    destination out          context fillStyle     fa          context fillRect           image png      destination atop 設定destination atop設定はsource atop設定と同様に重なり合う部分が残ります また source atopとは逆に重なり合う部分は先に描画した方が残され 且つ後から描画した方の重なっていない領域も残されます    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context globalCompositeOperation    destination atop          context fillStyle     fa          context fillRect           image png      lighter 設定lighter設定は重なり合う箇所が明るくなる形になります 描画モードの加算に近い感じでしょうか    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context globalCompositeOperation    lighter          context fillStyle     fa          context fillRect           image png      xor 設定xor設定は重なり合う部分は透明となり 重ならない部分のみ描画が残ります    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context globalCompositeOperation    xor          context fillStyle     fa          context fillRect           image png      multiply 設定multiply設定はいわゆる描画モードの乗算です 重なり合う部分が暗くなります    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context globalCompositeOperation    multiply          context fillStyle     fa          context fillRect           image png      screen 設定screen設定はいわゆる描画モードのスクリーンです 計算内容までは今まで把握していなかったのですが どうやら重なり合う部分の色を反転させ それぞれを乗算した上でさらに反転させる   という計算のようです 結果的に重なる部分は明るくなります    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context globalCompositeOperation    screen          context fillStyle     fa          context fillRect           image png      overlay 設定overlay設定はいわゆる描画モードのオーバーレイです それぞれの重なり部分で先に描画されている領域が後から描画される領域の色よりも暗ければより暗く 明るければより明るくなる計算となります 結果的にコントラストの強い画像になります     html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context globalCompositeOperation    overlay          context fillStyle     fa          context fillRect           image png     strokeStyle設定strokeStyle属性は線の色を設定できます fillStyleと同様に進数でのカラーコードやredなどの色名 rgba     といった指定などができます    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context strokeStyle     af          context lineWidth            context strokeRect           image png     lineWidth設定lineWidt設定では線幅を設定できます    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context strokeStyle     af          context lineWidth            context strokeRect           image png     lineCap設定lineCap属性は線の端に設定するスタイル設定です 文字列で指定します SVGなどと同様に以下の設定が存在します   butt  デフォルトの設定です 端に対して特にスタイルを設定しません   round  線の端を丸くする形のスタイル設定です   square  線の端に四角を追加する形のスタイル設定です buttと見かけ上は似ていますがbuttに対して線幅分だけ線が長くなります round設定時と同じ長さになります  以下のコードではそれぞれ上からデフォルト butt  round squareを設定しつつ線を描画しています beginPathを使っていますがbeginPathは本記事では割愛します 次の記事などで触れる想定です     html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context strokeStyle     af          context lineWidth            context moveTo             context lineTo             context stroke           context lineCap    round          context beginPath           context moveTo             context lineTo             context stroke           context lineCap    square          context beginPath           context moveTo             context lineTo             context stroke     image png     lineJoin設定lineJoin属性は折れ線などの折れの部分 連結部分 のスタイルを設定します 文字列で以下の値を受け付けます   miter  デフォルト値です 折れの部分が鋭い感じになります   round  折れの部分が丸くなります   bevel  折れの部分が削られて斜めになります ベベル  bevelとかは文字での説明よりも結果を見たほうが分かりやすそうです 以下のコードでは左の折れ線はデフォルト miter  真ん中にround 右にbevelを設定しています    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context strokeStyle     af          context lineWidth            context moveTo             context lineTo             context lineTo             context stroke           context lineJoin    round          context beginPath           context moveTo             context lineTo             context lineTo             context stroke           context lineJoin    bevel          context beginPath           context moveTo             context lineTo             context lineTo             context stroke     image png     miterLimit設定miterLimit設定は折れ線の折れの部分のmiter設定での上限比率が設定できるようです 小さい値などを設定した際には尖った折れの部分ではなくbevelのように削られた感じになります    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context strokeStyle     af          context lineWidth            context miterLimit            context moveTo             context lineTo             context lineTo             context stroke     image png    描画の各インターフェイス   四角を描画するfillRectメソッドfillRectメソッドでは四角を描画します 塗りの領域のみで境界線などは描画されません 第一引数はX座標 第二引数はY座標 第三引数は幅 第四引数は高さの指定となります    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect           image png     strokeRectメソッドstrokeRectメソッドでは四角の境界線部分を描画することができます 塗り部分は描画されません    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context strokeStyle     af          context lineWidth            context strokeRect           image png  なお 他の図形などでも同じような形となりますが塗りと線両方描画したい場合にはfillRectとstrokeRectの両方のメソッドを続けて描画する形となります 以下の例では fillRect と strokeRect メソッド両方を同じ座標とサイズで指定しているので塗りと線両方が設定されます    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     ef          context strokeStyle     af          context lineWidth            context fillRect                 context strokeRect           image png     clearRectメソッドclearRectメソッドでは指定した四角の領域の描画を削除します 第一引数はX座標 第二引数はY座標 第三引数は幅 第四引数は高さとなります 以下のコード例では context clearRect          という指定をしており 四角の中の一部の描画が削除されます    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context fillRect                 context clearRect           image png     fillメソッドfillメソッドは現在設定されているパス設定に対して塗りの描画を行います パスを描画するrectやarc lineToメソッドなどを呼び出した後に実行します rectやlineToメソッドなどのパス描画のメソッドは複数回呼び出しておいてからfillメソッドを回だけ呼んでパスのセットに対して描画を行うといったことも可能です また rectやlineToメソッドなどはパスの設定のみで実際の塗りや線の描画は実行されません fillや後述するstrokeメソッドを呼び出した時点で初めて描画が実行されます 以下の例ではrectメソッドで四角のパスを設定し fillメソッドで描画を行っています rectメソッドに関しては後の節で詳しく触れます  ※説明のためにシンプルな記述としてありますが これだけであればfillRectの方が同じ内容をシンプルに描画ができます    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context rect                 context fill     image png     strokeメソッドstrokeメソッドはfillメソッドの線描画版のメソッドとなります 設定されているパスに対して境界線の描画を行います 複数件のパスに対して一括で描画が行える点などの挙動はfillメソッドと同じような挙動になります    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context strokeStyle     af          context lineWidth            context rect                 context stroke     image png     rectメソッドrectメソッドでは四角のパスを設定します これだけだと描画はされないので別途fillやstrokeメソッドなどの描画のインターフェイスを呼び出す必要があります 他の四角関係のインターフェイスと同様に第一引数はX座標 第二引数はY座標 第三引数は幅 第四引数は高さとなります    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context fillStyle     af          context rect                 context fill     image png     arcメソッドarcメソッドでは円弧のパスを設定します これだけだと描画はされないので別途fillやstrokeメソッドなどの描画のインターフェイスを呼び出す必要があります 第一引数は円弧の中央のX座標 第二引数は円弧の中央座標のY座標 第三引数は円弧の半径 第四引数は開始角度 第五引数は終了角度 そして第六引数は省略可となりますが描画方向を半時計周りにするかどうかの真偽値となります デフォルトでは時計周りとなります  第四引数と第五引数の値はラジアンの値となります 度から変換する場合には 度の値 × π    といった計算が必要になります また 開始角度は上からではなく右からスタートとなるようです 以下の例では円弧の中央座標にを設定し 半径にを設定し 度 度に対して描画の設定をして半円を描画しています 注意点として円弧の中央座標 → 開始座標 → 終了座標とパスが設定されるのではなく開始座標 → 終了座標とだけ描画されます そのため終了座標が度未満の場合には中央座標付近は描画されなくなります その辺りの描画が必要な場合には別途パス設定などが必要になります 以下の例では終了角度を度にしているため円弧の中央付近は描画されていない状態になっています   image png  第六引数の真偽値はtrueを設定すると描画が半時計周りの方向に対して実行されます 以下の例ではtrueを指定しているため半時計周り方向になり 半円の描画方向が下ではなく上に変わっています lineToメソッドは指定された座標に対して線のパスを設定します 第一引数はX座標 第二引数はY座標となります 複数回連続して呼び出すことで折れ線グラフなどの表現ができます また 最初のlineTo 最初のパス設定 の分は描画されないようです 最初は後述するmoveToメソッドを使う方が直観的かもしれません  以下の例では回lineToを呼び出しています 最初の context lineTo      の分は描画されていません       から      へは描画されていないことを確認できます     html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context strokeStyle     af          context lineWidth           context lineTo             context lineTo             context lineTo             context stroke     image png     moveToメソッドmoveToメソッドはパスの位置を指定された座標に移動させます 現在位置から移動先の位置へはパスは設定されず その後に続くパス設定は新たなパスとなります 第一引数は移動先のX座標 第二引数は移動先のY座標となります 以下の例ではlineToの間にmoveToメソッドを挟んでいるため つの線のパスは連結されておらず別のパスとなっています    html        let canvas   document getElementById  canvas           let context   canvas getContext  d           context strokeStyle     af          context lineWidth           context moveTo             context lineTo             context lineTo             context moveTo             context lineTo             context lineTo             context stroke     image png    参考文献 参考サイト,1,2022-05-12
169,169,ゼロから作る物体検出(object detection),画像処理,https://qiita.com/shushin/items/35dbbf8274cbfae1c18f,   はじめに  ゼロから作るdeep learning ――フレームワーク編   で解説されたdeep learningのフレームワーク DeZero  を使って 物体検出を行います 本記事のプログラムと使用したデータセット 画像とアノテーション はこちらからダウンロードできます Dezero自体がpytorchと似ているので Dezero部分を書き換えるとpytorchでも使えます  pytorchを使ったほうが便利な機能が多数   tensorflowはﾜｶﾘﾏｾﾝ 実際の検出例がこちら   ダウンロード    png  ダウンロード    png  実装及び学習を簡単ににする目的で ここで作る物体検出はYoloを参考にしつつも  Yolo v  よりもさらにシンプルなものを作ります 具体的な制限としては以下のようなものがあります   backboneはVGGで古い DeZeroで使えるpretrained modelの制限   クラスしか検出できない 今回は写真中から玩具の車を抜き出します   一つのグリッド内で複数の物体を検出できない  損失関数も簡略化なので 本記事で最新鋭の物体検出モデルができるわけではないです 使用しているのはこれだけです python   DeZero   numpy   cupy   pillow   matplotlib   Dezeroは簡単にpip install dezeroでインストールできます cupyは必須ではないですが 学習時間が大幅に短くなるので かなり効率が違います OSはwindowsで GPUとしてGeforce GTX GBを使っています      データをpythonに取り込む物体検出では 画像と画像中の物体の位置のデータが必要となります このような感じですね   aa jpg  このうち物体の位置のデータ アノテーション の記述方法に関しては MS COCO等いくつかの形式がありますが 今回はPascal Voc形式を使います このPascal Voc形式に関してはxmlで保存されています ここで重要となるのはまず上から行目の＜path＞ ＜ path＞になります ここに このxmlファイルに対応する画像のファイルパスが記述されています なので このxmlファイルと画像が一対一で対応していることになります    xmlまた画像中の物体の情報は＜object＞ ＜ object＞の間に記述されています この中には＜name＞car＜ name＞という物体のクラス名 今回の場合は car  と 以下のような座標情報が記述されています     ＜bndbox＞        ＜xmin＞ ＜ xmin＞        ＜ymin＞ ＜ ymin＞        ＜xmax＞ ＜ xmax＞        ＜ymax＞ ＜ ymax＞    ＜ bndbox＞これがつの物体に関する情報で＜object＞ ＜ object＞がつあるので つの物体 car が画像中にあることを示しています このxml形式を解析するのにpythonのを使います 具体的な使用方法に関しては他に譲りますが 今回は先述のpathとobjectを抜き出して 辞書形式でreturnするclassを作ります このclass を使ってxmlファイル中から 画像のファイルパスと座標情報 bouding box を抜き出します 今回のファイル構成は以下のようになっています 同じフォルダにjpgファイルとxmlファイルが含まれています ここからglobを使ってxmlファイルのリストだけ抜き出します  xmlファイルのpathを抜き出し trainデータとtestデータに分割 trainデータのpathからつ取り出し targetデータを出力これの結果は 以下のように出力されます これで画像と それに対応するbounding boxをpythonに取り込むことができました 次にoriginalの画像は×のサイズですが 画像サイズが大きくなると学習や推論が遅くなるので 今回×にリサイズして使うことにします またbouding boxを表示する関数としてを用意してありますので これを使います  画像の表示出力は以下のような形になります   ダウンロード png  ×に対応したbounding boxが得られています      xyxyとxywhの変換  affdddfeedefdedafeefdfffddddea jpg   で得られたbouding boxの座標は上画像の左の  x  min   y  min   x  max   y  max   でした ただし 後述のtargetを生成するには右の  x  y  w  h  のほうが便利になります ここでは  x  min   y  min   x  max   y  max   を入力すると   x  y  w  h  を出力する関数を作ります      データオーギュメンテーションこの章のソースコードは主として以下のリンク先にあります  物体検出の場合 bouding boxも定義しないといけないので 画像分類よりも教師データを作るのに手間がかかります そのため 画像の枚数が十分に足りない場合が多くなり 画像を左右反転させたり resizeしたり 画像にフィルタをかけたりするのが さらに重要になります このような基本的な画像処理はPILを使って実現することができます また物体検出の場合 bounding boxの座標も一緒に変換する必要があります  例えば 左右反転させる場合 入力を  x  y  w  h  にすると   y  w  h  についてはそのままで  x だけ以下のような変換をすることで新しいbounding boxになります    mathx    画像の横幅 pixcel     xこれを関数とすると 以下のようになります 最初に の乱数を生成して それがpより大きい値であったら 左右反転させずに入力をそのままreturnします 次にrandomに画像のサイズを変更する 縮小する 関数は以下になります resizeした後に画像のサイズが変化すると困るので PILで一面灰色の新しい画像を作り その画像上に縮小した画像を張り付けています 色味を変更するなど bounding boxに変更がない場合はもう少し簡単になります 次の関数は 一定の確率で画像をモノクロに変更する関数です 上記のデータオーギュメンテーションをひとまとめにした関数を作成しておきます このデータオーギュメンテーションを実行してみます これにより 教師データとなる画像の枚数を疑似的に増やすことができ 物体検出の精度向上が期待できます オーギュメンテーションの例はこのようになります   ダウンロード    png       教師データの作成この章以降の内容のソースコードは主として 以下にあります まずは画像分類と物体検知の教師データの違いについてみていきたいと思います 画像分類の場合は画像枚全体を見て 画像を分類します 一方 物体検知の場合には画像を複数のグリッドセルに分割して そのグリッドセル毎に分類します 今回の場合では×に分割されたグリッドセル毎に物体 車 が存在しているか それとも背景かのどちらかに分類します   プレゼンテーション png  もっと具体的にはbounding boxの中心が存在するグリッドセルだけを 車  その他のグリッドセルは 背景 にします   プレゼンテーション png  ただしこれだけでは 細かい物体の中心が分かりません そこで グリッドセルの位置に細かい位置ついても同じように出力します なお×の画像を×のグリッドセルに分割するので セルあたり×となります ただし出力が になるようにするためにグリッドセル内の座標 ピクセル をで除したものを教師データとしています 下図の場合  x 軸が     y 軸が   です   プレゼンテーション png  大きい物体の w  h は少しずれても大丈夫ですが 小さな物体ルートの w  h がずれると正解との差が大きくなるので ルートをつけることにより 小さい物体を強調しています 以上の内容をまとめると 一つのグリッドセルにつき  物体の中心　or not  グリッドセル内の中心の位置 x axis   グリッドセル内の中心の位置 y axis のつの数字が教師データとなります すなわち教師データとして 枚の画像につき  ××  のデータを作成します ここでDezero及びPytorchなどではチャンネルファーストなので が一番最初になっていることに注意してください これを図で示すと以下のようになります   プレゼンテーション jpg  上記の内容を元に  x  y  w  h  のデータからtargetを作成する関数を作ります          cellのピクセル数    imsize   int or float         画像辺あたりのピクセル数       の配列の作成        セルの位置を計算         セル内の中心位置を計算 最初にnp zerosをつかってtargetの配列をつくり 物体 の中心 が存在するグリッドセルだけ数字を上書きしているので この関数でreturnされるターゲットはゼロだらけになりますが  物体の中心　or not のチャンネルを除いて 学習時に物体が存在しないグリッドセルは無視しますので ゼロだらけでも問題ありません targetを  x  y  w  h  に戻す関数も作成しておきます targetの 物体の中心　or not のチャンネルで閾値を超えたグリッドセルについて  x  y  w  h  の値をreturnするような関数です 後述のnon max supressionに使いますので  物体の中心　or not のチャンネルの値もscoreとして出力できるようにします 関数の確認をします まず初期の  x  y  w  h  をprintすると 写真中に物体 車 がつ存在していることが分かります make targetして  物体の中心　or not のチャンネルのみをprintしてみます   x  y  w  h  の順番が最とは逆になっていますが 値は同じです   ×× のサイズの画像を入力すると ×× を出力するネットワークを作成します ここでは画像分類で用いられるVGGを利用します ImageNetで画像分類のために学習されたネットワークは 画像の特徴をうまく捉えることができるネットワークになっており 物体検出のタスクにも転用できることが知られています このネットワークのことをbackboneということもあります VGGのネットワークを以下に示します   プレゼンテーション png   A review of deep learning in the study of materials degradation  VGGでは層のCNN層と層の全結合層が存在しています 最終的にImageNetのクラスに分類することができます 今回は××のデータが欲しいのですが ちょうど層のCNNが終わって最後のmax poolingしたデータ 全結合層の前 が××になっています 今回はここから派生させて ××のデータを出力できるようにします ここではCNNを層追加しています 最後は が出力されてほしいので 活性化関数にsigmoidを使用します ネットワークは以下のようになります   プレゼンテーション png   dezero models  にはclass VGGが用意されているので このclassを継承して新しいクラスを作成します 足りないCNN層については   init  で追加で定義します またforwardについては上書きして ××のデータが出力できるようにします      損失関数の設定ここでのポイントはloss   グリッドセル内に物体の中心が存在する or not のloss以外では mask画像を使用して 物体の中心が存在しないグリッドセルの損失を計算しないようにすることです これにより make target由来のゼロだらけの値を学習しなくなります loss は 物体の中心が存在する or not の値分類なので binary cross entropy 他は回帰なので MSEを使用しています Dezeroにはdataloaderを簡単に作れるようにするが用意されています 使い方はpytorchと似ています またDezeroにはPIL形式の画像をVGGで推論できる形に変換する機能がありますので それも利用します          VGGの前処理 GPUが使える場合のみ今回 batch size でイテレーションにつき枚の画像を学習させています GPUのメモリが小さく学習できないときには batch sizeを小さくすることにより 学習することができます ただし バッチサイズが極端に小さくなると 学習が不安定になると言われていますので 注意が必要です      学習ここまでの内容を元に学習させます  データの記録     test data こちらでは学習させないlossは以下のようになっています   ダウンロード png       non Maximum Suppressionによる重なりの除去testデータを用いて 実際に可視化してみます ここでは事前に用意している関数を使います  はVGGの前処理の逆の処理を行って matplotlibで綺麗に表示できるように変換する関数です 赤色が強いほど 物体の中心が存在する確率が高いことを示しています   ダウンロード png  ここでは複数のセルが｢救急車のおもちゃの中心はココだ ｣と主張しています このままですと つの物体に対して複数のbounding boxが重なりあってしまいます このbounding boxの重なりを除去するのが non Maximum Suppressionです non Maximum Suppressionでは まずbounding boxが重なっているかを判定する必要があります この重なりを判定するのがIoUと呼ばれる値で つのbounding boxの座標から 以下のように計算できます   iou png  ちなみに計算に必要となるx  yの座標は以下のように求めることができます   iou png  このIoUを計算する関数は以下のようになります      重なり合う部分がないとき をリターン     それぞれのbdの面積を計算     intersection部分の面積計算に必要な座標を計算IoUがしきい値を超えた場合 scoreが小さいbounding boxが削除される側になります 次の関数ではitertoolsを使ってbounding boxの組み合わせを全通り求めています またiouの計算の途中でbounding boxを配列から削除すると 配列のインデックスがずれてしまいます 故に ここでは非効率的ですがIoUを全部計算した上で 最後に必要なbounding boxのみを抽出するようにしています 最後にtestデータの推論結果について いくつか結果を表示します   ダウンロード    png    ダウンロード    png    ダウンロード    png     終わりにDezeroで物体検出が出来ました タイトルが煽り気味でｽﾐﾏｾﾝﾃﾞｼﾀ 今回の物体検出はかなり簡略化されているので 様々な工夫でさらに良くなるハズです ここで作った関数のいくつかは  torchvision  には最初から用意されています 実用的な観点からすると  torchvision  を使った方が効率がよいとは思います 今回の内容は こちらの講座も参考にしました 実装に関する説明はほとんど無いですが 理論面では非常に参考になりました 今回より さらに高性能な物体検出を作りたい方は 以下の教科書が参考になると思います ,8,2022-05-09
170,170,【車載動画予測×深層学習】PyTorchで実装する動画予測モデルPart2,画像処理,https://qiita.com/satolab/items/bac43905f3427910d057,  概要自動運転が実用化に近づく中 ドライブレコーダ等の車載カメラから取得できる動画像の活用は 今日ますます重要なタスクになっていると思います 当記事では CNNやGRUを用いた予測モデルを構築し 実際どこまでできるのか検証します 今回は Part  の結果を踏まえてモデルの改善をし その性能を評価したいと思います PartではCNNとGRUを用いたシンプルなモデルで予測を実施してみましたが 結果は良いものではありませんでした 前回記事と重複しますが 以下に動画予測のシステム全体像を示します   図形    jpg  詳細に関しては Part  を確認いただければと思います また 実装はすべて こちら  で公開しています   動画予測モデル今回実装するモデルを以下に示します．  図形    jpg  前回との大きな違いは全結合層の撤廃です また GRUではなく Convlutional LSTM  にしました LSTM層の結合が畳み込み層となっているため 画像の位置情報特徴を欠落せず保持できます これで前回の抽象的な予測結果の改善を期待します なおモデルの実装 PyTorch は下記のとおりです ConvlutionalLSTMの実装は こちら  を参考にしています すべて掲載すると長くなるので 実装を試したい方は上記のgithubをご確認ください   また 学習実行時にはmain pyのモデル定義部分をSeqseqGRU→SASTANGenに変更してください     モデルの学習 検証上で定義したモデルを学習します データセットは前回と同様にウェブ上に一般に公開されているものを用います 学習はepoch程度行いました 定量的な評価は行っていませんが 前回と同様に損失の推移を観察すると収束した印象です 気になった点は前回より圧倒的に計算時間が伸びた点です 大きな変更点はConvLSTM層なので ここの計算量が膨大なようです LSTMの内部変数の次元数を調整することで多少改善されましたが GRUと比較するとLSTMは計算量が多いデメリットがあります   Conv GRU  があるようので 計算量が気になる方はそちらで実装してもいいかもしれません 今回はやりませんでしたが なお 添付のリンク先の実装は詳しくは確認していません  パラメータは前回と一致させました 具体的には以下の通りとなります   バッチサイズ   学習率 e   画像サイズ ×  シーケンス長T   N   データ総数：約 枚以下 学習済みモデルの予測結果となります      正解データ　     予測結果 上：今回のモデル CNN Conv LSTM  下：前回のモデル CNN GRU  結果は前回よりかなり改善された印象です Conv LSTMの導入が功を奏しました より細かな画像中の特徴 看板や車体 柵等 が生成できていることが分かります 一方で多少のぼやけは残存しており まだまだ改善の余地はありそうです また 予測としての精度は画像の品質以外にも定量的な誤差等で評価する必要もありそうです というのも 極論もっとも直近のフレーム画像をそのまま出力すれば画像の品質的には問題ない一方 それは将来の予測をしたことにはなりません 今後はモデルの出力が良くなれば 評価方法もアップグレードしていきたいと思います   考察 まとめ今回はCNNとConv LSTMを用いた動画予測モデルを構築し 車載カメラ画像の予測を行いました 予測画像の品質は前回と比較して改善されました 一方でまだ画像の輪郭や細かな部分はぼやけが見られますので 品質面での改善の余地があります 次回は敵対的生成ネットワーク GAN を導入してさらなる予測品質の改善をしたいと思います 最後までご覧いただきありがとうございました ,12,2022-05-08
171,171,寿司ネタを判別するウェブアプリの作成,画像処理,https://qiita.com/iori-sushisushi/items/d2d81d30253ad9adb816,  やりたいこと某プログラミングスクールで 機械学習による画像識別とウェブアプリの構築を学習したので 自分が好きな寿司を題材に実践してみることにしました まずスモールスタートで初めて 徐々にネタの種類や精度を発展させていくことができそうな題材ということも大きいです   実行環境次のとおりです   Google Colaboratory  heroku データ収集 前処理ネットでざっと調べたのですが icrawlerを使うと 日本語対応も含め手間をかけずにできました 画像を収集したい寿司ネタのリストを作り 関数に流し込みます    Pythonneta list     まぐろ    サーモン    いか    海老    蛸    鯛                鯵   数の子    はまち    納豆巻き    かっぱ巻き                かんぴょう巻き    ネギトロ軍艦    うに軍艦    いくら軍艦  結果を見てみると寿司ネタあたり くらいの画像が収集できていました ここから目検確認 手作業で不適当な画像を除いていきます… 結構時間かかる  全然関係ない画像も結構収集されてしまっています… 続いて 画像を学習用に前処理していく関数を作り 処理していきます 画像サイズは   にしていますが 小さくしてもよいと思います     でもそれなりの精度に最終的にはなりました scratchで水増し処理を重ねていくのですが 水増ししすぎた感があるので 多いネタで万枚以上    取捨したほうが良いですね       ハイコントラストLUT作成      ローコントラストLUT作成      関数と画像を引数に 加工した画像を元と合わせて水増しする関数トレーニングデータとテストデータを用意します  で分割します 結局ここで各寿司ネタから枚までに絞っています 一番データが少ないネタがそれくらいなので アンダーサンプリングしているような感じでしょうか   データの分割  手法 アルゴリズム最終層を除いてVGGを延用し これに独自で構築したモデルを接続して一つのモデルとします モデルの構築と訓練の部分のコードは次のとおりです accuracyスコアを見ながら訓練させました ちなみに 冒頭のmagniは画像のサイズもハイパーパラメータのように捉え 調整を容易にするために設定しています まったく経験的なものです かつ処理が重くて 少ない試行回数  なお 訓練時の様子は次のとおり あとは予測関数を作っておいて 新規の画像を投入したり 性能を手作業でも確認しました すると 水増しのしすぎだと思いますが データセット以外のデータへの精度が悪い… かといって 画像収集もなかなか骨が折れるので 各寿司ネタの画像データ数を含むハイパーパラメータの調整で できるところまでやっていきました あとはherokuにデプロイすれば完了です なお herokuにデプロイする際にHエラーが出ましたが opencv pythonではなくopencv python headlessを入れると解決しました   考察 反省まずデータ収集をもっと頑張らなければいけないと思います 実は不要画像を除くとオリジナルの画像は寿司ネタによりますが 各 枚程度 圧倒的に足りないです 処理やモデルの改善は そのあとかなという気がしています また 今回はまぐろとうに軍艦のように 人間が見たらすぐ判別がつくものばかり集めています 図鑑的な感じで役に立つかもしれませんが 本当は白身魚とか ちょっと判別が付きづらいものとかマイナーなネタをやった方が役に立つと思います 今後の課題にしたいと思います 最後にウェブアプリはこんな感じです ,1,2022-05-07
172,172,Elixir で OpenCV (Evision) を使った画像処理（移動、回転、フィルターなど）,画像処理,https://qiita.com/RyoWakabayashi/items/45a7daccf064b8720ad0,   はじめにこれまで Python での画像処理や AI の学習 推論は実務でも扱ってきましたまた  Elixir は Phoenix を使った REST API に数年使っていますしかし  Elixir で画像処理  AI というのは未経験ですというわけで  Nx と evision で Elixir での画像処理を実装してみました実装したもの Docker 上に環境構築      note info   更新最新のモジュールを使うように更新しました参考記事    Nx とはElixir で多次元配列 テンソル を使うためのライブラリPython の numpy のような感覚で使えるため 画像処理に向いています上記のコードを Livebook で実行するとこんな感じ  スクリーンショット       png  テンソルの各要素が  で割られているのが分かりますね   evision とはElixir 用の OpenCV ラッパーPython の OpenCV と同じことが Elixir 上で実行できますNx とも連携できるため  Python と同じ感覚で画像処理できます   実行環境  MacBook Pro  inchi      GHz クアッドコアIntel Core i     GB  MHz LPDDR  macOS Ventura     Rancher Desktop       メモリ割り当て  GB    CPU 割り当て  コアLivebook    の Docker　イメージを元にしたコンテナで動かしましたコンテナ定義はこちらを参照   実行方法リポジトリーをクローンして  docker compose up  するだけです   bashgit clone cd elixir learningdocker compose upビルドが終わると以下のように localhost の URL が表示されるので ブラウザで開いてくださいこんな感じで Livebook が開きます  スクリーンショット       png  Elixir 学習用のリポジトリーなので他のファイルもありますが Docker 上では evision インストールに必要なもの   他で使うので Phoenix を入れていますしかし  Livebook 上から evision をインストールすると分以上ビルドに時間がかかってしまったため 以下のようにフラグを立て ビルド済のものを使うようにしました    evision のインストール実行他に使うものも併せてインストールします   elixirMix install      httpoison              evision              kino              nx            画像生成Nx を使ってテンソルから画像生成できます  Evision のマトリックスに変換  見やすいように拡大  スクリーンショット       png     画像のダウンロードHTTPoison を使って画像のバイナリデータを取得します  スクリーンショット       png  バイナリデータをデコードします  スクリーンショット       png     画像の書込   画像の読込Python の OpenCV と同じように画像ファイルを読み込みます  スクリーンショット       png  画像サイズも以下のようにして取得できます   elixirsize   Evision Mat shape img  Evision Mat to nx  でテンソル化することもできます   リサイズリサイズも Python と同じように記述可能です   elixirEvision resize img         スクリーンショット       png     グレースケールグレースケールでの読込もシンプルに記述できます  スクリーンショット       png  読込済のマトリックスをグレースケールに変換する場合は  Evision cvtColor  を使います定数は  cv   で evision に定義されています   二値化閾値を指定しての二値化も同様です  スクリーンショット       png     平行移動平行移動にはアフィン変換を利用しますアフィン変換用の変換行列は以下のように定義しますまず次元リストで定義した後 あとは Python と同じように  wrapAffine  に変換行列と出力サイズを与えるだけです   elixirEvision warpAffine img  affine         スクリーンショット       png     回転回転の場合は  getRotationMatrixD  に回転の中心座標 角度 スケールを指定して変換行列を取得します   elixiraffine   Evision getRotationMatrixD                Evision warpAffine img  affine         スクリーンショット       png     ぼかしその他 各種フィルター系も Python と同じように記述できます    通常のブラー   elixirEvision blur img         スクリーンショット       png      中央値ブラー   elixirEvision medianBlur img     スクリーンショット       png      ガウシアンブラー   elixirEvision gaussianBlur img           スクリーンショット       png     図形描画四角形や楕円などの図形も描画できます    線   elixirimg  直線    線の太さ  thickness    矢印    線の太さ  thickness       頭の大きさ  tipLength     スクリーンショット       png      四角形   elixirimg  四角形    線の太さ  thickness       線の引き方 角がギザギザになる     線の太さ  thickness       線の引き方 角が滑らかになる     塗りつぶし  thickness     スクリーンショット       png      楕円 扇形   elixirimg  円    半径    色 R  G  B     塗りつぶし  thickness     楕円     長径  短径     回転角度    弧の開始角度    弧の終了角度    色 R  G  B     線の太さ  thickness    扇形     長径  短径     回転角度    弧の開始角度    弧の終了角度    色 R  G  B     塗りつぶし  thickness     スクリーンショット       png     文字描画文字も書けます    文字列   Lenna      左下 x  y     フォント種類    フォントサイズ    文字色    文字太さ  thickness    スクリーンショット       png     Nx との連携  スクリーンショット       png    スクリーンショット       png     おわりにEvision を使うことで  Python と同じことが同じように Elixir で実装できましたElixir で画像処理が簡単に実装できるため バックエンドの実装の幅が広がりますね,39,2022-05-06
176,176,【GradCAM】ResNetで学習させた都市景観画像の判断根拠,画像処理,https://qiita.com/satolab/items/7bcb957774f1446ae0b7,  概要私事で恐縮ながら 社会人になって一人暮らしを始めました 今住んでいるのは都心の近くなのですが 実家は郊外なので それぞれの景観は異なってきます 路地を歩いているとなんとなく街並みが○○区っぽいな とか ここは○○市らしいなとか 感じたことがある方も多いと思います とはいえ その差は結構微々たるもので その市区町村にあるな 雰囲気 の差のようなものがある気がします 今回は東京都の町田市と中央区の画像を教師あり学習で識別モデルを学習し GradCAMを用いて判断根拠が何なのか確認してみます モデルには定番のResNetを用います 実装や詳細は下記のサイト M Tech Blog様 を参考にさせていただきました  機械学習で逆ストリートビューを作り 写真から港区らしさを判定する  ご指摘 疑問などございましたらコメントよろしくお願いいたします   学習  図形    jpg  上記のようなモデルを考えます 都市の景観画像データセットを用意し ResNetに学習させます 識別はパターンで 東京都町田市のものと 東京都中央区のものを用意します パラメータ 詳細は下記のとおりです  学習率：e  バッチサイズ： データ総数：約  枚  フレームワーク：PyTorch学習の定量的評価は行いませんでしたが 町田市と中央区は人間の目で見ても景観の乖離が大きいので単純なタスクかと思います そのためvalidation dataに対する正解率は を超えていました   学習データの収集今回最も面倒だったのがデータの収集です 数千枚の景観データを手作業で収集するのは現実的ではないため Googleのstreetview APIを使用しました locationに位置座標 sizeに取得したい画像サイズを指定することで簡単に景観画像を取得可能です ご注意いただきたいのが 一回叩くごとに USD請求されます  上記サイト参照 私はあまりに気にせず叩きまくった結果 万円を軽く超える請求が来たため 慎重なご利用をお勧めします  Google Cloud Platformの無料利用枠対象内でしたので実質無料でしたが   また 折角お金をかけて叩いたのに 取得できないケースがあります 取得画像に sorry we have no image here と表示される データセットに悪影響を及ぼすため こうした画像は排除することをお勧めします   GradCAM大変わかりやすい記事がQiitaにも多くありますので詳細は割愛しますが 平たく言うと識別モデルの判断根拠を出力する手法です 下記の画像の例では  cat の判断根拠をヒートマップ画像で可視化しています 猫の下半身周辺にモデルが注目し  cat と識別していることがわかります   image png  上記画像は下記論文から確認できます  Grad CAM  Visual Explanations from Deep Networks via Gradient based Localization    結果東京都町田市の画像をランダムに抜き出し  町田市 と判別された画像の根拠をGradCAMにより可視化しました   図形    jpg  ResNet的には緑の多い畑周辺に注目していることがわかりますね 反対にアスファルトで舗装された道や 空には注目していません 中央区にも共通して確認できる特徴だからでしょうか 中央区も同様にトライしてみました   result png  少し偏りがありますが 高層ビル周辺に注目しているようです 道路や空には目もくれていません   考察 おわりに今回はResNetに都市画像を学習させ GradCAMで判断根拠を可視化しました 町田市→緑 中央区→ビル群に着目していることがわかりました うまくいくかわからなかったので単純なタスクを設定しましたが 今後はもう少し差がない区や市でも判別が可能か そして判断根拠はどこなのかやってみようと思います 最後までご覧いただきありがとうございました ,2,2022-05-04
180,180,【PowerPointで作成した論文画像】様々な出力方法で画質を比較してみた,画像処理,https://qiita.com/Radley/items/caa2f9fc15e9a0eead71,  何の記事 パワーポイントは論文にとても便利ですが 出力する際に画質が低下していまうことが指摘されています 画質の低下に対しては色々な対策方法がネット上に散見されますが どの方法が最も良いかについては全く検討されていませんでした そこで報告されている色々な方法について 出力された画像の見た目の綺麗さを比較検討してみました   前提多くの科学論文の画像では 投稿時にdpi  dot per inch 以上の画質が要求されています 例として カラム 幅cmの出力を想定しました 縦横比は としました このサイズだと 横のピクセル数は  inch　× dpi   pxということになります   スクリーンショット       png    画像出力の方法以下の方法を検討しました   パワーポイントからPDFを出力 MACのPreviewで開いてdpiでTiffを出力  パワーポイントからPDFを出力 PhotoShopで開いてdpiでTiffを出力  パワーポイントで画像サイズを倍にしてPDFを出力 PhotoShopで開いてdpiでTiffを出力  パワーポイントからPDFを出力 Adobe Illustratorで開いてdpiでTiffを出力それぞれの方法について簡単に解説します      パワーポイントから直接dpiのTiff画像を出力するデザイン→スライドのサイズ→ページ設定から ページの幅をcm 高さを cmに設定します で スライドの編集をします 保存するときは ファイル→エクスポートから Tiffを選択します PowerPoint for Mac ではピクセル数を設定できるため 横幅をpxにして出力   スクリーンショット       png    スクリーンショット       png       パワーポイントからPDFを出力 MacのPreviewで開いてdpiでTiffを出力PDFで出力することにより 文字がベクター形式で保存されるので いくら拡大してもカクカクにはなりません 上図  そのためPDF形式だと保存するたびに文字が荒くなっていくことを防げます 保存したPDFをMacのアプリであるPreviewで開いてdpiで保存   スクリーンショット       png    スクリーンショット       png       パワーポイントからPDFを出力 PhotoShopで開いてdpiでTiffを出力パワーポイントでPDFとして出力した画像を Photoshopで開きます 開くときに ファイルの横幅をpx 解像度をpxに指定しておきます モードはRGBで良いですが 論文によってはCYMKが指定されているものもあります 画像を開いたら  別名で保存 から画像をTiff形式で保存します 画像圧縮はLZW レイヤーの圧縮はRLEを選択しました   スクリーンショット       png       パワーポイントで画像サイズをcmにしてPDFを出力 PhotoShopで開いてdpiでTiffを出力以前のパワーポイントでは保存時のデフォルト画素数がdpiとなっており もともとの画像サイズを大きくしておくことが推奨されていました 目標画素数dpiと比較して ぐらいなので ざっと画像の大きさを倍ぐらいにする必要があると考えました 具体的には パワーポイントのスクリーンサイズを横cm 縦cmで設定し スライドを作成した後に PDFとして保存します それをPhotoShopで開いてTiffに変換 ファイルの横幅をpx 解像度をpxで開くので 最終的な画像のサイズはと一緒です      パワーポイントからPDFを出力 Adobe Illustratorで開いてdpiでTiffを出力Illustratorの強みは ベクター画像をそのまま扱えることです すなわちPDFをillustratorで開けば 文字を拡大してもカクカクになりません 書き出し→書き出し形式→Tiffを選択 保存時に解像度を聞かれるので高解像度 ppi を選択します アンチエイリアスは アートに最適 スーパーサンプリング を 圧縮形式はLZWを選択しました  結果左から 元画像 ①パワーポイントで直接出力 ②Previewを介して出力 ③Photoshopを介して出力 ④画像サイズを上げてPhotoshopで出力 ⑤Illustratorで出力 です   スクリーンショット       png     文字の鮮明さ②Photoshopを介して出力 が最もぼやけが少ないように見えます 初めから目的のサイズで設定しておいた方が画質が良さそうです ,1,2022-05-01
182,182,【Python + OpenCV】簡易寸法測定(画像に寸法描写),画像処理,https://qiita.com/youichi_io/items/cc90778d89d7b715339d,   はじめに画像を用いた寸法測定ツール作成は カメラからの遠近によるサイズミスマッチや精度の問題があって敬遠しておりました しかし先日 面白そうな記事を見つけました    ちょっとした改良か月前に  commitされているのみで 開発が進んでいるのか止まっているのかわかりません ひとまずfork   git clone し自分でも使ってみて 必要に思った機能を追加しました 詳しい操作方法などはREADME mdをご覧ください 二値化して面積求めたりする機能もあるようですが 私には必要なさそうなので説明を省略しています 元リポジトリには一切の説明がありません       メイン画面webcamの画面をスクショして測定するツールらしいです   pic JPG       実スケール入力 追加した機能 いわゆるキャリブレーション 基準となる長さが  mmのみでは不便に感じ 変更できるようにしました   pic JPG       簡易測定結果基準となる長さを適当に取りすぎた 定規から離れているし 斜めになっている のと 定規より消しゴムの方がカメラに近いため 寸法が多少異なっています 実寸すると縦横 x mmでした    pic JPG  三次元物体を測定するのは避けた方がいいかもしれませんね きっちり測りたいなら市販されているツールを購入して利用するのが確実です ただし 例えば測定する対象をローカルで読み込んだCAD画像にすると カメラからの遠近によるサイズミスマッチなどの問題は気にする必要がなくなります また 線分長さの総和を算出できるようにすると応用の幅が広がるかもしれません    おわりに簡易寸法測定ツールの改良についてまとめました 開発中とのことで完成を気長に待ちたいと思います    追伸protopedia net の記事を引用しております 参考元の作者様にコメントでコンタクトを取ろうと思って新規登録しようとしましたが 受信確認用のメールが届きません    ライセンスの関係でまずいことになりそうであれば記事を非公開にします ,4,2022-04-24
183,183,DOBOT×AI グリーンバックから対象物の輪郭を抜き出す,画像処理,https://qiita.com/shimamotosan/items/57bdc00464d6ec703fc7,  やりたいこと以下のテキストでは AIを用いた画像認識により対象物を分類し ロボットアームにより仕分けを行っています   DOBOT Magician AIｘ画像認識ｘロボットアーム制御  上記テキストでは 画像認識によって対象物の領域を取得する際に カメラの画像をグレースケールに変換した後 値化して その後 領域を取得しています その場合でも カメラの調整や環境 照明の明るさ 外乱光等 の調整をすることでおおよそ認識することが可能です テキストでは グレースケールした際に明暗がハッキリするように カメラに映るところに黒い下敷きを敷いています ただ 対象物が黒い場合 上記の方法ではうまく認識できないことがあります 今回は 黒い下敷きを任意の色のついたものに変えて その色を抜き出すことで対象物と背景を切り分けてみたいと思います   できたもの今回は 明るめの緑色の画用紙を背景に使用しました 画用紙の色が抜き出せるように範囲を調整を行いました 色が複雑に入り込んでいるものや外郭が黒いものも正しく認識できています   プログラム   修正前          グレースケールに変換              サイズに縮小          グレースケール画像を表示する          値化              サイズに縮小          値化画像を表示する          輪郭を抽出             グレースケールに変換             値化             輪郭を抽出      修正後          グレースケールに変換              サイズに縮小          グレースケール画像を表示する          値化              サイズに縮小          値化画像を表示する          HSV色空間に変換          色を抽出する          ノイズ除去          黒白反転           サイズに縮小して表示          輪郭を抽出             グレースケールに変換             値化             HSV色空間に変換          色を抽出する          ノイズ除去          黒白反転           サイズに縮小して表示          輪郭を抽出      抜き出す色の調整上記 プログラムでは緑色を抜き出す際の調整になっています 動かす照明環境やそもそも別の色を抜き出したい場合は 以下の部分の調整を行ってください    python classifier py          HSV色空間に変換,0,2022-04-22
184,184,After Effectsのエフェクトプラグインを作ってみよう！　その１,画像処理,https://qiita.com/genkaigakuseiprogrammer/items/4b3f2f5a86cbe2e914d6,  この記事を読む前にAEのプラグインを作成するにあたって 記事が古かったりそもそも日本の記事があまりにも少ない印象だったので書いてみました 初めてこういった記事を書くので慣れていない部分が多く とても読みづらくなってしまうかもしれませんがご容赦頂ければ幸いです   また間違った知識の元書いてしまっている部があった場合はご指摘いただけると大変助かります   はじめにAEのエフェクトプラグインを作るにあたって 以下のような知識が前提となります  C  である程度コーディングができる  画像処理の基礎的なアルゴリズム  並列処理プログラムが書けると言ってもハードなプラグインを作ろうとしない限りC  でコーディングができることと画像処理の根本的な仕組みが分かっていればある程度の処理プログラムは書けます  実際自分も超専門的なアルゴリズムを理解しているわけではないです笑  まあそんなに頭固くして物事考えても仕方ないので取り合えず触れてみる精神が大切だと思いますので興味があれば十分です 今回はいきなり難しいことをやってもよくわからなくなるだけなので 簡単に色素の順番を入れ替えるだけの処理書いていきます   環境今回は以下の環境で開発していきます Adobe社のソフトウェア 拡張機能 プラグイン のSDKは こちらのサイト  からダウンロードできます   Windows   Adobe After Effects   Microsoft Visual Studio   October  After Effects SDK Windows  プラグインテンプレートのあれこれ    概要After Effects のプラグインSDKにはいくつかのテンプレートが用意されています 例えば After Effects その物を拡張する専用のものなどがありますが ここではそこまで触れないので興味がある方は こちらの記事  で詳しく紹介されているのでご覧ください     解凍After Effects のプラグイン専用SDKをダウンロードしてきたら今後プラグインの開発をしていく為のディレクトリを用意して解凍しておきます 例：C \AESdk\　など     ※注意解凍するときの注意点としてAESDK は配布されているテンプレートそのものを改造してコーディングしていくので 将来的に移動したくなる場合などを考慮して解凍することをお勧めします     テンプレートの選び方今回触る部分はいわゆる エフェクトプラグイン の開発なので 主に   Examples   フォルダの中の   Effect   フォルダ内のテンプレートと   Template   フォルダ内のテンプレートを選ぶと良いです    png  今回は基礎的な内容が簡単に書かれている   Template   フォルダ内の   Skeleton   テンプレートを改造していくことにします   Skeletonテンプレートの改造    テンプレートのコピー今後開発するにあたって いちいちテンプレートをダウンロードしてくるのはめんどくさいので最初にコピーしてから改造することをお勧めします コピーする際はプロジェクトのフォルダごとコピーしてしまいましょう 今回はコピーしてからフォルダ名を   color manager   とします   プロジェクトのオープンコピーできたらフォルダを開き   Win   フォルダの中にある   Skeleton sln   の名前も変更してしまいます 先ほどと同様に   color manager sln   といった具合に変更します 無事変更できたらそのままソリューションファイルを開きます 開いた際に VisualStudio から Skeleton に対するセキュリティ警告 といったポップアップが出現する場合がありますが 特に気にせずOKで大丈夫です 開いたら画像のようなポップアップが出現するのでお使いの Windows SDK のバージョンなどに合わせてOKをクリックします   通常はデフォルトのままで問題ないです     png  無事再ターゲットが完了すればソリューションが問題なく開かれます ※ここで失敗するようであれば先ほどの Windows SDK などの設定が悪い可能性があるのでやり直してみてください     プロジェクトの名前の変更まずビルドした際の拡張機能の名前を変更していきます   なんだかんだ言ってここが一番めんどくさかったりします笑  まずソリューション内に   Skeleton   という名前の C   プロジェクトがあるのでこいつの名前を変更してやります 普通に右クリックから 名前の変更 をしてやれば問題なく名前が変更されます ただこれだけでは After Effects に読み込ませたときにプラグイン名が変わっていないで 次にそっちの設定もしてやる必要があります      Skeleton cppC   プロジェクト内の   Skeleton cpp   を開きます こいつは簡単に言ってしまえば After Effects がこのプラグインを読み込んだ時に どんな内容のプラグインなのかを確認するための関数です ローカル変数 result に詳細な名前などが設定されているので こいつらを変更してやります   Name   プラグイン名  Match Name   After Effects が内部でコールする際の名前   Category   エフェクトの収納カテゴリー名グローバル変数に   g str   というものがあります これはエフェクトプラグインの情報部分にあたるもので 以下のように表示されるテキスト部分を設定しています    png  辿っていくと分かりますが 厳密にはSkeleton cppのAbout関数で定義しているようですが こうして別ファイルで定義しておけば書き換えがスムーズになるという意図があるのでしょう 下の二つ   GainとColor   に関してはエフェクトコントロールに表示されるテキストの名前を定義しています これに関しても上で語っているものと同様です Skeletonテンプレートではこのような手法をとっているようです まあ適当にそれっぽく書いておきますこれでOK と思いきやまだ一段階あるようです   はぁ〜〜〜〜 クソデカため息 ここにきてなぜR言語 と思った方も多いことでしょう   正直自分も思いました笑  ここに関してはよくわかりませんが おそらく After Effects の構造上Rのリソースファイルで読み込むことが都合が良いのでしょう まああまり深く考えることでもないのでそっとしておいてやります笑中身の  行目  あたりから  行目  と   行目  あたりの部分を先ほどのSkeleton cppの時と同様の値を設定してやります これでようやく名前の変更が終わりました いやめんどくさすぎるやろ     メインプログラムのコーディングメインプログラムをコーディングする前に各関数の処理内容を説明したいと思います     それぞれの関数の役割色々とややこしそうな関数軍がありますが それぞれ以下のような役割を担っています PluginDataEntryFunction   プラグインエントリーポイントEffectMain   AEからコールされた際に毎回呼ばれるメイン関数  About   プラグインの概要を設定  GlobalSetup   互換性などの設定  ParamsSetup   エフェクトコントロールのUIを設定└─ Render   メインの画像処理   └─ MySimpleGainFunc および    それぞれのカラービット数に応じた画像処理他にも  out flags  フラグなどを立てることで処理内容は変わったりしますが 今回は扱わないことにします     Render関数の引数リストここでにメインの画像処理を行いたいので   Render   関数に移動します Render関数の引数は以下のような内容で構成されています    in data   入力画像の詳細情報 ※画像データではない   out data   出力画像の詳細情報 ※こちらも画像データではない   params   UIで設定されたパラメータの構造体ポインタ   output   出力画像データのポインタややこしいのが  in data  や  out data  は入出力画像の詳細情報の記された構造体ポインタであり 画像データその物ではないという点です 反対に  output  が出力画像データのポインタです つまり  in data out data  から画像の縦横幅などを取得し その情報をもとにforループなどで  output  の出力画像の行列データを処理していくことになります まああまり言葉で書き綴ってもわかりづらいだけなので実際にプログラムをコーディングしながら説明していきます なんかごちゃごちゃと書かれていますが ざっくり説明すると AEGP SuiteHandler クラスを用いてそれぞれ カラービットの配列をイテレータを利用して  MySimpleGainFunc    に処理させているといった具合です ですがとても見にくいので  ローカル変数errとreturn部分を除いて  中身を一度空にしてしまいます   err  とは処理中に何かしらの不具合が起きたときに万が一スタックオーバーしないようにAEに知らせるために設定するフラグです     入力画像の取得勘の良い方はお気づきかと思いますが この状態では入力画像データがありません なので取得してあげます 取得方法はいたって簡単で引数の  params  配列に    を指定し 中身のレイヤーデータをポインタ変数で保管してあげます uとかldとかその辺はおまじないとしておきます     必要なデータの割り当てと色深度分けまずそれぞれ必要なデータをわかりやすくローカル変数にまとめます wとhはそれぞれ画像の横幅 width と縦幅 height です rは画像データが格納されている配列の幅です srcとdstにそれぞれ入出力画像データにアクセスする構造体ポインタをキャストしておきます   PF WORLD IS DEEP  マクロは現在のカラービット数がビットの場合に  true  を返します つまりビットの場合は  PF Pixelのサイズ   ビットの場合は  PF Pixelのサイズ  で割ってあげる処理になっています   PF Pixel   はつの画素が格納されている構造体です     それぞれ red  green  blue  alpha のチャンネル分格納できる構造体です     鉄板の重ループ処理画像処理と言ったらやっぱり重ループ 基本的には列と行をそれぞれfor文でループするのが鉄板なのではないでしょうか  タブン ということでfor文の重ループで入力画像の画素にアクセスし 同じ位置の出力画像の画素に割り当てていきます 画像処理に慣れている方からすれば当たり前の話ですが 画素へのアクセスには一癖あります 例えば   列 行目   の画素にアクセスするには以下のように記述します 今回は  RGBの順番を入れ替える操作をしたい  ので   redをblue  blueをred   で割り当てます   ビルドの前に    プリプロセッサの定義さて これで一応の処理は書き終えました 早速ビルドしてみましょう   とその前に   Skeleton h   を開き  pragma once の後に次の定義を書き加えておきます これを書き加えないとビルド時にセキュリティチェックが働きビルドできない場合があります 本来は何かしら対策をするべきなのですが そもそもSDKのテンプレートがそのような対策をとっていないようなのでどうしようもありません     環境変数の設定After Effects のSDKではデフォルトで  AE PLUGIN BUILD DIR  という環境変数を抽出場所として設定されています なのでご自身の環境変数に追加してやる必要があります   まあそれか単にAEのプラグインディレクトリを自力で指定してやっても良いです  システム環境変数に  AE PLUGIN BUILD DIR  変数を追加し ディレクトリパスを  After Effects のプラグインディレクトリ  を指定して保存します 例：C \Program Files\Adobe After Effects \Support Files\Plug ins\※環境変数の設定方法は割愛します ※場合によってはVisualStudioを再起動させる必要があります   ビルドそれではプログラムをビルドしてみます 無事にビルドできたら After Effects を起動します   ここでVisualStudioでデバッガをAEに設定してあげると楽に起動できます  起動したら適当に画像などを読み込んでおきます エフェクトメニューを開くと 先ほど設定したカテゴリー名でメニューが追加されて無事エフェクトが認識されました    png  ではこいつを適用してやりましょう    png  すると無事色素の順番が入れ替わったエフェクトが適用されました   おわりに今回書いたプログラムは GitHub  にて公開してますので 良ければそちらも参考にしてください また より深い知識や既存のクラスなどを活用したプログラムを書きたい場合は こちらのガイド  を参考にしてみてください またやる気が起きたらもうちょっと深い部分を書いてみるかもです   超絶気分屋なのであんまり期待しないでください笑  お疲れさまでした   参考   アニメ制作者のためのAfterEffectsプラグイン作成入門 第回 　プラグインの改造 修正  スケルトンの作成  ,6,2022-04-21
185,185,【論文紹介】食品のカテゴリ・材料・調理法の知見を活用した料理画像による食品カロリー推定・マルチタスクCNN,画像処理,https://qiita.com/_kazuyan/items/494c6d3598989506fa13,   はじめに　本記事はマルチタスクCNN を料理データに適用した論文の論文紹介となります 　簡単にではありますが 論文の内容を引用ベースで紹介させていただく形となっております    論文紹介　本記事で参考にした論文は以下になります   Ege  T     Yanai  K     October   Image based food calorie estimation using knowledge on food categories  ingredients and cooking directions  In Proceedings of the on Thematic Workshops of ACM Multimedia   pp          著者Ege  T     Yanai  K    概要 ABSTRACT 　本研究では 食品のカロリーカテゴリー 食材 調理方法を同時に学習することで 食品画像からカロリー計算をするモデルを提案します 一般に 食品のカロリーはカテゴリ 食材 調理方法に強い相関が存在するため これらを食品画像とともに学習することで 精度向上を図ることができるでしょう このことから 本研究では  マルチタスクCNN   という手法を用いて 日本とアメリカの料理データセットについて性能を比較します    導入 INTRODUCTION 　食事に対する健康志向の高まりから 日常の食事を記録するためのモバイルアプリケーションが多くリリースされています その中には 食品画像認識を用いて 食品名だけでなく 食品のカロリーを推定できるものもあります しかし これらのアプリケーションでは 食品の分類や大きさ 量などの情報を入力する必要があるため 手間がかかることや評価が主観的になりやすいという問題があります 　多くの場合 推定されたカロリーは 推定された食品カテゴリと関連付けられているか あるいはユーザが手動で指示した食品カテゴリの標準サイズと比較した相対的なサイズであるに過ぎません 　食品のカロリーは 食品のカテゴリ 量 材料 調理法などに強く依存し これらは図に示すように 完成した料理の外観に現れます 食品カテゴリーが同じでも 使用する食材や調理方向によってカロリーは異なります このように 食品カテゴリーを推定するだけでは解決できない食品カロリーの推定作業において 外観からカロリーを推定することは重要であると考えられます 食品写真から直接カロリーを推定することで 図に示すようなカテゴリー内の違いを考慮できる可能性があります   image png  図  同一カテゴリ内でもカロリーは大きく異なる　食品のカロリーと食品のカテゴリ 食材 調理方法の情報には強い相関があるため それらを同時に学習することで 独立した単一の学習よりも性能が向上することが期待されます この目的のために 我々は マルチタスクCNN   を用いる 先行研究により マルチタスクCNNを用いて食品カテゴリと食材を同時に推定することを提案し 同時に推定することで両タスクの推定性能が向上することを証明されています この研究に触発され 我々は食品カロリー推定のためのマルチタスクCNNを導入します 食品カロリー推定は 食品写真を入力し 食品カロリーを出力する回帰問題として扱われます      要点をまとめると  ユーザーが手動でカテゴリや料理の分量を入力するのは手間であり 課題である  シングルタスクCNNよりもマルチタスクCNNを用いることで調理法や 分量などの食品周辺の情報を用いてカロリー推定することができる   マルチタスクCNN の概要　CNNのアーキテクチャは VGG 層の畳み込みNN   をベースとしています 図に示すように 全結合層 fc は全てのタスクで共有され fc層は各タスクに分岐され 各タスクはfc層と出力層 fc を独立に持つように構成されます 本研究ではカロリー カテゴリ 食材 調理法をベースで分析するため 以下のような構成になります   image png  図：本研究で用いたマルチタスクCNNの構成また 損失関数は以下のように定義されます 各変数についての損失関数の定義についてはここでは省略します 本文内 p を参照してください    使用したデータセット　食品画像と食品カロリーの両方を含むデータセットは 私の知る限り 現在 公開されていないようです そのため 我々はWebからのデータ収集に着目しました 実際 市販の料理レシピサイトでは カロリー付きのレシピが提供されています また レシピごとに食材リストや調理方法の説明も提供されている 本論文では このような情報をいくつかの商用Webサイトから収集し カロリーが付与されたレシピデータセットを作成します また マルチタスク学習の効果を十分に確認するために 日本人向けのサイトとアメリカ人向けのサイトの種類のデータセットを作成し それぞれを実験に用いました    実験 EXPERIMENT  本論文では VGG   を拡張し 図  に示すようなマルチタスク CNN を実装しました fc 層と fc 層では Dropoutの代わりに Batch Normalizationを使用しています また Batch Normalization層とfc層以外の層では ImageNetの分類タスクの事前学習モデルを初期値として使用しています CNNの最適化には 運動量  ミニバッチサイズで SGD 確率的勾配降下法   を用います  損失項の重みは初期値をに設定し まず回学習さます 学習では 各反復の損失の値を保存していきます 最後に 各タスクの損失項の重みとして 全反復の損失の平均値の逆数を使用しています テストでは 学習時の最後のk回の反復から回間隔で得られた個のモデルを用い 各モデルから得られた推定値の平均値を最終的な推定値としました    日本人向けのレシピサイトのデータセット分析結果  image png  図  シングルタスクCNN に比べていずれの変数を考慮した場合においてもマルチタスクCNN の相関が高いことがわかる  image png  図：相関プロット  a がシングルタスクCNN  b がマルチタスクCNN　図 a と図 b を比較すると  信頼楕円からマルチタスクCNNにより精度が向上していることが確認できます   image png  図：分析の成功例  image png  図：分析の失敗例   アメリカ人向けのレシピサイトのデータセット分析結果  image png  日本語レシピデータセットと同様に マルチタスクCNNの優位性が確認することができます    議論 DISCUSSIONS 　本研究では オンライン料理レシピサイトからカロリーの付与された料理写真を収集し 料理カロリーの値をグランドトゥルースとして学習 テストを行いました しかし この食品カロリーの値は正確性を保証するものではなく 誤りも多く含まれていると考えられます つまり 高品質なデータセットを作成することが急務であると考えており 大規模なカロリー注釈付き食品写真データセットをどのように構築するかは最大の課題となっています    結論 CONCLUSIONS 　マルチタスクCNN を用いて 料理のカロリー カテゴリ 食材 調理方向を同時学習することで 料理写真から料理のカロリーを推定することを提案しています また Web上の日本のレシピサイトから収集したカロリー付きレシピのデータセットと アメリカのレシピサイトから収集したデータセットの種類を作成し 実験を行いました 本実験では いずれのデータセットにおいても マルチタスクCNNの性能が 独立したシングルタスクCNNの結果を上回りました 　今後の課題として 食品カロリー推定における体積推定と 大規模かつ高品質なデータセットの構築を予定しています 高精度な食品カロリー推定には 食品の検出とセグメンテーションが重要であり の予め登録された参照物体を用いることが必要となるでしょう また 複数の視点からの画像から食品の体積を推定することも考えています このように 体積 サイズに基づくカロリー推定に マルチタスクCNNによるカロリー推定を導入することで より精度の高いカロリー推定が可能になると期待されます いかがだったでしょうか 以上が論文の内容となります ここまで読んでいただきありがとうございました 以下に本論文の要約を記載します    まとめ  タイトル  食品のカテゴリ 成分 調理法の知見を利用した画像による食品カロリー推定  著者  Ege  T     Yanai  K   研究の動機    先行研究による料理画像データからカロリーに対するマルチタスクCNN の適用の拡張 カテゴリ 材料 調理法   研究の手法    アメリカ人向けと日本人向けのレシピサイトからデータを集め マルチタスクCNN を用いてカロリー予測を行った  貢献    シングルタスクCNN と比較して マルチタスクCNN の方がどのデータに対しても精度が向上し 有用性が確認された  課題    大規模で正確にアノテーション付きデータセットを整備し 正確な有用性を把握する  食品のサイズ 体積を把握できるような指標を追加し 表現の幅を広げる   参考文献    決定版 スーパーわかりやすい最適化アルゴリズム  損失関数からAdamとニュートン法   ,6,2022-04-21
186,186,Teachable Machine／上司にバレないように、秘密のサインでこっそり会話する in zoom会議,画像処理,https://qiita.com/7ENDO/items/29d07cf75a7fc226202d,   帰りたい  話長い  このあとランチいこ を仲良しの友達や同僚だけにチャットで送るのは非常にキケン  私は全員チャットに送ったことがある  もちろん話が長いその人には嫌われた でもその後長い話を聞かなくて済んで結果オーライだった でももしもあなたが私みたいな失態を起こしたくないというのなら お互いにしかわからないサインをこっそり送れば ほ ら安心 angel tone   zoom会議中  話長い  帰りたい をこっそり仲良しの同僚だけにボディサインで伝えるさあ さっそくやってみよう    私のサインは友人に伝わるか実験動画はこちら    Teachable Machineでポーズを読み取りGoogleの Teachable Machine  で簡単 無料で画像認識や音声認識の機械学習がweb上で作れちゃう 私が学習させたポーズとサインの意味はこんな感じ      png          png          png  最後のは真面目にオンライン会議に出ている時用 たまには真面目に聞くんです    そしてTeachable Machine内で実験   Code PenはこちらTeachable Machineで作った機械学習のURLをCode Penに入れれば ほーら できちゃった ちなみに   映像の輸入はOBSをかませたら解決しました     Untitled   on Teachable Machine Code Penらくちん    そして楽しい     色々遊びたくなるので 特に 意味なし系アイデア で活用できそう と思いました ,18,2022-04-20
187,187,【高速なnumpy】WindowsでJAXを使おう【Python】,画像処理,https://qiita.com/FORETUNE/items/a7d39eb9ab4c48d6b4dd,この記事ではJust In Timeコンパイルにより高速計算を実現するライブラリ  JAX  を  Windows  に導入する手順を説明します JITコンパイル JAXの説明はしません  参考       jaxlibのwhlファイルをダウンロードする から自分の環境にあったjaxlibのwhlファイルをダウンロードします  該当ファイルがどれかわからない という方は次のコードを実行することでCPU cuda jaxlibのバージョン cp等を確認してください     Pythonpip debug   verbose      whlファイルをインストールするダウンロードしたwhlファイルのパスをコピーしpip installします ※Windowsではshift 右クリックでパスのコピーを選択できる    Pythonpip install  C \Users\      whl      jaxをpip installする状況に応じてjax gpu やバージョンの指定が必要です  詳しくは  実際には import jax numpy as np や jnp と使うことが多いと思いますが np randomのようにnumpy同様に使えない処理もあります 今後に期待しましょう ,4,2022-04-20
188,188,Googleの最新のAIエンジンは、画像処理を新たな次元に進化させる,画像処理,https://qiita.com/AkiraMiyazaki/items/6f0f4e0f577122037c93,こちらの記事は sciencealert Blog 英文の翻訳です 皆さんもSF映画やテレビ番組をご覧になったことがあるのではないでしょうか  顔のアップやナンバープレートなど 細部を強調するためにズームインするシーンもあります そして いわゆる拡散モデルに基づくGoogleの最新のAIエンジンは このような芸当ができるようになっているのです  これは 他の類似画像から超知的に推測し カメラが当初とらえられなかったディテールを写真に加えるというもので 実は習得が難しいプロセスなのです この技術をGoogleはnatural image synthesisと呼んでいます  このAIモデルは小さく ブロック状 ぼやけた ピクセル化された写真を自然により鮮明な画像に加工しくれます 原画と完全に一致しないかもしれませんが 人間の目としては十分に本物に近いと思います    jpg  Googleはこの作業のために 実はつの新しいツールをリリースしています  SR Super Resolution via Repetitive Refinement と呼ばれるもので 画像にノイズや予測できないものを加え それを逆に除去することで 画像編集ソフトが休日の写真をシャープにするような効果が得られます  SRは 膨大な画像データベースと機械学習による確率的な計算によって 低解像度のブロック状画像からフル解像度の画像を生成することができるのです つ目のツールは CDM 拡散モデル です  Googleは これらを パイプライン と呼び SRを含む拡散モデルを誘導して 画像の解像度を高品質に向上させることができるとしています また 画像の画質も上げます  強化されたモデルを使って より大きな画像を作ることができるのです  Googleは 異なる解像度で異なる強化モデルを使用することで CDMは他の画像拡大方式に勝てるとしています     jpg  要するに SRとCDMの最終結果は印象的です  人の人間のボランティアを対象とした標準的なテストでは SRが生成した顔画像が本物の写真と間違われる割合は約 で 完璧なアルゴリズムであれば のスコアを達成することが期待されることを考えると あまりにも印象的な結果であった  この画像は オリジナルと完全に一致するものではなく 高度な確率論的数学に基づいて慎重に計算されたシミュレーションであることです Googleは つのツールを互いに戦わせて結果を洗練させで GAN Generative Adversarial Network などの他の選択よりも良い結果をもたらすと述べています  Googleは 新しいAIエンジンとその関連技術から 顔やその他の自然物の画像を向上させるだけでなく 確率的モデリングの他の分野でも さらなる成果を上げることを約束しています 同様の画像拡大ツールは ソフトウェアのダウンロードを必要とせず オンラインのみで処理できる手軽なものが登場しています  例： VanceAI    Waifux  などです ,2,2022-04-19
189,189,OpenCVを使ってある色以外は白黒にしてみる,画像処理,https://qiita.com/Guarneri009/items/08d08860b1c99df37af7,OpenCVを使ってある色以外は白黒にしてみる よくある処理に ある色のみカラーにして他の色は白黒にして  ちょっと雰囲気のある写真にしたててみる オレンジ色のみ残して他の色を無くす グレースケール にしてみた   ある色といってもRGBの数値でオレンジと示されても人間にはわからんので  直観的ででわかりやすいHSV色空間へいったん変更する   OpenCVでは色相 Hue は   の範囲なので注意    HSV色空間へ変換した画像の色相 Hue くらいまでのピクセル以外の  他の色のピクセルを全て彩度 Saturation をにすると色が無くなるのでグレースケールになる   HSV色空間へ変換したあとRGB色空間へ戻すのをお忘れなく       cpp  画面をだすよvoid display cv  Mat image         名前をつける      画面出た        なにかキーをおして       整理整頓   ここから      HSV色空間へ      取り出す色を決定     まで      マージ      画面へオリジナル画像  AA jpg  オレンジだけカラー画像,0,2022-04-18
190,190,OpenCVを使って画像を縮小、拡大してみる,画像処理,https://qiita.com/Guarneri009/items/f6b8581ca23c1a378632,画像を小さくしたり 大きくしたりしたいけど 正方形なら　cv  resize　を  使えばいいのだけど  長方形の画像などでは縦横比を維持したまま縮小 拡大したい      cpp  画面をだすよvoid display cv  Mat image         名前をつける      画面出た        なにかキーをおして       整理整頓   ここから      xの画像に変換      前処理　余った余白は黒で埋める      画面読み込み      縦横どっちか長い方は       割合      リサイズ      中心をアンカーにして配置      画面へ変換前画像  x    png  変換後画像  x ,1,2022-04-18
191,191,OpenCVを使ってLenaさんを減色にする,画像処理,https://qiita.com/Guarneri009/items/bc2607ea56e4ccd36a93,Lenaさん画像はxxの色だが 減色して色にしてみる   をで割ると各色種類 xx 色 減色にともなって暗くなるので  ちょっと明るくする加工を少し追加    cpp  画面をだすよvoid display cv  Mat image         名前をつける      画面位置固定      画面出た        なにかキーをおして       整理整頓   ここから      画面に出して      display image  オリジナルLenaさん,0,2022-04-18
192,192,CUDAとプロセス並列化で超高速画像処理 with Julia ,画像処理,https://qiita.com/dnkit/items/8bfa4cad2cf3729fae37,  この記事でやりたいことNVIDIA GPUで大量の画像処理 万枚の画像をフーリエ変換するとか をしたいとき CUDA C C   で書くと Glob を使いたくなるし Python だと CuPy の遅さが気になるしで一長一短です．Juliaでいいとこ取りしましょう．さらに Distributed jl の    pmap      を使ってプロセス並列化することでファイルアクセスのオーバーヘッドを軽減でき さらに高速化できます．  実装例   julia fftOnGPU  マスタープロセスでだけ使うパッケージのロードusing Globusing Distributed  ワーカープロセスを起動．　今回は並列．addprocs    ワーカープロセスで行う処理は  everywhere をつける．  出力したい画像サイズのbit BMPヘッダを予めpathに用意した画像から読み込む  bit BMPを保存するための関数． あらかじめ用意したヘッダとデータで無理やり作るよ．  path の画像をGPUでFFTしてスペクトル分布を保存する関数      処理するファイルのパスを表示    println path       白黒画像ファイルの読み込み．cu  でGPUに配列をコピー．      二次元フーリエ変換と fftshift  画像中央が低周波数域になるように変換する処理       値域を に      Array   でGPUからホストへ配列をコピー．      保存用パスの準備    hoge ディレクトリのbmpファイルのパスをリストで取得  画像一辺の長さdataLen     sample bmp は出力と同型の画像．ヘッダと流用します．header   getBMPHeader    sample bmp    個のワーカープロセスに処理を分散．  補足  Images jl の   save     ではモノクロbitBMPの保存はできません．なので出力と同じサイズのbit BMPを事前に ImageJとかで 作成して ヘッダを流用することで出力しています．  Juliaでは画像輝度値は の実数値で扱います．  参考   高速な計算にむけて     Juliaで超単純にマルチプロセス  ,5,2022-04-17
193,193,超いまさら感があるけど、アニメ風写真→イラスト変換AIの学習済みモデル,画像処理,https://qiita.com/tanreinama/items/af91edc0c9db3f274cac,  画像スタイル変換画像のスタイル変換と言えば Neural Image Style Transferで有名になった かなり昔の技術です   result a png  ゴッホの星月夜スタイルの画像とか有名ですよね 古い実装ですが 私の 著作  でも紹介しています    Cycle GANによるスタイル変換ところが Neural Image Style Transferは逆伝播によって画像を生成するので 生成の度に異なるスタイル画像を指定可能であるものの とても遅いという問題があります そこで Cycle GANで画像の変換タスクを予め学習させておき そのGeneratorを使ってスタイル変換を行おう という手法があります この手法のメリットは 実行速度が速いことで 例えばスマートフォンアプリなどのモバイルデバイスでも十分に実行可能な速度で画像のスタイル変換を行うことが出来ます 一方 予めアニメ風ならアニメ風のスタイルを学習させる訳なので その都度異なるスタイル画像を指定することは出来ません   cycle gan    anime style transferを持ってきたさて Cycle GANでもってスタイル変換を行うモデルですが どうせ誰かが学習済みモデルを作成して公開しておいてくれるだろうと思ってみたのですが GitHubで探してみたけど 以外とありそうで無い   やっと見つけたのが  こちら  のリポジトリです 学習済みモデルも公開されているし サンプル ↓ を見る限り 中々よさそう やったー あった と思っていざ使おうと思ったら おや 環境構築が難しいぞ    学習済みモデルの pb形式って TensorFlowの遙か昔のバージョンのじゃ    色々と試行錯誤してみたものの 当時のTensorFlow環境を現在のサーバー上で再構築するのは めちゃくちゃ大変と言うことがわかりました これは 折角モデルを公開してくれているのに 気軽に使うことが出来ないぞ       変換すべしということで PyTorchおよびtensorflowで使えるように モデルファイルを変換しておきました 自分で作ったモデルじゃないので あまり偉そうに出来る話じゃないですが もし何か需要があればご利用ください ,4,2022-04-16
194,194,ほぼ自動で画像認識アプリケーションを作ろう,画像処理,https://qiita.com/ryosao/items/44c8a6af21eb086d5782,  はじめにPythonなどのコードに知識なしに 事実上のノーコードで手軽に画像認識アプリケーションを作る仕組みを作りたいと思い 学習用の写真を用意するだけで google colabo   streamlit でオリジナルの画像認識アプリケーションを作る仕組みを作ってみました 無料環境のみなのであくまでその場だけで動作する簡易アプリケーションですが 簡単にAIアプリケーションが作れるということを体験出来るといいかと思います    用意するもの 学習用の画像データ　　画像認識で分類させたい項目毎に最低でも枚以上は用意してください 　　用意した画像データ jpg形式 を分類項目毎にフォルダ分けして 全体をzipファイルで圧縮します 　　フォルダ名は英数字にしてください 例├── 全体のフォルダ│   ├── dog│     │   ├── dog 画像データ │     │   ├── dog 画像データ │   ├── cat│     │   ├── cat 画像データ │     │   ├── cat 画像データ  googleアカウント google colaboratory を使うため    学習用のプログラムの起動以下のリンクより学習用のプログラムを開いてください  学習用プログラム  上記のページの下記のボタンをクリックして google colaboratoryを起動します  googleアカウントでログインしてください   image png     学習の実施学習の実施方法は プログラムページに記載していますが 実施する作業は以下のつだけです   ctrl F またはツールメニューの ランタイム から 全てのセル の実行  ファイルの選択 ボタンをクリックからZIP形式で圧縮したファイルのアップロード学習が完了したら自動的に学習済みモデルのダウンロードが開始します なお 上記方法でアップロードすると非常に時間がかかりますので google driveを参照する手順がわかる方はgoogle driveを使うことをお勧めいたします    画像認識アプリへの学習モデルのアップロード以下のリンクより 画像認識アプリケーションを開き ダウンロードした学習モデルをアップロードします  スマートフォンの場合はリアルタイムで撮影した写真を判定することができます   画像認識アプリ    image png     最後に上記の内容で自分オリジナルの画像認識アプリケーションを作成することができます アプリケーション自体も以下のgithubにおいております  アプリケーション     次にstreamlit cloud上で動いているための様々な制約があるので google colaboratory上で完結できるwebアプリを作成できればと考えています ,1,2022-04-15
195,195,画像２値化の前処理にCLAHEを使う,画像処理,https://qiita.com/ydclab_0003/items/3bfaaa364d9e9e1664a5,  値化は画像処理の基本だが･･･オープンソース化が進み 高度な画像処理が誰でも簡単に試せるようになった昨今ですが 値化処理は画像処理の基本として必ずおさえておきたい技術です  画像の値化処理とは 所定の閾値をもって画像を白と黒の色に変換する処理です  値化画像の入力を前提とする画像処理アルゴリズムが多くある  値化画像を題材とした加工事例が豊富にある  通常画像の特定部位を値化画像でマスキングすることで 領域を限定した加工もできる思い通りの値化画像を手にすれば その後の画像加工のアイディアもグンと広がります しかし       期待したような値化が上手く出来ない    これもまたよくあります 苦笑 例えば こんな画像で考えてみましょう   キャプチャ JPG  さて このリング部分を値化で捉えることを考えてみます 画像値化の閾値 thresh を段階的に調整してみました   キャプチャ JPG  しかし どの閾値で値化しても上手くリングを捉えることができなかったようです   リングの左上部を残す閾値だとリング右下部が残らない  逆に右下部を残そうとすると左上部が残らない  CLAHEによる明るさムラの調整思惑どおりの値化が出来なかったのは 画像内での明るさにムラがあり変化しているためです  実践においては あるある ですね こんな時にはCLAHEを試してみましょう CLAHEは Contrast Limited Adaptive Histogram Equalization の略称で 日本語にすると コントラスト制限付き適用的ヒストグラム平均化 です  CLAHE   Wikipedia   OpneCVチュートリアル：CLAHE  Contrast Limited Adaptive Histogram Equalization   さあ では早速試してみましょう 左がCLAHE適用前 右が適用後です   キャプチャ JPG  CLAHEは 一旦画像を細かい区画に分け 個別に明るさ調整した後に再合成するため 加工結果は少しぎくしゃくとした状態にはなりますが リングの部分はより鮮明になったように見えます   値化に再チャレンジさあ CLAHE適用後の画像で改めて値化をしてみましょう   キャプチャ JPG  今度は閾値 thresh が  あたりで見事にリングの値化に成功しました 以上 画像に明るさムラがあり値化が上手くいかなかったときのヒントとなれば幸いです 最後までお読みいただき ありがとうございました   サンプルコード最後に本記事のサンプルコードも載せておきます   値化閾値を変化させplot  検証用のoriginal画像の生成  CLAHE適用後画像の値化を試行動作確認環境：,5,2022-04-14
196,196,OpenCV-Pythonで画像処理　～二値化～,画像処理,https://qiita.com/ToppaD/items/c0bd354bc7dfcc4318a4,    はじめに最近OpenCVでの画像処理を始めるようになったので 勉強したことを覚書として 残していきたいと思っています     画像作成今回わかりやすいように にグラデーションする参考画像を作成しました   グラデーション画像を作成  画像を保存cv imwrite  grad png   img   grad png      単純な閾値処理単純な敷居処理として OpenCVでは以下の関数が用意されています  返り値 内容  ret 適用された閾値  dst 二値化された画像  引数 内容  src 入力画像  threshold 閾値  max value 閾値以上の値がとる値  二値化処理のタイプ 閾値より大きい 閾値より小さい 上記画像の生成コード    大津の二値化 では手動で閾値を決めていましたが 画像によって適切な閾値を決めるつのやり方に 大津の二値化というものがあります 第引数にcv THRESH OTSUを入れることで実行できます       詳細  image png  全体の画素数 P  all  とし 閾値以下のものをクラス 以上のものをクラスとする クラスに含まれる画素数 P      クラスに含まれる画素数 P    とすると 全体におけるクラスの割合 R  と全体におけるクラスの割合 R  はになります 全ての画素の輝度  \sim   の平均を \mu  all   クラス内の平均を \mu      クラス内の平均を \mu    とした時 クラスとクラスのクラス間分散 S  b   は以下のように定義されています またクラス内の分散を S    クラスの分散を S   とすると 各クラスごとの分散をもとにした全体のクラス内分散 S  in   は以下のように定義されています ここで全体の分散 S  all  S b    S  in   を考えると 全体の分散は閾値 t に依らない値なので ここでは定数と考えることができます なので分離度 X を変形して とすると 分離度 X を最大化するには 全体の分散 S は定数なので  S b  を大きくすれば良い ということが分かります つまり最適な閾値 t はが最も大きくなるようなものであることになります なので大津の二値化では \sim  のすべてを閾値に選んだ時 最も S b  が大きくなる数値を適切な閾値として選択するようになっています     適応的二値化処理全体を見て 部分的に光の当たり方が違う際等は 局所的に適切な閾値を設定するほうが 明確な結果を得られる場合があります dst   cv adaptiveThreshold src  maxValue  adaptiveMethod  thresholdType  blockSize  C  返り値 内容  dst 二値化された画像  引数 内容  src 入力画像  maxValue 閾値以上の値がとる値  adaptiveMethod 閾値の種類  blockSize 閾値計算に利用する近傍領域サイズ  C 計算した閾値からCを引いた値を最終的な閾値にする adaptiveMethodには以下のつがあります  二値化処理のタイプ 内容 また blockSizeは近傍領域の辺のサイズで 奇数で設定する必要があります blockSize   であれば Xの近傍が対象となります 以下の写真をブロックサイズを変えて二値化していこうと思います 適応的二値化はこのように部位によって明るさが異なるような例に使用するとよいとされています   cat png  下は上記画像をそれぞれブロックサイズを変えて二値化した例です ブロックサイズが小さいほど block size is   周辺の輝度の差に敏感に反応するため ノイズのようなドットが出てきます 逆にブロックサイズが大きいと block size is  左側が全体的に黒くなってしまい 物体の輪郭が読み取れません この例ですと block sizeがやの際がよいと考えられます 上記画像の生成コード    まとめ画像処理の基礎的なところですが 考え始めると色々と奥が深いなと感じました ,6,2022-04-13
197,197,Streamlit Cloudで画像認識AIアプリをデプロイする(2022.4.12),画像処理,https://qiita.com/tan0ry0shiny/items/c14f02fb1a3d00734348,  背景：Streamlitを使い始めました前回は Streamlitを使って colabからアプリのUIを確認する事が出来ました 今回はリモートサーバーである   Streamlit Cloudにデプロイして スマホから使用したいと思います  Streamlitを使ってスマホから開発環境を作る        目的：streamlit cloudにデプロイする  資料     まで公開  Streamlit Colab 人工知能Webアプリを手軽に公開しよう  Section  Live  AIRS Lab       結果：スマホのカメラで撮った画像を分類出来ました デプロイ出来たのでスマホからでも使えます   image png  寿司を撮ってみました 結果は以下の通りです   image png  CIFARに寿司の画像が無い様で 変な回答をされました ただ AIを実装したアプリが簡単に実装出来たのにはビックリしました   方法今回も 参考にしたYouTubeの大まかな説明をさせて頂きます 詳細は我妻先生がYouTubeで分かり易く説明されています ざっくりとした項目は以下になります   ローカルから実行して確認：我妻さんの資料が分かり易いのでご覧下さい   我妻     ai webapp section   webapp ipynb  GitHub   これだけだと分かり辛い方は 我妻先生の無料公開されている動画   Streamlit Colab 人工知能Webアプリを手軽に公開しよう    が分かり易いと思います   Streamlit Cloudへデプロイ 参考  Streamlit SharingとGitHubでStreamlitアプリを公開する方法    ：結局 我妻さんの動画の公開期間後の追記となりました いつもどおりGoogle先生にお聞きしたところSakizoさんの記事を提案して頂き 無事 再現出来ました ngrokの登録やトークンの発行などがまだの方は 我妻先生の動画  や 前回の記事  を参照して下さい   注意点  ローカル colab から実行する際 エラーとなる事がありました その時は数回リロードすると動きました   ローカルから実行する際はChromeに注意されます 詳細は前回の記事 Streamlitを使ってスマホから開発環境を作る      をご覧下さい   その他で今回 引っかかった事は特になかったと思います 動画の指示通りにすれば実行できました ただ 公開が明後日までですので 詳細な説明はそのうち追記したいと思います   感想：AIアプリのUIを考えるきっかけになりましたとりあえず コードの動きを簡単に見たい時には便利そうです UIがstreamlitに依存してそうですが そもそもUIについてそこまで考えたことが無かったので これを機にUIも考えてAIアプリをどのように使っていけるか考えたいと思います   参考     まで公開  Streamlit Colab 人工知能Webアプリを手軽に公開しよう  Section  Live  AIRS Lab        Streamlitを使ってスマホから開発環境を作る      ,0,2022-04-12
198,198,g-hフィルタ（α-βフィルタ）について,画像処理,https://qiita.com/mi_daiki/items/063f5629fd32ac259c16, 今回  こちら  を参考にg hフィルタについて学んでいく これはRoger R  Labbe 著 Kalman and Bayesian Filters in Pythonです 英語版は CC BY   International ライセンスで公開されています   g hフィルタとはg h フィルタとはあるデータの値をもつ時系列順のデータからそのデータの未来の値を予測するアルゴリズムである g と h はどちらもスケーリング係数を表す g は観測値に対するスケーリング係数であり h は観測値の変化に対するスケーリング係数である g と h ではなく α と β が使われることもあり そのときは α β フィルタ と呼ばれる     固定された g と h を使った g h フィルタを単一の状態変数に対して適用する     data   フィルタリングを行うデータ     x   状態変数の初期値     dx   状態変数の変化率の初期値     g   g h フィルタの g 係数     h   g h フィルタの h 係数     dt   タイムステップの間隔          予測ステップ          更新ステップ 参考：g h フィルタは膨大な種類のフィルタの土台になっている カルマンフィルタ 最小二乗フィルタ ベネディクト ボードナーフィルタなど  これらのフィルタは g と h に異なる方法で値を割り当てるものの その点を除けばアルゴリズムは変わらない 例えばベネディクト ボードナーフィルタは g と h に特定の範囲に収まる定数を割り当てる またカルマンフィルタのように g と h を各ステップで変化させるフィルタも存在する   直感的な考え方g h フィルタの考え方の前にまずは真の値の推定においての直感的な考え方として体重計を例にとる     同じ正確さの体重計つを使用例えば ほぼ同じ正確さ 誤差が同じ と仮定したつの体重計があったときにそれぞれの体重計で体重を測った 体重計Aでは kg  体重計Bでは kg とつともばらばらな値が観測された この場合どちらの体重計が正確かわからない 体重計が二つとも正確でないなら そして体重計で真の値より大きい値が観測される確率と真の値より小さい値が観測される確率が同じなら 真の値は A と B の間にあることが多いはずだ そのため多くの人の場合は 自分の体重の推定値を採用するときその二つの値の範囲の間にあると推定するはずである  体重Aの方が軽いのでそちらを信じて採用する人もいると思うが この考え方は数学の  期待値  の考え方である もし この計測を百万回行なったとしたとき 普通 ならなにが起こるだろうか もちろん 真の体重の値よりもつの体重計は大きい値や小さい値を取りうることはあるだろうがほとんどの場合 真の値をまたぐことになるだろう たとえ またがなかったとしてもAとBの間を選ぶことで悪い方の観測値の影響を和らげることができるためどちらか片方選んで誤差が大きくなるよりは優れた推定値が得られるはずである   スクリーンショット       png   参考：    異なる精度の体重計を使う次に今までの体重計よりもより正確 誤差が少ない な体重計 C を使用した場合を考える 先程の体重計Aよりも正確なため体重計Bを無視してAだけを使って推定値を得れば良いか この答えはつの体重計を使うことにより さらに正確になる である つの体重計 A C の観測点がそれぞれkg kgとする Aの誤差が±kg Cの誤差が±kgとすると以下のようなグラフになる   スクリーンショット       png   参考：上のグラフからAとCのエラーバーの交わるところのどこかに真の値は存在であろう また 今回の注目すべき部分は重なっている部分に今回の観測値のkg kgのどちらも含まれないことである つまり Cの方が正確だからと言ってAの観測値をそのまま推定値に使ったり AとCの平均値である kgを推定値と扱うと体重計の正確さ 誤差 に関する知識からこのつの値はどちらも真の体重の値としてありえないものであり推定値はkg〜kgの間の値とすべきである このことから  つのセンサー 今回のケースだと体重計 があると たとえ片方のセンサーがあまり正確でなくても     つのセンサーより推定値は正確になる  ことがわかる     同じ体重計を使う持っている体重計がつでそれで何度も体重を測った場合について考える 同じ体重計を使う場合は同じ正確さの体重計つを使用する場合と同様にそれぞれの観測値の平均を取るべきであることは説明した ではつの体重計で体重を 回計測した場合どうなるか 一回の測定で真の値より大きい値が観測される確率と真の値より小さい値が観測される確率は同じだと仮定しているので 観測値の平均値が真の値に非常に近くなるはずである  参考：ここでこの体重計の仮定として置いている 真の値より大きい値が観測される確率と真の値より小さい値が観測される確率は同じ について考える これはつまり 真の値がkgであったときに体重計はkgとなる確率とkgになる確率が等しいと仮定しているということである しかし 現実の体重計でこのことがあり得るだろうか 現実のセンサーは真の値に近い値を観測する確率が高く 真の値から遠い値を観測する確率は真の値からの距離が大きくなるに従って小さくなるはずだ つまり 体重計のとりうる値は正規分布にしたがって真の値の近辺を観測しやすいと考えると 参考：こちらもに非常に近い ただこの方法は何度も体重を測る必要があるため真の値を表す推定値として現実的ではない     体重を日間計測するある体重計を使って毎日体重を測定したところ                    という観測値が得られたとしよう 直感的にどう思うか  毎日kgずつ体重が増えていて ノイズ 誤差 により同じような体重が観測されてた 可能性もあれば  毎日kgずつ体重が減っていて ノイズ 誤差 によって同じような体重が観測された 可能性もありうる しかし そういった可能性が高いとは思えない コインを投げて回連続で表が出る確率を考えれば これは非常に低い この観測値だけでは確かなことは証明できないが 体重はほとんど変わらなかった可能性が非常に高く思える 次のグラフは今回の体重の観測値＋誤差をつけたエラーバーと緑の波線はあり得そうに見える真の体重 つまり観測値で説明できる理にかなった推定値 を表す   スクリーンショット       png       次に別パターンでのシナリオを考える 観測値が                            だった場合のグラフから何かわからないか考えよう    pythonplt errorbar range                                                          xerr   yerr   fmt  o   capthick   capsize  plt ylim      スクリーンショット       png   参考：このグラフの観測値から見ると明らかに体重は増加傾向にみえる 実際にこのグラフに体重が変わっていない仮説 緑線 と体重が増加する仮説 赤線 をグラフに表してみる そうすると可能性としてはやはり 体重が増加した 可能性が高そうだ これからわかるように観測値からでも体重の予想はある程度たてられることがわかる    pythonplt errorbar range                                     xerr   yerr   fmt  o   capthick   capsize  ave   np sum                                   len                                xs   range   len                                  line   np polyd np polyfit xs                                    plt plot  xs  line xs   c  r   label  hypothesis  plt plot        ave ave   c  g   label  hypothesis  plt ylim      スクリーンショット       png   参考：  体重の予測を行う一日に約kg体重を増すと知っている状況を仮定する 初日の観測値はkgだとする 最初は何も情報がないためこの値を推定値とする 次の日の体重はいくつになると予想できるか 一日に約kg体重が増えると仮定しているため次の日の体重はkgと予想できる さてこの予測がどれほど正しいだろうか もちろん  一日に約kg体重が増える のが全体に正しいと決め込んで日間の予測値を計算することもできるが 体重計を使っておいてその観測値を捨てるのは馬鹿な真似に思える そこで観測値も見てみる 次の日に体重に乗ると kgを指した   スクリーンショット       png   参考：このグラフからわかるように予測値と観測値が合っていない しかし これは驚くようなことでもないはずだ 予測値が必ず観測値と一致するなら 観測値がフィルタに新しい情報を追加することはない ではどうするか 観測値だけから推定値を計算すると 予測値が結果に影響せず予測値から推定値を計算すると観測値が無視されてしまう つまり つの値を活用するには  予測値と観測値を何らかの方法で混ぜ合わせる  必要があるこのつの値を混ぜ合わせるーこれは前に触れた体重計つを使用する状況とよく似ている そのときの結果によれば予測値と観測値の間にある値を推定値として選ぶことは良い選択だと言える では 予測値と観測値の間のどの位置を推定値とすべきだろうか 今回の場合 体重計の正確さと予測値の正確さはことなり一般的に観測値よりも予測値の方がある程度正確であることが多いように思える  一日で前回の推定値からkgも増えるだろうか  そのため 前に触れた精度の異なる体重計の結果から推定値は観測値よりも予想値に近い値を取るべきだと言える この考え方をグラフに示す   スクリーンショット       png   参考：推定値をスケールする係数を適当に選ぶ  観測値より予測値の方がある程度精度がよい という信念のもと推定値の式は以下で計算することにする    math推定値   予測値   \frac     観測値 予測値 ここで観測値と予測値の違いは  残差   residual と呼ばれる 上述した推定値の算出方法 g 推定値をスケールする係数 を適用したとき今回の観測結果に対してどのような結果になるかコードとその結果のグラフを示す また 今回もうつ定めなければならない係数 h 観測値の変化に対するスケーリング係数 については単位は 重さ 時間 なので日を時間ステップとしている ※今回 仮定として一日あたりの体重増加はkgと仮定する 今回 真の体重データはスタート時の体重をkg 体重の増加は日あたりkgになるように作成しました つまり 初日 日目 の真の体重はkg 二日目 日目　 軽量初日 の真の体重はkgとなります なお 初日の体重の初期値はkgと設定する    python  日間分の体重計の観測値      フィルタ結果の格納場所      フィルタリングの文献では観測値に z を使うことが多い          前回の推定値から予測値の算出          フィルタ更新部分           結果の格納 初期値の設定  スクリーンショット       png   参考：この結果から推定値は真の体重にかなりフィットしているように見える まず 青い実線はフィルタからが出力する推定値を表し初期推定値はkgとしている 赤い波線は予測値であり前回の推定値から計算した予測値を表す 体重の増加は一日kgと仮定しているため第日目では前日の推定値kg 初期値 からkgとなっている 黒い実線は真の体重の増分を示す ここで確認するのは推定値は必ず観測値と予測値の中間にあることである 計算される推定値を結んでも直線にはならないが 観測値を結んだものよりは振れ幅が小さく 観測値を生成するのに使った値に近くなっている また時間が進むにつれ推測値が正確になっているようにも見える ただ この結果はふざけたものである もともと体重の増分を一日kg増加すると真の体重の増加推移と同じ仮定しているため結果が正しくなるのはとうぜんな結果である では先程の関数に体重の増減分を一日kg減少すると推測するとどうなるか   python 初期値の設定  スクリーンショット       png   参考：これはあまり良い推定ができているとはいえない 推定値は観測値からすぐに離れていっている ここからわかることはデータの変化率を正しく推測しなければならないフィルタというのは明らかに役に立たない さらにたとえ最初の推測が正しかったとしても その後の変化率が変わればフィルタは値の推定が失敗してしまうということになる ただここで注意してほしいのはフィルタはうまく適応できないが適応自体は起きている 調整が追いついていないだけ ということである では このフィルタを改善するために一日の体重増加率の初期推定値であるkgの代わりに既存の観測値と推定値から計算するという方法がある 日目の予測値と日目の観測値から体重の増加率を考えるとする 日目の体重の増加はkgではなく kgの体重増加を示しています      のため  この情報を利用することは理にかなっています 結局のところは体重の測定自体は体重の実測値に基づいているので有用な情報になります ノイズが含まれているかもしれませんが ただ何の根拠もなく一日kg増加 もしくは減少 すると推測するよりは優れた推測になるはずです では 新しい体重増加量 日を kgにすべきでしょうか ちなみに前日の増加量はkgです せっかくつの数値があるので今までのようにつの数値の中間の値を選んでみます  今回は  h 観測値の変化に対するスケーリング係数 にしてみます  ただ 今回のこの体重増加量は割合 日あたりの増加量 のため単位を合わせるために以下のような式にします    mathnew gain   oldgain   \frac    \frac  観測値 予測値   day    pythonweight        体重初期値gain rate         先程うまくいかなかった一日kg減少すると仮定した増加率      予想ステップ      更新ステップ  スクリーンショット       png   参考：このグラフはいい感じになっています 最初の方では体重増加の推測が となっているためフィルターが正確に体重を予測するのに時間がかかってますが体重を正確に追跡できています     g hフィルタ以上までがg hフィルタと呼ばれているアルゴリズムのg と h の二つのスケーリング係数  weight scale と gain scale  を表しています ここまでの要点で重要なことは以下のことです   単一のデータ点を使うより複数のデータ点を使う方が正確なので どんなに不正確であってもデータ点を捨てない   データ点が二つあるなら 必ずその中間にある値を推定値として選択して正確さを向上させる   現在の推定値と変化率の推定から 次の観測値と変化率を予測する   予測値と次の観測値をそれぞれの正確さでスケールして混ぜ合わせ 新しい推定値を計算する     固定された g と h を使った g h フィルタを単一の状態変数に対して適用する     data   フィルタリングを行うデータ     x   状態変数の初期値     dx   状態変数の変化率の初期値     g   g h フィルタの g 係数     h   g h フィルタの h 係数     dt   タイムステップの間隔          予測ステップ          更新ステップ 参考：ここで最初に述べたg hフィルタの関数を示す ここまでの流れから難しいところはないはずである 体重問題に対するコードに含まれていた変数をすこし変化しただけで他の部分と変わっていないはずである ,1,2022-04-11
201,201,顔向き(頭部姿勢)の取得,画像処理,https://qiita.com/nom-7/items/f06d1f838940383163c9,  目的ライブへ行った際 ステージ上手下手だとどちらが良い席なのかと考える時がある これは アーティストが左右どちらを向いている時間が多いか統計を取りたいと思い始めた 当記事は 第一段階として顔向きを測定することを目的とする   環境準備  python で仮想環境作成  コーディング   参考上記記事からコードを参照しfacedirection pyを作成DEVICE ID     　使用するカメラのID は標準webカメラ 学習済みdatファイルのパスをコピペwhile True    カメラから連続で画像を取得する    ret  frame   capture read    カメラからキャプチャしてframeにコマ分の画像データを入れる        for  x  y  in shape   顔全体の箇所のランドマークをプロット         回転行列とヤコビアン         計算に使用した点のプロット 顔方向のベクトルの表示  実行結果顔向きを取得することに成功 次回は 背景からステージとアーティストの向きを推定したい ,2,2022-04-09
203,203,python+opencvで画像処理の勉強9 パターン認識,画像処理,https://qiita.com/tanaka_benkyo/items/43ef63f54f3dc191e64b,pythonとopencvを使って画像処理を勉強していきます 今回はほとんど機械学習がメインとなります 細かい理論などはここでは説明しません 説明が不十分であったりコードが見づらい部分もあると思いますがご了承ください 誤字や間違いは気づけば修正しますが お気づきの点があればご指摘ください 前回python opencvで画像処理の勉強 パターン 図形 特徴の検出とマッチングまず 画像を読み込む関数と円形度とRGB各色の平均値を計算する関数を定義しておきます     パターン認識の基本的なアプローチ     パターン認識の流れ画像処理における  パターン認識  とは 観測された画像の特徴を用いて あらかじめ定められたクラスにその画像を識別する処理のことです パターン認識では パターンを同類の画像が共通に持つ特徴の組とします   クラス  とは 同じ画像が属する集合のことで 学習のためにはクラスをあらかじめ定める必要があります パターン認識には 距離計算を用いるアプローチと機械学習を用いるアプローチのつがあります      画像からの特徴抽出  特徴抽出  とは 入力画像からパターン認識に役立つ特徴を取り出す処理です 取り出された K 個の特徴量をそれぞれ x i とすると   特徴ベクトル  は \boldsymbol x   x  x  \cdots x K  T となります リンゴ みかん バナナの画像の赤みの度合いと円形度を計測し それらを特徴とした特徴ベクトルをつくり プロットすると図のように同じ種類のものが集まってクラスとなります この特徴ベクトルで構成される空間を  特徴空間  と呼びます      プロトタイプ法による識別分類結果を表示する関数を定義します こちらの種類の画像群を使用します K近傍法により対象画像に元も近いデータの分類ラベルを予測値とします これ以降は使用する特徴量は画像全体のRGB各色の平均値となります   標準化  K近傍法     クラスの分布を考慮した識別平均値までのユークリッド距離が同じでも 広く分布しているクラスに識別したほうが誤りが少なくなることが直感的にわかります たとえば 学習用の入力画像群の分布に基づいた  マハラノビス距離  により識別を行うことがあります まず 入力画像群の分布から 平均値と分散 共分散行列を求めます ここで クラス c に属し  K 次元の N   c   個の学習用入力画像群をとすると クラスの平均ベクトル  \boldsymbol M    c   と  分散共分散行列   \boldsymbol S    c   は次のようになります ただし ただし 各クラスの平均ベクトル \boldsymbol M    c   をプロトタイプとしたとき テスト画像 \boldsymbol x  から各プロトタイプへのユークリッド距離の乗は マハラノビス距離は となります マハラノビス距離は 平均値までの距離が同じでも広がりの大きい分布に対して短くなる距離です ここで定義に従い計算と可視化を行います マハラノビス距離にしたとき最も近いデータのラベルを予測値とします   平均ベクトルM   X mean axis  M   X mean axis  M   X mean axis    分散共分散行列S    X M  T  X M    len X S    X M  T  X M    len X S    X M  T  X M    len X   ラベルlabels y  色の設定  各ガウス分布における等高線の表示  マハラノビス距離が大きいインデックスを分類結果とする  NN法  とは ニアレストネイバーの略で最近傍の意味であり テストデータに対して最も近傍の登録データを検索し そのデータが属するクラスにテストデータを識別する テストデータの近傍の k 個の登録データを検索し 帰属するサンプル数が最も多いクラスへテストデータを識別する方法が  kNN法  である NN法は 最近傍探索を行う際にテストデータとすべての登録データの距離を計算するため 計算時間を要するという問題があります そこで 木構造を用いて高速な最近傍探索を実現する  kd tree法  が利用されています 多次元の特徴空間にある N 個のデータの集合 \boldsymbol P  \ p  p  \cdots p N\  を分木により分割し 末端ノードにデータを格納します 作成した分木にテストデータを入力してトラバーサルすることにより最近傍探索を実現します kd tree法による木の構築方法は次のようになります   分割する特徴次元を選択する  選択した特徴次元において 中央値となる登録データを選択する そのデータを通過し 座標軸に直交する超平面を用いて分割する   超平面により 分割したデータを左右の子ノードに保存する   子ノードにおいても の処理を繰り返す      線形判別分析  線形判別分析  は クラス間をよく識別する特徴を選択する手法です クラス間を離し クラス内の入力画像を集める評価基準で規定を求めます クラス c のパターン数を N   c    平均値を \boldsymbol M    c   とし 全サンプル数を N  その平均ベクトルを \boldsymbol M  としたとき 全サンプルの分散共分散行列 \boldsymbol S  は以下のようになります クラス c の分散共分散行列 \boldsymbol S    c   は 以下のようになります クラス内分散共分散行列 \boldsymbol S  w とそのクラス間分散共分散行列 \boldsymbol S  b は 次のようになります クラス間の分離度を大きくするような d 個の基底を求める問題は 以下の \boldsymbol S  b\boldsymbol S  w     の固有ベクトルを求める問題と同じになります LDAの実行例を示します   標準化を行う左側が実行前で右側が実行後です  ここでは大きく違いがみられませんでした       部分空間法特徴空間の変換そのものを利用して識別する部分空間法を説明します      SELFIC法学習用の入力画像数に比べて特徴の数が多いと 特徴空間が広すぎ 真のクラスのまとまりが正確に推定できず 識別能力が劣化することがあります そのときには 主成分分析を用いて少数の特徴量にし 識別に不要な特徴を取り除いて テスト画像への識別能力を上げることができる場合があります このように低次元にした特徴空間で プロトタイプとの距離やマハラノビス距離などのクラスの近さに基づく識別方法を  SELFIC法  と呼びます 主成分分析を行い 低次元空間へと変換 といっても次元から次元です  します 左の図が元の分布 右側が主成分分析後の空間となります これを利用して前に説明した識別などを行います     機械学習の概要     教師なし学習  教師なし学習  は 学習データのみからデータの性質を理解することであり クラスタリングや次元圧縮に用いられます クラスタリングは 学習データに内在するクラスタを見つけ出す問題である 次元圧縮は 学習データの多次元情報を その意味を保ちつつ より少ない次元の情報に削減する問題であり データの圧縮や可視化に用いられます      教師あり学習  教師あり学習  は 学習データとその正解情報からモデルを学習して未知の情報を予測することであり クラス分類と回帰問題のつの問題設定を行う クラス分類は 学習データを入力し その学習データが属するクラスラベルを出力するように識別モデルを構築して 未知データが属するクラスを求める問題である 回帰問題は 学習データを入力し その学習データの出力である実数値を出力するように回帰モデルを構築し 未知データの情報を予測する問題です     教師なし学習とクラスタリング教師なし学習は 学習データのみからデータの性質を理解することです      k means法によるクラスタリングある特徴空間のなかでクラスごとに識別対象の入力画像が集まって存在することを仮定して クラスごとに属する画像を同時に求めます この処理を  クラスタリング  と呼びます   k means法  は 入力画像を分割するクラス数をあらかじめ k 個と設定して分割し これを初期状態として分割を繰り返し修正することで よりよい分割を探し出す方法です   標準化を行う     主成分分析による次元圧縮  主成分分析  は 多次元の特徴空間に分散する多数の学習用入力画像から 分布をよく表現できる低次元の特徴空間を求める手法です この低次元の特徴空間を  部分空間  と呼びます ここで  N 個の学習用の入力画像群の特徴量を K 個とすると 入力画像は 特徴ベクトル  となる その平均ベクトル \boldsymbol M  と分散共分散行列 \boldsymbol S  を以下により求めます 主成分分析では 入力画像の特徴空間での分布において 平均値となる点を通り 広がりの最も大きい方向の直線である第主成分を求めます 次に その第主成分に直交しかつ平均を通る番目に広がりの大きい方向の第主成分の直線を求めます これは 分散共分散行列を用いて固有値 \lambda j の大きい方からそれに対応する固有ベクトル \boldsymbol u  j を d 個選ぶと d 次元の主成分が求まる 顔の画像で主成分分析を行ってみる 平均画像とつの主成分を画像化したものは次のようになります 平均画像が少し怖くなってしまいました 苦手な方はすみません    python  平均画像plt imshow X mean axis     gray    image png  主成分画像は次のようになります   主成分画像    教師あり学習     アダブースト複数の識別器を組み合わせてつの強力な識別器を学習するアンサンブル学習のつとして   アダブースト  があります アダブーストは クラス分類問題に対して 逐次的に学習サンプルの重みを更新しながら識別器を選択することを繰り返し 最終的な識別関数を学習します 個々の識別器を  弱識別器  と呼び それらを組み合わせた識別器を  強識別器  と呼びます まず 学習サンプルに対して均一の重みを与える 学習が始まり つの弱識別器が選択されると 正しく識別できるサンプルは重みが小さく 誤識別したサンプルの重みは大きくなる 次の弱識別器の学習では 学習サンプルの重みを考慮して 誤識別した学習サンプルを正しく識別する弱識別器が選択されます この処理を繰りかえして複数の弱識別器が選択されます      サポートベクタマシン汎化能力の高い統計的学習手法のつに   サポートベクターマシン SVM    があります SVMでは クラスの分布を分ける超平面を決める際に マージン最大化という考え方を導入しているため 高い識別性能をもちます      ハードマージンSVMハードマージンSVMの線形識別器の識別関数 f \boldsymbol x   は 以下のように表すことができます    mathf \boldsymbol x   sgn \boldsymbol w  T\boldsymbol x  b  \boldsymbol x \in R d は d 次元の特徴量  \boldsymbol w \in R d は重みベクトル  b\in R d はバイアス項であります 識別関数 f \boldsymbol x   は 特徴量と重みベクトルの内積にバイアス項を加えた値がよりも大きい場合に 小さい場合に を出力する つまり 特徴空間を超平面によりつに分けることを表します 学習ではマージン d を最大とするような超平面の重みベクトル \boldsymbol w  とバイアス項 b を求めることになります      ソフトマージンSVMハードマージンSVMは 学習サンプルの線形分離が可能であることを仮定しています しかし すべての問題において線形分離ができるわけではありません そのような場合 ある程度の誤差を許容して超平面を決定する手法として ソフトマージンの概念を導入した  ソフトマージンSVM  を適用します ソフトマージンSVMでは ハードマージンSVMの目的関数にスラック変数 \xi i をペナルティ港として付き加えて 最小化を行います      カーネルトリック特徴量を非線形写像により高次元空間に写像し 高次元空間において線形分離可能な超平面を決定することを考えます こうして求められた超平面は非線形な境界線となるが 計算量が爆発的に増えます この計算量の問題を防ぐテクニックとして  カーネルトリック  があります カーネル関数を計算するのみで非線形な識別器の学習が可能となります      ランダムフォレスト  ランダムフォレスト  とは 複数の決定木構造を持った多クラス識別器を構築するアンサンブル学習アルゴリズムであります ランダムフォレストは 学習サンプルをランダムサンプリングにより作成したサブセットごとに決定木を構築し 複数の決定木の結果を統合して識別します     機械学習による画像認識の応用例     物体検出物体検出とは あるカテゴリに属する物体が画像中のどこに存在するかを求める問題です 顔検出や人検出などの物体検出法では 検出ウィンドウのラスタスキャンにより 検出対象の位置を検出します カメラと検出対象の物体の距離に応じて画像中の検出対象物体の大きさが変化するため 同一サイズのウィンドウでは異なる大きさの物体を検出することはできない そこで 入力画像からイメージピラミッドを作成し 各解像度の画像に対して検出ウィンドウをラスタスキャンすることで マルチスケールの物体検出を実現します      顔検出と顔識別顔画像の特徴を捉える特徴量として 明暗差に着目した  Haar like特徴量  が用いられています Haar like特徴量は 以下に示すように つの矩形領域の平均輝度の差を特徴量とします    mathH r r  S r  S r ここで  S r  は領域 r の平均輝度を算出する関数です   イメージファイルの読み込み      顔を検知          検知した顔を矩形で囲む     人検出人検出では 情報をヒストグラム化した  HOG特徴量  が利用されています HOG特徴量を画像から求め 画像をセルと呼ばれる矩形に分割し セルごとに勾配方向ヒストグラム h heta   を作成します 最後に複数のセルで構成されるブロックを用いて 勾配ヒストグラムを正規化します そして学習サンプルからHOG特徴量を抽出したら SVMによる識別器の学習を行います      類似画像検索画像検索とは 入力画像を画像データベースと照合し 画像内の物体情報を探し出すことです このような画像検索には   BoVW Bag of Visual Words    という画像特徴表現と最近傍探索により実現できます BoVW表現とは SIFTなどの特徴ベクトルをk means法によりクラスタリングする そして 各クラス中心をvisual wordsとして辞書を作成する 入力画像から抽出した局所特徴からvisual wordsごとの出現頻度をヒストグラムで表現するアプローチがBoVWです まず 各画像の局所特徴量をクラスタリングしてクラスタを作成します    python  BoVW表現の計算  k の最近傍法  各クラスタの中心centroids   bowTrainer cluster  次に各画像に対して各クラスタの出現頻度のヒストグラムを求めます    python  各局所特徴量の出現頻度のヒストグラムの計算 train   各局所特徴量の出現頻度のヒストグラムの計算 test 各画像のヒストグラムを比較して類似度を求めます    python  ヒストグラムの比較を行い類似度を計算する各画像に対して類似度の高いものを表示して結果を確認します 今回はあまりうまく計算できていないようです    python  類似度の近いものを表示ヒストグラムを用いて分類モデルの作成も行ってみます こちらもかなり精度が低いものとなりました    python  局所特徴量の出現頻度を使って分類モデルの作成    次回深層学習による画像認識と生成    参考ディジタル画像処理 改訂第二版    ディジタル画像処理編集委員会  本   通販   Amazon,11,2022-04-05
204,204,Lambdaで画像分類AIをサーバレスAPI化して得た5つの知見,画像処理,https://qiita.com/moritalous/items/db7d3a7ace4321078794,AWS上でAIを実現する場合は通常SageMakerを使いますが 軽量なモデルを使う場合であればサーバーレスで実現することもできると思い環境を構築しました 実際に構築することで得た知見をつ紹介します   image png  以下のチュートリアルで作成した犬と猫を見分けるモデルを使用しました    余談ですが 言語表示を英語にすると 犬と猫の識別ではなく 花の分類に題材が変わります   TenslorFlowのライブラリーはCPU版を指定するLambdaでの推論はCPUで行いますので TensorFlowのライブラリーもCPU版を指定します また 内部で使用するKerasのバージョンも合わせておかないとエラーとなります Kerasとのバージョン不一致時のエラー  参考サイト  LambdaのパッケージはZIPではなくコンテナを選択するLambdaのサービス制限で以下の決まりがあります   項目   条件    デプロイパッケージ   zip ファイルアーカイブ  のサイズ    MB  zip 圧縮済み 直接アップロード  MB  コンソールエディタ     コンテナイメージのコードパッケージサイズ    GB  圧縮済みzipでMB超 展開後でGB超のサイズとなるため zipでのデプロイは失敗します コンテナイメージのサイズは約 GBですのでデプロイ可能です   参考サイト  API Gatewayの種類はHTTP APIだと画像の扱いが簡単API GatewayにはREST APIとHTTP APIがあり どちらもバイナリデータを受け取ることが可能です REST APIの場合は明示的にバイナリデータを受信する設定が必要ですが HTTP APIでは特に設定なく受信ができました 少し横道にそれますが 以下のような 古き良き フォームで画像データをアップロードする場合は  multipart form data 形式でのアップロードとなります multipart form data形式の場合はHTTPボディに画像のバイナリ以外の情報も含まれるため 考慮が必要です 自前で行うのは大変なので  requests toolbelt というライブラリーを使用することで簡単に画像部分が取得できます   参考サイト  コールドスタートは気になるが使えないことはないLambdaで気になるのがコールドスタートですが 数回試した限り Initializationで秒程度でした lambda handler内の推論処理は秒かからず終わっていますので 簡単な処理であれば実用的かもしれません  メモリの割当はGB    Untitled png    AutoGluonで作成したモデルは動作しないAutoGluonはAutoML の OSS のフレームワークです 行でモデルが作れるよというのが売りです 犬と猫を見分けるAIについてはデータセットのダウンロードから含めて行です モデルは簡単に作れたのですが 残念ながらLambda上では推論実行時に以下のエラーモデルは簡単に作れたのですが 残念ながらLambda上では推論実行時に以下のエラーとなり 実行できません となり 実行できません ログを見る限りPyTorch内部でこの処理が使われているため 回避することができませんでした AutoGluonのサイトにはLambdaへのデプロイ方法が紹介されていますが おそらく画像を使わないテーブルデータに対する推論の場合のみ適用できるのだと思います 残念   参考サイト,7,2022-04-04
206,206,CSS または JavaScript におけるブレンドモード対応,画像処理,https://qiita.com/kerupani129/items/13b56fd35e4d228da8a2,過去記事から分離 追記 一部変更 参考  ImageMagick で PSD のレイヤー情報を JSON 形式で取り出す   関連情報   Qiita   本記事で使用した ImageMagick のバージョンは    です     ブレンドモード対応      早見表ブレンドモードの表記の対応は以下の表のようになっています 記載する内容は以下の通り   ブレンドモード名      ImageMagick  Compose      Photoshop      Photoshop  Blend Mode Key      JavaScript  PSD js  CSS または JavaScript から利用するためのブレンドモード名      CSS      JavaScript  Canvas      JavaScript  PixiJS表中で注意が必要な項目は以下のように表記しています   は動作が間違っているもの  は諸事情により使用できないもの    太字  は名前や動作に注意が必要なもの※理由はそれぞれ後述  加算  発光           覆い焼き  リニア    ※全てのブレンドモードを掲載しているわけではありません ※ PixiJS は   Canvas レンダラの強制  が前提です  後述   ※  Over    overlay  でない  や  source over  等は厳密にはブレンドの指定ではありません  別記事参照   参考  アルファ値を含むブレンドモードの画像合成の計算式   Qiita   	  lddg    linear dodge    lighter       メモ  画像編集ソフトウェアにより合成方法が異なるメンテナンス性を考えると上記のコードが良いと思いますが 実際に使用する場合は  Map  に変換すると処理速度的に良いと思います    None  の場合 レイヤーが  非表示  であることが分かるが 表示したときのブレンドモードを  取得できない     PSD の仕様的には 減算 合成も 除算 合成も対応しているが ImageMagick では 通常 扱い  バージョンによって変更される可能性あり      通過 フォルダは 通常 フォルダ扱い    LinearDodge  は本来 単なる 加算 合成をさすが ソフトウェアによっては 加算  発光  合成として使用される     CSS または JavaScript から利用する際の注意点      CSS で指定する場合     現在  加算  加算  発光   減算  除算 合成は非対応 参考   lt blend mode gt    CSS  カスケーディングスタイルシート   MDN         Canvas API で描画する場合     現在  減算 合成と 除算 合成は非対応    lighter  は説明文だけ見ると 加算 合成と間違えやすいが 仕様の数式を見ると   加算  発光    合成 参考   加算合成 と 加算  発光  合成 の違い   Qiita   参考      Lighter   Compositing and Blending Level          PixiJS で描画する場合     現在の PixiJS のバージョンでは   Canvas レンダラを強制  しないとほとんどのブレンドモードを使用できないため 注意      現在の PixiJS のバージョンでは Canvas レンダラ使用時に  ADD  で  lighter  を代わりに使用する実装になっているため  加算 合成でなく   加算  発光    合成になる    PIXI BLEND MODES NONE  は一部のクラス   State  など  のオブジェクトを非表示にするために使用される値で 通常のスプライト等に使用するものではないため Compose の  None  と意味が異なる 結局 Canvas で操作するのとやっていることが変わらないため PixiJS を使用しなくてもいいかもしれません 参考  PIXI Application   PixiJS API Documentation    ※  options forceCanvas  true  で Canvas レンダラを強制 参考  pixi js mapCanvasBlendModesToPixi ts at dev · pixijs pixi js · GitHub   参考  pixi js mapWebGLBlendModesToPixi ts at dev · pixijs pixi js · GitHub   参考  pixi js State ts at dev · pixijs pixi js · GitHub   参考  pixi js Sprite ts at dev · pixijs pixi js · GitHub       その他のブレンドモード本記事で扱わなかった Photoshop や PSD のブレンドモードとして 以下のようなものがあります ,0,2022-03-31
207,207,Crowd Counting用の自作データセットの画像に正解ラベル（アノテーション）を付与する,画像処理,https://qiita.com/HKondo0804/items/de8557a6b7020aec509d,  概要本記事では 機械学習による群衆カウント Crowd Counting の中でも MCNNやCSRNetに代表される   CNNベースの密度推定モデル  を使用する際に利用するデータセットを自作するための  アノテーションツール  を紹介します 今回紹介するアノテーションツールは  Matlab  を使用します   Crowd Counting用のデータセット密度推定モデルとは 群集の画像から密集度をヒートマップとして表現した群集密度マップ density map を作成し ピクセル単位で数の計測を行うモデルです ここ最近の機械学習による群衆カウント Crowd Counting モデルの大半は  CNNベースの密度推定モデル  であり CNNベースの密度推定モデル向けのデータセットもたくさん発表されています  代表的なデータセットの詳細は こちら    群集カウントの最新サーベイ論文 年月末時点    CNNベースの密度推定モデルは教師あり学習なので ラベル アノテーション 正解 付きのデータセットが必要です 密度マップ推定のための Crowd Counting 用データセットの大半は 研究者などの人の手によって手動でアノテーション 正解ラベルの付与 が行われています 今回は自分で用意した画像をCNNベースの密度推定モデルの学習に使用するためのラベル付けの方法を説明します   アノテーションツール今回紹介するのは GitHub上でオープンソースコードとして公開されているCrowd Counting用アノテーションツールの princenarula Crowd Annotation  です このアノテーションツールを使用することで 自作した群衆画像データに対し mat 形式の教師データを手動で付与することができます UCFやShanghaiTechなど 有名な密度マップ推定モデルのCrowd Counting 用群衆画像データセットの多くはこの mat 形式の教師データによってラベル付けされています ラベル アノテーション の内容は群衆内の人の頭部の座標データです   使用方法アノテーションツールの使用方法はREADMEに英語で書かれていますが ここでは日本語で説明します まずはgit cloneコマンドで上のアノテーションツールのリポジトリをローカルに複製してください それができたら gt mat gen mなどを移動させ 以下のようなディレクトリ構成にしてください  元からあるground truthやimagesのフォルダは中にサンプル画像などが入っているので 使用しない場合はサンプルデータを消すか data annotation下に新たに別のground truthやimagesの空のフォルダを作成してください 以下の説明でground truthフォルダやimagesフォルダはdata annotation下のものを指します  次に 用意したラベル付けをしたい群衆画像データをimagesフォルダ下に置きます このとき 画像データの名前は IMG  num  jpg のようにリネームします  方法は ファイル リネーム 一括 などで検索してください MATLABでgt mat gen mを開き 行目のt   の部分に例の場合は 例の場合はを入力します 行目を変更できたら gt mat gen mを実行します 画像が順番に表示されるので カーソルを人の頭部に合わせてクリックすることでアノテーションを付与します 画像内全ての人の頭をクリックしてラベル付けが完了したらEnterキーを押し 次の画像に進みます あとはこれを繰り返します 全ての画像のラベル付けが終わったら終了ですが 途中で中断しても 例えばgt mat gen mをt で実行すれば IMG  jpg の画像からラベル付けを再開できます   スクリーンショット    png  ラベル付けをしている最中の画面のスクリーンショットです ラベル付けが終わった画像の正解データ mat 形式の教師データ はground truthフォルダに生成されます   参考文献,1,2022-03-31
210,210,画像内の群集の数を計測する「群集カウント」（Crowd Counting）を実装する,画像処理,https://qiita.com/HKondo0804/items/af48a3c86b96bbc10b80,  概要本記事では   Crowd Counting   群集カウント 群衆カウント という 機械学習を利用した群衆画像内の人数推定モデルの実装について説明していきます 実装するモデルは この道では おそらく わりと有名な  CSRNet  というCNNベースの密度推定モデルを使用します すべてのコードをじっくり解説 というよりはCrowd Countingに興味はあるけど何から手をつけて良いかさっぱり  という初級者 中級者の学習や研究のはじめの一歩として見ていただければと思います なお 本記事は こちら  のページの実装部分の内容を日本語で説明しているものになります 英語が読める方は元の記事にも目を通してみてください   環境の準備jupyter notebook形式  ipynb のファイルを扱うので jupyter notebookが使用できるように環境の準備をしてください いろんなライブラリを使用するのでAnacondaをインストールし 仮想環境上での実装を推奨します 実装にはPyTorchとGPUを使用するのであらかじめGPU環境の準備 CUDAなど をお願い致します 自分が実装に使用した環境はPython    pytorch  ですが 必ずしも全く同じバージョンである必要はないと思います ただ Pythonを使用する前提でコードの修正などの説明をするので Python環境を使用する場合などはその限りではないことをご承知おきください   CSRNetについて  CSRNet  は年に発表されたCrowd Countingのための  CNNベースの密度推定モデル  です   密度推定モデル  とは 群集の画像から密集度をヒートマップとして表現した  群集密度マップ   density map を作成し ピクセル単位で数の計測を行うモデルです CSRNetは特徴量抽出に特化したフロントエンドのCNNと プーリング演算の役割を担うバックエンドのCNNを組み合わせたネットワーク構造が特徴であり それまでの手法による精度を大幅に更新しています CSRNetを何もない状態からすべて実装するのはかなり大変なので 今回はGitHubのコードを使用して実装を行います GitHub上のコードを使用する場合はgit cloneコマンドが便利です ご使用のコマンドプロンプト ターミナル で以下を入力します git cloneコマンドの詳細はここでは省略しますが 第引数にクローンしたいリポジトリを 第引数にクローン先のディレクトリを指定することで 指定したディレクトリにGitHub上のリポジトリ コードなど を複製します なお 第引数は省略可です  Command  git  not found   などと返ってきたら gitライブラリをインストールする必要があります conda またはpip  install gitなどでパッケージをインストールしてください  以下 このディレクトリを作成済みという前提で説明が進みます   データセットの準備実装するモデルは教師あり学習なので ラベル アノテーション 正解 付きのデータセットが必要です 今回はShanghaiTech Datasetというデータセットを使用します このデータセットは年に上海科技大学のYingying Zhangらによって発表されたCrowd Counting用データセットで Part AとPart Bのつに分けられています 本記事投稿時点では こちらの kaggleのページ  からダウンロードすることができます  kaggleのアカウント登録とサインインが必要です もちろん無料    Ground truth density mapを生成するまずは密度マップの正解データ Ground truth を生成します make dataset ipynbを使用しますが Pythonで書かれているため いくつか訂正箇所があります 全ての修正箇所に対して詳細に解説はしきれないので 各自下の修正済みコードを見て変更箇所を探して都度修正するか 下のコードを写経するか コピペしてください まず 番上のセルを実行してください command not found系のエラーが出たら 各自で必要に応じてcondaやpipでライブラリのインストールをしてください   ライブラリのインポート番目のセルが実行できたら 番目のセルを実行してください おそらくSyntaxErrorが出たと思います printの中身を  で囲む必要があります この修正はこの先いくつかの箇所で必要なので その都度修正してください   画像から密度マップを作成するための関数番目のセルでは rootに自分がデータセットをダウンロードした場所のパスを代入します このrootは次のセルで使用します ここはコピペせずに各自ShanghaiTechを保存してあるフォルダの絶対パスを入力してください   データセットのパスを入力番目のセルではpart Aとpart Bの学習データ テストデータそれぞれのパスに修正が必要です  上記のkaggleからデータセットをインストールする方法以外でデータセットを入手した場合は 特にパスに誤りがないか確認してください 番目のセル ここは修正なし番目のセル エラーがなければ このコードでPart Aの密度マップの正解データ Ground truth density map を生成します 実行完了までしばらくかかるので コーヒー片手にリラックスしときましょう  番目のセルではサンプルとしてPart Aの画像 密度マップ 正解の人数が表示されます  番目のセル Part Bも同様の手順で密度マップの正解データ Ground truth density map の生成を行います 生成にはPart Aのときと同様にけっこうな時間がかかります 実行完了したらGround truth density mapの生成は完了です   パスをSHT Bのものに変更  SHT Bの画像から密度マップを生成  モデルの学習 訓練 train pyを使用してモデルの学習を行います モデルが生成した密度マップと さっきまで生成していた密度マップの正解データとの誤差を最小化することで学習を実行しています 少しだけmodel pyとimage pyの内容を修正します model pyは 行目を image pyは次のように変更してください git clone先のディレクトリ上で次のコマンドを実行し モデルを学習させます  初めて学習を実行する際には 同ディレクトリ内のjsonファイルを書き換える必要があります コマンドを実行する前に下の jsonファイルの書き換えに関して を読んで下さい  shanghaitech dataset part Bを学習に使用したい場合は 上記のpart Aの部分をpart Bに置き換えれば良いと思います 番目以降の変数はGPU番号などですが よほど特殊な状況でない限りはそのままで大丈夫なはずです       jsonファイルの書き換えに関して上記のgit cloneによってディレクトリを作成していれば ディレクトリ内にshanghaitech dataset用のjsonファイルが複数あると思います jsonファイルには各画像データの絶対パスがリストで書かれています しかしデフォルトの状態では絶対パスが自分の環境のものと異なるため実行できません 適宜自分の環境に適した絶対パスの書きかえを行ってください 最近のテキストエディタであれば 同じ文字列の一括編集 同時編集 が出来ると思うので上手く活用すると良いでしょう  書き換え完了後に上のコードを実行してJSONDecodeErrorが出力されたら 書き換えの際に余計な文字などが入っていてjsonファイルに問題がある可能性が高いです    モデルの評価 検証モデルの学習が完了していると使用しているディレクトリ上にmodel best pth tarみたいな名前のファイルが出来ているかと思います その中に学習したモデルのパラメータ 重み が保存されています val ipynbを使用してこのモデルの評価をしてみましょう val ipynbもいくつか修正箇所があるので make dataset ipynbのときと同様に各自で変更をしてください 最初のつのセルはそのままで大丈夫です     python val ipynb ライブラリのインポート次のつはデータセットのパスに合わせて 各自変更をお願いします path setsの中身には評価したいデータセットを入力します     python val ipynb  データセットのパスを入力次も特に変更なしです 次のcheckpointの中には 自分のディレクトリ上にある学習済みモデルのパスを入力してください     python val ipynb  学習したパラメータ 重み を代入次のコードで指定したデータセットのMAE 絶対平均誤差 を算出します 正しく学習が完了していれば Part Aのテストデータなら前後 Part Bのテストデータなら前後になるかと思います 次のコードを実行すると 指定したテストデータ画像のモデルによる推定人数 モデルが作成した密度マップ 正解の人数 密度マップの正解データ 元のテストデータ画像が出力されます   行目のパスは適宜変更してください 次のコードは上のコードを参考に作成した 未知の画像に対する学習済みモデルの推定人数 モデルが作成した密度マップ 元の画像を出力するコードです 人数を推定したい画像のパスを org data jpg の部分に入力してください   最後にここまで読んでいただきありがとうございます 自分なりに説明したつもりではありますが たくさんのコードを使用しているのでどこかしらで予期せぬエラーが発生してしまったかもしれません それでもめげずに試行錯誤する過程にこそ 学びがあると思います 自分の環境ではなぜかtqdmが上手く動作しなかったので tqdmを全てのコードから外しました こんな感じで臨機応変に対応していただければと思います   参考文献など  AnalysticsVidhya   It s a Record Breaking Crowd  A Must Read Tutorial to Build your First Crowd Counting Model using Deep Learning  この記事の親記事です ,4,2022-03-27
211,211,python+opencvで画像処理の勉強8 パターン・図形・特徴の検出とマッチング,画像処理,https://qiita.com/tanaka_benkyo/items/f65ffabc32538020ba20,pythonとopencvを使って画像処理を勉強していきます 今回もschikit imageなども使用します 説明が不十分であったりコードが見づらい部分もあると思いますがご了承ください 前回python opencvで画像処理の勉強 領域処理    テンプレートマッチングによるパターンの検出     テンプレートマッチングつの画像が同じかどうかを判断するために 画像を重ね合わせて違いを調べるような処理を一般に  マッチング  と呼びます 画像の視覚的特徴や画素値そのものをパターンと呼び パターンの存在や位置を検出することをパターンマッチングと呼びます あらかじめ標準パターンを  テンプレート  として用意しておき 入力画像とのマッチングを行うことを  テンプレートマッチング  と呼びます テンプレート画像を画像全体に対して移動し それぞれの位置で類似度を調べるとき これを画像の左端から水平方向に それを順次下の行に向かって探索することを  ラスタスキャン  と呼びます      類似度テンプレートマッチングでは つの画像間の  類似度  または  相違度  を調べるために SSDやSADを利用することが多いです これらは 一致したときにとなるので 相違度を表しています 類似度として 以下のNCCを利用することもあります テンプレートと対象画像のベクトルを \boldsymbol T  と \boldsymbol I  と考えると SADは先端間の市街地距離 SSDはユークリッド距離の乗 NCCはベクトルのなす角を表しています また 以下の相互相関係数を類似度として利用することもあります ここで  \bar I  と \bar T  は領域内の画素値の平均値です      サブピクセル位置推定類似度や相違度の最大値または最小値を与える位置から 画素単位でテンプレートの位置が得られます さらに精密に求めるためにフィッティング関数で補間し サブピクセルの位置を求める手法が  サブピクセル位置推定 類似度補間手法   です 相違度が最小の位置における相違度の値を R    その隣接位置での値を R    R   とします 等角直線補正パラボラフィッティング   math    \hat d  \frac R    R    R    R   R        高速探索法     残差逐次検定法SADを計算するとき 領域内の差の絶対値を加算しています 加算の途中で残差があるしきい値を超えたら 検出位置ではないと判断し 加算を打ち切りつぎの位置での計算に移る方法が  残差逐次検定法  です      疎密探索法画像情報を何段階かの解像度で表現し 効率的に探索する方法を  粗密探索法  と呼びます イメージピラミッドを構成し 低解像度画像から順に探索を行います      参照画像の選択による高速化テンプレートに含まれるがそから同時生起行列を用いて選択された独自性の高い画素を用いて類似度計算することで 高速にマッチングを行います 同時生起行列の確立を計算し 確率が小さいパターンほど独自性が高いと言えます cv matchTemplateでテンプレートマッチングを行います 左がテンプレート画像 番目の画像が計算結果の画像 一番右の画像が検出結果です     エッジ情報とヒストグラムによるパターン検出     エッジ情報を用いたチャンファーマッチングエッジ相違度を用いて行うテンプレートマッチングを  チャンファーマッチング  と呼びます まず エッジ画像を作成し 距離変換処理を行い距離変換画像を作成します 入力画像から作成した距離変換画像とテンプレート画像から作成したエッジ画像を 相違度に基づいてマッチングを行います    python  値化処理  距離画像  テンプレート画像の作成  テンプレートマッチング  距離画像に対して エッジ画像のテンプレートでマッチング     ヒストグラム情報を用いたアクティブ探索     カラーヒストグラムの類似度  カラーヒストグラム  は カラー空間を量子化した際の各色番号に対する頻度を表したものです   ヒストグラムインタセクション   は 以下のようにカラーヒストグラムの各色番号における頻度の大小を比べ 小さいほうの頻度の和です   バタチャリア係数  では 各色番号における頻度を掛け合わせて その総和を求めます 以下に例を示します ヒストグラムインタセクション 交差  バタチャリア係数の他にカイ乗と相関による類似度の計算結果も示します まず ヒストグラムの計算とヒストグラムの類似度を計算する関数を定義します 次のつの画像で計算を行います ポスタリゼーション処理をしたものを使用します     ポスタリゼーション用のLUTを作成    相関        カイ乗        交差 ヒストグラムインタセクション         Bhattacharyya距離        img vs img    相関        カイ乗        交差 ヒストグラムインタセクション         Bhattacharyya距離        img vs img    相関        カイ乗        交差 ヒストグラムインタセクション         Bhattacharyya距離       特徴点検出画像間の対応位置を求めるときに 画像中の特徴点をまず検出して その特徴点の対応を探索することがあります コーナーや特徴点 輪郭線を検出する方法を説明します      コーナー検出     ハリスのコーナー検出画像からコーナーを検出する代表的な手法である  ハリスのコーナー検出  の手順を示します   入力画像 I に対し ガウス関数 G \sigma  をx yの各方向で微分した G  x  \sigma  G  y  \sigma  を畳み込み 勾配画像を求める     各勾配画像の積により 各方向における勾配の大きさを算出する 以下に示すようなガウス関数 G \sigma   による重み付き和を用いることが多い   以下の行列 M x y  を定義する   画素 x y がコーナーである場合 行列 M x y  の固有値 \lambda    \lambda    はともに大きい値となる そこで コーナー関数 R を以下のように定義する  k は調整パラメータである   Rの値が局所的な最大値となる画素をコーナーとして検出する 適当なしきい値を設け 有効なコーナーだけを選択する      FASTによるコーナー検出  FAST Feature from Accelerated Segment Test    のコーナー検出は 決定木をトラバーサルしてたどり着いた末端ノードが保持する情報により コーナーを検出する方法であります まず 注目画素 p x y  を中心とする周囲画素を考えます 周囲画素の画素値が注目画素の画素値よりも明るい場合はBrighter 暗い場合はDarker 類似している場合はSimilarと値化します BrighterまたはDarkerが一定数以上連続する場合に 注目画素をコーナーとします FASTでは決定木を用いた判定を行います      Shi Tomasiのコーナー検出ハリスのコーナー検出を改善したもの 物体追跡に有用です   ハリスのコーナー検出  FASTによるコーナー検出     DoG画像を用いた特徴点とスケールの検出画像中に拡大縮小があると 画像間の特徴点領域の濃淡パターンが変化するため 特徴点の対応付けができません 特徴点とその領域の大きさを表すスケールを検出する必要があり 複数のDoG画像で計算することができます DoGはスケールの異なるガウス関数 G \sigma  と入力画像 I を畳み込んだ平滑化画像 L の差分により求めます  k は \sigma の増加率であり スケールを少しずつ大きくして複数のDoG画像を求めます ここでは DoGフィルタによる特徴点の検出に加え LoGフィルタ そしてDoHフィルタを用いた特徴点の検出の例を示します LoGフィルタは ガウシアンフィルタで平滑化した画像に対し ラプラシアンフィルタを適用するフィルタです DoHフィルtは ヘッセ行列を用いた特徴点の検出法です     特徴点の記述とマッチング     スケールと回転に不変な特徴記述 SIFT   SIFT  の特徴記述は 検出したスケールい合わせた領域に対して 回転に不変な特徴量を記述するためにオリエンテーションを算出します オリエンテーションは特徴点における方向を表します 特徴点のスケールに合わせた局所領域内の平滑化画像 L x y  から勾配強度 m x y  と勾配方向 heta x y  を求めます 勾配強度と勾配方向から 重み付き勾配方向ヒストグラムを作成します 勾配方向は方向に量子化し 中心に近いほど高く重み付けして投票します オリエンテーション方向に特徴記述する矩形領域を回転し 勾配情報に基づく特徴量を記述します オリエンテーション方向に座標軸を合わせた領域で特徴量を記述するため 回転に不変な特徴量です   SURF  は SIFTの高速化版であり SIFTはLoGをDoGを用いて近似していたところを   ボックスフィルタ  を使ってこの近似を行ったものです   使用するためには設定が必要です ここでは行いません     SURFは商用利用不可のアルゴリズムです siftは商用利用可能となっています        KAZE特徴量 AKAZE特徴量非線形拡散フィルタを用いて算出する特徴量です AKAZEはKAZEを高速化したものとなります      値特徴量SIFT特徴量は計算に時間を要するというデメリットがあります そこで特徴量を値ベクトルで表現する手法が提案されています   BRIEF  は ランダムに選択された点の画素値の差の符号かｒ値特徴量を生成します   ORB  は BRIEFの処理におけるサンプリングペアを教師なし学習で決定します   BRISK  は BRIEFを発展させた方法で スケール不変性と回転不変性を得ています   BRIEFは商用利用不可のアルゴリズムです    image png      その他の特徴量  ブロブ  を検出する特徴量の例を示します      対応点マッチング異なる画像間で検出された各特徴点の特徴量を比較することで 画像間の対応付けが可能となります 画像 I    の特徴量を x  画像 I    の特徴量を y としたとき つの特徴量の類似度は ユークリッド距離の乗distにより算出できます 画像 I    のあるつの特徴点と画像 I    の全特徴点との距離を算出し 最も小さいもの 番目に小さいものを比率テストを行い 信頼の高い特徴点と対応点を判定します  k を小さくすると対応点が減少します   つの画像で特徴点の検出  マッチングmatcher   cv BFMatcher  matches   matcher knnMatch descriptor  descriptor  k    マッチング精度が高いもののみ抽出    図形要素検出     ハフ変換xy画像空間中の直線は  \hat a  を傾き  \hat b  を切片としたとき 以下のように表されます    mathy \hat a x \hat b abパラメータ空間では xy画像空間中の直線を点で表すことができます xy画像空間中の直線 l 上の点  x  i  y  i   は 以下によってabパラメータ空間に写像されます abパラメータ空間では xy画像空間中の点を傾き  x  i   切片  y  i   で表すことができます 複数の点をabパラメータ空間に写像すると 点の数だけ直線が描画され これらの直線は点  \hat a  \hat b   で交差します このように直線検出原理を  ハフ変換  と呼びます ハフ変換の処理は以下のように行われます   エッジ検出処理などで 出力画像を用意 値化処理で 線の上に位置する可能性がある画素を用意する   abパラメータ空間を小さなセルに分割し 線候補画素をabパラメータ空間に写像したときに生成される直線が通過するセルの値を増やす この処理は  投票  と呼ばれる   この処理をすべての線候補画素について行う 結果 abパラメータ空間は直線の通過した回数を値として持つ 投票度数    abパラメータ空間で 投票度数が大きなセルを探索すると その座標がxy画像空間中の直線の傾きと切片に相当する また xy画像空間中の直線を以下のように表し パラメータ空間の大きさを制限することができます    math\hat ho  x\cos \hat heta   y\sin \hat heta  ただし  \hat ho  は原点から直線までの符号付距離 \hat heta は原点から直線への垂角です 一方 xy画像空間中の直線 l 上の点  x  i  y  i   は  hoheta 空間では正弦波に写像されます     直線検出例まずは 標準ハフ変換による検出例を示す 確率的ハフ変換による直線検出例を示す      円検出例    顕著性マップ     特徴統合理論  特徴統合理論  とは 形状 色 明るさなどの情報に分解し 注目した領域に対して組み合わせることで物体を認識するというものです      顕著性マップ  顕著性マップ  とは 特徴統合理論に基づき 各特徴マップ生成時にそれぞれの特徴マップが独立に注意を引き付ける性質を持っていると考え この性質を数値化したものです イッチらの計算モデルでは 画素値 色空間 勾配方向のつの特徴マップを生成し それぞれのマップから注目度を計算します 顕著性マップを利用することで画像中の重要な物体を捉えることができるため シームカービングや人の注視点推定などに用いられます     次回パターン認識    参考ディジタル画像処理 改訂第二版    ディジタル画像処理編集委員会  本   通販   Amazon,32,2022-03-24
214,214,# 画像処理 エッジ検出器(2),画像処理,https://qiita.com/kotabook/items/93169cc5a67eebb19745,前回の記事に続いて 本記事では二次微分によるエッジ検出器について記していきます      二次微分を利用したエッジ検出の理論前回と同様に 二次微分の数学的表現からフィルタの形を求めてみます 二次微分では 前回の記事にも記したように 色の急激な変化点にゼロ交差点が発生します そのゼロ交差点を用いてエッジ検出を行っていきます   picture png  連続系の x 方向  y 方向の二次微分の和をラプラシアンといい  \nabla  で表されます    math\nabla    \frac \partial  \partial x     \frac \partial  \partial y  また 離散系の二次微分は 前回記事の I x  y  を用いて以下のように計算ができます 以上のことから ラプラシアンのカーネルは以下のように表されます 斜め方向を考慮した場合は以下のようになります ただ このままだと ノイズの影響を避けられないため 通常はガウシアンフィルタと併用されます そのため ガウシアンフィルタで平滑化を行い ラプラシアンフィルタでエッジ検出する流れをラプラシアン ガウシアンフィルタ Laplacian of Gasussian filter  LoG と言います ラプラシアン ガウシアンフィルタは通常以下の式で表されます      エッジ検出のPythonでの実装      ラプラシアン ガウシアンフィルタの実装前回のコードとあまり変わりません 後半を修正したという感じになります   カーネル関数  ガウシアンフィルタ用のカーネルを作成する関数      カーネルの規格化  ガウシアンフィルタの計算      カーネルサイズを取得    line  column   kernel shape      フィルタをかける画像の高さと幅を取得    height  width   img shape      畳み込み演算をしない領域の幅を指定      出力画像用の配列      フィルタリングの計算  フィルタ計算する関数      カーネルサイズ    m  n   kernel shape      畳み込み演算をしない領域の幅              畳み込み演算  画像を白黒画像としてインポート  カーネルを定義 行数と列数は奇数である必要あり   ガウシアンフィルタを適用  ラプラシアンフィルタのカーネルの定義  ラプラシアンフィルタを適用上記でラプラシアン ガウシアンフィルタが適用できます 計算結果を表示してみます 元画像と得られた画像の比較は以下のようになります   output png  この画像では少し分かりにくいので 結果をもっと分かりやすくするために更なる手順が必要だと考えられます      まとめ二次微分を用いたエッジ検出である ラプラシアン ガウシアンフィルタ について実装を行いました 問題点などあれば指摘していただけると幸いです 以上になります ,2,2022-03-21
215,215,segmentation_models_pytorchの使い方と実装例,画像処理,https://qiita.com/tchih11/items/6e143dc639e3454cf577,segmentation models pytorchというsegmention用のライブラリについて 基本的な使い方を解説後に VOCデータを使用して実際に実装していきます なお VOCデータでの実装コードはgitにもコードを上げています このあとの記事と同じ内容  大した精度が出ているわけではありませんが 実行結果を見ながら進めたい方はこちらもご参照ください   基本的な使い方まずはモデル定義 学習の実行までの方法について記載します パラメータやインスタンスを少し調整するだけで様々なモデルが簡単に作成できます    Dataset DataLoaderの作成まずはDataset DataLoaderの作成です 基本は普通にpytorchでモデルを作成する際の手順と同じですが 正解ラベルはsegmentationを行うラベル毎にOne hotになるように作成するのがポイントです Batchの形は batchサイズ  正解ラベル数  height  width になります イメージは下記の画像の通りです   image png   参照：Dataset DataLoaderの作成方法はデータセットによってまちまちなので ここではDataloaderの中身がどうなっているかだけ確認し コードの詳細は後半の実装編に譲ることにします    モデルの定義DataLoaderの作成が終わったらモデルの作成に移ります 使用するモデルやEncoderの種類 重みなどを指定します 使用できるモデルとEncoderは本家gitを参照しましょう 今回はUnet  にしましたが 例えばPSPNetに変更したい場合は smp UnetPlusPlus を smp PSPNet とします  参考 使用できるモデルとEncoderの種類：  パラメーター  モデル定義   学習時の設定を定義続いて学習時の各種設定を定義します segmention時によく使用されるDicelossやIoUも簡単に指定できます    python  学習時の設定   学習の実行 モデルの保存ここまで来たら事前に作成したDataloaderを入れてfor文で回すだけです 簡単ですね    python  学習実行      IoUスコアが最高値が更新されればモデルを保存実行過程の様子はこんな感じ   image png     作成したモデルでの推論作成したモデルはロードして使用することができます    python  モデルのロード  前処理済みの画像imgをミニバッチ化：torch Size          x   torch tensor img  unsqueeze    推論の実施  VOCデータでの実装ここからはVOCデータでの実装となります 環境はGoogle Colaboratoryです データの内容や構造については下記の記事が大変参考になるのでご参照ください ポイントはmaskデータがインデクスカラーを使って作成されている点です  U NetでPascal VOC の画像をSemantic Segmentationする  TensorFlow      事前準備まずはインポートやデータをDLなどの下準備です    python  必要なライブラリのinstall  使用データのDLDatasetを作成するにあたり 事前にaugmentationと前処理関連の関数を定義しておきます 前段で作成した関数を使いながら Datasetを定義します 基本的な使い方でも記載しましたが maskデータはOne hotに整形するのがポイントです また VOCデータにはbackgroundを含めるとクラス存在しますが そのうち事前にリストで指定したクラスのみをモデリング対象とする形で今回は実装を行いました    python class VOCDataset data Dataset        VOCデータセットのクラス名          元画像の読み込み 整形          maskの読み込み 整形          maskデータの境界線を表すは扱いにくいのでに変換        masks   np where masks        masks           maskデータを正解ラベル毎のOne hotに変換          前処理の実行   モデル定義関数の定義が終わったら 続いてモデルを定義します 今回はUnet  を使って ご覧の各種パラメータにてモデリングを行いました また segmentationを行うクラスは下記の通りリスト化します 今回はbackground person catのつだけですが 必要に応じて PREDICT CLASS のリストに加えたり減らしたりすることで変更します    python  モデルの各種設定  Unet  でモデル作成  モデリングで使用するデータ周りの作成続いてモデリングで使用するデータ周りの作成を行います 具体的にはtrain valid別に画像のpath取得→Dataset作成→Dataloader作成の流れとなります 基本的な使い方では解説がありませんでしたが  smp encoders get preprocessing fn で各Encoderが学習したときと同じ前処理を実施してくれる関数を呼び出しています    python  encoderに合わせた前処理の取得  データ周りを格納する辞書   学習の実行ここまで準備ができたらあとは回すだけです  TrainEpoch と ValidEpoch で学習 検証時の設定を行い for文で学習を実行します 今回はついでに  Early Stopping  Epochを超えたタイミングでの学習率の変更を実装してみました 今回はvalidのIoUは 程度でしたが ここら辺をもう少し調整することで精度向上が望めるかもしれません    python  学習時の各種設定      IoUスコアが最高値が更新されればモデルを保存   推論と可視化による検証最後にモデリングが上手くいっているかを定性的にも評価するため 可視化による検証を実施します 検証データから今回のモデリング対象であるpersonとcatが含まれる画像を抽出した後に 作成した関数を可視化を行います    python  モデルのロード  検証データから cat   person を含む画像を取得  該当の対象物があればpathをリストに加える  検証用の関数を作成      前処理後の画像とmaskデータを取得      前処理後の画像を表示    ax   imshow img transpose           DataloaderのmaskはOne Hotになっているので元に戻してパレット変換    mask   np argmax mask  axis      mask   Image fromarray np uint mask   mode  P      mask putpalette PALETTE     ax   imshow mask       推論結果の表示        x   torch tensor img  unsqueeze     推論のためミニバッチ化：torch Size                推論結果は各maskごとの確率 最大値をその画素の推論値とする      パレット変換後に表示  ラベル毎に実行して結果を確認   推論結果結果は下記の通りとなりました 学習データが多いpersonはまぁギリギリセーフかな と思う一方で catの精度は悪いです もう少しチューニングやらデータの水増しやらで工夫が必要そうですが ざっくり作った一手目としてはこんなもんなのかなとも思います   image png    image png    image png    感想Dataset Dataloaderさえできてしまえば あとは比較的簡単に色々なモデル パラメータを試せる点がとても良いですね 一方で segmentationをやったのは初めてですが中々思い通りの精度を出すのは難しいことも分かりました 良いモデルを作るためにはもう少し細かいチューニングやaugmentationの方法などを考える必要があり その方法については別途勉強をする必要があるなと感じます   参考 公式gitのexample   画像データ拡張ライブラリ   albumentations     PyTorchによるMulticlass Segmentation   車載カメラ画像のマルチクラスセグメンテーションについて．  ,26,2022-03-21
216,216,【画像処理】くっついている硬貨(コイン)を検出してみよう,画像処理,https://qiita.com/spc_ehara/items/afba011e15392c7851f6,     告知 年月日に予定しております当社のオンラインセミナーでは この記事の内容について私が登壇してご説明させていただきます ご興味のある方はぜひご参加いただければと思います 参加は以下のURLからお願いいたします 第部  AIを使わない  物体検出   第弾 硬貨 コイン を検出してみよう第弾 硬貨 コイン の種類を判別してみよう前回 前々回の記事 上記参照 で 硬貨 コイン の検出 判別ができるようになりました しかし 実際には硬貨同士がくっついて置かれることも十分に考えられるかと思います そこで今回は 画像に写っている硬貨を特定しよう シリーズの第弾で くっついている硬貨 コイン を検出していきたいと思います これによって 以下のように前提条件を少しだけ緩く設定することができます    各硬貨同士が重なるまたは隣接 ピクセル以内 して置かれることはない     各硬貨同士が重なって置かれることはない  それぞれのバージョンはPython    OpenCV   になります また 今回の記事の内容はOpenCVの 公式ドキュメント  を参考にしています   前提条件今回検出対象の画像は以下の前提条件を満たしているものに限定していますので 他の条件ではうまくいかない可能性があります   硬貨の種類は年月現在 日本銀行から発行されている有効な硬貨種類 円硬貨 円硬貨 円硬貨 円硬貨 円硬貨 円硬貨 を対象とする ただし 令和年発行の新円硬貨は除く   背景は模様の少ない黒一色で硬貨に対する白飛びはないものとする   画像には背景と硬貨のみが写っている   画像に対する硬貨の大きさは画像の分の以上 分の以下とする   各硬貨同士が重なって置かれることはない   硬貨を正面から撮影した画像である   くっついている硬貨の検出第弾の  硬貨 コイン を検出してみよう   で行った検出アルゴリズムを少し変更して くっついている硬貨を検出していく過程を順に説明していきます    元画像   png  コインがくっついている画像を使います    値化①ガウシアンフィルタで背景にある細かいノイズを除きます ②適応的二値化により 影や光等の影響を抑えながら 値化します ③モルフォロジー変換をして背景を大きなつのオブジェクトにくっつけてしまうと同時に 硬貨のふちが切れてしまうのを防ぎます ここまでは前回の値化処理と大きく変わったところはありません    ラベリング処理閾値外の大きさのオブジェクトを除外するために ラベリング処理を行い 硬貨と思われる丸いオブジェクトのみを残します ただし 今回は硬貨がくっついている場合を考慮して max areaの値を少し広げます また 各オブジェクトの輪郭線が切れないようにモルフォロジー変換でくっつけておきます この時に硬貨が同士がくっついてしまいますが 直後の処理でそれらを分割していきます    円の検出と分割オブジェクトの輪郭を抽出し 各オブジェクトを塗りつぶします ハフ変換による円検出で 硬貨のある部分を検出し 入力画像 bin img に対して円の輪郭を黒で描画します これで各硬貨がくっついていた部分を切り離すことができました    輪郭抽出オブジェクトの輪郭を抽出し その面積と輪郭の中心点からオブジェクトを包む最小の円の面積を比較します 面積がほぼ同じ＝円の形をしたオブジェクトのみを抽出します    検出結果以上の手順でくっついている硬貨の検出ができるようになりました  以前のアルゴリズム 今回のアルゴリズム    tt jpg  動画で処理をしてみた結果が以下の通りです 以前のアルゴリズム↓今回のアルゴリズム↓以前のアルゴリズムと比較してみると 硬貨がくっついている場合もしっかり検出できているのが確認できるかと思います    実装実際の実装はこのようになります   今回追加したところ  さいごに今回はくっついている硬貨 コイン を検出していきました 前提条件が減って ロバスト性が上がったと思います 今後も画像処理に関しての記事を投稿していきますので 引き続きよろしくお願いいたします 目次は以下の記事からご覧になれます ,4,2022-03-16
217,217,OpenCVでレシート画像を切り抜き （射影変換）その２,画像処理,https://qiita.com/kazzzu/items/ebaa3f42c91d9ade3ee5,  目的  レシートと背景が写った写真からレシートをくりぬきたい  OpenCVの理解  なんか画像処理っぽいことしたい  前回のあらすじ写真の中からレシート部分を認識 線で囲う まで行った 今回は射影変換をやってみたいと思いました   環境  Windows Home  VSCode  Python   pip opencv python      本編射影変換をやってみたいと思いました 参考：   輪郭抽出から前回の輪郭抽出の結果はこんな感じ   射影変換これを射影変換します なんか度回転してしまいました 頂点はどのように出てくるのだろうと思い　printしてみました print areas   輪郭抽出の頂点ですがどうやら　右上→左上→左下→右下となっているようです 多分一筆書きで方向は順不同 で 設定した射影側の頂点は　右下→左下→左上→右上なのでヘンテコになるのはそりゃそうですね    改善点これを踏まえて以下の改善を行う   輪郭抽出の頂点をソートし頂点の固定化　レシート画像なので縦位置で縦長レシートと仮定する   縦横の比率をもとのレシートになるだけ合わせる ちょっと冗長なプログラムになってしまいましたが何とかなりました       重心を求める      切り取り画像サイズを求める         右側             右下             右上            h    i          点の順番を求める　tmp             右側                 右下                 右上             左側                 左下                 左上                tmp append           射影変換   改良―射影変換の結果これがこういう風にこれもこうなりました,2,2022-03-14
218,218,python+opencvで画像処理の勉強7 領域処理,画像処理,https://qiita.com/tanaka_benkyo/items/0a607c01fcbe8e0a934f,pythonとopencvを使って画像処理を勉強していきます 今回はschikit learnやschikit imageなども使用します 前回python opencvで画像処理の勉強 値画像処理    領域処理のための特徴量     領域のテクスチャ繰り返しの輝度パターンを  テクスチャ  と呼びます コンピュータがテクスチャを扱うためにはその特徴量を数値化する必要があります      次元フーリエ変換による周波数特徴量次元フーリエ変換の結果 F u v  を用いて パワースペクトル p u v   その極座標 p r heta  を求めます さらに 幅 \Deltaheta を持つ角度hetaの扇状領域 p heta  と 幅 \Delta r を持つ半径 r の同心円状領域 p r  を 以下の式により求めます ここでは パワースペクトルと極座標変換した結果を示します 極座標変換はcv warpPolarを使用します        パワースペクトル          極座標へのマッピング        ガボールフィルタによる局所周波数特性  ガボールフィルタ  とは 正弦波 余弦波にガウス関数で窓をかけた関数のことです パラメータを操作して様々な方向と強度を持つフィルタを多数準備して特徴抽出を行います このフィルタ群を  フィルタバンク  と呼び 抽出された特徴量の群は  texton  といいます 次元のガボールフィルタの式を以下に表します ガボールフィルタのフィルタバンクの例を示します 様々な方向を持つフィルタ群となります    python  cv getGaborKernel  ksize  sigma  theta  lambd  gamma  psi   ksize　フィルタのサイズ  sigma　波の出てくる幅方向は同じで波長の異なるフィルタの例を示します これらのフィルタバンクで変換された画像を示します これらを足し合わせた画像は次のようになります      同時生起行列を用いた統計的特徴量テクスチャの統計的特徴量を求めるために   同時生起行列  を用いる方法があります これは 離れたつの場所にある画素対の値から 画素値の一様性 方向性 コントラストなどの性質を表す特徴量を求めるものであります ある画素 i と  i から離れた位置にある画素 j の画素対を考え 画素 i と画素 j の相対的な位置を \delta  とします それぞれの画素値を L  i    L  j  とし 画素値の対  L  i  L  j   が生じる出現頻度で ある同時生起行列 \boldsymbol H  \delta   L  i  L  j   を考えます ディジタル画像では  \delta  は離散的な値を取り  d  の場合  heta がとりうる値は ° ° ° ° ° ° ° °となります 点対称の関係も同じものとみなすと  heta がとりうる値は ° ° ° °となります 同時生起行列の例を示します 以下のような値画像の場合 のようになります ここで 縦方向は画素 j の値であり上から  横方向は画素 i の値であり左から に対応しています       主な特徴量同時生起行列を用いて計算する主な特徴量に 以下の種類があります ここで  L は画素値のレベルを表します ただし ここではschikit imageを使用して計算を行います また 上で紹介したもののうち一部のみを計算します 実行には少し時間がかかります               境界値処理              xのウィンドウで画像を切り取る              度と度の同時生起行列を計算する              テクスチャ特徴量を計算計算結果を表示します 別の画像でも試してみます     領域分割処理     領域分割処理のアプローチ画像上で近傍の画素を特徴量に基づいて 階層的に統合する方法 画素の持つ属性値のパラメータ空間でクラスタリングする方法などがあります      隣接画素の階層的な統合による領域分割処理類似した特徴量を持ち空間的に隣接した画素の集合に画像を分割する代表的な手法が  領域統合法  です   ラスタスキャンによって ラベルの付いていない画素を注目画素とし 新しくラベルを付けます   注目画素の  近傍で 同じ画素があれば注目画素と同じラベルを付けます   新しくラベルを付けた画素を注目画素として の処理を行います   の操作で新しくラベルを付ける画素がなければ の処理に戻ります   すべての画素にラベルが付いたところで処理を終了します   同じラベルを持つ画素の画素値の 平均値を全て求めます   すべての未処理のラベルを持つ画素の集合および 統合されていない画素の集合に対して 隣接する画素の集合のなかで   上記平均値の差分が最小の画素の集合と統合し 新しいラベルを統合された領域のすべての画素に付けます   すべての画素の集合を未処理のラベルとして  の処理を繰り返します      画素特徴量のパラメータ空間でのクラス分けによる領域分割処理画素の空間的な位置関係を全く考慮せず 画素単位に特徴量を計算し 画素単位でクラス分けをします 詳しくは   パターン認識  で説明します      画素特徴を効率よくクラス分けするミーンシフトここでは ミーンシフト法を用いた画像の領域分割の例を示します まず 使用する画像と画素値の分布を確認します 各画素のRGBの画素値と座標を説明変数とします 比較のため k meansとMean shiftでクラスタリングを行います Mean shiftは画像サイズが大きいと時間がかかります 画素値のプロットとクラスタリングの結果を確認します k meansに比べてMean shiftの方がバランスよく分けられていることが分かります 画像で確認します 黒と緑がうまく分類できていませんが 色ごとに分けられていることが確認できます      対象物と背景の間のエッジを利用した領域分割処理対象領域がエッジで禍根れているときは エッジを閉曲線として抽出する  スネーク 動的輪郭モデル   によって領域分割が行えます スネークは 対象物体を囲む閉曲線を初期値として与え 徐々に閉曲線が縮んでいき エッジの境界に張り付いていきます   閉曲線のエネルギー E  S  を以下のように定め  E  S  が最小になるように閉曲線を求めます  E  internal  は 閉曲線の連続性や滑らかさを表すエネルギー E  image  は 画像のエッジの強度に基づくエネルギー E  external  は ユーザーが望むような任意のエネルギーを付加するための項である  E  internal  は 連続性のエネルギー E  continuity  と 滑らかさのエネルギー E  curvature  の加重和で 以下のように表されます  E  continuity   d\nu ds   は  \nu s  の次微分値の乗であり 閉曲線の長さを短くしようとする  E  curvature   d \nu ds    は  \nu s  の次微分値の乗であり 閉曲線の滑らかさを表す  \alpha と \beta はそれぞれエネルギーの重みである ここでは schikit imageを使用して実装を行います オレンジの点線が初期位置として赤い線が求められています      グラフカットを用いた領域分割処理ある画像において あらかじめ対象と背景の一部の位置が与えられている条件下で 対象と背景には画素値に違いがあるという仮定のもとで 全画像を対象と背景の種類にラベル付けする方法として   グラフカット  があります まず 枠線を与えて求める例を示します 次にマスク画像を与えて求める方法を示します 番目の図がマスク画像です      Watershedアルゴリズムを使った画像の領域分割最後にWatershedアルゴリズムを使用した領域分割の例を示します 値化処理の後に 距離画像へと変換してさらに値化処理を行いラベリングをします そして cv watershedを使って領域分割を行います   image png     python  値化  距離画像  値化  ラベリング    次回パターン 図形 特徴の検出とマッチング    参考ディジタル画像処理 改訂第二版    ディジタル画像処理編集委員会  本   通販   Amazon,13,2022-03-12
219,219,触れ合っている対象物を個々に認識して、AIにより判別して仕分ける,画像処理,https://qiita.com/shimamotosan/items/2b35b6290c0e959fa52a,  やりたいこと以下のテキストでは AIを用いた画像認識により対象物を分類し ロボットアームにより仕分けを行っています   DOBOT Magician AIｘ画像認識ｘロボットアーム制御   前回の記事  で触れ合っている対象物を分離した状態で認識し 仕分けるようにしました 対象物が種類だけの場合は これだけでも問題ありませんが 今回は複数種類を判別して 仕分けるようにしたいと思います   できたもの  手順  学習データの収集  前回の記事で作成したプログラムを使用して 学習データを集めます 手順はテキストの手順と同じです   学習  テキストと同じ手順です   分類  前回の記事で作成した領域を分離する手順を今回のプログラムにも適用します 値化した画像から領域を分離する手順を実行し 分離した領域から画像を切り抜き モデルへ入れ 判別します   プログラム以下のようにプログラムを修正してました 前回の記事に引き続き 一部ソースコードを分割していますのでご注意ください   学習済みモデルを使って仕分ける        学習したモデルの読み込み             モデルを読み込み          パラメーターを読み込み          モデル構成を表示          モデルがディレクトリにない場合は 終了する      DOBOTの初期化処理          加工なし画像を表示する        cv imshow  Raw Frame   frame           グレースケールに変換             値化               触れ合っているオブジェクトを分けて検知する          Unknown 背景でも前景でもない           輪郭を抽出             各輪郭に対する処理              ノイズを除去する                 輪郭の領域を計算            area   cv contourArea contour               ノイズ 小さすぎる領域 と全体の輪郭 大きすぎる領域 を除外              フレーム画像から対象物を切り出す                 回転を考慮した外接矩形を取得する              回転行列を取得する              切り出す              リサイズする              画像の前処理                 分類する                 輪郭に外接する長方形を取得する                x  y  width  height   cv boundingRect contour               長方形を描画する                 ラベルを表示する                  P キーが押されたときの処理                     回転を考慮した外接矩形を表示                  確率を表示              外接矩形の中心点を描画                 中心点の座標をカメラ座標系からロボット座標系へ変換             描画した画像を表示        cv imshow  Edited Frame   edframe           キー入力をms待つ        k   cv waitKey             ESC   キーを押す          プログラムを終了する           C キーを押す          WEBカメラのゲイン値 露出の値を調整する           P キーを押す          各ラベルの確率を画面上に表示する／再度押すと消える           H キーを押す          DOBOTをホームポジションに移動させる 位置リセット            S キーを押す          最後に取得した矩形とその結果を元にDOBOTでピックアップする      終了処理         DOBOTの終了処理    dc finalize        キャプチャをリリースして ウィンドウをすべて閉じる,2,2022-03-09
220,220,触れ合っている対象物を個々に認識して、ロボットアームで移動させる,画像処理,https://qiita.com/shimamotosan/items/3a3ddddeb3ff3f6c7fe3,  やりたいこと以下のテキストでは AIを用いた画像認識により対象物を分類し ロボットアームにより仕分けを行っています   DOBOT Magician AIｘ画像認識ｘロボットアーム制御  ただ 判別したい対象物が触れ合っていると以下の画像のようにつの塊として認識したり その上で大きいサイズとして無視されたりしてしまい 上手く対象物を判別できません 触れ合っている対象物を分離した状態で認識し 仕分けるようにしたいと思います 本記事では AIによる判別を行わず 同じ形の対象物を触れ合った状態で個々に認識してつずつ移動させるプログラムを作りたいと思います 次の記事で 触れ合っている対象物を分離して認識した上で AIによる判別を行いたいと思います  触れ合っている対象物を個々に認識して AIにより判別して仕分ける    参考情報以下のサイトを参考にプログラムを作成いたしました    Watershedアルゴリズムを使った画像の領域分割     Image Segmentation with Watershed Algorithm     物体セグメンテーションアルゴリズム watershed を詳しく    できたもの  対象物を分離する手順おおよその流れを以下に示します 細かい説明は 参考情報をご確認ください WEBカメラの画像を取得し 値化します    python          VideoCaptureからフレーム読み込む           ret  frame   cap read          ret  edframe   cap read            加工なし画像を表示する        cv imshow  Raw Frame   frame           グレースケールに変換              サイズに縮小          グレースケール画像を表示する          値化              サイズに縮小          値化画像を表示する値化した画像の白い部分を少し膨張した領域 明確な背景 を作ります    python            触れ合っているオブジェクトを分けて検知する値化した画像の白い部分を少し小さい領域 明確な前景 を作ります 明確な背景と明確な前景を抜き取ったどちらでもない領域を抽出します    python          Unknown 背景でも前景でもない 個々の対象物の輪郭を取得します 上記を組み込むと以下のように対象物が触れ合っていてもきちんとそれぞれを判別します   プログラム以下に修正したプログラムを示します テキストに記載のプログラムからソースコードを分割等していますのでご注意ください dobotClassifier pyは Z軸調整用に関数を追加しました    python dobotClassifier py  Z座標  オブジェクトの大きさ 環境に合わせて変更する一部の関数を別のPythonファイルへ分割しました 下記ソースコードをコピーし ProcessImage py として 実行するプログラムがあるフォルダに保存してください     補正をかけた画像を保存する      リストに格納された長方形を画像 png データで保存する      モザイク      ぼかし      高コントラス調整      低コントラスト調整      ガンマ調整      ガウス分布に基づくノイズ    モザイク処理       に縮小      元サイズに拡大    ぼかし処理    ガウス分布に基づくノイズ処理      塩モード      胡椒モード    ハイコントラスト調整      ルックアップテーブルの生成      ハイコントラストLUT作成    ローコントラスト調整      ルックアップテーブルの生成      ローコントラストLUT作成    ガンマ調整      ガンマ変換ルックアップテーブル  画像のサイズ調整    切り取った矩形の長辺に合わせて短辺を伸ばす    伸ばされた部分は 黒色＝RGB      で塗りつぶす      長辺のサイズを取得      横のほうが大きい場合      縦のほうが大きい場合一部の関数を別のPythonファイルへ分割しました 下記ソースコードをコピーし TransformCoordinate py として 実行するプログラムがあるフォルダに保存してください     入力した座標 カメラ座標系 を変換行列を使用して DOBOTの座標 ロボット座標系 に変換する今回のプログラムは shoot trainingData py を一部修正して 作成しました 元々は 画像を切り抜いて保存するだけでしたが 識別した対象物をピックアップして移動できるようにしました  G キーを押すと認識した対象物を移動させます   教師データの作成            加工なし画像を表示する        cv imshow  Raw Frame   frame           グレースケールに変換              サイズに縮小          グレースケール画像を表示する          値化              サイズに縮小          値化画像を表示する            触れ合っているオブジェクトを分けて検知する          Unknown 背景でも前景でもない           輪郭を抽出             各輪郭に対する処理              輪郭の領域を計算               area   cv contourArea contour               ノイズ 小さすぎる領域 と全体の輪郭 大きすぎる領域 を除外                 フレーム画像から対象物を切り出す                 回転を考慮した外接矩形を取得する              回転行列を取得する              切り出す              輪郭に外接する長方形を取得する            x  y  width  height   cv boundingRect contour               輪郭に外接する長方形を描画する              長方形の各頂点を描画する              輪郭データを浮動小数点型の配列に格納              中心を描画              情報を描画          描画した画像を表示        cv imshow  Edited Frame   edframe           キー入力をms待つ        k   cv waitKey             ESC   キーを押す          プログラムを終了する           C キーを押す          WEBカメラのゲイン値 露出の値を調整する         画像の保存             S キーを押す         そのまま切り取って画像を保存する                  リストに格納された矩形を長辺に合わせてサイズ調整する                  サイズ調整した正方形を画像 png データで保存する           A キーを押す          補正を加えた画像を保存する                  取得した矩形を長辺に合わせてサイズ調整する                  サイズ調整した正方形に補正を加えて保存する           R キーを押す          画像を回転させた上に補正を加えた画像を保存する                  取得した矩形を長辺に合わせてサイズ調整する                  画像の中心位置                  画像サイズの取得 横  縦                   リストに格納された長方形を画像 png データで保存                  回転 °  °  °  ° して 変換処理した画像を保存                      回転変換行列の算出                      アフィン変換           G キーを押す          最後に取得した矩形とその結果を元にDOBOTでピックアップする      キャプチャをリリースして ウィンドウをすべて閉じる,3,2022-03-09
221,221,numpy形式の２つの画像をマージする方法のメモ,画像処理,https://qiita.com/LittleWat/items/60c5236586fa130a08ac,人生生きていると画像をマージしたいタイミングに出逢うこともあるかと思います そういった時はこちらでマージできます 基本的なnumpyの操作ですが備忘録として残しておきます        画像にマスクをマージする     Args         original  np ndarray   cv imread  で読み込んだRGB channel 画像        mask  np ndarray   cv imread  で読み込んだグレースケール channel 画像        thresh  int   マスクのしきい値     Returns         np ndarray  マージされたRGB channel 画像       マスクする部分を初期化       マスクを加える 必要に応じて色を変更してください  ,1,2022-03-08
222,222,OpenCVでレシート画像のロゴを探す（Template Matching）,画像処理,https://qiita.com/kazzzu/items/b379f74d7f51c5806f09,  目的  ファミマのレシートを探したい  OpenCVの理解  なんか画像処理っぽいことしたいOpenCVにTemplate Matching機能があると聞き どんなもんだろうと思いお試ししてみました  環境  Windows Home  VSCode  Python   pipopencv python    numpy             本編   参考文献ありがとうございます お世話になりました   プログラム    元の画像いろいろなレシートを混ぜて取りました 大きさの違うロゴを左上に貼り付けて これも探せるといいな     テンプレート画像これを探したい     scoreによる足切り   python      閾値による足きりまあまあうまくいったのですが 大きさが違うと探せないのかな     一番scoreが高いもの      検索結果の信頼度と位置座標の取得      検索結果の左上頂点の座標を取得      検索結果の右下頂点の座標を取得      検索対象画像内に 検索結果を長方形で描画ここのロゴを切り抜いたので正解です    まとめ  まったく同じものを探し出すことはできる  大きさが違ったりすると探せない ,1,2022-03-08
224,224,MAEでコラージュ画像作ってみる,画像処理,https://qiita.com/shiba_inu_/items/70ac20516728ac50430b,  概要MAEのコラ画像生成の能力を測るため 軽い実験をしてみます．ちなみにMAEとはBERTのように入力画像のMASK部分を復元するようなモデルです．  用意facebook research公式の こちら  のスクリプトを使います．  ソース  元のソースコードでは入力画像をランダムマスクするため これをマニュアルにしましょう．  今回は上に張り付ける画像の境界部分を手作業で指示します．             マスクするパッチを手作業で指示します．          マスクパッチの指示は以下のように番号を参照します．  この場合は         みたいな境界のパッチをマスクしましょう．  画像は 朝日新聞デジタル様  と マクドナルド様  からお借りしました．  結果上記のコードを Run MAE on the image の直前に実行してから  Run MAE on the image を実行してみます．グレーでマスクされた部分がモデルにより復元されています．   png  なんとも言えない結果ですね．別の画像でもやってみましょう．  スクリーンショット     png  ちょっと位置調整をします．   png  waifuxで超解像してみました．画像は Diamond online様  と 自由民主党様  よりお借りしました．  まとめ本記事はfacebook research様が公開されているMAEの学習モデルのコラ画像生成の能力を見てみました．結果は   実用できるほどではありませんね   ,0,2022-03-07
225,225,論文の勉強14 EfficientNet V1,画像処理,https://qiita.com/tanaka_benkyo/items/23da937de708090bfa0e,EfficientNet Vについて構造の説明と実装のメモ書きです ただし 論文すべてを見るわけでなく構造のところを中心に見ていきます 勉強のメモ書き程度でありあまり正確に実装されていませんので ご了承ください 学習もうまくできなかったので何か致命的なミスがあるかもしれません 自分の実力不足で読み解けなくなってきています 難しいです 以下の論文について実装を行っていきます  i 番目の畳み込み層をと定義します ここで F i はオペレータ  Y i は出力となります また  X i は入力でサイズは  となります  H i W i は空間方向の次元 幅 高さ   C i はチャンネル数です 畳み込みネットワーク N は 次のように層の積み重ねで表すことができます 畳み込みネットワークは同じ構造を持った複数のステージに分けられることが多いので と定義します  F i  L i  は F i がステージ i では L i 回繰り返されることを表します ネットワークの長さ length  L i  チャンネル数 width  C i  解像度 resolution   H i W i  を モデルの精度を与えられたリソースで最大化するためにスケールすることを考えます また  \hat F  i \hat L  i \hat H  i \hat W  i \hat C  i はベースラインとなるネットワークで定義されたパラメータとなります      conpound scaling methodそれぞれのパラメータのスケーリングは独立していません 例えば画像の解像度が高い場合は チャンネル数も増加させなければなりません 従ってそれぞれのパラメータをバランスよく調整する必要があります 新しいconpound scaling methodはcompound係数 \phi を使用して width depth resolutionを一様にスケールします    mathdepth d \alpha  \phi \\width w \beta  \phi \\resolution r \gamma  \phi \\s t \ \alpha \beta  \gamma \approx \\\alpha\geq \beta\geq \gamma\geqここで  \alpha \beta \gamma はグリッドサーチなどで決められる定数となります 今回は  \alpha \beta  \gamma \approx  と制限しました      構造ベースラインとなるネットワークの構造は次のようになります これをEfficientNet Bと表します ここで MBConvはMobileNetで扱ったmobile inverted bottleneckです   image png  これを起点としてconpound scaling methodを次のように適用します EfficientNet B  \phi   での最適な \alpha \beta \gamma は \alpha   \beta   \gamma   と計算できました そして    mathd \alpha  \phi \\w \beta  \phi \\r \gamma  \phi \\より 算出された d w r によって 異なる \phi でスケールアップされた B から B のネットワークを得ることができます 本来各 \phi で最適な \alpha \beta \gamma を計算すべきですが 計算コストなどを考えて固定とします   今回の実装では    の値を参考にしています  というより本当の実装はこちらを参照ください         学習最適化手法としてRMSprop decayは  momentumは です weight decayはe とし 学習率は で エポックごとに の減衰率で減少させていきます SiLU Swish  を活性化関数として使用しています Dropoutの確率はスケールに応じて増加させます  Bで  Bで       実装実装していきます 改めて勉強目的であるため正確でないことご承知おきください 内容自体はMobileNetとほぼ変わりません      keras まずkerasです 必要なライブラリのインポートをします 活性化関数及び SE blockの実装をします これらはすでに以前の記事で紹介しました メインはMobileNetで使用したブロックとなります B Bでチャンネル数とブロック内のレイヤ数の倍率が異なります Bのものをベースに引数で渡す倍率だけ増加させます 各モデルの倍率です さきほど紹介した実装をもとにしています    pythonR   B     B     B     B     B     B     B     B   D   B     B      B      B      B      B      B      B       LW   B     B     B      B      B      B      B      B     CP   B      B      B      B      B      B      B      B    試しにBの構造を確認します あとは学習の準備をして終わります 実際に用意したデータで学習させる場合はこれらのパラメータは調整する必要があります 学習済みモデルが用意されています 実際はこちらを使うことになると思います pytorchでも同様に実装をします Hardsigmoidなどはもともと用意されています 倍率を引数にしてモデルの作り分けをできるようにします 構造を確認します Bを見てみます 学習の準備をして終わります 学習の目的に応じて書き換える必要があります       New  テストデータに対するエポックごとの処理学習済みモデルが用意されています 最後の層を目的に応じて書き換えます 学習させてみようと思いましたが うまくいかなかったので悩み中です ,5,2022-03-07
226,226,【画像処理】硬貨(コイン)の種類を判別してみよう,画像処理,https://qiita.com/spc_ehara/items/b0c913a89877615d30b2,     告知 年月日に予定しております当社のオンラインセミナーでは この記事の内容について私が登壇してご説明させていただきます ご興味のある方はぜひご参加いただければと思います 参加は以下のURLからお願いいたします 第部  AIを使わない  物体検出   前回は 画像に写っている硬貨を特定しよう シリーズの第弾   画像処理 硬貨 コイン を検出してみよう   で 硬貨 コイン を検出しました 結果として 硬貨を検出することはできましたが 種類の判別まではできていませんでした そこで今回はその続編 画像に写っている硬貨を特定しよう シリーズの第弾で 硬貨 コイン の種類を判別していきます それぞれのバージョンはPython    OpenCV   になります また 今回の記事の内容はOpenCVの 公式ドキュメント  を参考にしています   前提条件前回と同様に 今回検出対象の画像は以下の前提条件を満たしているものに限定していますので 他の条件ではうまくいかない可能性があります   硬貨の種類は年月現在 日本銀行から発行されている有効な硬貨種類 円硬貨 円硬貨 円硬貨 円硬貨 円硬貨 円硬貨 を対象とする ただし 令和年発行の新円硬貨は除く   背景は模様の少ない黒一色で硬貨に対する白飛びはないものとする   画像には背景と硬貨のみが写っている   画像に対する硬貨の大きさは画像の分の以上 分の以下とする   各硬貨同士が重なるまたは隣接 ピクセル以内 して置かれることはない   硬貨を正面から撮影した画像である   硬貨の識別硬貨を識別していく過程を順に説明していきます    硬貨 コイン の検出  test png  上記画像のように 前回の記事   画像処理 硬貨 コイン を検出してみよう   でコインの検出までは終わっていますので ここからは各コインの特徴量を抽出して 硬貨 コイン の種類を判別していきます 各コインの特徴として挙げられるのは大まかに以下の通りだと思います   形 穴が開いているかどうか   色 黄色っぽいのか白っぽいのか   大きさ 他のコインと比べて大きいのか小さいのか   表面 裏面の情報 ex    平成十九年  等 これらすべてを使えば精度の高い硬貨の種類判別ができそうですが 今回は 簡単 がテーマなので その中でも形と色の情報のみを使って識別をしていきたいと思います    形の特徴硬貨に穴が開いているかどうかを判別します ここでは輪郭を抽出した後に 硬貨の内側にある輪郭群の中から穴を探しています 穴 孔 の特徴は以下のつです   穴の面積 硬貨の面積と孔の面積比は決まっている   穴の丸み きれいな丸みを持っている 結果として 穴の判別によって以下の通りに分けることができるようになります 穴あり→円硬貨 円硬貨穴なし→円硬貨 円硬貨 円硬貨 円硬貨  comress png     色の特徴各硬貨の情報をまとめて取得し それらの情報をもとに硬貨の種類を判別します extract feature関数で 各硬貨のピクセル情報 穴の有無 硬貨の面積を抽出します 対象となる各硬貨のピクセル範囲は以下のようになります    png    output gif  そしてdetermine coin type関数で 各硬貨のピクセル範囲の色情報と穴の有無から種類を判別しています    描画検出した硬貨 穴 種類を画像に描画します    検出結果以上の手順で硬貨の識別ができるようになりました 試しに動画で処理をしてみた結果が以下の通りです 概ね硬貨を識別できているのが確認できるかと思います    実装実際の実装はこのようになります   さいごに今回は硬貨 コイン の識別をしました 前提条件が多くありますが 比較的簡単に識別ができたかと思います ただし ここから実用性を求めるためには もう少し前提条件を細かく設定する ロバスト性の高いアルゴリズムを実装する必要があります 前者ではカメラと光源に対して一定の制限を設けて できる限り撮影条件が安定するようにします 後者ではある程度撮影条件にぶれがあっても幅広く対応できるようなアルゴリズムを実装します 特に色の判定は光の具合やカメラによっても大きく変わり 非常に難しいため 前提条件にもっと強い制限 カメラと光源を固定する等 を加えるか 色の情報を使わずに処理を行う等のもうひと工夫が必要になります 今後も画像処理に関しての記事を投稿していきますので 引き続きよろしくお願いいたします この講座の構成は 以下の記事より確認できます ,4,2022-03-07
227,227,Nuxt.jsにて表示する画像を圧縮してパフォーマンスを向上する方法,画像処理,https://qiita.com/soicchi/items/fa888c3e72bc9a449f37,   概要今回はポートフォリオ作成した際にフロントのパフォーマンス改善した方法の一つを紹介します 結論から言うと   aceforth nuxt optimized images というライブラリを追加し 表示する画像を圧縮することでパフォーマンス改善を行います    環境   バーション  nuxt       導入方法まずは先程のライブラリをインストールします   注意点として上記ライブラリをインストールするだけでは画像は圧縮されません   圧縮したい拡張子によって追加でインストールが必要です 今回は jpeg   png を圧縮すると仮定して下記のライブラリを追加します yarn add imagemin mozjpeg    imagemin pngquant imagemin mozjpeg の最新パージョンの    はバグがあるようで    を指定しています 詳しくは以前の記事を参照してください たったこれだけで表示される jpeg と png の拡張子を持った画像は圧縮されます    さらなる圧縮方法 webp に変換することでさらに容量を小さくすることもできます  webp とは jpeg などと同様な拡張子です     Step  ライブラリの導入先程のライブラリに追加して下記を追加します このライブラリは  webp がついている画像URLをwebp形式に自動変換してくれます     Step  画像表示用のタグを用意下記の用に template にタグを記載します 上記のようにすると webp の拡張子画像で表示されます 詳しい仕様等は下記ドキュメントを参照してください    まとめポートフォリオを作り終わってブラウザのパフォーマンスをLighthouse等で確認したときにパフォーマンスが    になっていて 上記の圧縮を実行したら    までとりあえず上げることができました アプリの作成には最適化も考えながら構築しないといけないことですね ,0,2022-03-06
230,230,imagemin-mozjpegに関しての不具合,画像処理,https://qiita.com/soicchi/items/569482fcc2fe2fa68a3f,   概要ポートフォリオ開発で画像を圧縮するために imagemin mozjpeg をインストールした際にエラーが発生したのでその対処法をメモします    エラー内容どうやら node modules 内のファイルがrequire  をサポートしていなくてimportに変えなさいというエラーが出ている しかし node modules 内のファイルなのでビルド時に生成されるので変更できないのではないだろうか    解決法ググってみたところ下記の記事を参考にし バーションを最新の    から    にダウングレードすることで解決しました どうやらESM関係のバグなようです なので imagemin mozjpeg を使う際は現状    のバーションを使用したほうが良さそうです ,3,2022-03-04
231,231,OpenCVでレシート画像を切り抜き（改良の余地あり）,画像処理,https://qiita.com/kazzzu/items/d9684319d2d0b64d6f31,  目的  レシートと背景が写った写真からレシートをくりぬきたい  OpenCVの理解  なんか画像処理っぽいことしたい  環境  Windows Home  VSCode  Python   pipopencv python     追記：パラメータなど調整しました   グレースケール化の方法を変更　cv cvtColor→cv decolor  二値化のパラメータ変更　cv THRESH OTSUに変更  本編   参考文献ありがとうございます お世話になりました   プログラム    元の画像パンを買ってきました その時のレシートをパシャリ背景の影響とかありそうだな      グレースケール化ちょっと変更 従来からある方法から最近できた方法 ※ でグレースケールしています    python        グレースケール化    二値化   python        二値化案外うまくいっている     輪郭抽出        輪郭抽出        面積の大きいもののみ選別               輪郭の近似おおっ 輪郭は描画された   よく見ると輪郭は描画されてのですがなんか余計なものも描画されているな    改良の余地ありです   パラメータ調整したらだいぶいい感じにできました つづく      おまけレシートだけではつまらないので明治神宮へ行った時のおみくじ写真に実施してみました かなりうまくいっています これはまるでダメパターン レシートと背景のコントラストが無くうまく二値化できなかった ,10,2022-03-03
232,232,【画像処理】硬貨(コイン)を検出してみよう,画像処理,https://qiita.com/spc_ehara/items/e627f5633d0e9d98a39a,     告知 年月日に予定しております当社のオンラインセミナーでは この記事の内容について私が登壇してご説明させていただきます ご興味のある方はぜひご参加いただければと思います 参加は以下のURLからお願いいたします 第部  AIを使わない  物体検出   今回は 画像に写っている硬貨を特定しよう シリーズの第弾で 硬貨 コイン を検出していきます それぞれのバージョンはPython    OpenCV   になります また 今回の記事の内容はOpenCVの 公式ドキュメント  を参考にしています   前提条件今回検出対象の画像は以下の前提条件を満たしているものに限定していますので 他の条件ではうまくいかない可能性があります   硬貨の種類は年月現在 日本銀行から発行されている有効な硬貨種類 円貨 円貨 円貨 円貨 円貨 円貨 を対象とする ただし 令和年発行の新円貨は除く   背景は模様の少ない黒一色で硬貨に対する白飛びはないものとする   画像には背景と硬貨のみが写っている   画像に対する硬貨の大きさは画像の分の以上 分の以下とする   各硬貨同士が重なるまたは隣接 ピクセル以内 して置かれることはない   硬貨を正面から撮影した画像である   硬貨の検出硬貨を検出していく過程を順に説明していきます    元画像  coin png     値化ガウシアンフィルタで背景にある細かいノイズを除きます   gaus png  適応的二値化により 影や光等の影響を抑えながら 値化します 今回のように画像内で輝度値の差が大きい場合 明るい部分と暗い部分が両方ある場合等 は 一律で閾値を決めてしまうと画像全体をうまく二値化できません 適応的二値化は決められたカーネルサイズごとに閾値を決めるため 画像全体の輝度値の差に影響されずに二値化することができます   adap png  モルフォロジー変換をして背景を大きなつのオブジェクトにくっつけてしまうと同時に 硬貨のふちが切れてしまうのを防ぎます   morph png     ラベリング処理閾値外の大きさのオブジェクトを除外するために ラベリング処理を行い 硬貨と思われる丸いオブジェクトのみを残します    輪郭抽出オブジェクトの輪郭を抽出し その面積と輪郭の中心点からオブジェクトを包む最小の円の面積を比較します 面積がほぼ同じ＝円の形をしたオブジェクトのみを抽出し 元画像に描画します    検出結果以上の手順で硬貨の検出ができるようになりました 試しに動画で処理をしてみた結果が以下の通りです しっかり硬貨の検出できているのが確認できるかと思います    実装実際の実装はこのようになります   さいごに今回は硬貨 コイン を検出していきました 前提条件はあるものの 比較的簡単に検出ができたかと思います 今後も画像処理に関しての記事を投稿していきますので 引き続きよろしくお願いいたします 目次は以下の記事からご覧になれます ,13,2022-03-03
235,235,# 画像処理 エッジ検出器(1),画像処理,https://qiita.com/kotabook/items/0036c3bdc94970173690,前回の空間フィルタリングに続いて エッジ検出についての理論とPythonによる実装方法を掲載しています コードに間違いなどがあればご指摘いただけると幸いです      エッジ検出器とはエッジ検出器とは 画像に写っている物体の輪郭を検出する処理のことです 身近な例で言うと コロナ禍になってZoomなどのオンラインでの会話が増えましたが 背景以下の写真のように変化させる手法も エッジ検出の応用例です   zoom jpeg  写真引用元    デザイン情報サイト JDN   本記事では このようにモノの輪郭を検出する基礎的なエッジ検出器について記していきます      エッジ検出の適用例ここでは 白黒画像の明るさが急激な変化をしているところを物体の縁として検出することを目的とします 例として 一次元で明るさが以下のグラフの  a  のように変化している場合を考えます グラフの  a  のグラフについて 一次微分したグラフを  b   二次微分したグラフを  c  とします   graph png  すると   b  のグラフでは 色が急激に変化している箇所のみが高い値となること そして  c  のグラフでは 同じところにおいて符号が変化するゼロ交差点があることがわかります このように 一次微分の計算または二次微分の計算をすることによって エッジ検出が可能であることが考えられます 本記事では主に一次微分のエッジ検出について記しております 二次微分を利用したエッジ検出では次回以降の記事にで記していこうと思います      一次微分を利用したエッジ検出の理論位置 x における f x  の微分の式は以下のように表されます 連続値の場合の微分の式はこのようになりますが 実際に画素について扱う場合は離散的なグラフとなりますので  \Delta x   ± として  f x ±   を位置 x に隣り合う画素値とすると 以下のように単純な差分についての式が得られます この二式から f x  を消去すると 以下の式が得られます    mathf  x    \frac f x       f x       上記のことを I x  y  について適用すると  x 方向と y 方向の微分の式 I x と I y はそれぞれ以下のように表されます この式から 注目画素 I x  y  に対して 隣り合った画素の差分を  で割ったものを計算し それを可視化することによってエッジ検出が可能であることがわかります   calculate png  上記の式は以下のカーネルを適用することと同値です  x 方向と y 方向の検出結果をまとめるには 各画素の微分値の二乗平均平方根を計算します ただ このままでは薄い縁まで検出されてしまうため この ∇I に閾値を設けて信頼性の高い縁のみを残す処理 Hysteresis Threshold処理 必要になります また 上記の画像のようにエッジが太くなってしまうためそれを細くする処理 Non maximum Suppression処理 が必要になります 後述するPythonではこれらの処理は施しておりませんが 後ほど記事にしていく予定です また 上記のカーネルではノイズに弱いため 注目画素の周辺の画素値の微分も計算し それらの値の平均を取る方法が考えられています 代表的なものがプリューウィットフィルタ Prewwit Filter  ソーベルフィルタ Sobel Filter が挙げられます      キャニーエッジ検出器 Canny Edge とはキャニーエッジ検出器とは 前回の記事で説明したガウシアンフィルタと上記のソーベルフィルタを組み合わせたエッジ検出器です 上記のプリューウィットフィルタとソーベルフィルタはノイズ対策としてカーネルを再検討された結果ですが それでもノイズの影響を避けるには限界があります それを防ぐために ノイズ対策として 前回の記事で説明したガウシアンフィルタを施した後にソーベルフィルタでエッジ検出する方法が考えられます これによってノイズに強い適切なエッジ検出が期待できます      エッジ検出のPythonでの実装       プリューウィットフィルタ Prewwit Filter    ソーベルフィルタ Sobel Filter この二つはカーネルを変更するだけでできますので 同じソースコードで実装します   フィルタ計算する関数def filterD src  kernel        カーネルサイズ    m  n   kernel shape      畳み込み演算をしない領域の幅              畳み込み演算  画像を白黒画像としてインポート  プリューウィットフィルタのカーネルの設定  ソーベルフィルタのカーネルの設定  プリューウィットフィルタを適用  ソーベルフィルタを適用  x yの検出結果をまとめる   filter D   関数を使用することで カーネルに基づいた画素の計算ができるようになっています また 各画素の微分値の二乗平均平方根を計算してまとめます これによってソーベルフィルタとプリューウィットフィルタを適用させることができます 計算結果を表示してみます    matplotlib pyplot   モジュールの   imshow   関数を使用します   元の白黒画像を表示  フィルタ適用後の画像の表示得られた画像は以下のようになりました   元の画像  gray png    フィルタ適用後の画像  ps png  プリューウィットフィルタとソーベルフィルタの双方に大きな差はない結果となりました また 双方ともランドクルーザの大まかな輪郭がかたどれていることがわかります        キャニーエッジ検出器 Canny Edge キャニーエッジ検出器も 上記のソーベルフィルタと前回の記事のガウシアンフィルタを組み合わせるだけなので 特に難しくはないかと思います   キャニーエッジ検出器    ガウシアンフィルタ  カーネル関数  ガウシアンフィルタ用のカーネルを作成する関数      カーネルの規格化  ガウシアンフィルタの計算      カーネルサイズを取得    line  column   kernel shape      フィルタをかける画像の高さと幅を取得    height  width   img shape      畳み込み演算をしない領域の幅を指定      出力画像用の配列      フィルタリングの計算    ソーベルフィルタ  ソーベルフィルタの計算      カーネルサイズ    line  column   kernel shape      畳み込み演算をしない領域の幅              畳み込み演算  入力画像を読み込み  カーネルを定義 行数と列数は奇数である必要あり   ガウシアンフィルタを適用  ソーベルフィルタのカーネルを定義  ソーベルフィルタを適用  x y方向に分けて計算した結果を一つに集約上記のコードでキャニーエッジ検出器が適用できます 計算結果を表示してみます 元画像と得られた画像の比較は以下のようになります   キャニーエッジ検出器  canny png  フィルタリング処理を施しているため 他のものよりもノイズが少ない結果となりました      まとめエッジ検出についてと その代表例である プリューウィットフィルタ と ソーベルフィルタ  さらにフィルタリングと組み合わせた キャニーエッジ検出器 のPythonによる自力実装を行いました 次回は二次微分を用いたエッジ検出について記していこうと思います 以上になります ,3,2022-03-01
237,237,Python(Pillow)を使って複数の画像ファイルをリサイズする処理,画像処理,https://qiita.com/Hoshiy/items/9bcbcfa7d0d30eaa0afd,  目次  はじめに    はじめに   使用環境    使用環境   リサイズ    リサイズ   ファイル取得    ファイル取得   処理    処理   ファイルを保存    ファイルを保存   おわりに    おわりに     はじめに画像を一括でリサイズ処理することがあったので 備忘録としてQiitaに載せておきます 今回はPythonとその画像処理用ライブラリであるPillowを使用しました    参考 使用したサイト一覧   Python  Pillowで画像を一括リサイズ 拡大 縮小     それ pythonでできるよ －画像のリサイズ－   Pillow公式ドキュメント   指定した幅のサンプル画像を作成してくれるーplacehold jp      使用環境  Pillow公式ドキュメントより引用    リサイズPillowはPILをインポートして使用します そして今回は画像のリサイズを行うためにresize  を使用します resize  width height  resample のように引数を指定して使用します width heightの部分にはピクセル値を打ち込んで使うこともできるし 変数を使ったり元サイズの値を参照して指定することもできます    ピクセル指定     pythonimage resize         変数     リサイズ元画像を使用して指定 横幅を元画像の倍 縦を    引数resampleでは リサンプリングする際に使われるフィルターを指定できます デフォルトではNEARESTが使われます フィルターの種類は  NEAREST  BOX  BILINEAR  HAMMING  BICUBIC  LANCZOSです     ファイル取得今回はPATHを指定してファイルを取得していきます ここではglob  を使用して複数ファイルを一括取得していきます これでカレントディレクトリのhogeの中にあるpngファイルを全て指定できます 拡張子を複数指定する場合の処理も後述します     処理先ほどglob  で受け取ったファイルをresizeで処理していきます まずpngファイルのみを処理します    拡張子を指定して処理   リサイズ処理これでhogeディレクトリ内の pngファイルがxにリサイズされます 処理したい拡張子がひとつのときはglob  のほうで指定するだけで可能です 次に複数の拡張子を指定して処理します    複数拡張子を指定して処理     python PATH取得 ここではすべてのファイルを対象とします      指定した拡張子にだけ処理を加える         リサイズ処理これでhogeディレクトリ内の jpg   png   jpegファイルがxにリサイズされます     ファイルを保存処理が終わったら保存先を作成して保存しましょう 趣旨と少しはなれるので ここはちゃちゃっといきます まずリサイズした画像の保存先のファイルを作成します 複数方法ありますが ここではmakedirs  を使って作成します    保存先ディレクトリの作成  この書き方だと  hoge hogehoge resized imagesのように指定して 途中に存在しないディレクトリを挟んだとしても resized imagesまでのディレクトリを作成してくれます では保存先のディレクトリに保存していきます save  でファイルの保存ができます    保存     さきほどの処理と合わせて記述すると   保存先の作成 処理 保存これでhogeの指定した拡張子ファイルがxにリサイズされて resized imagesに保存されます     おわりに最後に自分が実際に書いた処理を載せておきます パスを取得して jpg  png  jpegを指定したサイズの正方形にリサイズする処理です 保存するときに  resized 値  をファイル名に追加しています  リサイズした画像の保存先ディレクトリ作成         リサイズ処理         リサイズした画像のファイル名に  resized 値 を追加正方形なので リサイズしたい値が変更したときにimg sizeに代入する値を変えれば一回で変わるようにしました ファイル名に追加する部分も変数の値が変われば変更されます また余談ですが 正方形のリサイズ時はwidthとheightの値がひとつでもいけるのか気になって試したところと記述するとエラーを吐きます しっかり横x縦を書きましょう  追記 は可能です   追記 ご覧いただきありがとうございました 感想 アドバイスございましたらコメントでお願いいたします ,0,2022-02-26
240,240,Android端末で新と旧アプリをpython+Appiumで自動実行させて得たエビデンス画像をExcelファイルに貼り付けて新旧画像の比較結果を示した,画像処理,https://qiita.com/jun_higuche/items/42156965311e54fb9693, ゜д゜ ﾉやぁーどもです 今回は 割と結構実用的なプログラムを作ってみましたよ 年くらい前に↓の記事で Android端末でアプリケーションをpython＋Appiumで自動実行させることをしましたが 今回 これを使って作った自動実行プログラムで 新アプリと旧アプリそれぞれで貯めたエビデンス画像をExcelファイルに新と旧で並べて貼っていって  新と旧で差異がないよね ってことの確認を支援するプログラムを作ってみました    背景私が担当したエンハンスプロジェクトで 現在のアプリ 以降 旧アプリ で使っていたAPIの全てを別のAPI そのとき開発中のもの に置き換えるという案件を実施しました 新APIでは 単純に置きなおしすればよいというものではなく そのつつの動作仕様が違うものやパラメーターも違うものもあり こちら側のプログラムロジックもだいぶ修正することになったのです そして 旧アプリと新アプリでまったく同じ動作を保証しなければなりませんでした まず この連結試験で 以前使ったpython＋Appiumで自動実行プログラムを開発して 旧アプリと新アプリで実行してみて そのエビデンス画像の比較をすればテスト工数がだいぶ省けると考えたのですが さらに そこで そのエビデンス画像を手作業でExcelファイルに貼って その結果を人の目で確認していくという作業に工数が掛かっているという所に目を付けたのです 人が判断する箇所も必要でしょうが 新と旧の画像で同一の名前のものを確認してExcelファイルに貼っていくというのは 人でなくてもできることです なので そこをpythonにさせようと思いました さらに 人の目で判断する上で 画像の差分を示してあげることでより確認作業が楽になるのではないかと思い 比較した類似度と 比較した差異画像を生成するということも同時に行ったのです 以下 pythonにさせた事と人がやることにした作業の切り分けです    今回作ったプログラムの概要 前作業 python＋Appiumで作った自動実行プログラムを新アプリと旧アプリで動作させ それぞれのエビデンス画像を別フォルダで貯め込んでおく  今回の範囲  pythonで 新アプリ 旧アプリの結果フォルダを参照して 同一名称の画像を並べてExcelファイルに貼りつける  pythonで その画像の類似度を算出して Excelファイルに記録 pythonで その画像の差異を示す画像を作成して Excelファイルに貼り付ける　　 ※この時点でExcelファイル完成  人 開発メンバー がExcelファイルを開いて 新と旧の差異を見て確認していく結果となるExcelファイルを開くと ↓このような感じで差異を示すことができます    cac fe fe a ecdee png     afad fd ea c bffeecfe png  上部にファイル名と算出した類似度を示すとともに 新と旧のエビデンス画像を左右に貼り付け 真ん中に差異を示す画像を貼り付けてます    技術解説    類似度の算出↓類似度の算出は   画像の画素値を比較し類似度を算出 の方で実装しました 理由は 今回は新アプリと旧アプリで同じ端末を利用するつもりなため 同じ動きであったならば画素的なズレは全く無いはずだからです もし 新アプリと旧アプリで別端末を利用するのであればヒストグラムでの算出の方が得たい結果に近くなるのではないかと思います     差異画像の生成↓の  画像比較して異なる箇所が分かる画像を生成するプログラム の  diff center png のやり方の方で 実装しました     Excelファイル作成pythonでExcelファイルを作成するに当たっては  openpyxl というライブラリを利用しました ググったら色々サンプル見つかります    プログラム↓Gitに格納しておきました ダウンロードして頂いて実行環境がWindowsの方は install library bat を実行することで 必要なライブラリがインストールされるはずです LinuxやMacの方は install library bat の中身を実行してみてください そのあと  main py を実行していただければ 動作するはずです  tmp と tmp に新アプリと旧アプリを想定したサンプル画像が入ってます    まとめ今回 新アプリと旧アプリの動作比較という割とありがちな案件で使えるプログラムを作ってみました 実は まだ実運用で利用していなくって この記事を書いている カ月後くらいに実行する予定なのです 実案件で利用後にまたレポート書いてみます この記事を読んでて 新アプリと旧アプリの比較という用件のある方は是非使ってみて頂きたいです ,1,2022-02-23
241,241,DOBOT×AI 分類結果が一定確率以下の場合、無視する,画像処理,https://qiita.com/shimamotosan/items/3e97fbf6ed3d7d8612da,以下のテキストでは AIを用いた画像認識により対象物を分類し ロボットアームにより仕分けを行っています   DOBOT Magician AIｘ画像認識ｘロボットアーム制御  分類した結果として 最も該当している それらしい 確率が高い ラベルが出力されるため ぎりぎり該当したようなものも学習したいずれかのラベルに分類されてしまいます そのような状態を回避したい場合 学習データをもっと集めて より良いモデルを学習させる方法もありますが 今回の記事では 該当している確率が低いものは無視するようにプログラムを修正したいと思います   修正前   Python classifer py              分類する   修正前は 似ている対象物が異なるラベルとして出力されることがあります 以下の画像では 左下のパーツは本来 beam  が正しいラベルとして学習していますが  beam x という別のパーツとして認識しています   修正後   Python classifer py              分類する                   if p         確率が 以上の場合 フラグを立てる分類結果として出力されているそれぞれのラベルである確率を順番に確認し  以上のものがなければ以降の処理をスキップするように修正しています 以下の画像のように に満たないものは判別結果として表示されません ,0,2022-02-22
242,242,DOBOT×AI ネジやボルトを認識して仕分ける,画像処理,https://qiita.com/shimamotosan/items/86560396dea14f699643,以下のテキストのソースコードは 判別対象を吸引カップで移動させるようになっています   DOBOT Magician AIｘ画像認識ｘロボットアーム制御  対象物が小さい あるいは細長く 吸引カップでは保持できないことがあります そのような場合 DOBOT magicianに付属する空気式グリッパーで掴む必要があります 以下の記事でグリッパーで掴めるように修正方法を示しました   DOBOT×AI 判別対象をグリッパーで掴む  また以下の記事で 細長いLEGOパーツを対象物とし パーツの向きを取得して 掴みやすいようにグリッパーを回転させるようにプログラムを修正しました   DOBOT×AI LEGOパーツを認識して仕分ける  今回の記事では上記にて修正したプログラムを使用し 対象物をより小さいネジやボルトに変えて 判別して仕分けることが可能かを試してみます 対象物が小さい場合 カメラの位置が高すぎると対象物が小さく映るため 判別が難しくなる可能性があります そのような場合 カメラの位置を調整してください ,0,2022-02-22
243,243,DOBOT×AI LEGOパーツを認識して仕分ける,画像処理,https://qiita.com/shimamotosan/items/98de5729ee260eb8a678,以下のテキストのソースコードは 判別対象を吸引カップで移動させるようになっています   DOBOT Magician AIｘ画像認識ｘロボットアーム制御  対象物が小さい あるいは細長く 吸引カップでは保持できないことがあります そのような場合 DOBOT magicianに付属する空気式グリッパーで掴む必要があります 以下の記事でグリッパーで掴めるように修正方法を示しましたが 対象物の向きによっては掴むことができません   DOBOT×AI 判別対象をグリッパーで掴む  本記事では 細長いLEGOパーツを対象物とし どのLEGOパーツかを認識することに加えて パーツの向きを取得して 掴みやすいようにグリッパーを回転させるようにプログラムを修正したいと思います   修正前              中心点の座標をカメラ座標系からロボット座標系へ変換             最後に取得した矩形とその結果を元にDOBOTでピックアップする    指定された座標のオブジェクトを取り ラベルごとに仕分ける      オブジェクトの真上に移動      オブジェクトを取れる位置まで移動し オブジェクトを取る      オブジェクトの真上に移動  修正後上記の箇所を以下のように追記 修正します               中心点の座標をカメラ座標系からロボット座標系へ変換                 輪郭データを浮動小数点型の配列に格納              開始点              終了点              角度の基準 度 を描画              主成分ベクトルを描画          最後に取得した矩形とその結果を元にDOBOTでピックアップする    指定された座標のオブジェクトを取り ラベルごとに仕分ける      オブジェクトの真上に移動      オブジェクトを取れる位置まで移動し オブジェクトを取る      オブジェクトの真上に移動実行すると認識したオブジェクトの右側にオブジェクトの角度が表示されます 取得したオブジェクトの角度を使用してグリッパーの向き R座標 を回転させて オブジェクトを掴みます ,0,2022-02-22
244,244,cufftComplexに画素値を代入してみる,画像処理,https://qiita.com/toroi-ex/items/66d3985c844c71f9cd79,  概要  関連記事  コード      画像をグレースケールで読み込み 作業がしやすいので        入力       画素値代入用変数       画素値代入       すべてのcopyが終わるまで待機    解説コードの方を少し解説しますとGPUに入力する画像の大きさ分のメモリを cudaMalloc で確保    cpp   入力   画素値代入用変数その後画素値を代入    cpp   画素値代入これで画像を使ってcufftの入力に加えたりすることができます ,0,2022-02-21
245,245,pythonで２つの画像を比較してその差異を分かりやすく示すための画像を新たに生成するプログラムを作ってみた,画像処理,https://qiita.com/jun_higuche/items/e3c3263ba50ea296f7bf,pythonでつの画像を比較してその差異を分かりやすく示すための画像を新たに生成するプログラムを作ってみました ちなみに ディレクトリ名を日本語にしたかったので 日本語のパスにも対応するように作ってます ソースコードはこちら↓ の  差分画像の生成 のプログラムです    背景この記事の前に書いた記事とほぼ同じ背景です 付け足すと 差分を分かりやすく示す画像を生成しておいた方がより確認がし易いと思い 作ってみることにしました     画像比較して異なる箇所を赤枠で囲んだ画像を生成するプログラムこちらでは つの画像を比較させて その差異の箇所に赤枠を付けた画像を生成するというプログラムになります 画像比較して異なる箇所を赤枠で囲む 画像読み込み 画像を引き算 値化 輪郭を検出 閾値以上の差分を四角で囲う 画像を生成    画像比較して異なる箇所が分かる画像を生成するプログラムそして こちらは つの画像を比較させてその差異のある箇所 ピクセル的な を色を変えて示すというプログラムになります 画像比較して異なる箇所を別画像で表示↓を参考に  画像をリサイズする  画像の差異を計算  単純に差異をそのまま出力する  差異が無い箇所を中心 灰色： とし そこからの差異を示す  差異が無い箇所を中心 灰色： とし 差異をで割った商にする 差異を  にしておきたいため    それぞれのやり方で差異を示す画像を生成してその特徴を比較してみた Android端末で 適当なスクリーンショットを撮ってサンプル画像として使ってみました     サンプルの画像：    比較結果      画像比較して異なる箇所を赤枠で囲んだ画像を生成するプログラムちょっとわかりづらいかもですが ↓の画像上方に差異の箇所を赤枠で示すことができました このサンプルの場合は 差異の箇所が文字の部分だけですので これでも十分わかりますよね       画像比較して異なる箇所が分かる画像を生成するプログラムちょっと解説ですが   diff jpg は ただただ差異を描画しようとしてみたのですが どうもマイナス値については 全て 黒 になってしまうようで 差異があるかどうかが分かりませんでした なので   diff center jpg のように 中間値を  灰色  として そこからの差異として示してみました ただ ここでも 差異が  となるために 差異がマイナス値となってしまい 黒潰れてしまうことがあります なので 次に   diff center jpg のように差異の最小値を とするために で割った商を予め計算してみました これで 割と差異が見易い画像になったかと思います     まとめ今回 差異を示すために 赤枠を付けた画像の生成と 差異のある箇所を色付けして示すという事をやってみました 差異の箇所が限定的な場合には 赤枠を付けた画像 で良さそうですが 多くの場所に差異が出てくる場合にはつ目の 差異のある箇所を色付けした画像 の方が良さそうです 今回は以上です 次は 前回ご紹介した類似度の算出と 今回の差異画像の生成のつを利用して 新と旧の両方で動かしたアプリのキャプチャ画像を比較させて まとめてExcelファイルに貼り付け 結合テストの結果エビデンスとしてしまうプログラムを紹介したいと思います ,6,2022-02-20
246,246,透過pngを手軽に作成できる無料オンラインツール,画像処理,https://qiita.com/hailiziyema/items/7d4d13ad31b86ef64fe1, 透過pngを手軽に作成できる無料オンラインツール皆さんこんにちは 透過pngを作成したい人がたくさんいるでしょう この記事は透過pngを手軽に作成できる無料オンラインツールと使い方についてご紹介します 役に立てれば幸いです   事前準備編集したい画像PicWishの公式サイト　 無料 オンライン このソフトは完全無料ですので ご興味のある方はお試しください   使う手順  PicWishの公式サイト  を開きます  編集したい写真を選択し アップロードします   image png   数秒待つと 自動的に画像処理が行われます    image png    ダウンロード をクリックすれば 透過pngの画像をダウンロードできます ,0,2022-02-18
247,247,CSSやJSで画像リサイズ,画像処理,https://qiita.com/MIYAKEZAKA/items/11c93bceb18cd8e3ff50,フロント側で画像リサイズ方法を紹介します 取得した画像が大きすぎるため小さくして表示したい 送信したい場面に役立つかと思います    画像をリサイズして表示する表示するだけであれば わざわざ容量を落とす必要もないためCSSで調整可能です 新たなCSSプロパティであるobject fitでのリサイズがオススメです     デモ  Untitled   on object fit  cover でアスペクト比を維持したまま横幅と高さを自動で調節してくれます 値によってリサイズの種類を設定できます MDN object fit    IE対応IEに対応する場合はポリフィルを読み込む必要があります    画像本体をリサイズして送信する送信する際は 見栄えだけではなく本体の容量を落とすケースが考えられます Canvasを使用してbase形式でリサイズが可能です     デモディベロッパーツールで画像を確認すると画像がリサイズされているのがわかるかと思います ※デモの画像だと元画像の大きさと大差がないので baseのため容量は上がっています      最大幅 最大高さを設定     Imageのインスタンス生成     img srcを読み込まれてから安全に実行       読み込み画像のアスペクト比調整       コンテキスト canvasの描画に必要なもの Dグラフィックを描画するためのメソッド を取得       canvasに計算したアスペクト比で画像を描画する       挿入用の画像を生成 〜 出力送信する際はFileAPIで読み込むケースが多いかと思いますが また別の機会に載せたいと思います 最後までご覧戴き ありがとうございました ,2,2022-02-15
248,248,Halide小ネタ（compute_with編）,画像処理,https://qiita.com/HrysTkg/items/143aac4429da124a32d5,  はじめに前置きは ここ  ．Halideのチュートリアルを終えたばかりの方は  compute なんとか というスケジュールメソッドについて compute inline compute root compute atはご存じかと思います 一応チュートリアル終わった人向けに書いているつもりなので これらの詳細はここでは省略 ．しかし computeシリーズはもう一つ存在しており それが本記事のメインである  compute with  です．compute withは有能な機能ですが チュートリアルでは触れられていないためか あまり知られていない機能 な気がする ので 布教の手助けになればと本記事を書きました．compute withは あるFuncに対して異なるFuncとループを共有させるスケジュールです．詳しくは  公式ドキュメント  にて説明されていますので そちらを参考にしてください．下に そのドキュメントからコードを交えた解説を引用します．このように ループ構造が一部または全て一致しているつのFuncについて 引数を用いてどのループレベルを共有するかを指定したうえで ループの冗長性を減らします．上の解説では 超単純な計算パイプラインでのデモでしたが 実際に扱うHalideコードの計算パイプラインをもっと複雑なはずです   よね おそらくHalidユーザの方が息を吐くように使用している vectorizeやunroll parallelといったスケジュールメソッドは compute withと共存させるのに 少し工夫が求められます． それらの対処法についての備忘録としても書いてます チュートリアルで登場する compute なんとか は純粋定義でしか使用できませんでしたが compute withは純粋 更新二つの定義に対して使用できます．そのため 畳み込みなどで使用するRDomが生成するループもcompute withはまとめることができます．下のコード例では 同じループ構造を持つ関数fと関数gに対して 純粋 更新定義のループをまとめます． コードについて 処理内容に特に意味はありません     実行結果このように fとgで更新定義を含め ループを共有していることが分かります．また 上の例ではfとgは完全に同じループ構造を持つため gについてcompute with f x とすれば ループ全てを共有させることも可能です．また compute withの使用制限として fとgのように  完全に独立   互いに干渉していない している必要があります．なのでfとgを消費するhとはcompute withでループをまとめることはできません これはcompute atの仕事 ．．．．一方 おそらく先ほどのコードを見たHalide使いの方は  vectorizeとかもしたくね     と思われたはず．しかし先述の通り 通常のやり方ではvectorizeとcompute withは共存が難しいです．プログラムを高速化する定番の手法であるSIMD命令によるベクトル化 これをHalide上で行うにはvectorizeメソッドを用います．このvectorizeメソッドは ちょっと厄介な挙動をするため compute withを共存させるのは少しテクニックがいります．例えば 次のように先ほどのコードにおいてスケジュール部分を変更させて xを幅でベクトル化さる場合  なにも考えなければ 次のようになります．    実行結果エラーが返ってきました．違う変数どうしてcompute withできないと怒られました．違うとされているx vとx vですが これらはアルゴリズム記述では登場しておらず スケジュールメソッドで自動的に生成されたものだと推測できます．この謎の変数x vはvectorizeメソッド 特にベクトルサイズとしてfactorを引数に持たせたときに出現するものです．vectorizeは本来 引数に与えられた変数のループをベクトル化するもので factorが設定された場合はsplitで変数を分離した上でvectorizeを設定します．これをコードで示すとしたのようになってます．   C     vectorize with factorf x  y    hoge f vectorize x       above equal belowf x  y    hoge Var xv  x v   f split x  x  xv    vectorize xv  このように factor付きvectorizeはsplitを内包しており 上のコードで使用したxvがvectorize内部で自動的に生成され 変数名に固有名が割り当てられます．Halideコンパイラでは 変数名が異なる場合異なるループと認識するらしく これが原因でcompute withがはじかれます．ベクトル化をしつつcompute withも適用させるために 今まで空気な存在だったhが活躍します．上手く動作するコードを下に示します．    実行結果この方法では hに対してxをベクトル化し fとgをhのベクトル化したxにcompute atします．すると fとgが持つループがRDomによるループだけに限定されるため fとgでcopute withが通ります．一方 この方法では hのベクトル化した変数を得るところが 一般的なHalideの記述ではできないため 少しトリッキーな形になります． 別記事  で少し紹介した通り  Halide  Func の本体は Halide  Internal  Function が担っており スケジュールにおけるループ の次元 は Function  definition   schedule   dims   で取得できます．この戻り値は vector で Funcのスケジュールにおける次元を内側から格納されています．したがって この番目 つまり最内側 を取得することで ベクトル化したxの情報を引っ張ってくることができます．このように fとgを消費するhにcompute atを行うことで 解決できますが これは常に有効に行えるとは限りません．また ベクトル化した変数の取得がめんどくさいですが これは後述する別の解決策と組み合わせれば何とかなります．   解決策     vectorizeとsplitを分離vectorizeがsplitを内包し 隠蔽するため 名前が異なるVarが生まれるので vectorizeに含まれるsplitもユーザ側で行えば解決できます．    実行結果このように ベクトル化を含んだ状態で compute withを適用することができました． ただし hへのcompute atもしたほうがキャッシュ効率的にはいいかも     vectorizeのように ループ構造を変形して最適化をかけるスケジュールメソッドとして unrollやparallelがありますが これらもvectorizeと同じく factorを設定すると内部的にsplitも発行されます．そのため これらとcompute withの共存は同じ問題が発生しますが 同じように解決できます．先ほどまでの例において fとgで共通して消費される関数があった場合 その関数を事前計算して再利用した方が効率が良い場合があります．下にコード例を示します．    実行結果このように fおよびgで消費されるeをcompute atさせ fとgでの再計算を防ぎます．また スケジュールでは   eはfに対してcompute atする  ことが必要です．gに対してcompute atした場合は 下のようなエラーがでます．このように fにcompute atすることが求められます．  おわりにcompute withの説明と 記述方法や応用を紹介しました．compute withの弱点   として 今回のコード例におけるhのような fとgを両方消費する関数が必要になります．つまり fとgを別々にrealizeしたい場合などでは compute withできないようになっています  ここ  で議論されてました ．これが解消されると いろいろ応用範囲が広がる気もしますが．．．あと vectorizeがsplitを内包する点はチュートリアルにも書いてありましたね．．． さっき見て気がついた ,0,2022-02-14
249,249,# 画像処理 エッジ&コーナー検出のための空間フィルタリング(2),画像処理,https://qiita.com/kotabook/items/35832a7d01c1bd21ca79,本記事では 前回投稿した記事の続きでノンローカルミーンフィルタについて記していきます 前回記事は こちら  です     ノンローカルミーンフィルタとはノンローカルミーンフィルタの考え方としては 注目する画素の周辺と 他の画素の周辺の類似度を調べ 類似度が高ければ重み付けを大きくし 逆に低ければ重み付けを小さくするという考え方になっています このフィルタリングを用いることで 残すべき輪郭を残しながらノイズの平滑化を行えるようになっています   nlmf png  この写真は  こちら  の OpenCV のホームページから引用させていただきました そのため カーネルの大きさ 上記画像の大きい枠 ×  と 比較する周辺範囲の大きさ 上記画像の小さい枠 ×  のつを決めてフィルタリングを行いますので このつの大きさをあらかじめ定める必要があります     ノンローカルミーンフィルタの理論注目する画素 p に対する周辺画素 q の重みを w p  q  とするとき 重み w p  q  は以下の式で表されます  Z p  は規格化するためにカーネルの総和で割るために計算をしています また  v p  と v q  はそれぞれ注目している画素とその周辺の画素の画素値の情報です その差分についてノルム計算を行うことによって類似度を計算します ノルム計算結果が小さいほど類似度が高いと判断できます また パラメータとして \sigma と h のつを調整する必要があります  \sigma はノイズの分散の値になるように調整すると良いとされております また  h は画素比較の敏感さを表しております 値が大きくなるほど違いに対して鈍感 値の違いが大きくても似ていると判断 になり 逆に小さくなるほど敏感 値の違いが少しでも異なると違うものとして判断 するようになります このつのパラメータは 画像ごとに適宜調整していく必要があります   カーネル関数      配列を一次元化      ノルム計算      より大きいかどうかを比較  カーネルを作成する関数  ノンローカルミーンフィルタ      フィルタをかける画像の高さと幅を取得    height  width   img shape      カーネル作成に必要な範囲を計算      それぞれの幅を計算      出力画像用の配列      フィルタリングの計算              カーネルを作成              カーネルを用いて計算少しごちゃごちゃしていますが 自分なりに数式を再現した結果がこのソースコードになりました これらの関数を用いてフィルタリングを行います    python  入力画像を読み込み実行結果は以下のようになりました バイラテラルフィルタの時と同様 輪郭をしっかり残した状態でフィルタリングができていることがわかります     OpenCV   のライブラリより精度が劣った結果となってしまっていますが    計算量が他のフィルタに比べ増加しているため 実行にはかなり時間を要してしまう問題点があります また    OpenCV   のノンローカルミーンフィルタでは  \sigma と h のパラメータをノイズ推論を用いて最適になるように調整する設計となっているようです このソースコードは推論する機能はないので最適解を実験を通して調べていきましたが 調整がかなりシビアになってしまう問題があります     まとめ本記事では 前回のガウシアンフィルタとバイラテラルフィルタに続き ノンローカルミーンフィルタについて理論について勉強し そしてPythonで自力実装をしてみました これはあくまで機械学習でいう前処理的な立ち位置ですので 次回以降では このフィルタを応用したエッジ検出などについて書いていこうと思います 以上となります ,1,2022-02-13
250,250,ゴッドファーザー風の画像の作り方。,画像処理,https://qiita.com/yoshi-hanzo/items/502c9e43854bf6ea53bc,この記事は リンク情報システム  の TechConnect 年月 のリレー記事です engineer hanzomon のグループメンバによってリレーされます  リンク情報システムの Facebook    Twitter  もヨロシクです  TechConnect 年月のインデックスは こちら    はじめに年月日に全米公開されるや当時の興行記録を塗り替える大ヒットを記録し アカデミー賞部門を制した不朽の名作 ゴッドファーザー  記念すべき公開周年を迎え  特設ウェブサイト  にてシリーズ三部作の特別上映やリマスターブルーレイセット発売が発表されるなど 映画ファンの間で大いに盛り上がっていますね しかも主人公マイケルの声を吹き替えているのは かの名優 野沢那智 あなたも映画史上最高傑作の真の完結編を目撃せよ    白黒画像への変換がうまくいかない俺たちの……俺たちのドン コルレオーネが帰ってくるっ 　ヒューッ 　ヒューッ  数十年前にTV放映されて以来 商品化されず幻と化していた野沢那智バージョンの吹替収録を知って感動に打ち震えていた私 テンション上がりまくりな中でゴッドファーザーの映画ポスター風の壁紙を作ることを思い立ち  よーし パパ 記念壁紙つくっちゃうぞぉ と呟きつつ作業に取りかかったのですが そこで思わぬ結果を見ることに   o  png  　    png   左：期待していたイメージ 右：実際に出力された画像のイメージ モノクロビットマップで保存すると 何度やっても画像が黒つぶれしてしまう……ロールシャッハテストかな おぼろげながら浮かんできたんです……暗がりに佇む男のイメージが……シルエットが浮かんできたんです    ディザリングを試してみるペイントでは高度な編集ができません ここはC で変換プログラムを組んでみましょう 原因について調べていくうちに  値化して bppの白黒画像を作成する   という記事にたどり着きました   極端に言えば 画像によっては真っ白 あるいは真っ黒になってしまう恐れがあります よって 画像によってしきい値を変更した方が良い結果が得られます なるほど ピクセルの色の明るさが設定したしきい値を越えれば白くし 越えなければ黒くするという方法 固定閾値法 だと 黒い箇所と白い箇所が両極端になってしまい その中間が表現されないということのようです これを解決する方法として紹介されている ディザリング Dithering を試してみましょう  オーダー法  オーダード ディザリング   で変換した結果  誤差拡散法  フロイド スタインバーグ ディザリング   で変換した結果 前者は格子状のパターンが表れているのが特徴です 漫画のトーンっぽい  後者は点描状のパターンが表れているのが特徴です 新聞のドットの荒い写真っぽい  モノクロビットマップとは思えないほどの見事な仕上がりですが ちょっと期待したイメージとは違うかな……   モノトーン化の後に変換してみる別の方法を探ってみます  ImageAttributes SetThresholdメソッドを使ってしきい値を設定する方法も考えられます この方法で画像を値化するには まず 画像をグレースケールに変換して表示する のように画像をグレースケールにしてから SetThresholdメソッドでしきい値をセットしたImageAttributesオブジェクトを使って画像を描画します先にグレースケール 白から黒までの明暗 に画像を減色してから値変換するとのこと   画像をグレースケールに変換して表示する   の記事を基に実装してみます  ほうほう ColorMatrixクラスを使うっと  ──えっ 行列部分の値を変えるだけでセピア調 半透明化 ネガポジ反転 ガンマ補正 コントラスト変更もできるのかっ  オオッ  す すごいぃぃぃ と寄り道しまくった結果がこちら  左：オリジナル画像 中央：グレイスケール変換結果 右：セピア調変換結果 NTSC加重平均法によるグレースケール画像の見事な出来に 期待が高まります 成功間違いなしと普通は確信するよね……ところがどっこい 準備した壁紙を変換してみると 出力された結果はまたもや黒画像 なんでや   判別分析法で算出した閾値でモノクロ化してみる先に結論を書いてしまうと 値化するのに結局は閾値が必要で 適切な閾値は画像ごとに異なるから 事前に減色しても効果があるとは限らないってことでした そりゃそうだ   むぅぅと唸りながら次の手を考えます  適切なしきい値を計算する方法としては 判別分析法 大津の二値化 などがあります 興味のある方は 調べてみてください  判別分析法   analysis method は大津の二値化とも言われ 分離度という値が最大となるしきい値を求め 自動的に二値化を行う手法とのこと OpenCVにも組み込まれているメジャーなアルゴリズムのようです 試行錯誤の末 huskworksさんの記事   C  判別分析法を用いて画像値化の閾値を算出してみる   の実装を組み込んだところ ついに期待したイメージに近い変換結果を得ることに成功しました   まとめ今回のチャレンジにより 以下の知識を得ました 　  モノクロ画像への変換アルゴリズムはいっぱいあるよ 　  ただし 値化するための適切な閾値は画像ごとに異なるよ 　  中間色を表現するためにディザリングという手法もあるよ ※変換ツールのソースコードを置いておくので 検証したい方はどうぞ  サンプルコードの寄せ集めですが……先人の皆様に感謝  概要  　　      開発環境  Visual Studio     開発言語  C   WindowsForm    対応画像  BMP JPG PNG    使用方法 ウィンドウズFormで新規プロジェクトを作成し Form Load関数を作ったらソースコードをコピーする     C  Form csusing System using System Collections Generic using System ComponentModel using System Data using System Drawing using System Drawing Imaging using System Linq using System Text using System Threading Tasks using System Windows Forms namespace WindowsFormsApp    public partial class Form   Form        MenuStrip menuStrip         PictureBox pictureBox         Bitmap imageDefault   null         Button   buttonConvert         Button buttonUndo         int pictureBoxMaxWidth            int pictureBoxMaxHeight            public Form              InitializeComponent               フォームのLoadイベントハンドラ              コントロールを作成する            this menuStrip   new MenuStrip               this pictureBox   new PictureBox                 レイアウトロジックを停止する            this SuspendLayout               this menuStrip SuspendLayout                  メニュー生成               ファイル  F  メニュー項目を作成する            ToolStripMenuItem fileMenuItem   new ToolStripMenuItem               fileMenuItem Text    ファイル  F                 MenuStripに追加する            this menuStrip Items Add fileMenuItem                 開く  O     メニュー項目を作成する            ToolStripMenuItem openMenuItem   new ToolStripMenuItem               openMenuItem Text    開く  O                    ショートカットキー Ctrl O を設定する            openMenuItem ShortcutKeys   Keys Control   Keys O             openMenuItem ShowShortcutKeys   true               Clickイベントハンドラを追加する               ファイル  F  のドロップダウンメニューに追加する            fileMenuItem DropDownItems Add openMenuItem                 名前を付けて保存  A     メニュー項目を作成する            ToolStripMenuItem saveAsMenuItem   new ToolStripMenuItem               saveAsMenuItem Text    名前を付けて保存  A                    ショートカットキー Ctrl A を設定する            saveAsMenuItem ShortcutKeys   Keys Control   Keys A             saveAsMenuItem ShowShortcutKeys   true               Clickイベントハンドラを追加する               ファイル  F  のドロップダウンメニューに追加する            fileMenuItem DropDownItems Add saveAsMenuItem                セパレータを追加する            fileMenuItem DropDownItems Add new ToolStripSeparator                   終了  X  メニュー項目を作成する            ToolStripMenuItem exitMenuItem   new ToolStripMenuItem               exitMenuItem Text    終了  X                 ショートカットキー Ctrl X を設定する            exitMenuItem ShortcutKeys   Keys Control   Keys X             exitMenuItem ShowShortcutKeys   true               Clickイベントハンドラを追加する               ファイル  F  のドロップダウンメニューに追加する            fileMenuItem DropDownItems Add exitMenuItem                フォームにMenuStripを追加する            this Controls Add this menuStrip                フォームのメインメニューとする            this MainMenuStrip   this menuStrip                ピクチャーボックス生成            this pictureBox Width   this pictureBoxMaxWidth             this pictureBox Height   this pictureBoxMaxHeight             this pictureBox Location   new Point                 this MainMenuStrip Location X                  this MainMenuStrip Location Y   this MainMenuStrip Height              this Controls Add this pictureBox                 ボタン生成              ボタンコントロール配列の作成            this buttonConvert   new Button               string   buttonLabels   new string                     固定閾値法    判別分析法                    オーダー法    誤差拡散法                    加重平均法    セピア調                      サイズと位置を設定                  コントロールをフォームに追加              戻すボタンの作成              レイアウトロジックを再開する            this menuStrip ResumeLayout false              this menuStrip PerformLayout               this ResumeLayout false              this PerformLayout                開く  O     メニュー項目のClickイベントハンドラ                      画像ファイルの読み込み                    Image isrc   CreateImage filePath                        縮小サイズの計算 最大サイズに合わせて縮小                       リサイズ画像の作成                    Bitmap bmpResize   new Bitmap isrc   int  isrc Width   scale    int  isrc Height   scale                         リサイズ画像表示                      変換前画像をバックアップ            指定したファイルをロックせずに System Drawing Imageを作成する             終了  X     メニュー項目のClickイベントハンドラ            名前を付けて保存  A     メニュー項目のClickイベントハンドラ            戻す  U  ボタンのClickイベントハンドラ            画像変換ボタンのClickイベントハンドラ            値化画像に変換 固定閾値法             値化画像に変換 判別分析法         Bitmap ToBinaryByOtsu Bitmap bmpBase             float th   GetThreshold bmpBase                 指定の閾値を使用して値化を行う            値化画像に変換 オーダー法         Bitmap ToBinaryByOrdered Bitmap bmpBase                配列ディザリングを使用して値化を行う            値化画像に変換 差分法         Bitmap ToBinaryByDiff Bitmap bmpBase                誤差拡散法を使用して値化を行う            値化画像に変換 加重平均法             値化画像に変換 セピア調             判別分析法により閾値を求める        int GetThreshold Bitmap img             BitmapData imgData   null             try                byte   buf   null                    変換する画像のピクセルあたりのバイト数を取得                PixelFormat pixelFormat   img PixelFormat                 int pixelSize   Image GetPixelFormatSize pixelFormat                        変換する画像データをアンマネージ配列にコピー                   ヒストグラム算出                           ピクセルで考えた場合の開始位置を計算する                           ピクセルの輝度を算出                   全体の輝度の平均値                double ave   sum   cnt                    閾値算出                       クラスとクラスのピクセル数とピクセル値の合計値を算出                       クラスとクラスのピクセル値の平均を計算                    double ave    sum          sum   n                      double ave    sum          sum   n                         クラス間分散の分子を計算                       クラス間分散の分子が最大のとき クラス間分散の分子と閾値を記録            指定された画像からbppのイメージを作成する        Bitmap CreatebppImage Bitmap img  float th               bppイメージを作成する            Bitmap newImg   new Bitmap img Width  img Height                 PixelFormat FormatbppIndexed                Bitmapをロックする            BitmapData bmpDate   newImg LockBits                 new Rectangle     newImg Width  newImg Height                  ImageLockMode WriteOnly  newImg PixelFormat                新しい画像のピクセルデータを作成する                      明るさが閾値以上の時は白くする                          ピクセルデータの位置                          白くする              作成したピクセルデータをコピーする            IntPtr ptr   bmpDate Scan             System Runtime InteropServices Marshal Copy pixels    ptr  pixels Length                ロックを解除する            指定された画像からbppのイメージを作成する        Bitmap CreatebppImageWithOrderedDithering Bitmap img               しきい値マップを作成する            float     thresholdMap   new float                    new float    f f  f f  f f  f f                  new float    f f  f f  f f  f f                  new float    f f  f f  f f  f f                  new float    f f  f f  f f  f f               bppイメージを作成する            Bitmap newImg   new Bitmap img Width  img Height                 PixelFormat FormatbppIndexed                Bitmapをロックする            BitmapData bmpDate   newImg LockBits                 new Rectangle     newImg Width  newImg Height                  ImageLockMode WriteOnly  newImg PixelFormat                新しい画像のピクセルデータを作成する                      しきい値マップの値と比較する                          ピクセルデータの位置                          白くする              作成したピクセルデータをコピーする            IntPtr ptr   bmpDate Scan             System Runtime InteropServices Marshal Copy pixels    ptr  pixels Length                ロックを解除する            指定された画像からbppのイメージを作成する        Bitmap CreatebppImageWithErrorDiffusion Bitmap img  float th               bppイメージを作成する            Bitmap newImg   new Bitmap img Width  img Height                 PixelFormat FormatbppIndexed                Bitmapをロックする            BitmapData bmpDate   newImg LockBits                 new Rectangle     newImg Width  newImg Height                  ImageLockMode WriteOnly  newImg PixelFormat                現在の行と次の行の誤差を記憶する配列            float     errors   new float                      new float bmpDate Width                     new float bmpDate Width                  新しい画像のピクセルデータを作成する                      ピクセルの明るさに 誤差を加える                      明るさが閾値以上の時は白くする                          ピクセルデータの位置                          白くする                          誤差を計算 黒くした時の誤差はerr なので そのまま                         err    f                       誤差を振り分ける                  誤差を記憶した配列を入れ替える                errors     errors                   errors     new float errors   Length                作成したピクセルデータをコピーする            IntPtr ptr   bmpDate Scan             System Runtime InteropServices Marshal Copy pixels    ptr  pixels Length                ロックを解除する            NTSC加重平均法を使用して             指定した画像からグレースケール画像を作成する        Bitmap CreateGrayscaleImage Bitmap img               グレースケールの描画先となるImageオブジェクトを作成            Bitmap newImg   new Bitmap img Width  img Height                newImgのGraphicsオブジェクトを取得            Graphics gr   Graphics FromImage newImg                RGBの比率 YIQカラーモデル               各Pixel値 RGB の輝度 明るさ を計算して 階調のグレー色に置き換える            System Drawing Imaging ColorMatrix cm                  new System Drawing Imaging ColorMatrix                     new float                             new float   r  r  r                              new float   g  g  g                              new float   b  b  b                              new float                                     new float                          ImageAttributesオブジェクトの作成            System Drawing Imaging ImageAttributes ia                  new System Drawing Imaging ImageAttributes                 ColorMatrixを設定する            ia SetColorMatrix cm                ImageAttributesを使用してグレースケールを描画            gr DrawImage img                 new Rectangle     img Width  img Height                      img Width  img Height  GraphicsUnit Pixel  ia                リソースを解放する            指定した画像からセピア調の画像を作成する        Bitmap CreateSepiaImage Bitmap img               グレースケールの描画先となるImageオブジェクトを作成            Bitmap newImg   new Bitmap img Width  img Height                newImgのGraphicsオブジェクトを取得            Graphics gr   Graphics FromImage newImg                ColorMatrixオブジェクトの作成              輝度計算後 階調のセピア色に置き換えるための行列を指定            System Drawing Imaging ColorMatrix cm                  new System Drawing Imaging ColorMatrix                     new float                             new float    F   F   F                              new float    F   F   F                              new float    F   F   F                              new float                                     new float                          ImageAttributesオブジェクトの作成            System Drawing Imaging ImageAttributes ia                  new System Drawing Imaging ImageAttributes                 ColorMatrixを設定する            ia SetColorMatrix cm                ImageAttributesを使用してグレースケールを描画            gr DrawImage img                 new Rectangle     img Width  img Height                      img Width  img Height  GraphicsUnit Pixel  ia                リソースを解放する   参考文献   DOBON NET          値化して bppの白黒画像を作成する    ──様々なディザリング手法の紹介        画像をグレースケールに変換して表示する   ──グレースケール変換 カラーバランス調整   HUSKING   kotteri           C  判別分析法を用いて画像値化の閾値を算出してみる   ──大津の二値化の実装        レナ 画像データ    ──画像処理の分野で広く使用されている標準的なテスト画像      経済産業省が立ち上げた日本の美しい風景写真の公開サイト  クリエイティブ コモンズ CC  ライセンスで提供         兵庫県 日和山海岸 日の出    ──ディザリングの項で出てきた風景画像の出典   パブリックドメインQ：著作権フリー画像素材集           ダンディーな外国人男性が持つ拳銃   他──男性画像の出典,15,2022-02-13
251,251,Detectron2ではじめる画像の物体検出とセグメンテーション,画像処理,https://qiita.com/siganai_poteto/items/ecaf7feef0336a988875,   はじめに最近  Detectronを用いて画像の物体検出とセグメンテーションを行ったのですが  日本語の記事が少なく実装に苦労した部分があったため  今回は物体検出とセグメンテーションに関して基本的な操作をまとめておきたいと思います    note info今回は実装に焦点を当てるため画像処理やニューラルネットワークの専門的な話は盛り込んでいません    Detectronとは Detectronとは FacebookAIが開発したPyTorchベースの物体検出ライブラリで 物体検出 セグメンテーション その他の視覚認識タスクのためのプラットフォームです  現在はオープンソースとして公開されています  詳細は こちら   で確認できます    記事概要 Google Colaboratoryの立ち上げ Detectronの設定 画像の物体検出とセグメントの実装   必要事項 Googleアカウント Pythonの基礎知識 PyTorch Numpy CVの基礎知識 対象の画像ファイル   GoogleColaboratoryの立ち上げ     GoogleColaboratory  以下Colab  とはGoogleが提供しているGoogleドライブ上でのpythonの実行環境  JupyterNotebook仕様  で 時間制限はあるものの高性能なTeslaのGPUを無料で使うことができます  Colabの詳細は こちら  で確認できます    上記セル実行後表示されるURLにアクセス  アカウントを選択しドライブの接続を許可  ドライブが実行環境にマウント以上の手順を踏むことでドライブ上のファイルやフォルダーをColab上で操作できるようになります    note warn以降のコードはランタイムにGPUを使用します   Detectronの設定     Detectronのインストール コードはDetectronの チュートリアル  を参照しますDetectronのモジュールをインストール  訓練済みモデル選択モジュールModelZoo 続いて画像を扱うモジュールをインストール  colab上ではcvオブジェクトの表示に特にcv imshowを用いる     学習器の作成 model   学習モデルの読み込み モデルはCOCOフォーマット   学習器を作成 model predictor   DefaultPredictor cfg 今回の目的は物体検出とセグメントなので  モデルは訓練時間が比較的早く  精度も割に高めのmask rcnnのR FPNを用いる設定になっています   その他のモデルについては こちら  で確認できます    画像の物体検出とセグメントの実装それでは実装していきます  今回使用する画像はこちらです  宇宙猫withブロッコリー jpg  この画像からcvオブジェクトを作成し  学習器を通してみましょう   学習器で推定するoutput   predictor img outputについて出力してみます      出力結果の詳細を確認するここから  outputの中身を覗いていきます  まず  outputの構造は辞書形式でその中にInstanceオブジェクトが入っています    Instanceオブジェクトの詳細は こちら   そして  Instanceオブジェクトの中にはnum instance  image height  image width  fieldsが確認できます  また  fieldsにおいて次の要素が確認できます   名前   構造   内容       推定された物体名を確認この要素をもとに  推定された物体名を確認してみましょう 物体名はリストで保存されています  これらはdetectronのdataにおけるMetadataCatalogで取得することができます   このclasses listのindexと物体のカテゴリーIDが対応しています  推定した物体名のリスト     物体名   類似度       broccoli           cat           broccoli           broccoli           broccoli           bird           bird           broccoli           セグメンテーションを行う先ほど予測した物体が画像ではどこになるのか可視化してみましょう   boundign boxの取得    物体の 左上座標  左下座標  右上座標  右下座標   上書き用に画像をコピー  検出した物体数反復する    左上座標    右下座標    範囲を枠線で囲む出力ここで  物体名の際に作成した表と  位置を比べてみると が鳥と誤認されていますね  予測した物体名が必ずしも正しいとは言えないでしょう      再掲       物体名   類似度       broccoli           cat           broccoli           broccoli           broccoli           bird           bird           broccoli      より特定の物体を正しく推定するためにファインチューニングを行うといった方法がありますが  今回は触れません  興味のある方は こちら  の記事を参考にするとよいかもしれません   ビットマップの抽出      論理値の反転      cvオブジェクトはnumpy配列なので 正 true を灰色に変換出力  左上から右に出力順  出力結果  neko jpg  ここまで出来たらいろいろな画像処理ができそうですね    さいごに今回は画像の物体検出とセグメンテ―ションをDetectronを用いて実装していきました  画像処理やニューラルネットワークの専門的な知識がなくても手軽に実装でき  画像処理の技術は誰でも使える時代になりつつあるなと感じました  技術者でなくとも画像処理の活用を積極的に検討していきたいですね  ,17,2022-02-13
252,252,# 画像処理 エッジ&コーナー検出のための空間フィルタリング(1),画像処理,https://qiita.com/kotabook/items/30173d8d40d5e5734dfd,記事名に  をつけたのは まだ実装できていないノンローカルミーンフィルタを後ほど実装したいと考えているためです  追記    　ノンローカルミーンフィルタについての記事を書きました 当ページでは 理解を深めるために    OpenCV   に頼らないフィルタの実装方法を掲載しています コードに間違い等があればご指摘していただけると幸いです     空間フィルタリングとは空間フィルタリングとは 画像処理を使ってエッジやコーナー検出を行う前に画像にあらかじめ施しておく処理のことです これを行う理由には 写真を撮った時に発生するノイズによる影響をできるだけ防ぐために行われます 夜景を撮る時など 低光量であるときにノイズの影響はより大きく鮮明に現れるようになります 夜景を撮ったら以下の写真のように暗い部分が鮮明な暗さではなくなったという経験はあるのではないでしょうか    png  これはカメラの不良などによるノイズが含まれてしまった結果です  ちょっとわかりにくいですが このノイズは性能が悪いカメラほど顕著に現れます だったら 性能が良いカメラで撮ったものを使えばいいじゃないか となるかもしれませんが 実際にはドライブレコーダーや防犯カメラなど 画質は劣るが安価で量産に向いており大雑把に撮影することで十分とされるカメラを対象とした場合は フィルタリングが不要なくらいに画質を求めるのは難しいと考えられます ゆえに 汎用性を高めるためには 空間フィルタリングが重要となるのではないかと個人的には考えています     空間フィルタリングの適用例空間フィルタリングは その注目している画素値とその近傍の画素値を入力値として 出力する画素に対応する画素値を計算する方法です 周りの値を用いて何かしらの計算を行うことで ノイズの影響を軽減することができます 例えば 以下のような白黒画像を検討します マスは一つの画素であり それらはからまでの値で明るさを表しており 大きければ大きいほど明るいと考えてください 試しに 注目する画素つとその周りの画素つの値の平均を取るようなフィルタを検討します その場合だと 各画素は以下のように計算することができます    png     png  そしてフィルタをかけた後の画像は以下のようになります    png  すると 黒しかない画像のなかに白いノイズ点が含まれていた場合でも 空間フィルタリングをかけることによって この白い点をノイズとして平滑化を行い 周りと同じような色にして目立たなくしてくれることがわかります これが主なフィルタリングの役割です ただ このフィルタリング効果が強くなってしまうと 色の濃淡がはっきりとすべき箇所にまで影響を及ぼしてしまう ピンボケが酷い画像のようになる ため 適切な範囲で施す必要があります     空間フィルタリングの数学的定義空間フィルタリングを数学的定義に基づいて書くと以下のようになります 入力画素の値と出力画素の値をそれぞれ  I J  とします 座標   x  y    近傍領域の範囲を   N  imes N    として 空間フィルタリングをかけた時の画素の計算方法は以下のように表示できます  ただし  N  は以上の整数 ここでの  F  はカーネルと呼ばれ 注目する画素に対してどのような計算方法を適用するかを行列で定義します  N  の値はカーネルの行数と列数によって決まります また カーネル  F  を計算する方法として 相関と畳み込みの種類があります 相関とは カーネルと入力画素の行と列が一致しているもの同士を計算する方法で 畳み込みは行と列を逆に並び替えたカーネルと入力画素を計算する方法です 文字で説明してもわかりにくいので 以下の画像を元に理解されると良いと思います 数式で表すと以下の通りです 画素  I  とカーネル  F  がそれぞれである時の計算方法は以下の通りです 相関 畳み込み数式で書くと複雑になってしまいますが 先程の実際に計算してみた例では  N    としてそれぞれの画素の計算を行うことと同じ意味となります 単純な計算でも 数式だと難しいことをやっているように表記できますが 実際のところは大したことをやっていないというのが現実です     空間フィルタリングの種類今回はさまざまな空間フィルタリングをPythonで実装していこうと思います なお 適用する画像は以下のランドクルーザーの写真を用います また 結果をわかりやすくするために あえてノイズを付加しております  画像は こちら  の Response さんのランドクルーザーの記事から引用させていただきました       平均化フィルタ  中央値フィルタ平均化フィルタは 先ほど例に示したような 決めた範囲について平均値を画素値に適用するフィルタののことであり 中央値フィルタは 決めた範囲の中央値を画素値に適用するフィルタのことです これらは 先程の例のように単純な計算をすれば実装できますので Pythonでの実装は割愛します       ガウシアンフィルタガウシアンフィルタでは 注目する画素から近いと大きい重み付けを 遠いと小さい重み付けをするようなカーネルを用いたフィルタリングを行います 計算式は以下のように表され 分散 \sigma の値を変えることで 注目画素とその周りの画素との重みの差異を変えることが可能です 式は正規分布に似ています    mathF x  y    \frac    \pi \sigma     \exp \Bigl   \frac x    y   \sigma    \Bigr この式の x と y に対する F x  y  の分布は以下のグラフのようになります 中央であるほど値が大きく 離れるにつれて小さくなっていることが確認できます   output png  この分布をカーネルに適用してフィルタリングを行います   必要な関数の定義  カーネル関数  カーネルを作成する関数      カーネルの規格化  ガウシアンフィルタ      カーネルサイズを取得    line  column   kernel shape      フィルタをかける画像の高さと幅を取得    height  width   img shape      畳み込み演算をしない領域の幅を指定      出力画像用の配列      フィルタリングの計算上記では必要な関数を定義しています    create kernel   では カーネルの大きさと分散の値に従ってカーネルを計算します    gaussian filter   では 上記つの関数を使って実際に画像にフィルタをかける関数です    python  入力画像を白黒画像として読み込み  カーネルを定義  sizeにはカーネルを適用する計算範囲を sigmaには分散を渡す  ガウシアンフィルタを適用先程のつの関数を使ってフィルタをかけると フィルタを適用した画像のデータが得られます フィルタ適用前と適用後にわけて比較を行いましょう 可視化には   matplotlib   ライブラリの   imshow   関数を用います    python  元の画像とフィルタ適用後の画像を比較結果は以下のようになると思います 微妙にフィルタ効果があるんですが わかりにくいですね   使用する画像が悪かったかもですが   あとよーく見ると フィルタをかけた後に車の輪郭がぼやけていることがわかると思います ガウシアンフィルタでは 平均化フィルタよりぼやけ具合は抑えられますが それでもまだ少しはぼやけてしまうという問題点があります このぼやけを無くす つまり 必要なところだけ平滑化を行い 輪郭をしっかり残そうとして考案された方法が 次に紹介するバイラテラルフィルタです       バイラテラルフィルタバイラテラルフィルタとは ガウシアンフィルタの考え方に加えて 画素値に依存する重み付けを行うフィルタのことを言います 注目画素との値の差異が大きければ重みを小さくし 逆に 差異が小さければ 重みを大きくすることによって ガウシアンフィルタでは問題となっていた輪郭ボケを抑えることができるようになります 計算式は以下の通りになっています  I x y  はフィルタをかける前の画素値を  ilde I  x y  はフィルタをかけた後の画素値を  W x y i j  は適用するカーネルを表しています  \exp の指数部分の第二項が新たに追加された部分になります 特徴としては 一つの画素の計算毎にカーネルが毎度変わることです 画素値に基づいてカーネルを計算するため 毎度カーネルを計算し直す必要があります   カーネル関数  カーネルを作成する関数  バイラテラルフィルタの演算を行う関数  バイラテラルフィルタ      フィルタをかける画像の高さと幅を取得    height  width   img shape      畳み込み演算をしない領域の幅を指定      出力画像用の配列      フィルタリングの計算              カーネルを作成              カーネルを用いて計算ガウシアンフィルタより少し複雑になっていますが 大まかな流れは同じです    create kernel   と   bilateral function   のつの関数を使って カーネルの大きさと分散の値に従ってカーネルを計算します    python  入力画像を読み込み  バイラテラルフィルタを適用  sizeにはカーネルを適用する計算範囲を sigma dとsigma rにはそれぞれ距離に対する分散と画素値の差異に対する分散を渡す先程の関数を使ってフィルタをかけると フィルタを適用した画像のデータが得られます    matplotlib   ライブラリを使って可視化してみましょう 結果から フィルタによる平滑化ができており かつ 輪郭がぼやけずにしっかりと残っていることがわかります ただ 計算効率が非常に悪いためか フィルタ適用に分もかかってしまいました 計算を早めるためには工夫や改修が必要になるかと思われますが そもそも   OpenCV   ライブラリを使えば速度は解決するため ここでは計算ができればよしということで     まとめ空間フィルタリングについてと その代表例である ガウシアンフィルタ と バイラテラルフィルタ のPythonによる自力実装を行いました    ノンローカルミーンフィルタ などもありますが これについては実装が出来次第記事にしようと思います         冒頭にある通り ノンローカルミーンフィルタについての記事は投稿済みです 以上になります ,2,2022-02-12
253,253,iPhoneでPython + OpenCVを試してみる,画像処理,https://qiita.com/tnar-f/items/3a1e3044cb8211235658, はじめにiPhoneでPython   OpenCVができるというのを読んで やってみることにしました 環境構築は yamamidoさんの記事を参考にしました 使用するのは手持ちのiPhone SEです 今となってはもう古い機種ですが つの高性能CPUコアとつの高効率CPUコア GPUはコア Neural Engineはコアを備えた 立派なコンピュータです  Appleの公式情報ではありません  OSは由緒正しいバークレーUNIXの末裔です ひと昔前は こんなスペックのワークステーションは大学か大企業の研究室にしかなかったんですよねえ  準備PytoをApp Storeからダウンロードします これを書いている時点    では¥でした 起動して新しいディレクトリを作り そこを作業ディレクトリとします CreateメニューからBlankで空のファイルを作り 前回の記事でMacで使ったコードをメモ帳経由でコピペします 前回の記事作業ディレクトリに実験用ビットマップをコピーしておきます このディレクトリがコード実行時のカレントディレクトリになります これまでの実験で使ってきた x 約万画素のグレースケールのBMPファイルです 画面右上の実行ボタンで実行開始します  実行  スクリーンショット       png    ミリ秒 速っ これってWindowsでCore i  GHzを使ってやったときの倍近く速く M Mac  の時の半分の速度です 以下 結果画像の一部を拡大したものです しっかりBlurしてますよね   スクリーンショット       png  いやもう 驚愕のひとことです OpenCVを開発しているみなさんありがとう Pytoの開発者さんありがとう Appleさんありがとう Jobsさんありがとう ,3,2022-02-12
254,254,Deep Metric Learningでゲームキャラを分類する,画像処理,https://qiita.com/Yukiho_P/items/cc875aba845290641712,  目的　ゲームのスクリーンショットから切り出したキャラクターの顔を分類できるようにする．この際 新キャラクターが追加された場合に行う学習コストを下げられるような手法を選択する．  環境   機械学習の実行  Python     Pytorch    cu  torchvision    cu  GPy     plotly     windows  home  Intel i KF  RTX super   画像の準備  Mac Mini M  macOS Monterey   写真 app  プレビュー app  導入　待ちに待ったアイマスの家庭用機向け新作 通称スタマス．以下 本作 が年月に発売されました．今作はシナリオ量が過去最大とも言われており スキップせずにプレイすると約時間はかかるほどです．真エンディングまでですと時間弱とたっぷりプレイできます．　本作に限らずゲームをプレイしているとスクリーンショットを撮る機会は少なからずあると思います．本作ではお気に入りのアイドル 衣装 楽曲 ステージを組み合わせてMVを鑑賞することが可能なため 必然的にベストショットがフォルダに数千枚と蓄積されていきます   　せっかくのベストショットも未整理で数千枚とあれば見返す機会が限られてしまいます．そこで積み上がったベストショットを整理すべく 画像分類タスクに取り組むことにしました．　ところでただ画像を分類したいだけなら 深い畳み込み層の後に全結合層を接続したニューラルネットを作成するだけで十分ですね ですがこの手法には欠点があります．分類したいクラスが増えて再学習を行おうとすると 深い畳み込み層の再学習コストが大き過ぎるのです   ．本作のようにDLCでアイドル＝新クラスが追加されるような場合には由々しき事態です．この問題に対処するため 本記事ではDeep Metric Learning 以下 DML という手法で顔空間を別の低次元空間に埋め込みました．同じ顔は近くに 異なる顔は遠くに埋め込まれるような変換を最初に学習することで 再学習の際には埋め込み空間を分割する全結合層だけを学習すれば十分になることを期待します   ．     生体認証identification     取り組みの全体像  全体の流れ  本作の真エンディングまでクリアする  MV鑑賞でベストショットを撮りまくる 本記事では千枚以上   ベストショットから顔部分を切り抜く  アイドルごとのフォルダに仕分け 同千枚以上   DML実行 埋め込み空間の確認  DLCを購入 追加アイドルのベストショットを撮りまくる 枚程度     DML実行時に存在しなかった  追加アイドルが埋め込み空間でどう分布するかを確認する　顔の切り抜きは偉大な 先人の力  を借りることで簡単に実行できます．フォルダへの仕分けは気合いで手作業です．  これを自動化したかったのに本末転倒．  DMLもほぼコピペです．パラメータの調整とか手付けずです．埋め込み空間の確認に先人達との差別化がありますが これもパーツを組み合わせただけです．  分類対象の確認　画像の仕分けという泥臭い作業が完了したところから始めます    画像例　今回の学習対象となるアイドル人の画像と 未知データへの適用可能性を評価する用の追加アイドル人の例です．例示画像は に縮小しています．   Amana png     Chihaya png     Kaori png     Mami png     Ranko png     Shizuka png     Yayoi png  　本記事の投稿前日にDLCアイドルが追加されているのですが 画像の準備ができていないため省略します．   クラス分布　クラス間のデータの偏りです．トップだけで全体の  人 人で というのは教師データとしては不適切でしょうが仕方ありません．最上位陣は感覚通りですが 上位陣くらいからもう そうなの  という感じです．割とストーリークリアの都合で決まってる気がします．  cumulative png     画像の大きさ　ベストショットの構図によって顔画像の大きさは全て異なるのですが 基本K画質で保存してあるため大きめの画像が多いです．正方形で切り抜いたところ一辺の長さの中央値はピクセルでした．学習の際には中央値で大きさを統一しておきました．  hist png    学習　本質的には こちらの記事  モデル学習 と同じです．モデルをResNetに置き換え  こちらの記事  に従って高速化を施し 学習全体をクラスにまとめて  パラメータをコマンドライン引数として受け付けられる  ように書き換えてます．   モデル　Pytorchで 既に構築されているResNet  を使います．本来は調整すべきパラメータなのですが 最後の全結合層の出力は仮で個としておきます．すなわち顔空間を次元で再現することをDMLで学習します．   学習データと評価データの分離　ここで言う評価データとはDLCアイドルのことではありません．最初にDMLで学習するアイドル人の画像を 学習用と評価用に分離するということです．前述の通りデータ分布が著しく偏っているため ランダムサンプリングで分離すると評価が安定しなくなります．これを回避するために scikit learn で層化抽出を行います   ．学習データと評価データにそれぞれどの画像が選ばれたかを追跡するためにcsvでの保存も行っています．   自作DMLクラス　前述の通り 先人の業績  を整理しました． 保存機能や保存済みモデルの読み込み機能  も追加し  精神的安全のためにプログレスバーを実装  するなど欲張りクラスに仕上がってます．   学習　実行するだけ．パラメータチューニングしたいならスクリプトを書き換えずにコマンドライン引数から対応可能です   ．　私の環境でコマンドライン引数を与えずに実行すると batch epochの学習が約秒で計算できます．その後の評価やら保存まで込みだと秒ほどになりました．画像が と大きいためバッチサイズでもGBのVRAMがギリギリです．AMPを使わない場合はバッチサイズをにしないとメモリに乗りませんでした．  結果の可視化   評価データでtSNE　pixel   pixel   RGB    次元の画像がDMLによって次元空間に埋め込まれています．おおよそpixel    pixel   RGB相当の情報量ですが そう画像にしたところで顔とは分からないでしょう．埋め込みがうまくいっているかを  評価データで  確認します．次元そのままでは確認できないためtSNEで次元圧縮します．　次元に圧縮してる割には綺麗にクラスター アイドルが分離されてる気がします．またエポック以上は学習が進展していないようにも見えますね．実際 学習中に選択されるtripletがどんどん減少して途中から個のケースも散見されました．このまま分類用の層を接続すれば分類タスクもきっとうまくいくでしょう．たった千の教師データでもDNNは有用なんですね．  準備の手間は全く無視できませんでしたが．     次元に圧縮するGPLVM　次元圧縮がtSNEばかりでも芸がないため ガウス過程回帰を用いたGPLVMを試してみます．また次元の例もつまらないので次元にします．　面倒なことに GPy の想定している ploly はかなり古いようで 公式サンプル  をコピペしてもうまく描画できません．さらに面倒なことに APIリファレンスを読んでも埋め込み後の点を取得する方法が明記されていません．ひとまずそれっぽいところを辿って get x y var   なる関数を発見しましたが 明記されていない以上いつまで使えるか怪しいです．　残念ながらCodePenの上限 MB に引っかかってしまってグリグリ動かせて ポインタを当てるとラベルが表示できるplotlyのグラフを掲載できません．このため静止画のみ掲載します．  スクリーンショット       png  　なんか層になってました．しかし共通点も見出せなかったので偶然でしょう．   未知アイドルの埋め込み　本命．学習時に全く情報を与えていないアイドルが他のアイドルから離れた点になるかを確認します．これがうまく成功すると   DLCでどれだけアイドルが追加されようが    CNNを固定して低コストで分類機を学習できます．なんなら転移学習して他のゲームのキャラクター識別にも使えるはずです．先ほどの続きです．　さて静止画で確認です．  スクリーンショット       png  　いい感じ．  スクリーンショット       png  　アイドル Amana  Iori  Mirai の交点みたいなところに埋め込まれてしまいました．確かに髪の長さとか色とか似てます．その他 別のアイドルの外れ値も紛れています．ですが千程度の学習データから未知データをこれだけ分離できているのは十分有効な手法だと思います．    Plotly Chart Studioで描画　 グリグリ動かせるグラフを外部で保存しました．  グラフ用のGPLVMの計算を保存しておらず再計算したため上記の結果と全く同じではありませんが DLCアイドルのうちKaedeは分離できたのに対してKotohaはやはり他のアイドルに近い位置になりました．  結論　DMLで埋め込んだ空間は未知データもうまく分離できる．  データの公開　google driveで画像とpythonファイルと学習済みモデルを公開予定です．　 公開しました．  ,3,2022-02-11
256,256,python+opencvで画像処理の勉強6 2値画像処理,画像処理,https://qiita.com/tanaka_benkyo/items/b1f498a881958280187f,pythonとopencvを使って画像処理を勉強していきます 前回python opencvで画像処理の勉強 周波数領域におけるフィルタリング周波数フィルタリングについて学びました python opencvで画像処理の勉強 幾何学的変換今回は値画像処理を勉強していきます つ目の使用画像は旭川動物園のペンギンの散歩の写真です   image png      値化     値化の意味例えばグレースケール画像などで 中間値をなくし 白または黒の値の画像に変換することを  値化  とよびます   値画像  は 例えばある値   しきい値   以上の画素値を それ未満の画像をに変換することで得られます      p タイル法  p タイル法  とは 対象物の占める領域の画素数が あらかじめ予測できる場合に 予測された画素数に応じてしきい値を決める手法です 画素値の低いところから頻度値を積算し 予測された画素数を超えたときの画素値をしきい値とします 対象画像に対する事前の知識が必要であり 未知の画像を処理する場合には適していません      モード法グレースケール画像はつの山を持つことが多い   モード法  は山と山の谷の底をしきい値とします 画像に十分な画素数がないと山がはっきりせず 安定に底を検出することができません      判別分析法黒白どちらかのクラスに属する画素は あるしきい値tの左右に分かれて分布します その分布の分離度が大きくなるようにしきい値を決める方法を  判別分析法 大津の方法   と呼びます   分離度  は   クラス間分散   \sigma  b   とクラス内分散 \sigma  w   の日 \sigma  b   \sigma  w   で定義します 全体の画素値の平均と分散を m  t  と \sigma  t    黒画素クラスを m    と \sigma      黒画素クラスを m    と \sigma     とし  \omega    と \omega    をそれぞれ黒画素クラスと白画素クラスに属する画素の数とします クラス内分散は クラス間分散は 以下の式で表されます   全分散   \sigma  t   とクラス間分散 \sigma  b    クラス内分散 \sigma  w   には 以下のような関係があります したがって 以下の式が成り立ち しきい値tに関係なく全分散 \sigma  t   は一定のため クラス間分散 \sigma  b   が最大になるようにtを決めればよいことがわかります oprncvに値化処理の手法がいくつか用意されているので見ていきます      適応的値化処理閾値を固定せず 画素ごとに変化させる二値化処理 画素毎に異なる閾値を算出する手法です     値画像の基本処理と計測     連結性注目画素に対して上下左右の画素を  近傍  と呼び その近傍に対して注目画素の連結を定義したものを  連結  と呼びます 近傍に斜め方向の近傍を加えたものを  近傍  と呼び 連結を定義したものを  連結  と呼びます 連結している画素の集合は  連結成分  と呼びます 対象の連結成分のなかにあり 背景に連結していない白画素の集合を   穴  と呼びます 例を以下に示します つ目とつ目の画像は注目画像 赤 と近傍点 青 の画素を表しています つ目の画像に対してそれぞれの近傍点を見ていくと 連結 つ目の画像 ではつの穴 連結ではつの穴となります 連結では斜めのつながりを連結とみなしません      輪郭追跡連結成分の境界を求めることを   輪郭追跡  と呼びます     ラスタスキャン  によって 白画素から黒画素に変わる画素を探索します ラスタスキャンとは 画像の左上を起点に 左端から右に画素を調べ 右端についたら行をつ下がって左端から右に画素を調べる操作です 探索した方向を進入方向とします   進入方向を基点に番号順に右回りに黒画素を探索します   見つかった黒画素に移動する 黒画素が開始点で かつ 次の移動点が追跡済みの場合は 処理を終了し 追跡結果を登録します そうでない場合は の処理を繰り返します 連結成分の外輪郭の場合 追跡は右回りになり 内輪郭すなわち穴の輪郭の場合は 追跡は左回りとなります 輪郭追跡時に 開始点座標と方向コード 注目点右の画素位置をとし時計回りにまで を並べたものを  チェインコード  と呼びます      収縮 膨張処理      収縮背景または穴に接する対象の画素をひとまわりはぎとる処理を   収縮  と呼びます この処理を繰り返すと 対象の領域は小さくなります この処理で つの連結成分が複数に分割されることがある すなわち 連結性を保持しない処理です cv dilateを使って収縮の様子を確認します       膨張背景または穴に接する対象の画素をひとまわり加える処理を   膨張  と呼びます この処理を繰り返すと 穴は小さくなります この処理で 複数の連結成分がつの連結成分となることがある すなわち 膨張も連結性を保持しない処理です 膨張の場合はcv erodeを使用します       クロージング オープニング同じ回数膨張して収縮する処理を  クロージング  と呼び 画像中の小さな穴を除くことができます 同じ回数収縮して膨張する処理を  オープニング  と呼び 画像中の小さな連悦部分を除くことができます また cv morphologyExを使っても同様の処理ができます      ラベリング同じ連結成分を構成する画素に同じ番号を付け 異なる連結部分を構成する画素に異なる番号を付ける処理を  ラベリング  と呼びます      ラスタスキャンによるラベリング  ラスタスキャンを行い ラベルの付いていない画素を調べ 見つかったら注目画素とします   注目画素の上の画素 または左上の画素 がラベルを持つとき 上の画素 または左上の画素 の 左上の画素 ラベルを注目画素に付けます 左の画素 及び上 右上の画素 がラベルを持ち 注目画素のラベルと異なるときルックアップテーブルにそれらのラベルが同一連結成分に属することを記録します  右上の画素がラベルを持ち 注目画素のラベルが異なり かつ右の画素が白画素のとき ルックアップテーブルにつのラベルが同一連結成分に属することを記録します    注目画素の上の画素 および左上の画素 が白画素で 左の画素がラベルを持つとき そのラベルを注目画素に付けます   注目画素の上も左も 左上の画素 も白画素のとき 新しいラベルを注目画素に付けます   走査する画素がなくなるまでから繰り返します つぎに 再度ラスタスキャンを行い ルックアップテーブルを参照しながら 同一の連結成分に属するラベル群から 最も小さいラベルを付け直します      輪郭追跡によるラベリング輪郭追跡した外輪郭の結果で というラベルが付いているとき ラベルを除いた連結成分に対して 左上の画素から再度輪郭追跡結果にラベルを付けます さらに ラベルを除いた連結成分に対して 同様の処理を行い ラベルを付けるべき画素がなくなるまで処理を繰り返します 同じ連結成分に付いたラベルを統合して 同じラベル番号を振り直すことによって ラベリングを行うことができます ここでは クリオネの写真で例を示していきます   色を領域の数だけ用意  markersの値に応じて色を決めていくラベリング処理の結果では ノイズの部分にもラベルが割り振られています これを利用して面積でフィルタをかけ ノイズを除きます   色を領域の数だけ用意              面積が小さいものは対象外ディープラーニングじゃなくてもこんな画像がだせました      形状特徴パラメータ値化画像中の連結成分は 形状の特徴によって分類できます この特徴を数値化したものを  形状特徴パラメータ  と呼ばれます クリオネ 匹 の画像で見ていきます   値化  膨張 収縮処理kernel   np ones       np uint thresh   cv erode thresh  kernel  iterations    thresh   cv dilate thresh  kernel  iterations    thresh   cv erode thresh  kernel  iterations      輪郭の抽出  輪郭の特徴 モーメント の算出      重心連結成分の画素に等しい重さがあると仮定したときの 図形全体の重さの中心   pythoncx   int M  m   M  m   cy   int M  m   M  m   print cx  cy       面積連結成分を構成する画素の数   pythonarea   cv contourArea cnt area      周囲長輪郭追跡し一周する移動量   pythonperimeter   cv arcLength cnt True perimeter      円形度図形がどれだけ円に近いかを表す尺度 円に近いほどに近づく       オイラー数連結成分の数から穴の数を引いたもの      主軸画素の位置に重み付けをして合計した数値   モーメント特徴  を利用して主軸方向を求めることができます       外輪郭 外接長方形 外接円 主軸  主軸  外輪郭  外接長方形  回転を考慮した外接長方形  外接円  外接楕円  重心     距離と距離変換画像      距離の定義つの画素 A x  a  y  a   と B x  b  y  b   の間の距離は 経路の制約によって以下のような定義があります   ユークリッド距離  は AとBを直線で結んだ距離   市街地距離  は 近傍距離とも呼び上下左右の近傍の移動経路による距離です   チェス盤距離  は 近傍距離とも呼び 近傍の移動経路による距離です      距離変換画像連結成分の各画素の背景からの最短距離を与える変換を  距離変換  と呼び 変換後の画像を  距離変換画像  と呼びます     線画像のベクトル化     ベクトル化の流れ連結成分の連結性を保存したまま画素を削る処理を  細線化  と呼びます 細線化された値画像の画素は 特徴点である  端点     分岐点     通過点  の種類に分類されます 図に例を示す 赤い点が端点 緑色が分岐点 青の点が通過点を表しています      細線化手法対象を黒画素 背景を白画素とします 注目画素を中心に×の画素値のパターンを観測し 注目画素が以下のつの条件を満たすとき その値を白画素にします   境界上にある黒画素であること   白画素に変更しても連結性が保持されること   端点でないこと 画素値を更新する方法には 逐次法と並列法があります   逐次法  は 更新した結果を用いて すぐに次の処理を行います   並列法  は 消去可能かどうかを検証する画像と更新を行う画像を別々に持ち 更新用画像に処理結果を反映していきます 高速道路の図を使って説明していきます   image png  細線化は値化した画像に対してcv ximgproc thinningで処理します      細線の特徴点の抽出細線化処理の結果は 端点 分岐点 孤立点 通過点の特徴量に分類されます 特徴点と特徴点によって区切られた画素列を   セグメント  と呼びます テンプレートマッチングにより特徴点の抽出を行います 各特徴点のテンプレートを用意します    python  端点  分岐  分岐例えば端点であれば  image png  のような画像です 分岐と分岐がうまく分けられていませんが ある程度検出できました これで値画像処理についての勉強は終わります     次回領域処理    参考ディジタル画像処理 改訂第二版    ディジタル画像処理編集委員会  本   通販   Amazon,16,2022-02-10
257,257,DOBOT×AI 判別対象をグリッパーで掴む,画像処理,https://qiita.com/shimamotosan/items/d15dde18889b8dba2204,以下のテキストのソースコードは 判別対象を吸引カップで移動させるようになっています  DOBOT Magician AIｘ画像認識ｘロボットアーム制御  対象物が小さい あるいは細長く 吸引カップでは保持できないことがあります そのような場合 DOBOT magicianに付属する空気式グリッパーで掴む必要があります   変更前グリッパーで対象物を掴んで動かすプログラムに修正します dobot aiフォルダ内にある dobotClassifier py の以下の部分で対象物を移動させています    python dobotClassifier py  Z座標  オブジェクトの大きさ 環境に合わせて変更する    指定された座標のオブジェクトを取り ラベルごとに仕分ける      オブジェクトの真上に移動      オブジェクトを取れる位置まで移動し オブジェクトを取る      オブジェクトの真上に移動      ラベルに合わせた座標を指定      ラベルごとに仕分ける位置の真上に移動      オブジェクトを置く      カメラに映らない場所に移動  変更後以下の様に修正します    python dobotClassifier py  Z座標  オブジェクトの大きさ 環境に合わせて変更するz    ※ 上記の数値は オブジェクトの大きさや動かす環境に合わせて変更してください     指定された座標のオブジェクトを取り ラベルごとに仕分ける      オブジェクトの真上に移動      オブジェクトを取れる位置まで移動し オブジェクトを取る      オブジェクトの真上に移動      ラベルに合わせた座標を指定      ラベルごとに仕分ける位置の真上に移動      オブジェクトを置く      カメラに映らない場所に移動修正したプログラムは グリッパーの角度 R座標 は度のままなので 対象物や置き方によって変更する必要があります ,1,2022-02-09
258,258,DOBOT×AI カメラのセッティング方法,画像処理,https://qiita.com/shimamotosan/items/286f80808f30f026b466,以下のテキストを使用する際 カメラのセッティング等で上手くいかない場合があるので 解決方法を記載します  DOBOT Magician AIｘ画像認識ｘロボットアーム制御    測定マット上の色を読み取れないカメラの座標からロボットアームの座標へ変換するために測定マット上の色の座標を読み取り 変換マトリックスを生成するステップがあります その際 色の座標だけを上手く読み取れないことがあります 例えば 以下の画像の場合 黄色の座標は読み取れていますが それ以外の色は読み取れていません また 灰色の部分を誤認識しています    対策：余計な部分を隠す余計な部分を認識する場合は 誤認識する部分を隠すのが簡単な解決方法だと思います    対策：認識するサイズを調整する認識してほしいところを認識しない場合は 領域抽出用のサイズを調整すると正しく認識するようになることがあります ソースコード上の以下の箇所の最小サイズ 最大サイズの値を調整します   領域抽出用の最小／最大サイズ↓　変更後  領域抽出用の最小／最大サイズ  判別対象が認識しないAIによる対象物の判別をするために学習データを集めます その際 判別対象を認識しないことがあります    対策：認識するサイズを調整する以下の画像のようにバイナリー化 値化 した画像で物体がくっきり見えている場合は 前述した領域抽出用のサイズを変更することで認識するようになります   領域抽出用の最小／最大サイズ↓　変更後  領域抽出用の最小／最大サイズ,1,2022-02-09
260,260,論文の勉強13 MobileNet V3,画像処理,https://qiita.com/tanaka_benkyo/items/0d96e9f96a686f4f752a,MobileNet Vについて構造の説明と実装のメモ書きです ただし 論文すべてを見るわけでなく構造のところを中心に見ていきます 勉強のメモ書き程度でありあまり正確に実装されていませんので ご了承ください 自分の実力不足で読み解けなくなってきています 難しいです 以下の論文について実装を行っていきます SE blockについてはSENetをご参照ください      MobileNet VMobileNet VではMobileNet Vで導入したbottleneck構造に SENetなどで導入したSE Squeeze and Excitation  blockを組み合わせます また 非線形関数としては  h swish  を使用します SE blockでは非線形関数としてhard sigmoidを使い expansion layerのチャンネル数は入力の とします   image png    image png       h swish  swish関数  は以下のように定義されます で置き換えたものを  hard swish関数  と呼びます ここで  RELU  はMobileNet Vでも出てきましたが最大値をとしたRELU関数のことです   image png       構造構造を見ていきます 種類紹介されています つ目はMobileNetV Large  image png  つ目はMobileNetV smallです   image png       学習最適化手法はRMSpropでmomentumが とします 学習率の初期値は でエポックごとに の減衰率で減衰させます weight decayは とします 確率 のdropout層も挿入し すべての畳み込み層の後にはBatchNormalizationを入れることとします      実装MobileNet VにSE blockの追加をして 非線形関数を変更するだけです  シリーズものでは自分の実装を持っておくとこの辺が楽だなと思いました       keras必要なライブラリのインポートをします 次にsmallの実装をします 構造を確認します 学習の設定をします pytorchも同様に書いていきます 必要なライブラリのインポートをします hard sigmoidとhard swishはもともと用意されているので それを使います 構造の確認をします 学習の設定をします       New  テストデータに対するエポックごとの処理最後の出力層のみ書き換えて学習させます       New  テストデータに対するエポックごとの処理以上で実装を終わります ,1,2022-02-05
261,261,論文の勉強12 MobileNet V2,画像処理,https://qiita.com/tanaka_benkyo/items/ff87271a729b85234a88,MobileNet Vについて構造の説明と実装のメモ書きです ただし 論文すべてを見るわけでなく構造のところを中心に見ていきます 勉強のメモ書き程度でありあまり正確に実装されていませんので ご了承ください 自分の実力不足で読み解けなくなってきています 難しいです 以下の論文について実装を行っていきます タイトル：MobileNetV  Inverted Residuals and Linear BottlenecksMobileNet Vは前回扱いました     MobileNet V     Inverted Residual block bottleneck block MobileNet Vで扱ったDepthwise Separable Convolutionsにbottleneck構造を持たせます 図の d のように×のdepthwise convolutionをpointwise convolutionで挟み込んだ構造となります つ目のpointwise convでチャンネル数を増加させ つ目のpointwise convでチャンネル数を減少させます   image png  さらに下の図の右側のようにResidual接続を持たせます   image png  このbottleneckの構造を詳しく見ます   image png   k は入力  k  は出力のチャンネル数  t はチャンネル数の倍率  s はストライドを表します 非線形関数としてReLUが使用されています これは出力の最大値をとしてクリップする関数となります 次にMobileNet Vの構造を確認します  n はブロックの繰り返し数となります 複数回繰り返す場合は 最初のブロックでチャンネル数が拡張 strideがの場合も最初のブロックのみ適用となります bottleneckのresiduak接続は入力と出力の形式が同じときのみ適用とします   image png       学習最適化手法はRMSpropを使用しdecayとmomentumは とします そしてweight decayは です 学習率は初期値を としてエポックごとに減衰率 で減衰させていきます      実装MobileNetVと大きく変わりません      keras必要なライブラリのインポートをします 論文の構造の表に沿って実装できるようにbottleneckを実装します 構造の確認をします 学習の設定をします 全結合層を付け替えて 追加した部分だけを学習させます 設定は変わりません pytorchも同様に実装をします 論文のbottleneck部分を実装しておきます 構造の確認をします 学習の設定をします       New  テストデータに対するエポックごとの処理torch hubで学習済みのモデルを取得torchvisionで学習済みのモデルを取得torch hubのものと変わりません 最後の全結合層のみ学習させるものとして 学習の設定をします       New  テストデータに対するエポックごとの処理以上で実装を終わります ,1,2022-02-05
262,262,論文の勉強11 MobileNet V1,画像処理,https://qiita.com/tanaka_benkyo/items/37aadc03c4cee2e55c9e,MobileNet Vについて構造の説明と実装のメモ書きです ただし 論文すべてを見るわけでなく構造のところを中心に見ていきます 勉強のメモ書き程度でありあまり正確に実装されていませんので ご了承ください 自分の実力不足で読み解けなくなってきています 難しいです 以下の論文について実装を行っていきます 分解することで計算コストを削減することができます      depthwise convolutionチャンネルごとに異なるフィルタで畳み込み処理を行います      pointwise convolution×の畳み込み処理を行います   image png       構造構造の詳細を次の表に示します depthwise convolutionとpointwise convolutionの組み合わせが基本単位ですが 下の図の右側で示すように各conv層の後にはBatchNormalizationとReLUが挿入されます   image png    image png       学習RMSPropを使用し decayは   \epsilon は です 学習率は で エポックごとにrateを として指数的に減衰させていきます      実装     kerasまず 必要なライブラリのインポートをします 構造を確認します 学習の設定をします 学習済みのモデルを使用します 全結合層を付け加えて 追加した部分だけ学習させます 必要なライブラリのインポートをします 構造の確認をします学習の設定をします       New  テストデータに対するエポックごとの処理,1,2022-02-05
263,263,論文の勉強10 SENet,画像処理,https://qiita.com/tanaka_benkyo/items/fbc4c242a820885d84b9,SENetについて構造の説明と実装のメモ書きです ただし 論文すべてを見るわけでなく構造のところを中心に見ていきます 勉強のメモ書き程度でありあまり正確に実装されていませんので ご了承ください 以下の論文について実装を行っていきます タイトル：Squeeze and Excitation Networks    SENet     Squeeze and Excitation blockSqueeze and Excitation block SE block は次の図のような構造を持っています   image png  このSE blockをResNetなどに挿入することでSE 〇〇Netが構成されます また これらをまとめてSE Netと呼びます 入力を \boldsymbol X  として 構造を確認していきます  \boldsymbol U  は最初にチャンネル毎に画像全体 H×W領域 の平均値などが計算することにより チャンネル数分の数値に変換されます と書くことができます      excitationsqueezeにより得られた数値を線形変換し sigmoidで に変換します このチャンネル数分の数値をもとの特徴マップ \boldsymbol U  に掛け合わせます 線形変換部分ではつ目の変換 重み \boldsymbol W   \frac C  r ×C   でチャンネル数を C r に減衰させ つ目の変換 重み \boldsymbol W   C×\frac C  r    でチャンネル数を元の C に戻します その後 sigmoid関数 \sigma により変換を行います つ目の線形変換後はReLU関数 \delta を適用します これらの変換で得られる \boldsymbol s  ××C  は と表すことができます   最後に  \boldsymbol U  のチャンネル毎に対応する \boldsymbol s  の成分を掛け合わせていきます この変換で得られる出力を ilde X   ilde x   \cdots ilde x  C  とすると と書くことができます      SENetたとえば  \boldsymbol F   tr  としてInceptionを使えば 次の図のような構造となります 図のようなSE ResNetとなります 構造の詳細は次の表のようになります   a b  はつの線形変換の出力するチャンネル数となります   image png       学習SGDでmomentumは  学習率は としエポックごとに を掛けて減衰させていきます 入力はInceptionであれば× 他のものは×です      実装実装はSEResNeXt について行います 以前実装したResNeXtにSEBlockを追加するだけです      keras必要なライブラリのインポートをします           入力層          出力層モデルの確認をします 学習の設定をします 必要なライブラリのインポートをします 以下 kerasと同じです 構造を確認します       New  テストデータに対するエポックごとの処理これで実装を終わります ,0,2022-02-04
264,264,論文の勉強9 Xception,画像処理,https://qiita.com/tanaka_benkyo/items/45ac24b1c4bc342250fb,Xceptionについて構造の説明と実装のメモ書きです ただし 論文すべてを見るわけでなく構造のところを中心に見ていきます 勉強のメモ書き程度でありあまり正確に実装されていませんので ご了承ください 以下の論文について実装を行っていきます タイトル：Xception  Deep Learning with Depthwise Separable Convolutions    Xception     depthwise separable convolution図に示すようにinputに対し×の畳み込み層でチャンネル数をoutput channelsに増やし そのチャンネルを重複のないセグメントに分けそれぞれ×の畳み込みを行い 最後に各出力を結合します これを depthwise  separable convolutionと呼びます 本来の separable convolution は×の畳み込みのあとに×の畳み込みを行いますが 層を積み重ねていくのでその違いは重要でないと考えます Xceptionではチャンネルごとに×の畳み込みを行います   image png  プーリング層のないシンプルなInception 上図 を考えたときに このseparable convolutionで表現できる 下図 ことが分かります   image png    image png  構造の詳細を見ます   InceptionV であったような途中の層で分類させるような構造は持ちません      学習SGDでmomentumを とします 学習率は初期値が でエポックごとに をかけて小さくしていきます      実装     keras必要なライブラリのインポートをします SeparableConvDが実装されているので利用します つの畳み込み層とプーリングまたは畳み込み層を組み合わせます  回繰り返すのでそれも含めて実装します Xception本体の実装をします モデルの構造を確認します 学習の設定をします 必要なライブラリのインポートをします これ以降はkerasと同様の実装を行います 畳み込み層構造の確認を行う       New  テストデータに対するエポックごとの処理これで実装を終わります ,0,2022-02-04
265,265,論文の勉強8 Inception V3,画像処理,https://qiita.com/tanaka_benkyo/items/d78c37ba1de82f0c78ff,Inception Vについて構造の説明と実装のメモ書きです ただし 論文すべてを見るわけでなく構造のところを中心に見ていきます 勉強のメモ書き程度でありあまり正確に実装されていませんので ご了承ください 以下の論文について実装を行っていきます factorizationにより ある畳み込み層をそれより小さいkernel sizeの畳み込みに分解することで計算コストを削減しています また VはVを少し修正したものであり実装はほとんど変わりません ここではVの実装を行います     Inception V     Inception ModulesInception Vでは複数の種類のInception Moduleがでてきます まず Inception Vの構造を見ます   image png  表内の種類のInception以外にそれぞれのInception間にチャンネル数と画像サイズを変えるためのInceptionが挿入されます 従って全部で種類のInceptionが出てきますが論文内では特に名前が付いていないので  Inceptionx  Inceptionx  Inceptionx  Inceptionto  Inceptiontoとします さらに xInceptionの後には GoogleNet Inception V 同様にAuxiliary Classifierが挿入され この出力も使って学習を行います ただし Vではつありましたが今回はつです      Inceptionx下の図の通り×の畳み込み層をつの×の畳み込み層に分解することができて これにより計算コストを下げることができます   image png  次の上の図はGoogleNetのInception Moduleですが 今回はこの×の畳み込み層を下図のようにつの×の畳み込み層に分解します   image png    image png       Inceptionxさらに 計算コストを下げるため n×n の畳み込み層を ×n と n× のつの畳み込み層に分解します  × の畳み込み層の場合の例を図に示します  × と × のつの畳み込み層の出力は × の畳み込み層と同じになることが分かります   image png  これを利用するとInception Moduleは次の図のようになります   image png  この構造のInception Moduleは画像サイズが×のものに使用し そのときは n  とします      Inceptionxさらに 直列に分解するのではなく下図のように並列に分解したInception Moduleも使用します   image png       Inceptionto Inceptionto Inception Module間では画像サイズが半減し チャンネル数が増加します この処理は以下のような 畳み込みとプーリングを並列に処理する構造のInception Moduleを使用します   image png  ただし 図の構造はInceptionxとInceptionxの間で使用し InceptionxとInceptionxの間では別の構造のInception Moculeを使用します Inceptiontoの場合は ×の畳み込みがつ並んでいる部分のつ目の畳み込みが×と×の畳み込みのつの層に置き換えたものを使用します      Auxiliary Classifier論文では以下のように紹介されていますが 今回は参考サイトをもとに図のプーリングと畳み込みの間にもう一つ畳み込み層を入れて使用します   image png       Label Smoothing学習の際にLabel Smoothingと呼ばれる正則化を行います 正解ラベルを以下のように変形したものに変更します   image png  ここで K はクラス数  \epsilon は足し合わせる割合です  \delta は正解ラベルをワンホットエンコーディングしたものです      学習バッチサイズでエポックで学習を行います RMSPropを使用し decayは   \epsilon は です 学習率は で エポックごとにrateを として指数的に減衰させていきます 勾配をでクリッピングすると学習を安定させることができることもわかりました      実装     kerasまず必要なライブラリのインポートをします 畳み込み層の後にはBatchNormalizationとReLUをつけるので セットで定義しておきます kernel sizeが非対称になっています kernel sizeが非対称で 枝分かれが多いので注意します 次に特徴マップのサイズを変換する  Inceptionto  の実装をします もうつの特徴マップのサイズを変換する  Inceptionto  の実装をします ここでは学習で使用する  InceptionAux  の実装をします 今まで実装したモジュールを使用して  InceptionV  の本体を実装をします 作成したモデルの構造を確認します 学習の設定をします 学習は実行しません      pytorch必要なライブラリのインポートをします 非対称なkernel sizeの畳み込みではpaddingも非対称に設定します InceptionVの本体の実装を行います 構造を確認します 学習の設定をします       New  テストデータに対するエポックごとの処理学習は実行しません 以上で InceptionVの実装を終わります ,4,2022-02-04
266,266,MATLABで行うFFTその3 ~画像編~,画像処理,https://qiita.com/Hernia_Baby/items/31144f3163dba803f50c,  はじめに今回は画像のフーリエ変換をやっていきたいと思います  YouTube   の動画がものすごくわかりやすかったのでMATLABで実装します Gitの内容は見てないので各自 概要欄から確認してね  画像の読み込みまずは画像を用意するのですが サイズが大きいので×のグレースケールに変換します はい かわいいさて この絵の行目だけ抜き取って 画素値がどうなっているかを見てみましょう FFTにすると以下のようになります ylabel  空間周波数スペクトル  低周波数部分が強いのでここでは上限までとしました まあこんな風に画像も断面で見ると信号のようであることがわかります   次元フーリエ変換画像も縦と横の波をかけ合わせることで表現できます  \sin ax imes\sin by  のようなものと考えてください 例を以下に示します ※画像をきれいに見せるために余弦波の掛け算を行っています 　正弦波の場合は a   b のどちらかのときにはすべてになります 波形に重みをつけて 足し合わせることで画像を生成することができるということがわかりました   FFTをやってみようさて早速やってみましょう おっと取り乱しました はじめに行った行目FFTを思い出してください 低周波になるほどスペクトルどんどん大きくなってましたよね こういう時は対数をとります 四隅が低周波です でも着目したいのは四隅の方なので 真ん中に来るように移動させます これみるとなんとなくわかるんですが  あれ 高周波成分削ってもよくね  ってなります これが jpeg の画像圧縮につながるんですが これは別途改めて…  次元フーリエ変換の重ね合わせさてイメージをつかむために画像を一枚ずつ重ねていきます まず×の面をFFTした後  reshape   で直線にして 左上からスペクトル さっきの波 を少しずつ足していきます      ■アニメーション  アニメーションについては　 MATLABによるアニメーション作成  　を参考にしました こんな神記事書いちゃってさ…誇らしくないの ちなみにグラフであればライブスクリプトで何もせずともアニメーションができます そう…  Ra   ならね ただですね…普通にやると  imes     回やることになります CPUがぶちぎれます…    ■ステップ数決め 素因数分解 なのでここでは   回 を素因数分解して最初と最後がピッタリ終わるように飛ばし飛ばしで計算します から始まるので       を素因数分解します 行で素因数分解できるのやばくね それじゃあ なんとなく imes ステップずつ計算しますか      ■画像と空間周波数スペクトルの関係  周波数帯ではこういう風に足されてます ま まあそれっぽくでたからいいか笑  さいごにつぎあたりフィルタ STFT ほかの記事 音のA特性とかもやりたいな それでは,12,2022-02-02
269,269,画像をコラージュする,画像処理,https://qiita.com/john-rocky/items/76ee25daafe7609e39e3, 画像を組み合わせて一枚の画像を作る方法です 複数の画像から新しい画像を作りたい複数の画像を組み合わせて一枚の画像にするとなると はて どうすればいいのか   と改めて思いました  Contextに描画すればいい   swiftUIGraphicsBeginImageContextWithOptions CGSize width  outputWidth  height  outputHeight   false    imageA draw in  drawRectA imageB draw in  drawRectB imageC draw in  drawRectC let assembledImage   UIGraphicsGetImageFromCurrentImageContext  UIGraphicsEndImageContext  フリーランスエンジニアです お仕事のご相談こちらまでCore MLやARKitを使ったアプリを作っています 機械学習／AR関連の情報を発信しています ,0,2022-01-31
270,270,論文の勉強7 GoogleNet(Inception V1),画像処理,https://qiita.com/tanaka_benkyo/items/a4d64a168281195b2d76,GoogleNetについて構造の説明と実装のメモ書きです ただし 論文すべてを見るわけでなく構造のところを中心に見ていきます 勉強のメモ書き程度でありあまり正確に実装されていませんので ご了承ください 以下の論文について実装を行っていきます タイトル：Going deeper with convolutions    GoogleNet     Inception module× × ×の畳み込み層 そして×のMaxPooling層のそれぞれの出力を結合してつの出力とします      dimension reduction× ×の畳み込み層の前にチャンネル数を削減するために×の畳み込み層を追加します さらにMaxPooling層の後にも×の畳み込み層を入れることでチャンネル数を変換します   image png    image png       auxiliary classifier最後の出力層とは別にinception moduleaとdからの出力を使用した分類器を用意します つの出力に重みをつけて足し合わせて学習で使用する損失とします 重みは最後の出力層は 中間層からのつは とします 推論時にはこれらの中間層からの予測結果は出さないようにします      実装     kerasまず 必要なライブラリのインポートをします 畳み込み層の一連の流れをまとめてクラスにします 途中の層の出力を使用した分類結果を出力させるInceptionAuxを実装します つの枝分かれを最後に結合します GoogleNet本体の実装を行います モデルの構造を確認します 学習の設定をします つの出力に対してそれぞれ損失関数を指定し 重みも設定します 学習時はつの出力が出てきます それぞれにデータを指定します pytorchでもほぼ同様の実装となります 畳み込み層の実装構造の確認学習の際はつの出力それぞれの損失を計算し 重みをつけて足し合わせます       New  テストデータに対するエポックごとの処理学習の実行はしません かなりいい加減な実装ですので あまり参考にならないと思います ,5,2022-01-30
271,271,Laravelでの画像取得処理いろいろ,画像処理,https://qiita.com/manabuQiita/items/b47de64f17dc43181021,  背景Laravelでの画像取得処理を実装したときの個人的メモです    画像の保存方法  画面などからアップした画像を保存する  インターネット上に保存されている画像を保存する   画像の保存先以下つが考えられるが  の方がメジャーなようなのでそちらで実装する   画像ファイルをDBにそのままバイナリデータとして保存する  画像ファイルはサーバ上に保存しておきDBにはファイル格納パスだけを保存する データベースに画像を保存するのはありでしょうか        画面からアップした画像を保存する アバター画像をアップロードする           中略       リクエストオブジェクトのfileメソッドでname  avatar を取得し アップロード時に指定したオリジナルの名称を取得       リクエストオブジェクトのfileメソッドでname  avatar を取得し storeAsメソッドで名前を付けて画像ファイルをサーバに登録 引数はパス ファイル名        保存したファイルパスをDBに更新     インターネット上に保存されている画像を保存する画像ファイルの保存先URLがDBに格納されており それを使って画像ファイルを取得 保存し DBにパスを更新する という場合        中略          画像データ取得       画像データ保存       画像ファイルパスを作成  image idは一意の名前が格納されている        Storage Facadeを使い ドライバを指定してファイルを保存 ドライバの指定なしだとデフォルトのLOCALドライバが使われる ストレージドライバはconfig filesystems phpの設定による        DBカラムに画像ファイルパスを更新  userはuserモデル Laravelには大抵の処理にはヘルパ関数があるのでphpオリジナル関数の file get contents をつかわないとできないところが個人的に引っかかりました 何かベストプラクティスあれば教えて頂けると幸いです    参考 Laravel  x ファイルストレージ  ,1,2022-01-30
272,272,ピクセルごとの色相をEXCELで計算,画像処理,https://qiita.com/nekooosaka/items/8ee45ae5ebb29ee62b86,  ピクセルごとの色相をEXCELで計算し出力する   発端何かそんなツールがありそうなのですが 見つけられなかったのでやってみました 画像で使われている色の偏りを調べたかったからです 今回はあくまでも色相だけを見ています    RGB値の取得RGB値をピクセル毎に保存するツールを探したのですが GIMPを使って ppm形式だとよさそうでした      GIMPについて     ppmファイルについて画像をエクスポート＞PPM形式で出力  image png     ppmファイルをテキストファイルに変換するppmのファイルを選択し 拡張子を txtにします 拡張子を表示していない場合はOSの設定で変更してください txtに変換したファイルを開き 最初の行ないし行程度を削除します 空き行が無い方がいいかと思います   image png  画像は削除した部分をハイライトしています 削除後 保存します GIMPの場合は上から行を削除しました    内容を整形する正規表現で検索置換できるツールをつかってさきほど保存したファイルをひらきます MacですのでCotEditorを使っています このデータはがR がGとつの数字でピクセルとなっています まず 改行をカンマに変更します 検索ワードに　\n 改行 を入力 置換ワードに    カンマ を入力し置換します  変換前   image png   変換後   image png  R G B R G B      とデータが並んでいます これではエクセルで一気に変換できませんので R G B R G B R G B とつ区切りにします 検索置換で 検索ワードに                   　を 置換ワードに    \n を入れて置換します  変換前   image png   変換後   image png  これで 左からR G B  改行 のデータになりました ここでファイルを保存します ホソンしたファイルの拡張子を csvに変換します これでEXCELで読み込む準備ができました    EXCELで開く保存したCSVファイルをEXCELで開きます 行目に行を追加し AセルにR BセルにG CセルにBと入力しておきます   image png  RGBからHSBなどへの変換公式は検索してみてください まず最大値を取り出します   MAX A C 次に最小値を取り出します   MIN A C   image png  次にRGBのうち 最大となったセルがRかGかBかを判定します 　EXCELのINDEXやMATCH関数を使います  INDEX  A   C   MATCH MAX  A  C   A  C     image png  最大の値がRかGかBかで変わりますので IFを使ってHの値を計算します 負の値の場合を計算します 今回はおおかまな割合を見たいので 度ずつに分割します 度ずつに分けたピクセル数をカウントします   image png  円グラフにします   image png  度 〜度 が と多いですね 元画像 職場なのでぼかしてます はこちらです   image png  おつかれさまでした ,0,2022-01-27
274,274,論文の勉強6 ResNeXt,画像処理,https://qiita.com/tanaka_benkyo/items/9c3cd325038e7fa35e63,ResNeXtについて構造の説明と実装のメモ書きです ただし 論文すべてを見るわけでなく構造のところを中心に見ていきます あまり正確に実装されていませんので ご了承ください 以下の論文について実装を行っていきます 実装としてはResNetとほぼ同様です 畳み込み処理を一部grouped convolutionに設定したものとなります 以下のようなネットワークを考えます 入力をチャンネル方向に分割しそれぞれ変換を行い それを足し合わせるというものです  C は関数の数でCardinaryと呼びます   今回は T i はすべて同じ形式として bottleneck構造を使います それぞれの T i の最初の×の畳み込み層ではチャンネル数を減少させます   と書くことができます      grouped convolutionこれらの構造はgrouped convolutionを使って簡潔に表すことができます それを表したものが下の図です   image png  最初の×の畳み込み層はつにまとめることができます   そして複数のグループに分けて畳み込みを行い結合する操作は grouped convolutionで同様の結果を得ることができます 図の c はResNetなどで扱ってきた構造と同じ形になっています 構造の詳細は以下のようになります   image png       学習学習はSGDを使用し weight decayは でmomentumは  学習率は から始めて文献  通りにで割っていきます      実装ResNeXt の実装を行います ResNeXtブロックの実装をします ResNeXtの本体を実装します           入力層          出力層学習の設定を行います    python  学習率を返す関数を用意するパラメータ数も論文のものとだいたい同じになりました 学習は実行しません ResNeXtブロックの実装はkerasとほぼ変わりません 本体の実装を行います 構造の確認をします 学習の設定をして実装を終わります       New  テストデータに対するエポックごとの処理ここまで見ていただいた方 いい加減なまとめと実装で申し訳ありません ,0,2022-01-23
275,275,モダン開発環境構築,画像処理,https://qiita.com/openfields/items/37a032fa286b7bd49a78,  モダン開発環境構築アプリケーション 組み込み 画像処理をWin Mac Linuxのマルチ クロスプラットフォームで開発することが多いので その開発環境構築手法をまとめてみようと思います現時点    ではCMakeのみ記載してますが 今後IDE パッケージ クロスコンパイルについても記載していこうと思いますそれぞれの技術がネストし合っているので 全て揃って完璧になる感じはあるので少々お時間 お付き合いください  CMake編         インストール           変数 環境変数 分岐 ループ           ダイナミック スタティックリンク      書く理由画像処理エンジニア 組み込みエンジニアと仕事をすることが多いのですが  自分の環境ではビルドできる 環境を作る人が非常に多いので 少しでも正しくキャッチアップできるように情報発信できればと思います組み込み業界は未だにサクラエディタや秀丸を使う人も多々いますインチのモニタで作業することを苦と思わない人も多いです私の経験上 夜遅くまで仕事して休日出勤も多く外の世界を知る時間もないので そんな環境に慣れてしまっているのだと思います私の環境構築を参考にしてもらえれば 開発速度 他人との共有 整理など段違いでレベル上がります,4,2022-01-20
276,276,pythonでGANの勉強2 変分オートエンコーダ・畳み込みVAE,画像処理,https://qiita.com/tanaka_benkyo/items/073e32fea0b4780fffaa,pythonでGANの勉強をしていきたいと思います 自分の勉強のメモとなります コードが見づらかったり 正しくない場合があるかもしれません 実装については怪しい部分がありますので何か気づいたらご指摘いただければと思います 前回はオートエンコーダについて学びました 今回は 変分オートエンコーダについて勉強していきます  はじめてのディープラーニング をもとにkerasやpytorchでも実装してみるという流れとなります     変分オートエンコーダ   VAE 変分オートエンコーダでは 潜在空間は学習された平均値と標準偏差を持つ正規分布として表現される まずエンコーダにより入力から平均ベクトル \mu と分散ベクトル \sigma を求める これらを基に潜在変数 z が確率的にサンプリングされ  z からデコーダにより元のデータが再現される この z を調整することで連続的に変化するデータを生成できる      潜在変数のサンプリング潜在変数とは入力の特徴をエンコーダを使ってより低い次元に減らしたものである 平均値 \mu と標準偏差 \sigma を出力し これらを使った正規分布により潜在変数 z をサンプリングする      Reparametrization Trickサンプリングする処理はそのままではバックプロパゲーションを適用できない そこで Reparametrization Trickという方法では 平均値 標準偏差の正規分布からサンプリングされた \epsilon を使って以下のように潜在変数を定義する    mathz \mu \epsilon\sigmaこの式を利用することで バックプロパゲーションが適用できるようになる      誤差 潜在変数がどれだけ発散しているか  正則化項  E  reg  と 出力が入力からどれだけずれているかを表す再構成誤差  E  rec  を合わせてVAEの誤差とする       再構成誤差 x  ij  はVAEの入力  y  ij  はVAEの出力  h はバッチサイズ  m は入力層 出力層のニューロン数となる   ここで  \Sigma を省略して以下の通りに表す この誤差は 交差エントロピー と呼ばれる       正則化項 h はバッチサイズ  n は潜在変数の数  \sigma  ij  は標準偏差  \mu  ij  は平均値である  \Sigma を省略して以下の通りに表す     実装     平均値 標準偏差を出力する層標準偏差に関しては層の出力を標準偏差の乗 分散 の対数を表すこととする      サンプリング層順伝搬はReparametrization Trickの式z \mu \epsilon\sigmaに基づいて行われる これを書き換えると z \mu \epsilon\exp\frac \phi   となる   逆伝播を考えるため 誤差を以下の形で表す  \mu での微分は次のようになる  \phi による微分は次のようになる      出力層逆伝播では と計算できる 学習を実行します 今回はつの潜在変数を用いているので 平面上にプロットを行います つの潜在変数により同じ数字が固まってクラスタを形成していることが分かります 次に潜在変数を連続的に変化させて出力の変化を確認してみます 徐々に数字が変わっていくことが確認できます      pytorchまずはpytorchで実装を行います 必要なライブラリのインポートをしておきます データの準備をします 使用したデータは前回のオートエンコーダのときと同じものです エンコーダとデコーダの実装します エンコーダでは平均と分散の種類を出力するようにします デコーダはつの潜在変数を受け取り入力と同じ形で出力します 次にVAE本体の実装をします ここで サンプリング層と損失関数の定義を行います 学習の設定を行います 学習を実行します 学習の様子を可視化します 次に潜在変数を連続的に変化させて出力の確認をします 潜在変数をプロットしてうまく学習できているか確認します 同様に実装を行います まずは 必要なライブラリのインポートをします エンコーダとデコーダの実装をします VAE本体の実装をします 誤差をadd lossを使って定義しています  WARNING が出てしまうので あまりよくないかもしれません モデルを定義します    pythonmodel   VAE  model compile optimizer  adam  学習を実行します 学習の様子を可視化します 潜在変数を連続的に変化させて出力の変化を確認します 潜在変数をプロットして様子を見てみます     畳み込みオートエンコーダ今までは入力は画像をベクトル化したものを使用していましたが ここでは画像のまま入力できる畳み込み処理をいれた変分オートエンコーダの実装を行います 実装の内容自体はさきほど実装した変分オートエンコーダに畳み込み処理を追加するだけです pytorchとkerasのみで実装を行います      pytorchエンコーダでは Convd を使ってチャンネルの増加と画像サイズの削減を行い デコーダでは ConvTransposed を使ってチャンネルの削減と画像サイズの増加を行い元のサイズに戻します VAE本体はほぼ変わりません 損失を計算する際に 画像をベクトル化します データは画像のままで使用します 学習の設定をします 学習を実行します 学習の様子を可視化します 潜在変数を連続的に変化させたときの出力を確認します 潜在変数をプロットしたときの様子を確認します kerasもほぼ同様です エンコーダでは ConvD  デコーダでは ConvDTranspose を使って実装を行います 本体の実装をします 損失の計算をするときは画像をベクトル化します モデルを定義します    pythonmodel   ConvVAE  model compile optimizer  adam  データは画像のまま入力し 学習を行います 学習の様子を可視化します 潜在変数を連続的に変化させたときの出力の変化を確認します 最後に潜在変数をプロットしたときの分布の確認を行います ところどこと淡しい部分はありますが 概ね期待する結果が得られました 参考文献,0,2022-01-18
277,277,pythonでGANの勉強1 オートエンコーダ,画像処理,https://qiita.com/tanaka_benkyo/items/ca46cc68b1682827b6e7,pythonでGANの勉強をしていきたいと思います 自分の勉強のメモとなります コードが見づらかったり 正しくない場合があるかもしれません まずは オートエンコーダについて勉強していきます  はじめてのディープラーニング をもとにkerasやpytorchでも実装してみるという流れとなります     オートエンコーダオートエンコーダはエンコーダとデコーダのつの部分からなります   エンコーダ　訓練済みのエンコーダを使って 最初のデータ表現 例えば画像 x を入力とすれば その次元を ilde y   を ilde z   に減らす    潜在空間  z  　ネットワークを訓練するにあたって 潜在空間に何らかの意味が形成される　通常は入力より小さな次元で 中間ステップとして動作  デコーダ　元の表現と同じものを 元の次元で再構築する  　このステップにより z は x   に変換される  オートエンコーダの訓練は以下のように行われる   画像 x を取り出しオートエンコーダに入力する   x \   が出力される これは再構成された画像である   再構成誤差を計算する これは x と x \  の差である 　これは x と x \   の距離 例えばMAE として計算され 明示的な目的関数   x x \    が定義され 勾配降下法により最適化可能となる     実装まずはnumpyだけでの実装です 一番下にのせた参考書をもとに作成します まず 各層の実装です 中間層と出力層のみとなります 次に 順伝搬と逆伝播 パラメータ更新の定義をします 学習を行います 結果の確認をします 上の行は入力画像 真ん中の行は潜在空間 一番下の行が出力結果です 入力画像が再現され散ることがわかります      pytorch同じことをpytorchで実装してみたいと思います まず必要なライブラリのインポートをします データの準備ですが さきほどと同じデータを使用します オートエンコーダを定義します モデルの設定を行います    pythondevice   Nonemodel   AutoEncoder device device  to device criterion   nn BCELoss  optimizer   optimizers Adam model parameters   学習を実行します 学習の様子を可視化します 入力画像と出力画像の比較を行います kerasで同じものを実装します まず必要なライブラリのインポートをします データは他のものと同様のものもを使用します モデルの作成をします Sequential  を使用します モデルをコンパイルします 学習を実行します 学習の様子を可視化します 入力画像と出力画像を比較します 参考文献,0,2022-01-18
278,278,im2colの実装で出てくる式をわかりやすく視覚化してみます,画像処理,https://qiita.com/008_3104/items/b5e825c30f554190be70,有名な  ゼロから作るDL    第章に出てくる imcol  imcolの定義式について問う問題が 某E資格対策講座の教材で出題されましたので 図に書いて自分用にメモを取りました  視覚的に なんとなく 　わかってもらえたら幸いです  imcolのソースコードはどなたでも閲覧可能です   リンク先githubの〜行目が該当します imcolの概要については  ゼロつく 　やその他にお譲りします   今回はこのソースコード中のという式   だけ   に注目して 解説します  せまい  一応 E資格講座の質問担当様にも 理解に間違いがないかチェックしていただきました この式 一見簡単そうに見えますが よくよく見ると   　となるのはぼくだけでしょうか まずは簡単のため 前提条件はこんな感じとします    × の画像がRGBのチャンネルあるとします ストライドやフィルターの大きさは以下の通り pythonでは 例えば    という表記は  列目から列目まで つ飛ばしで という意味です の式の右辺の意味は  までで   あくまでなんとなく   ご理解いただけたかと思います では この右辺＝左辺　とはどういうことか 　というと   つの図は それぞれ右辺にあたるものが左側 　　　　　　　　　　左辺にあたるものが右側  にあるのでご注意ください   image png  以上が の視覚的な理解になります   おまけ  なお その次に続く 最後の式は 詳細までは力尽きて視覚化できませんでしたが 以下の図のような変換をしている式とのことです    x y      の時の例だけ載せています    image png  なんでこの式でこういう変換になるのか 途中を図示できる方はぜひぜひお願いします  ぼくにはわからない  おしまいです,3,2022-01-15
279,279,【画像処理】Numpyでテンプレートマッチング,画像処理,https://qiita.com/aa_debdeb/items/a3905a902263402ab8ea,Numpyでテンプレートマッチングを実装してみます まず マッチングを行う画像を読み込みます テンプレート画像を用意します  R  ssd  x y  は位置  x y  におけるSSDの値  I i j  と T i j  はそれぞれ位置  i j  におけるマッチング対象の画像の画素値とテンプレートの画素値   N M  はテンプレートの大きさとなります SSDが最も小さい箇所がテンプレートと最もマッチする箇所になります そこに四角形を描画しています 四角形の描画に関しては以下の記事を参照してください SSDと同様にSADの値が最も小さい箇所が最もマッチする箇所なので そこに四角形を描画します NCCは最も値が大きい箇所が最もマッチする箇所になるので そこに四角形を描画します ,1,2022-01-14
280,280,論文の勉強5 DenseNet,画像処理,https://qiita.com/tanaka_benkyo/items/a1e65a402af290010e2e,論文の勉強をメモ書きレベルですがのせていきます あくまでも自分の勉強目的です 構造部分に注目し その他の部分は書いていません ご了承ください 本当にいい加減 不正確な部分が多数あると思いますのでご理解ください 今回は 以下の論文のDense Netの実装を行います タイトル：Densely Connected Convolutional Networks    DenseNets    構造 \boldsymbol x   が畳み込みネットワークを通過することを考える 特徴マップのダウンサンプリングを行うため dense blockの間に 畳み込みとプーリングを行うtransition layerを挿入します この層はBN ×Conv ×Pooligで構成されます 入力のチャンネル数が m のとき transition layerでの出力を mheta とします ここで   heta\leq とし   heta  のときDenseNet Cと呼びここでは heta   とします   image png       Growth rate H l が k 個の特徴マップを作り出す場合 第 l 層への入力は k  k l   チャンネルとなります ここで  k  は入力層のチャンネル数である この k をgrowth rateと呼ぶ      bottleneck layersdense blockは k チャンネルの特徴マップを出力するが これは入力と比べて少ない dense blockの×Convの前に×Convを追加して bottleneck  ×Convへの入力チャンネル数を減らして計算効率を改善する bottleneck layersの構造は BN ReLU ×Conv BN ReLU ×Convとなり DenseNet Bと表します 今回は×Convで ×k 個の特徴マップを作ることとする      Implementation Details最初のdense blockに入力する前に 入力をチャンネル ConvNet BCの場合は growth rateの倍 に変換します      学習SGDで学習し 学習率は とする エポックが全体のエポック数の   となったら を掛けて減衰させる weight decayは とし Nesterov momentumを とする rateを としたdropout層を各畳み込み層の後に入れる      実装 k  とし DenseNet を実装します 使用する構造はbottleneck構造を持ったDenseNet BCです      keras必要なライブラリのインポートを行います まず BN ReLU Convのお決まりの流れを定義しておきます 回の畳み込み処理を行います Dense Layerを繰り返し 各Layerの出力を最後に結合します 畳み込みとプーリングを行い 特徴マップを変形します DenseNetの本体の実装を行います 構造の確認をします 学習の設定を行い 実行はせず実装を終わりにします    python  学習率を返す関数を用意する学習済みのモデルが提供されています 出力層を付け加えて使用します   学習率を返す関数を用意する実装の内容はkerasとほぼ同様です 必要なライブラリのインポートを行います 各モジュールの定義を行います モデルの構造を確認します     中略学習の設定を行い 実装を終わりにします       New  テストデータに対するエポックごとの処理pytorchでも学習済のモデルが提供されています       New  テストデータに対するエポックごとの処理これでDenseNet論文のメモ書きを終わります ,0,2022-01-13
281,281,【画像処理】NumpyでDoGフィルター,画像処理,https://qiita.com/aa_debdeb/items/c9e6ccecbb1b891f289c,NumpyでDoG Diffrence of Gaussian フィルターを実装してみます DoGフィルターでは  \sigma の値が異なるつのガウシアンフィルターをそれぞれ適用し その差分を求めます まず DoGフィルターを適用する画像を読み込んでグレースケール画像にします ガウシアンフィルターのカーネルを作成する関数と畳み込みを行う関数を定義します ガウシアンフィルターの詳細は以下の記事を参照してください   画像処理 Numpyで平滑化フィルター \  Qiita  DoGフィルターを適用します ,0,2022-01-13
283,283,python+opencvで画像処理の勉強5 幾何学的変換,画像処理,https://qiita.com/tanaka_benkyo/items/5840a36d0e97a8498388,pythonとopencvを使って画像処理を勉強していきます 前回python opencvで画像処理の勉強 周波数領域におけるフィルタリング周波数フィルタリングについて学びました 今回は幾何学的変換について学び 最後は応用として画像のつなぎ合わせを行います 使用画像は旭川動物園のペンギンの散歩の写真です   image png     線形変換    線形変換の一般系座標  x y  の位置の点が 変換により座標  x  y   に移動するとする そのとき 以下の式で表される変換を  線形変換  と呼ぶ     拡大 縮小  拡大 縮小  は 以下の式で表される ここで s  x  s  y  はx方向 y方向の拡大 縮小 率である     回転原点を中心に 反時計回りに角度 heta だけ  回転  する変換は 以下の式で表される ここでは 画像の左下を中心に回転した例を示す     鏡映ある直線に対して 対象な位置に反転する変換を  鏡映  と呼ぶ x軸に関する変換y軸に関する変換直線y xに関する変換    スキュー長方形を傾けて平行四辺形にするような変換を  スキュー  または  せん断  と呼ぶ x軸方向へのスキューは以下の式で表される 傾ける角度を heta とすると  b an heta  の関係がある また y軸方向へのスキューは以下の式で表される 傾ける角度を heta とすると  c an heta  の関係がある     合成変換ここまでに示した各種変換は ベクトルと行列を記号で置き換えることで 以下のように表される    math    \boldsymbol x   \boldsymbol A \boldsymbol x ここで 線形変換は×の行列 \boldsymbol A  で表現される 座標 x が変換 \boldsymbol A  により 座標 \boldsymbol x   に移ったとすると以下のように表される    math    \boldsymbol x   \boldsymbol A \boldsymbol x さらに 座標 \boldsymbol x   が 変換 \boldsymbol B  により座標 \boldsymbol x    に移ったとすると 以下のように表される    math   \boldsymbol x    \boldsymbol B \boldsymbol x  以下の式が導かれる    math    \boldsymbol x     \boldsymbol BA  \boldsymbol x  \boldsymbol Cx ここで  \boldsymbol C  は \boldsymbol C  \boldsymbol BA  なる×行列であり これもつの線形変換を表すことになる 任意の線形変換の組み合わせは やはり線形変換となる 一般に 個々の順番を入れ替えると 結果が異なることに注意する    同次座標とアフィン変換 射影変換    平行移動x軸方向の移動量 t  x   y軸方向の移動量を t  y  としたとき この  平行移動  は以下の式で表される 単純なものであるが 線形変換の一般系で表現することはできない     同次座標座標  x y  に対し 要素をつ増やした座標  \xi    \xi    \xi     を 以下の関係式を満たすように定義する ただし  \xi    \xi    \xi    の少なくともつはでないとする このように定義される座標を  同次座標  と呼ぶ 通常の座標に直したとき ともに  \frac \xi     \xi     \frac \xi     \xi      となる 同次座標系においては 定数倍をしても変わらないとみなせる このような関係を  同値  であると呼び 以下のように表す ベクトル表記では 頭に を付けて表す     アフィン変換同次座標を利用すると 平行移動は以下の式のように表すことができる また 線形変換は 同次座標を用いると以下のように表現される さらに 任意の線形変換と平行移動を組み合わせた変換を  アフィン変換  と呼ぶ アフィン変換の一般系は以下のように求めることができる アフィン変換の特殊な例で 任意の回転と平行移動を組みわせたものを  ユークリッド変換  と呼ぶ 対象の大きさを保ったまま 任意に移動するような変換であるユークリッド変換に対し 縦横の倍率が等しい拡大 縮小を加えたものを  相似変換  と呼ぶ     射影変換同次座標を用いて さらに一般的な変換を以下の式で表現することもできる この変換を  射影変換 ホモグラフィ   と呼ぶ これをベクトルと行列の記号を用いて表すと ただし 座標  x  y   を求めると     合成変換線形変換の合成変換の性質は 同次座標を利用した射影変換による合成変換に対しても成り立つ    画像の再標本化と補間    画像の再標本化ディジタル画像に幾何学的変換を施すと 元の画像の画素位置は 一般に変換後は標本化位置からずれる 変換後の画像を再び縦横等間隔に標本化された位置の値の集まりとして表現するために   再標本化  が必要となる   変換後の出力画像におけるある画素位置に対し 適用する幾何学的変換の逆変換を行い 元の入力画像に対する位置を求める   上で求めた入力画像での位置は 一般に入力画像の画素位置からずれている そこで その位置の値を  補間  により周囲の画素値から求める   上の処理を 出力画像上のすべての画素位置に対して行う     ニアレストネイバー求めたい位置に最も近い画素位置の値をそのまま利用する補間法を   ニアレストネイバー  と呼ぶ 求めたい位置を  x y  とすると その位置の画素値 I x y  は以下のように得られる    math    I x y  f  x     y    ただし  f i j  は入力画像の位置  i j  の画素値を 記号   はガウス記号で   内の数字を超えない最大の整数値を返すものである 処理が単純で高速であるが 滑らかなエッジがギザギザになって現れるジャギーが発生しやすい     バイリニア補間求めたい位置  x y  の値 I x y  を まわりの点の画素値を用い 以下のように求める これを  バイリニア補間  と呼ぶ まわりの点の画素値の重み付きの平均値を求めることになるため 平滑化の効果が生じる そのため ニアレストネイバーのようなジャギーが目立たなくなるが エッジがなまってしまう傾向がある     バイキュービック補間  バイキュービック補間  では 求めたい位置  x y  の値 I x y  を まわりの点の画素値 f    f    \cdots f    を用いて 以下の式により求める ただし 関数 h t  は sinc関数を次多項式で近似するもので 一般に以下の式が用いられる バイリニア補間に比べて よりシャープで自然な画像が得られる    イメージモザイキング    イメージモザイキングとその概略処理手順複数の画像をつなぎ合わせてつの画像とすることを   イメージモザイキング  と呼ぶ 概略の処理手順は 以下のようになる   特徴点と検出のマッチング  幾何学的変換の推定  画像の幾何学的変換と合成    特徴点の検出とマッチング各入力画像に対し 各々から特徴点を検出する つぎに 検出された特徴点に対し 画像間で対応を求める このように 画像間で対応を求めることを  マッチング  と呼ぶ     幾何学的変換の推定イメージモザイキングでは射影変換が良く利用される は 定数倍を許した方程式であるため h     とおき 整理することで以下の式を得ることができる 未知数の数は個なので 対応する特徴点が組以上あれば解を得ることができる まず 各対応点から得られる式を以下に示すように並べて行列表現する    math    \boldsymbol Ah  \boldsymbol b ここで 最小二乗解は 以下の式で得られる    math\boldsymbol h   \boldsymbol A   T \boldsymbol A      \boldsymbol A   T \boldsymbol b マッチングの結果には 一般には  アウトライヤ  と呼ばれる誤った対応が多数含まれる アウトライヤに影響されずに何らかのパラメータを推定する方法として   RANSAC RAndom SAmple Conensus   と呼ばれる方法がよく利用される RNSACによるパラメータの推定の手順を 以下に述べる     パラメータを推定するために必要最小限のデータをランダムに選択する 上の場合 組の対応点を選択する   選択した組の対応点座標を用いて 射影変換のパラメータ h    h    \dots h    を求める   求めた射影変換パラメータを用いて 他の全ての対応点が正しく変換されているかチェックする ここで 正しく変換された対応点は  インライヤ  と呼ばれる 用いたパラメータに対するインライヤの数を記憶する    の処理を繰り返し 最後に最もインライヤの数が多い射影変換パラメータとそのインライヤを結果として出力する 画像をつに分けて片方を変形してつなぎあわせてみます ここで特徴点のマッチングなどを行いますが これは次回以降で説明をします     画像の幾何学的変換と合成画像をつなぎ合わせたとき 画像のつなぎ目が目立ってしまうことがある 画像のつなぎ目の位置を適切に選ぶか 重なった部分で両方の画素値を混ぜあわせる方法がある 後者の例として 両方の重なった部分でアルファブレンディングの考え方を利用し 画素値を重み付き平均する方法がある 例えば 画像の画素値を I     画像の画素値を I     画像の画像端からの距離を d     画像の画像端からの距離を d    とするとき 以下の式により つなぎ目を目立たなくする これで幾何学変換についての勉強は終わります     次回値画像処理    参考,6,2022-01-12
284,284,【画像処理】Numpyでハフ変換による直線検出,画像処理,https://qiita.com/aa_debdeb/items/63a51e148370986face2,Numpyでハフ変換による直線検出を実装してみます 直線を検出する画像を読み込みます エッジ画像を作成します ここでは Cannyのエッジ検出を用います Cannyのエッジ検出については以下の記事を参照してください 直線を次のように  r  heta  で表します  r は原点から直線までの最短距離  heta は原点からの直線までの最短地点を結んだ直線と原点がなす角度です    mathr   x\cosheta   y\sinheta上式よりエッジ画素  x e  y e  を通る直線は以下のようになります   r  heta  パラメータ空間を一定間隔で区切り エッジ画素ごとに heta を少しづつ変えながら r を求めます 求めた  r  heta  の組み合わせをカウント 投票 していきます 多数の投票が集まった  r heta  が検出された直線になります 検出された直線を画像上に描画してみます 直線の描画については以下の記事を参照してください これでハフ変換による直線の検出ができました ハフ変換による直線検出をつの関数にまとめると次のようになります ,0,2022-01-12
286,286,【MediaPipe】Holisticのランドマークの具体的な場所、座標取得・保存まとめ【Python】,画像処理,https://qiita.com/Esp-v2/items/9c671ad29a263ce3675e,自分用にMediaPipeのHolisticでlandmarkを取得した結果をまとめました holisticを使うと画像中の人物に対してpose  face  right hand  left handのランドマーク推定をまとめて行ってくれます  下準備  初期設定resultsにすべての結果が格納されています 以下 resultsからそれぞれのランドマークを抽出します  pose公式ドキュメント   に載っていますが 以下のつどちらを実行しても 姿勢推定からの 鼻のx座標 を取得できるようです 鼻は番目らしい 上記の   mp holistic PoseLandmark NOSE   は 以下の図のランドマークの名前に対応しています  NOSE のように大文字にするといいです 出典：また 以下のコードからランドマークの名前一覧を取得することもできます     poseのランドマークの名前一覧faceについては 以下の画像のように点取得しているようです 本当に小さすぎて見えませんが ランドマークに赤の数字で 〜のインデックスが書かれています それぞれに固有の名前はおそらくありません 出典：座標を出力してみます     顔の座標 x   y   z   e   x   y   z      x   y    z    ランドマーク〜のx  y  z座標が順番に表示されます 上記のように取得した座標を一個ずつ取り出して listに格納してみます handについては個々のランドマークに名前がついています 出典：右手と同様です  まとめ 画像パス に対してmediapipeを適用し 上記すべてのランドマークの描画と座標を取得します 以下のコードを実行するとランドマーク付き画像とDataFrameが出力されます  入力画像  test jpg  Photo by  出力画像  holisticLandmark jpg  両手 顔 ポーズの一部が検出できてます MediaPipeすごい   初期設定      画像を整形      結果を出力  顔のランドマーク          ランドマークを描画する    else     検出されなかったら欠損値nanを登録する  右手のランドマーク  左手のランドマーク  姿勢のランドマーク  imageに対してmediapipeでランドマークを表示 出力する      姿勢→顔→右手→左手の順番でランドマーク取得偉大な先輩とストイックな同期に感謝します ,19,2022-01-10
288,288,Excel VBA 画像挿入する（繰り返し処理をして画像一覧を作る）,画像処理,https://qiita.com/hayasakakenta/items/62a459b83fe858eae1bb, はじめに画像一覧を作って共有したい 貼り付け作業をまとめてやりたい そんなときの元ネタになればいいなと作成したマクロになります ここからのアレンジは様々なのでベースとして利用してください  目次  使い方  コード  コード補足  大量のファイルのフルパスの取得方法 使い方ファイルのフルパスを書いたセルを選択してマクロを実行します 画像のファイルパスを書いたセルを選択して実行すると 左隣のセルに画像を挿入します 画像挿入後 一つ下のセルを確認して同様に文字列が書いてあれば 下方向へ繰り返し処理をします    PNG     PNG   コード   VBScript VBAマクロ 画像を大量に挿入するマクロSub PicInsertMacro  Const PADDING   Dim WorkingCell As RangeSet WorkingCell   ActiveCell デバッグ用に行数カウント     デバッグ用     Debug Print WorkingRow         LastRow         WorkingCell Value     WorkingRow   WorkingRow        画像挿入    ActiveSheet Pictures Insert WorkingCell Value  Select     縦長か横長かでサイズ指定を変える      セルの真ん中に移動してきれいに配置する    Selection Top   WorkingCell Offset      Top    WorkingCell Offset      Height   Selection Height        Selection Left   WorkingCell Offset      Left    WorkingCell Offset      Width   Selection Width         次のセルに移動する    Set WorkingCell   WorkingCell Offset    Wend デバッグ用  コード補足基本的には画像一覧を作るためにセルは正方形に近いことを前提に作っています PADDINGで余白をつけていて 両サイドの余白の合計を指定する内容になっています 画像の縦長か横長かを確認して 大きい辺を基準にサイズを決定します   PNG     PNG    参考 大量のファイルのフルパスの取得方法  つのフォルダにファイルが入っている場合 同一フォルダ ファイルをすべて選択してから   Shiftキー  を押して右クリックすると   パスのコピー  が可能です    PNG    サブフォルダがある場合ファイルが入っている場合 サブフォルダあり コマンドで取得できます    bat dos  batdir  s  b  a d  対象フォルダ REM ファイル数が多い場合には結果をファイルに出力することもできます dir  s  b  a d  対象フォルダ     保存したいテキストファイルの保存先 dir ファイル一覧を取得します s サブディレクトリも取得します b　ファイルのフルパスを取得します  sと組み合わせない場合はファイル名のみ  a d　ファイルのみ表示します 　AllからDirectory フォルダ を除くので ALL Directoryです    PNG  ファイル出力をする場合 画面上に文字は出てこないですが ファイルに書き込まれています   PNG  閲覧ありがとうございました ,0,2022-01-09
289,289,論文の勉強4 PyramidNet,画像処理,https://qiita.com/tanaka_benkyo/items/46ae658a58b937e8a3fb,論文の勉強をメモ書きレベルですがのせていきます あくまでも自分の勉強目的です 構造部分に注目し その他の部分は書いていません ご了承ください 本当にいい加減 不正確な部分が多数あると思いますのでご理解ください 今回は 以下の論文のPyramid Netの実装を行います   タイトル：Deep Pyramidal Residual NetworksほとんどResNetと変わりません     PyramidNets     ResNetResNetなどでは 特徴マップのサイズが減少するタイミングでチャネル数を増加させていました ResNetでは  n 番目のグループの k 番目のresidualユニットにおける特徴マップのチャネル数 D k は となります ここで  n k \in\    \   は  k 番目のresidualユニットが属するグループのインデックスを表しています 同じグループに属しているユニットの特徴マップは同じサイズであり  n 番目のグループは N n 個のユニットを含むものとします 番目のグループは RGB画像を複数のチャネルに変換するつの畳み込み層からなります n番目のグループでは  N n 個のユニットを通して特徴マップのサイズは半分に チャネル数は倍となります  ここで  N はresidualユニットの総数であり となります チャネル数は \alpha   N ずつ増加 各グループのユニット数が同じであれば各グループの最後のユニットでのチャネル数は   n  \alpha  と計算できます  \alpha はどの程度チャネル数を増加させるかを表す因子です また  \lfloor　floor は床関数と呼ばれるもので その値を超えない整数を表します additiveのものと比べ input側ではチャネル数はゆっくり変化するが outputに近くなるほど急に変化します すべてのユニットでチャネル数が増加するので shortcut connectionではzero paddingでチャネル数を合わせます inputは層目でチャネルに変換され ユニットを通るごとに   ずつ増加していきます 各グループで   ×チャネルずつ増加し 最終的には   × チャネルとなります さらに baseのもので N  N n        で \alpha  のものを実装します   image png       学習最適化手法はSGD Nesterov で学習率は  エポック数がとのときに を掛けて小さくしていきます weight decayは   momentumを に設定し バッチサイズはとします      実装     keras必要なライブラリのインポートを行います bottleneckブロックの実装をします           層目 × 畳み込み処理は行わず 線形変換  チャネル数をbneck channelsにします          層目 × 畳み込み処理を行います          層目 × 畳み込み処理は行わず 線形変換  チャネル数をout channelsにします          層目 × 畳み込み処理は行わず 線形変換  チャネル数をbneck channelsにします          層目 × 畳み込み処理を行います引数でbottlenackとbaseを指定できるようにします ブロックの数の計算が変わってきます           入力層          出力層モデルの確認をします     中略次に baseバージョンです     中略学習用の設定までを実装して終わります    python  学習率を返す関数を用意するほとんどkerasと変わりません           層目 × 畳み込み処理は行わず 線形変換  チャネル数をbneck channelsにします          層目 × 畳み込み処理を行います          層目 × 畳み込み処理は行わず 線形変換  チャネル数をout channelsにします          層目 × 畳み込み処理は行わず 線形変換  チャネル数をbneck channelsにします          層目 × 畳み込み処理を行いますまず baseモデルの詳細を見てみます     中略次に bottleneckモデルの詳細を見てみます     中略学習用のclassを定義して終わります 不正確 わかりずらい部分が多々あり申し訳ありません 自分の勉強のための活動ですのでご了承ください ,1,2022-01-09
291,291,【画像処理】NumpyでHarisのコーナー検出,画像処理,https://qiita.com/aa_debdeb/items/6aefb8112a1347f681c9,NumpyでHarisのコーナー検出を実装してみます コーナーを検出する画像を読み込みます 画像にフィルターを畳み込む関数を定義します 詳細は以下の記事を参照してください   画像処理 Numpyで空間フィルタリング\ 畳み込み演算\  \  Qiita  Sobelフィルターで横方向 縦方向の勾配画像 I x   I y をそれぞれ作成します  I  xx    I  yy    I  xy  から局所領域で平均化した画像 S  xx    S  yy    S  xy  をそれぞれ作成します ここではガウシアンフィルターを使用しています ガウシアンフィルターについては以下の記事を参照してください   画像処理 Numpyで平滑化フィルター \  Qiita  画素  x y  における行列 \boldsymbol M  を次のように定義して R を求めます この R が一定値以上であれば その画素をコーナーとみなすことができます  R が一定以上の値の箇所を抽出して そこに円を描画します 円の描画については以下の記事を参照してください つのコーナー付近で複数の検出があるので  R が局所最大となる画素のみを残します 以上 Harisのコーナー検出によりコーナーを検出することができました これまでの操作をまとめて一つの関数にします ,10,2022-01-07
293,293,【画像処理】Numpyで図形描画,画像処理,https://qiita.com/aa_debdeb/items/8178fef706e8dc7e30d2,Numpyで画像に図形描画してみます まず 使用する画像を読み込んでおきます    円塗りつぶしの円です 線の円です    四角形塗りつぶしの四角形です 線の四角形です    直線二点間を結ぶ直線です ,0,2022-01-06
294,294,Tkinterで画像処理ソフトを作ってみた,画像処理,https://qiita.com/dem_kk/items/df8dd8088fc228aa3950,  はじめに以前自作のデータセットを作ったのですが その際GUI上で簡単にトリミングができたら便利だなと思ったところから開発をスタートしました 上記の例では全て統一したピクセル数の画像を用意していました ただ実際の現場では 元画像のサイズ自体もそれぞれ違うことがあり クリックで簡単に処理したかったのです 一気に作るのは自分の力量的にしんどかったので 前回の内容をまず作ってそれから改良する形になりました   環境  内容メイン画面とサブ画面を作っています メイン画面：トリミングしたい部分の選択 表示などサブ画面：トリミング幅の指定 ここでは正方形のみ          サブ画面の作成     指定した座標を中心にトリミング       メディア gif  切り取った画像も 動画内で表示されている枠の部分と同じになっていることがわかる   まとめ何とか画像処理ソフトとしては形になったのではないでしょうか 細かい部分はさておき  気力を使い果たしたので 今日はもうやる気が出ない sleeping ,2,2022-01-05
299,299,画像表示について,画像処理,https://qiita.com/Ryuichi_natsume/items/dbf3b4dcb5329815f1a4,   画像を表示させる方法について   qiita rbimgはImageの略で img要素を用いることで Webサイト上に画像を表示することができます img要素には 画像の場所を指定するsrc属性 Sourceの略   画像が表示されなかった場合に代替テキストなどを表示するためのalt属性 Alternativeの略 も併せて使用します ,0,2022-01-04
300,300,【画像処理】Numpyで幾何学的変換,画像処理,https://qiita.com/aa_debdeb/items/c85fc4c0511bac519881,Numpyで画像の幾何学的変換を実装してみます まず 使用する画像を読み込みます 画像の変換は同次座標を用いてxの行列で表現することができるので 画像に行列を適用する関数を定義します この関数内では 変換後の画像座標に対して逆行列をかけて原画像での位置を求め 二アレストネイバー法で補間しています この記事では画像の原点は左上になっています    平行移動x方向に t x  y方向に t y だけ平行移動させる変換を表す行列は次のようになります 平行移動を適用します    拡大 縮小x方向に s x 倍 y方向に s y 倍の拡大 縮小を表す行列は次のようになります 拡大 縮小を適用します    回転原点を中心に heta だけ回転させる変換を表す行列は次のようになります 回転を適用します    変換の合成変換を合成するには行列の積をとります ,2,2022-01-03
301,301,【画像処理】Numpyでモルフォロジー演算,画像処理,https://qiita.com/aa_debdeb/items/9404d481d7b01cb7b41b,Numpyでモルフォロジー演算を実装してみます まず 使用する画像を読み込んでグレースケール画像に変換します 二値画像も作成します 二値画像作成の詳細については以下の記事を参照してください   画像処理 Numpyで大津の二値化\ 判別分析法\  \  Qiita  前景 白 を一回り小さくするモルフォロジー演算を収縮 Erosion と呼びます 注目画素とその近傍画素の最小値を注目画素の値にします 収縮を二値画像に適用します 収縮をグレースケール画像に適用します 前景 白 を一回り大きくする処理を膨張 Dilation と呼びます 注目画素とその近傍画素の最大値を注目画素の値にします 膨張を二値画像に適用します 膨張をグレースケール画像に適用します 複数回の収縮のあとに 同じ回数膨張する処理をオープニング Opening と呼びます オープニングを二値画像に適用します オープニングをグレースケール画像に適用します 複数回の膨張のあとに 同じ回数収縮する処理をクロージング Closing と呼びます クロージングを二値画像に適用します クロージングをグレースケール画像に適用します 元の画像からオープニングを施した画像を引く処理をトップハット Top Hat と呼びます トップハット画像にはオープニングにより削除された値が残ります トップハットを二値画像に適用します トップハットをグレースケール画像に適用します クロージングを施した画像から元画像を引く処理をブラックハット Black Hat と呼びます ブラックハット画像にはクロージングより追加された値が残ります ブラックハットを二値画像に適用します ブラックハットをグレースケール画像に適用します ,4,2022-01-02
302,302,【画像処理】Numpyで大津の二値化(判別分析法),画像処理,https://qiita.com/aa_debdeb/items/bc31f5aeb1ee7af0dcea,Numpyで画像の二値化を実装してみます まず 二値化する画像を読み込み グレースケール画像に変換します 任意の閾値で二値化をする関数を実装してみます これは numpy where を用いると簡単に実装できます 次に大津の二値化 判別分析法 を実装してみます 大津の二値化ではクラス間分散 \sigma b  とクラス内分散 \sigma w  の比である分離度 \frac \sigma b   \sigma w   が最大になる閾値で二値化を行います 黒画素クラスの平均と分散を m  と \sigma    白画素クラスの平均と分散を m  と \sigma   とし 黒画素クラスと白画素クラスの所属する画素数をそれぞれ \omega    \omega  とすると クラス間分散 \sigma b  とクラス内分散 \sigma w  はそれぞれ以下のようになります 全分散 \sigma t  は閾値によらず一定で クラス間分散 \sigma b  とクラス内分散 \sigma w  と以下のように関係にあります このことから分離度 \frac \sigma b   \sigma w   は次のようになり クラス間分散 \sigma b  が最大のときに分離度 \frac \sigma b   \sigma w   も最大になることがわかります 実装としては すべての閾値に対してクラス間分散 \sigma b  を計算して クラス間分散 \sigma b  が最大となるとき閾値で二値化を行います クラス間分散 \sigma b  の分母は一定なので 実装では計算を省略します ,3,2022-01-01
1,1,opencvでの画像の前処理からndarray変換を並列処理で高速化する。,画像処理,https://qiita.com/ittsu/items/0888a5c2def1de56542e,  はじめに   自己紹介　大学年生で情報系の学科にいます AIに興味があって AIエンジニアのインターンをしたり NLPの研究をちょっとしたりしています もし記事に不備があればコメントで教えていただけると幸いです    背景　万枚以上のtif画像から データの前処理をする際に並列処理で高速化できないかなと思って調べながら実装してみました   並列処理とは 　非同期処理 並行処理 並列処理のつはかなり混同しやすい概念ですが 参考文献 \ \         の記事がわかりやすいなと思ったので 並列処理自体の話は下記のサイトにお任せします   実装   pythonにおける並列処理の記述 pythonで並列処理を記述するにはconcurrent futuresとmultiprocessingのつの標準ライブラリが使えます 今回はmultiprocessingを使用しており concurrent futuresには触れません なお multiprocessingを使用したのは    そのため IO関係での待ち時間が結構ある場合にはProcessを使ったほうが速くなるケースがある  らしい と \ \   に記載があったためです 今回は画像の読み込みが律速になりそうだったのでProcessを採用することに決めました    並列で処理したい関数　今回は単純化のために単なるresizeのみとしますが 仮に以下の関数の処理を並列化したいとします わざわざimg npでndarrayとして画像を保存していく目的はデータセットとして以後扱う際にデータサイズを落とす 画像のメタ情報を削ぎ落とす ためです    並列処理のスクリプト    img paths        画像のパスのリスト      共有メモリの解放    共有メモリの作成並列処理するプロセス間でデータを共有するには 共有メモリを使う必要があります multiprocessingにはValue  Array  RawValue  RawArrayという共有メモリ上でデータを保持するためのクラスが用意されています しかしながらいずれのクラスにおいても ctypes  オブジェクトしか保持できません ですので今回はshared memory SharedMemoryクラスを用いて適切な大きさのメモリを確保し np ndarrayのbuffer引数で行列を共有メモリ上に配置する形で実装しました    参考文献  \ \ その並列処理待った 　 Python 並列処理 でググったあなたに捧ぐasync  threading  multiprocessingのざっくりとした説明     \ \ Pythonの並列処理 並行処理をしっかり調べてみた  ,1,2023-12-31
3,3,画像認識の処理の手順,画像処理,https://qiita.com/kakuteki/items/3bfbbc81e92ff0257e5b,画像認識は コンピュータが画像や動画内の物体 パターン 特徴を 識別する事を言います 本記事では 画像認識の処理の手順に関して取り扱っていきます      画像の取得目的  画像の取得 方法  WEBカメラ デプスカメラ 画像 動画を取得して入力として使用      画像の前処理目的  画像を分析しやすくするために ノイズを減らし 画像を標準化する 手法  グレースケール変換 ヒストグラム均一化 ノイズリダクション エッジ検出 リサイズ      特徴抽出目的  画像から識別可能な特徴を抽出する      特徴選択 削減目的  分析のために最も有用な特徴を選択し 次元の呪いを避ける 手法  主成分分析 PCA  線形判別分析 LDA  オートエンコーダなど      分類 認識目的  抽出された特徴を使用して 画像内のオブジェクトやシーンを識別する 手法  k 最近傍法 サポートベクターマシン SVM  ディープラーニング CNN  ランダムフォレストなど      結果の解釈と評価目的  認識結果を解釈し 性能を評価する 手法  混同行列 精度 リコール Fスコア ROC曲線      フィードバックと改善目的  システムの性能を改善し より正確な認識を実現する 手法  追加データの取得 アルゴリズムの調整 新しい特徴の統合    追加の処理セグメンテーション  画像を複数のセグメントに分割して 特定のオブジェクトや領域に焦点を当てます  ROI データ拡張  トレーニングデータセットのバリエーションを増やして モデルの一般化能力を向上させます オブジェクト追跡  ビデオ内でオブジェクトが時間と共にどのように動くかを追跡します ,1,2023-12-31
4,4,ROIとは,画像処理,https://qiita.com/kakuteki/items/1525ea4c3d2c14cbdc0a,ROI Region of Interest は 関心領域 と訳され 画像処理 画像認識 機械学習などで用いられる用語です 具体的には 画像または動画の中で特に注目して 処理や分析を行いたい部分を指します ひと昔前では このROIを手動で設定していましたが 現在は 色検出や差分などを用いてROIを設定する事が主流です    ROIの使用例 物体検出  画像内の特定の物体や特徴を識別する際 その物体が存在する領域をROIとして特定し 分析の対象とします 画像処理  特定の操作やフィルタリングを画像の特定部分にのみ適用する場合 その部分をROIとして定義します 追跡  ビデオ内の特定の物体を追跡する際 フレームごとの物体の位置を表す領域がROIとなります 特徴抽出  機械学習やパターン認識で 特定の特徴を抽出するためにROIを用いて 画像の特定の部分に焦点を当てます    ROIの選択方法 手動  ユーザーが直接 画像上で関心領域を選択します 自動  アルゴリズムが特定の基準 例：色検出 差分 エッジ検出 に基づいてROIを決定します 事前定義済み  特定のアプリケーションやタスクに基づいて ROIがあらかじめ定義されている場合もあります  例 定点カメラなどで 特定の座標を見ていればよい場合 その場所を事前にROIとして設定する     ROIの利点 効率性  全体の画像ではなく 関心領域のみを処理することで計算時間と計算コストを節約できます 精度  分析や処理をROIに限定することで ノイズや不要な情報の影響を減らし 結果の精度を向上させることができます ,1,2023-12-30
5,5,qrcodeライブラリで簡単QRコード作成術,画像処理,https://qiita.com/kakuteki/items/3b24bd240da50553dc94,QRコードを生成するためには Pythonのqrcodeというライブラリを使用することで生成ができます 本記事では QRコードを生成し それを画像として保存する方法を解説します ↓実際に生成したQRコード必要なものPython  プログラミング言語として使用 qrcode  QRコードを生成するためのライブラリ      qrcodeライブラリのインストールまず 必要なライブラリをインストールする必要があります コマンドラインまたはターミナルで以下のコマンドを実行してください      QRコードの生成以下のコードは data変数に入力したデータのQRコードを生成し それを画像ファイルとして保存します   生成するQRコードのデータdata      QRコードを生成    box size      ボックスのサイズ  QRコードを画像として出力  画像を保存このコードはqr code pngという名前のファイルでQRコードを保存します data変数に任意の文字列やURLを入力して QRコードを生成できます ,1,2023-12-30
6,6,AIによる背景判定処理の実験 前編 Python、PyTorch編,画像処理,https://qiita.com/amaman/items/5af159135983568ea2ea,  はじめにVRデバイスのMeta Questでどの程度のAI処理ができるか知るために実験したので記事にしました 前編はPCで期待する効果が得られるかテストします 具体的にはPythonとPyTorchを使ってAI CNN 画像認識 により背景か前景かを判定します  後編  ではMeta Questで実際に動作させます アマレコVR※の背景透過処理※への応用を想定した実験となっています    note info※ アマレコVR私が制作しているMeta Quest用 VR動画プレイヤー※背景透過処理再生中の動画の画像をリアルタイムで解析し 変化のない部分を透明にする機能他の動画やパススルー映像と合成することができます  参考記事この実験は私個人のAIの勉強を兼ねています 以下の記事から多くを学び参考にさせていただきました PyTorchではなくkerasの使用例です 正解率が高かったのでCNNモデルを参考にしました PyTorchの使用例です こちらの記事にそって実験しています   環境  Windows  Python Ver     PyTorch Ver    dev cu  実験内容あらかじめ犬や猫 車などの写真 一般例 を学習してから 入力された画像 未知の写真 に何が写っているかを判定する 画像処理系AIの代表例であるCNN Convolutional Neural Network を使います 最初に 犬や猫の代わりに 背景   背景画像 に属する画像と 前景 被写体    前景画像 に属する画像を CNNモデル   cnnモデル を使って学習し保存します 次に入力された画像を縦横分割 全部で分割 した各エリアに対し 保存した CNNモデル   cnnモデル を使って背景と前景のどちらの特徴が強いか推論します 上手くいけば次のような結果が得られるはずです    note info  背景 被写体が含まれないエリア どちらの数値 背景 前景 も低いか 背景の数値が高くなる  前景 被写体が含まれるエリア 含まれる被写体の面積により 前景の数値が高くなる  結果使用したサンプル動画  学習に使った画像   背景画像動画のスクリーンショットから被写体が映っていないところを切り取って背景画像にしました    前景画像動画から被写体だけを切り取り さらにできるだけ背景が残らないようにマスクします こんな画像を枚作成しました  ピンクの部分は学習しません    実際に学習する画像それぞれの画像からxの領域を切り出して 一つ一つを小さい枚の画像として学習します   ニューラルネットワークの定義          画像処理        self feature   nn Sequential               ブロック              ブロック          平滑化        self flatten   nn Flatten            全結合最終的に非力なスマホで使うので 性能 画像認識能力 を多少削ってでも計算コストが低くなるように設計しています 具体的には コンボリューション回数やパラメーター数が少なくなるようにしています コンボリューションは最初のRGBのch入力 ch出力のフィルターを除き Depthwise Separable Convolution  を採用しました 性能はあまり変わらず 劇的にコンボリューション回数を減らすことができます 続いて 画像処理のブロックの最後をMaxPoolから GAP  へ変更しました これもパラメータ数を劇的に減らすことができます 約万パラメータ xxx  を程度 x  まで減らしています そのかわり性能が低下します 参考記事後編ではこのCNNモデルをMeta Questで実行します ,2,2023-12-30
7,7,【Python】四分木の中で最も複雑な領域を分割し続けるアートを実装してみた,画像処理,https://qiita.com/bvv/items/543404fb894dcf09292c,  はじめに数ヶ月前に  このツイート  が目に留まりました 非常に魅力的で 自分でも作りたいと思ったのですが アルゴリズムや実装が公開されているにもかかわらず 実際にやっている人が少ないようでした そこで 本記事では Pythonの画像処理ライブラリPillow PIL を使用して 四分木の中で最も複雑な領域を分割し続けるアートの実装方法について解説します   アルゴリズム以下の操作を再帰的に繰り返します   キャンバス上のすべての矩形領域の中から 最も複雑な領域を選んで四分割する   新しくできた矩形領域において画像の複雑度 score と平均色を求め 領域を平均色で塗りつぶす 詳しくは元記事を参照してください   実装   Rectクラス Rect クラスは 長方形のフレームの座標情報を保持するクラスです  calc area は長方形のフレームの面積を計算するメソッドです  Quad クラスは 四分木 Quadtree のノードを表すクラスであり フレームの座標情報 rect と複雑度 score  平均色 color を持たせています heapqで使用するため    lt   により比較演算子を定義しています  QuadTreeGenerator クラスは メインの処理を実装したクラスです 画像の読み込み 分割処理 結果の描画などが含まれています         self heap         すべてのquadを管理する優先度付きキュー        heapq heappush self heap  root       再帰的にを四分木の中で最も複雑な領域を分割し続けるメソッド 指定された回数分だけ Quad を生成し 優先度付きキューに追加します       新しく生成された Quad をキャンバスに描画して  self gif frames にフレームとして追加するメソッド       指定された Rect から Quad オブジェクトを生成するメソッド 最小サイズ以下であれば 分割を終了します       指定された Rect と色のチャンネルに対して 平均色と複雑度を計算して返すメソッド       指定された Rect と色のチャンネルに対して 元の画像の色の分布を numpy histogram として返すメソッド 平均色や複雑度の計算に利用されます       指定された Rect が最小サイズ以下かどうかを判定するメソッド       指定された Rect を四分割して生成された 新しい Quad のリストを返すメソッド        self gif frames のフレームを繋いで アニメーションGIFを生成するメソッド  heapq heappop は最小の要素を取り出すことに注意してください 今回は複雑度が最も高い Quad を取り出すために スコアをあらかじめ   倍しています    コード全体        self heap         すべてのquadを管理する優先度付きキュー        heapq heappush self heap  root       再帰的にを四分木の中で最も複雑な領域を分割し続けるメソッド 指定された回数分だけ Quad を生成し 優先度付きキューに追加します       新しく生成された Quad をキャンバスに描画して  self gif frames にフレームとして追加するメソッド       指定された Rect から Quad オブジェクトを生成するメソッド 最小サイズ以下であれば 分割を終了します       指定された Rect と色のチャンネルに対して 平均色と複雑度を計算して返すメソッド       指定された Rect と色のチャンネルに対して 元の画像の色の分布を numpy histogram として返すメソッド 平均色や複雑度の計算に利用されます       指定された Rect が最小サイズ以下かどうかを判定するメソッド       指定された Rect を四分割して生成された 新しい Quad のリストを返すメソッド        self gif frames のフレームを繋いで アニメーションGIFを生成するメソッド   使用例   生成例背景がベタ塗りで 複雑度にコントラストがある画像を選ぶと上手くいくようです   カモメの画像    カモメの画像    カモメのGIF    カモメのGIF    おわりにいかがでしたか ぜひみなさんも好きな言語で実装してみましょう   参考,47,2023-12-29
8,8,グレースケールのアルゴリズムの比較,画像処理,https://qiita.com/mogamoga1337/items/b04fba468042ac27dc4a,   きっかけ今までグレースケール処理はRGBの平均値を使っていたが調べてみるといろいろと方法があることを知り それらの違いを理解したいと思った    比較するアルゴリズム平均法 加重平均法 輝度法をそれぞれ比較していきたい    平均法 Average Method ：R G Bの値の平均をグレースケール値とします  グレースケール値    R   G   B       加重平均法 Weighted Average Method ：R G Bの値に重み付けをして平均を計算します  グレースケール値       R       G       B   輝度法 Luminosity Method ：人間の視覚システムに基づいた方法で R G Bの値を異なる重み付けで合算します  グレースケール値       R       G       B引用元：比較するときには以下のサイト 自作 を利用した    比較していく    RGBが極値の色たちおそらく最も違いが分かりやすい画像平均法は赤 緑 青は同じ色になっているが 加重平均法 輝度法は青 赤 緑の順に薄くなっている 特に輝度法の黄色はほぼ白に近い     虹のイラスト七色の虹のイラスト各層がきれいに識別できるのは平均法 加重平均法 輝度法は中心の色が同じ色のようになってしまい区別がつきにくい     普通の画像特に差があるようには見えない     普通の画像加重平均法 輝度法のほうがクラゲが見やすい     しもんきん平均法が濃く 輝度法は薄い 加重平均法は平均法と輝度法の中間くらいの薄さに感じる     Lennaさんしいてあげるなら輝度法は影と光のメリハリが強いように見える    どう使い分ける シンプルに変換したいなら平均法人間の目による光の感じ方を忠実に再現したいなら加重平均法 輝度法を使うといいんじゃないかな といった感じ 終わり    チラ裏 輝度法 は きどほう と読む ,0,2023-12-29
9,9,手軽に実現！OpenCVとWebカメラでQRコードの位置を特定する方法,画像処理,https://qiita.com/kakuteki/items/609d80473315a9c2207d,本記事では OpenCVを使用してWebカメラからQRコードの位置を特定します   スクリーンショット     png  QRコードの生成に関しては以下の記事を参照してください  QRコードの生成      必要なものPython  プログラミング言語として使用 OpenCV  画像処理を行うためのオープンソースライブラリ      環境設定まず Pythonがインストールされていることを確認してください 次に OpenCVをインストールします コマンドラインまたはターミナルで以下のコマンドを実行します pip install opencv python     カメラからの映像取得Webカメラから映像を取得するためのコードを書きます OpenCVを使ってカメラにアクセスし 映像を取得し表示します   カメラのキャプチャを開始      フレームをキャプチャする      フレームを表示する    cv imshow  QR Code Scanner   frame        q を押して終了  キャプチャを解放cap release  cv destroyAllWindows       QRコードの検出と位置の特定OpenCVには QRコードを検出し その位置を特定する機能が備わっています 以下のコードは カメラのフレームごとにQRコードを検出し その位置を四角で囲む方法を示しています また コマンドラインには検出したQRコードの座標を出力します   QRコード検出器を初期化      QRコードを検出し その位置を取得          デコードされたテキストを表示このコードは Webカメラからの映像に対してリアルタイムでQRコードを検出し その位置を四角で囲み デコードされたテキストをコンソールに表示します プログラムを終了するには  q キーを押してください ,2,2023-12-29
10,10,Python: tifffileを使って、画像をImageJで読み込める形の.tifファイルとして書き出す,画像処理,https://qiita.com/nyunyu122/items/d27bf7cdc8a4e92b0a1a,作成日：言語：Python  tifffileとはpythonで出力した画像をtif形式で保存したい時は ライブラリ tifffile を使えば書き出しができます tifffileは通常のtif以外にも バイオ系でよく使われる画像ソフトや顕微鏡画像のフォーマットにも対応していて メタデータも含めて読み書きすることができます  メタデータがない場合はscikit imageのio imsaveなんかでも事足りそう      参考リンク 元コード 作者のGitHub はこちら 冒頭に一通り使い方の説明が書いてあります   インストール  サンプルコード     画像作成    出力結果今回はz stackが枚 大きさが × のRGB画像を想定しています 蛍光顕微鏡で撮影した画像等を  tif形式でImageJで簡単に開ける形で保存したいとします 今回はRGB画像なので データ型をnp uint  bitの非負の整数 にしておきます      画像の書き出しtifは次元の画像ですが 次元の順番はTZCYXSの順番にしておきます TZCYXSはそれぞれTime  Z stack  Channel 撮影時のチャンネル数   Y height   X width   Sample カラー のチャネルを指します Cがカラーではなくchannel GFPとRFPでチャネルで撮影した時などに使う であることに注意 Sがカラー RGB S  やRGBA S  等のカラー画像を保存する際は 忘れずに画像のデータ型を bitにしておきましょう     参考リンク：TZCYXSの意味については 作者のGitHubを参照しました   ここに作者本人が回答していたのから遡りました scikit imageの古いdocumentationにも簡潔にまとまっています  何故scikit imageのdocumentationに説明があるのか気になって軽く調べてみたのですが tifffileは現在は独立のパッケージですが 以前はscikit imageの一部として実装されていたようです 私の手元のskimage   では import skimage external tifffile ができましたが 最新バージョンではskimage external tifffileというモジュールはなくなっているようです  import tifffile するのが安全なように思います    補足  tifffileのその他の機能 tifffileを使うと 画像全体の読み込み 保存だけでなく 画像を直接開くことなく一部だけ確認したり読んだりすることもできます 最初のスライスだけ読み込むとか 成分だけにアクセスするとかの操作で大きな画像をいちいち開く必要がなくなるので 処理を高速化できます     参考リンク：Qiita記事 再掲 作者の元コードの冒頭にも一通り使い方が書いてあるので こちらもご参照ください ,0,2023-12-29
11,11,OpenCVでwebカメラの映像を表示する,画像処理,https://qiita.com/kakuteki/items/e48b47e8ce77332bc051,画像認識においてpythonでwebカメラの映像を使用することがよくあります そこで今回は基本的なwebカメラの映像をOpenCVで取り込み表示するコードを書いてみたいと思います まず PythonでOpenCVを使用するためには OpenCVライブラリをインストールする必要があります pip install opencv pythonを実行することでインストールできます 以下のコードはウェブカメラの映像を表示するための基本的なコードです   ウェブカメラのキャプチャを開始cap   cv VideoCapture    キャプチャがオープンしている間続ける      フレームを読み込む          フレームを表示        cv imshow  Webcam Live   frame            q キーが押されたらループから抜ける  キャプチャをリリースし ウィンドウを閉じるcap release  cv destroyAllWindows  このコードでは でOpenCVをインポート cap   cv VideoCapture  でウェブカメラのキャプチャを開始 はデフォルトのウェブカメラを示します        フレームを読み込むでウェブカメラが開いている間 フレームを継続的に読み込みます cv imshow  Webcam Live   frame で各フレームをウィンドウに表示 でユーザーが q キーを押すと ウィンドウを閉じてプログラムを終了 ウェブカメラの番号 上記のコードでは は 使用するデバイスや設定によって異なる場合があります 複数のカメラがある場合は  などに変更して試してみてください また このコードを実行するには コンピュータにカメラが接続されていて 適切なドライバがインストールされている必要があります ,1,2023-12-29
12,12,ToFカメラについての重箱の隅,画像処理,https://qiita.com/nonbiri15/items/064bbce3a2347f307119,さきに  市販ＴoＦカメラについて調査中 年版   の記事を書いた それに関連して 重箱の隅のようなことを別途記載する ToF Time of Flight カメラで 容易に深度 depth が測定できるようになってきているから 買ってきて接続すれば十分だろうと思う方も多いかもしれない しかし そう簡単ではないことを記す    nm のToFカメラは 太陽直射光の下でも使える   nm を採用しているToFカメラも増えているので それを用いれば屋外でも使える   nmを使うと 大気中の水蒸気の吸収によって 太陽光中のnmの光が抑制されている そのため照射したnmの近赤外光の戻り値が埋もれにくい  新型ToFカメラ 屋外 太陽光下 での評価状況 第段      遠方での距離の算出が苦手  遠方からの戻り光が減衰しすぎると距離が算出できない   中にはm先でもdepthが求まるToFカメラもあるので検索してみてほしい  ただし高価    黒くて光を吸収 散乱する対象物が苦手  顔が計測できていても 頭部の髪の毛が欠損値になることがある   黒のズボンも ズボンの染料の種類 布地の種類によっては 欠損値になってしまう    ToFの近距離側の算出距離の限界がある    cmという近距離を必要とする使い方の場合には 各社の中でそれをサポートするToFカメラを探してほしい   この距離は ToFのイメージセンサだけでは決まらない   その製品ごとに違ってくる   ステレオ計測の場合でも近距離側の限界があるので 別にToFに限ったことではない    近赤外光を透過するガラスが苦手  ガラスへの距離を算出できない   ガラスの先の対象物への距離を算出してしまう    鏡も苦手  鏡に写っている対象物への距離を算出してしまう    黒光りするものも苦手  通常の物体の場合と違った奥行き画像になってしまうことがある    反射率の低い黒い物体は 奥行きの計算にも影響することがある  ToF特集 第話 ToFカメラでこんな被写体 撮影してみました   の記事を読んでみてください     Dカメラ比較例 Dカメラ 種類を 様々な環境で比較しました  その① 屋外編    Dカメラ 種類を 様々な環境で比較しました  その② 屋内編     ユーザーの取るべき手段  欠損値が生じたり 期待しない値になるような状況が ユースケースでどう影響が出るのかを考慮すること   超音波センサなど他のセンサの併用についても考慮すること    関連記事 超音波センサの使い方について調査中     追記：　座標系の対応付けRGBカメラとDepthカメラで 微妙に座標系が違っている場合には 以下の記事にあるようにDepth画像でRegstrartion 画素の対応付け を行うとよい qiita  ROSのカメラ画像処理に関するメモ  ,0,2023-12-27
13,13,Juliaで画像処理（グレースケール化）,画像処理,https://qiita.com/inoshun/items/4f76eb9b5941bbd4dc50,  はじめにJuliaで画像処理したくてOpenCVが使えることがわかり 試したためまとめます ついでに別の方法もあったので併せて紹介します   OpenCVまずはOpenCVのパッケージがすでに存在するということで パッケージを追加    juliausing PkgPkg add  OpenCV  それではコードを書いていきます めっちゃOpenCV感ありますね ただし OpenCVで画像を読み込んだ際に Pythonでは画像が表示されてたと思うんですが Juliaではピクセルデータで表示されます 関数を作成した場合は表示されません  まあ しっかりグレースケール化して保存はできます   Imagesもう一つの方法は Images というパッケージを用いるやり方です 下記のパッケージを追加しておきます    juliaPkg add  Images  Pkg add  ImageMagick  それではコードを書いていきましょう 割とわかりやすいですね  Gray は単一のデータに対して変換を行います 読み込んだ画像は複数のピクセルデータから成るので ブロードキャスト     を用いて 配列の各要素に関数を適用させる必要があります  Gray  img    Julia感あって個人的にはこちらの方が好きです   結果  test png    gray png  できてますね    まとめ今回はグレースケール化をしてみましたが 画像処理にはいろいろな処理があります OpenCVでやるのが楽なのか その他の方法でやるのが楽なのかはまだわかりませんが ケースバイケースで使っていけるようになりたいですね それでは   ,1,2023-12-25
14,14,RGBD画像でのセグメンテーションを調査中,画像処理,https://qiita.com/nonbiri15/items/86d76d58bf12843b66c1,近年 RGBD画像が取得できるセンサが増えてきているので セグメンテーションもRGBD画像で行なった方が 格段に精度を改善しやすいと予想する そこで RGBD画像を用いたセグメンテーションについて調査中である 引用元  末尾に示したリンク先上記のつの値が 学習結果の精度を評価する指標である    気にするポイント  入力：RGBD画像の大きさ  処理時間：  入力画像フォーマット：      色画像の部分が センサによってはYUVなどの場合がある       RGBよりはYUVの方がセグメンテーションが楽な場合がある       ただ 学習がどのように訓練されているかに依存する  引用元    Screenshot from       png  どのようなデータセットで学習されているのかが記載されている   We timed the inference on a NVIDIA Jetson AGX Xavier with Jetpack    TensorRT     PyTorch     Jetson AGX Xavier での計測がされている  引用元  上記の種類のDEPTHセンサを用いている   Screenshot from       png  引用元　論文,0,2023-12-25
15,15,Cannyのエッジ検出の角度推定式を導出と検証する,画像処理,https://qiita.com/FallnJumper/items/9c688f042db1fecd74e6,Cannyのエッジ検出から角度を推定するアルゴリズムは以下だと知られている   対象画像にガウシアンフィルタをかける  縦横のSobelフィルタをかける  着目ピクセルにおける縦 横Sobelの出力を g y g x とすると  \arctan \frac g y  g x  が角度値になるなお で出てくる角度は縦線  g y 出力が が角度に対応することに注意する ところで このなぜこの処理を行うと角度がでるのかを説明している記事が少ない気がするので この記事で導出および検証してみよう   導出一旦 処理対象が画像であることは忘れて直線に沿って非ゼロ あとはゼロである関数を説明モデルとして定義しよう 直線角度が heta で表現されるとき を満たす場合のみ非ゼロとなるような変数関数を作ればいい ここで 角度が縦線に対応するように式を立てている ここで \delta はディラックのデルタ関数であり 以下の特性をもつ ただし  x  a x  とする いま これにガウシアンフィルタを畳み込む ここで  A B W U は x y には依存しない  hetaには依存する   ガウシアンフィルタは分散も指定できるが 適切な U に置き換えれば式の形は同じである いま Sobelフィルタの役割をx yそれぞれの偏微分に対応していると考える それぞれの偏微分は以下 これより 任意の x y においてが導ける   どこがキモか Cannyによる角度検出の処理のなかで  Gaussianフィルタを使う部分があるが このフィルタがキモ  だといいたい Sobelフィルタが微分に対応しているのは係数の形をみれば自明であるため 重要なのだがつまらない Gaussianフィルタが以下の特性をもつからこそ Sobelフィルタの比が anheta になるためである   直線上にGaussianを畳み込むと直線からの距離を変数としたGaussianが得られること  Gaussianを微分すると 距離の方向別偏微分が元の信号にかけ算した結果で出てくることこの特性によって  \sigma の値やpixelの位置によらず方向別の偏微分の比が an に依存するというのが興味深い   実装と検証以下のように 画像内に任意角度の一本線を描画して 各角度が上記式によって求まるかを調べてみよう 	  ラジアンに変換 	  定義式に基づいてdelta r のrを生成 	  ガウシアンフィルタ後  σは 	  公式を使ってthetaを逆算  単位は度に直す 	  線は から度で一周するので 値域に収まるようにを加減する 	  度付近の値の不連続がでにくいように  とする 	  理屈上はr となる点以外ならどこ使っても同じ値が出るが 線の芯から離れるほど信号が小さくなるので誤差が大きくなる    付近の点を角度vs計算結果のプロットの代表点として使用する 結果は以下 それぞれ 一番上の図は指定角度の直線にGaussianを畳み込んだフィルタ画像 中段は 各pixelで角度計算を行った結果である 下段は特定のpixelに着目して 指定角度vs計算結果をプロットしていった結果である  からまで順に描画されていく途中経過  中段において中央のpixelなどがになっているのは Gaussianの中心点は傾きがになってしまい角度計算ができないことを意味している  度のとき   tan  png   度のとき   tan  png   度のとき   tan  png  直線の傾きは 度とマイナス度で差がないため 最後は一周して が計算結果になっている というわけで 導出の通りの結果が得られていることがわかる   補足 実は一般的に成り立つ話だった任意の微分可能な関数 h についてとできるとき 一瞬で求まってしまった つまり やはりGaussianである必要はなく 任意の微分可能関数で定義された畳み込みフィルタを使えばOKということらしい ややこしく考えすぎただけのようだ ,0,2023-12-24
17,17,【Python】画像処理入門,画像処理,https://qiita.com/MandoNarin/items/dcd4303933d16b622ab1,  はじめにこちらの記事では 初学者向けにPythonを使った画像処理に触れていきたいと思います コンピュータはどのように画像を扱っているのか を実感できる内容を意識して書きました 自分のPC ローカル環境 上で実行していきます 具体的には 自分で作成したファイル中にPython言語を記述していき これを実行させます   準備以下二つを利用していきます まだの方がいたら 先に環境構築を済ませておきましょう  Anaconda VSCode  画像処理   Python実行フォルダー ファイルの準備今回は分かりやすいようにデスクトップ上にフォルダーを作成します     stepデスクトップの何もないところで 右クリック    新規作成    フォルダーで新規フォルダーをデスクトップ上に作成します 名前はpython practiceとしておきましょう まず VSCodeのアプリを起動します 機種によって微妙に見た目が違いますが 大体以下のような黒い背景のエディターが立ち上がっています   python png  左クリックでpython practiceを掴み Vscode上にドラッグ ドロップしてください   python png  そうすると 以下のようにVSCode内でpython practiceを開くことができました   python png      step今度は 今後記述していくPythonファイルを作成していきます   python png   PYTHON PRACRICE と書いてある箇所の下のスペース上で右クリックして New File を選択してください ファイル名はsample pyにします 同様にsample pyとsample pyとsample pyも作成します 以下のようにつのファイルが作成できれいればOKです   python png      step処理したい画像ファイルを用意します 本記事では以下の画像を例に用います 自分の好きな画像をダウンロードして利用してみてもOKです   python png  画像処理に利用する画像を先ほど作ったpython practiceフォルダー内にそのまま置きます 今記事で用いるファイル名はhuman pngにします   python png  以上で準備完了です これから画像処理のコードを記述していきますが まず 用いるライブラリや周辺知識を説明します    ライブラリ  python jpeg  ライブラリとは  繰り返し使う機能がまとまっている工具箱 の様なもので 簡単に利用することができます 今回使うライブラリは以下です①OpenCVコンピュータビジョンのライブラリ 画像を白黒にするなどの画像編集 加工ができる ②Matplotlibグラフ描画のために使われるライブラリ 画像の出力用に用いる ③Numpy計算処理に便利なライブラリ しかも高速    コンピュータはどのように画像を扱っているの 画像データは全て数字でコンピュータ内では扱われます 赤 緑 青の各色を の値で表すことで それらの組み合わせによって色が決まります   python png  それぞれの画素 Pixel の色がRGBで表されています   python png  これらは 一般的には以下画像のように 縦座標 x 横座標 x 色情報 のつの情報を扱う次元配列の形式のデータとして内部では利用されることになります   python png     ライブラリを使って画像表示OpenCVとmatplotlibを使って画像を表示します それと同時に実行方法も見ていきましょう     stepVSCodeでsample pyを開き 以下コードを丸々コピペします コピペしたら  Ctrl Command    S で保存します 詳しいコードの説明はコメントアウト部分に譲ります    python sample py  ライブラリのインポート  matplotlibのインポート 今後pltという名前で用いる   画像ファイルの読み込み  同じ階層にあるため ファイル名のみ記述filename    human png   画像配列の生成orig   cv imread filename   RGBの順に整形  配列の形を表示 たて座標 x 横座標 x 色の値 print src shape   画像の表示上記のPythonファイルを実行します いくつか方法はありますが 以下一例です Windowsの方は  Anaconda prompt Macの方は  ターミナル を起動します これらは コマンド 命令 を入力して実行してくれるものです Anacondaには MatplotlibとNumpyがデフォルトで入っていますが OpenCVは入っていないので まずこちらをダウンロードします 以下を入力して Enter を押します    sh ターミナルpip install opencv pythonすると 以下画面のようにOpenCVがダウンロードされました   python png  次に デスクトップ上に作った python practice の階層に移動し sample pyファイルを実行します 以下を一行ずつ順番に入力して Enter を押していきましょう   cd は change directly の略で 今いる階層を移動するコマンドです    sh ターミナル上記のようにsample pyを実行出来たら 以下のように配列の形の出力と 画像表示用のWindowが出てきます このようにして ローカル環境でPythonファイルを実行していくことができます   python png  また 新たにターミナルに入力するには 画像windowsを閉じるか  Ctrl command    C を入力します すると 以下のように入力できる状態に戻りました   python png     画像を加工する    blur ぼかし OpenCVを用いると blur ぼかし を簡単にかけることができます sample pyファイルに以下を丸々記入して保存します    python sample py  ライブラリのインポート  matplotlibのインポート 今後pltという名前で用いる   numpyのインポート  画像ファイルの読み込み  同じ階層にあるため ファイル名のみ記述filename    human png   画像配列の生成orig   cv imread filename   RGBの順に整形  OpenCVのblurメソッドを利用し ぼかしをつけるblurred   cv blur src         ぼかし画像の表示前回と同様 ターミナル上でsample pyファイルを実行します python practiceの階層の場所で以下を入力して Enter を押します    sh ターミナルpython sample pyぼかし画像は出てきましたか         python png      色の反転色の反転は numpyで計算を行います sample pyファイルに以下を丸々記入して保存します    python sample py  ライブラリのインポート  matplotlibのインポート 今後pltという名前で用いる   numpyのインポート  画像ファイルの読み込み  同じ階層にあるため ファイル名のみ記述filename    human png   画像配列の生成orig   cv imread filename   RGBの順に整形  numpyを用い 画像データ配列の各要素のRGB値をから引く   ブロードキャストにより全要素の r  g  b それぞれの値がから引かれるpixels   np array src pixels      pixels  色反転画像の表示同様に 以下実行していきましょう 画像全体の色が反転されます    sh ターミナルpython sample py  スクリーンショット       png      画像の一部のみに変更を適用する画像配列の一部の範囲内のRGB値を計算対象とします 座標の範囲を指定し 繰り返し処理を利用します   python png     python sample py  ライブラリのインポート  matplotlibのインポート 今後pltという名前で用いる   numpyのインポート  画像ファイルの読み込み  同じ階層にあるため ファイル名のみ記述filename    human png   画像配列の生成orig   cv imread filename   RGBの順に整形  以下画像配列の一部の座標範囲内のRGBを計算対象とする  繰り返し処理を利用  縦座標の範囲    横座標の範囲      各座標のピクセル値を変更      ブロードキャストにより r  g  b それぞれの値がから引かれる  画像の表示    画像の一部のみに変更を適用する その画像の一部を黒色で覆ってみましょう sample pyとして作ってみましょう   python png     python sample py  ライブラリのインポート  matplotlibのインポート 今後pltという名前で用いる   numpyのインポート  画像ファイルの読み込み  同じ階層にあるため ファイル名のみ記述filename    human png   画像配列の生成orig   cv imread filename   RGBの順に整形  以下画像配列の一部の座標範囲内のRGBを計算対象とする  繰り返し処理を利用  縦座標の範囲    横座標の範囲      各座標のピクセル値を黒色に変更  画像の表示  おわりにこの記事では Pythonとそのライブラリを使用して画像処理について書きました コンピュータは画像を縦x横xRGB値という次元配列で表現しており プログラミングで色々処理できることが実感できたと思います ちなみに動画はここに時間軸が加わるので次元配列となります 本記事で触れた処理内容自体は基本的なものでしたが 色々な画像処理ツールはこの基本原理に基づいて作られています どのような画像処理アルゴリズムが使われているのだろう と興味を持って色々調べていくと 新たな発見があるかもしれません ,2,2023-12-24
18,18,画像の補間方法,画像処理,https://qiita.com/momoteei_bioxdatascience/items/d60c4b76b420ce1ffb0e,   アップスケーリング時の画像補間方法深層学習を使ったメーカー研究員 ももてぃ 仮 です cherry blossom 深層学習を使って前処理をする際に画像の情報量を増やす機会があったので 基本的なことですがアップスケーリングの補間方法についてまとめました そもそもは CNNで学習する画像をアップスケーリングして情報量を擬似的に増やして性能向上に繋がるかどうか を知りたかったのですが 擬似的に解像度を上げてもあくまで補間しているだけなので 結果は変わらないのか 画像の解像度が擬似的に向上し 性能が良くなるのか 結果がわかったらまた共有します ご意見あれば是非欲しいです relaxed       この記事を読んで得られること OpenCVで実装できるアップスケーリングの補間方法の原理がわかる       アップスケーリング方法以下のテストコードを実行しました   アップスケールの倍率とメソッド  出力ディレクトリの作成  各メソッドで画像をアップスケールして保存print  アップスケールが完了しました   ちなみに methodの中の数字は以下のようになっています        最近傍補間最も単純な補間方法で 近くのピクセルの値をそのまま使用しています そのため拡大するとカクカクした画像になっていますね これだと情報量は増えていなさそう         バイリニア補間近くの×のピクセルの値を使用して線形補間を行う 標準的な補間方法としてよく使用される方法        バイキュービック補間双線形補間よりも滑らかな結果が得られるが 計算に時間がかかる バイリニアと比べて パッと見の違いはわかりません        エリア補間近く×個のピクセルの値を使用して補間する ピクセルの集合を平均して補間を行います 縮小時に用いると ノイズの影響を抑えることができますが 拡大時には適していません        ランチョス補間サンプリングウィンドウを使用して補間を行います 高い品質を提供しますが 計算が他の方法よりもコストがかかります ,1,2023-12-24
20,20,[コピペで試せる] Python OpenCV で行う、カメラキャリブレーション,画像処理,https://qiita.com/iwatanabe-7/items/8b29f5ea4fe2eb8bfa64,   初めに  法政大学情報科学部 Advent Calendar      日目の記事になります どうも iwatanabeeです 研究室では 画像処理の研究しています 今回は PythonとOpenCVで画像の歪みを補正する カメラキャリブレーション をしていきます 最近 研究室の同期が キャリブレーション を使うらしいので 少しでも参考になればいいと思い この記事を作成しました    概要画像処理をしたいときに 画像が歪んでいたり ズレていたりしたら処理に影響が出てしまいます カメラで撮った画像は 丸みを帯びていたり 歪みが生じている可能性があるので これらを修正するために カメラキャリブレーション という処理を行います  カメラキャリブレーション とは 内部 外部パラメーター レンズの歪収差係数を求め 画像を補正する処理です 簡単に言うと   どれくらい歪んでいるかを求めて 正しい画像に修正する処理  です この処理を行うために OpenCvのカメラキャリブレーションのライブラリを使用していきます 今回はわかりやすいように 歪みが大きいステレオカメラで撮影した画像を使います これは右斜め横から ステレオカメラで撮った写真 左 とiPhoneで撮った写真 右 です 左のほうは結構丸みを帯びています    カメラのパラメーター写真を撮るときに カメラは次元の情報を次元のデータに変換します その時に歪みなどが生じます ですので 次元のデータ 外部パラメータ や次元のデータ 内部パラメータ を元に歪みを修正します 内部パラメータとは カメラの焦点距離や写真の歪みの値などのことで 外部パラメータは 位置や向いている方向などの値のことを言います    手法 Opencvのドキュメント  にあるカメラキャリブレーションのチュートリアルを参考にしました   修正したいカメラで チェスボード を撮り Opencv の  findChessboardCorners   でコーナーの位置を見つける  image png    チュートリアルより↑     再投影誤差        カメラ行列      歪み係数           e    e    e    e    e     パラメーターをもとに undistort  で歪みを補正し 画像を変換します  image png    チュートリアルより↑    実際にやってみる    チェスボードを 修正したいカメラで約枚ほど撮る 左右上下いろんな角度から撮る チェスボードができるだけ大きく映るように撮る    コードを実装今回は Google colab 上でプログラムを実装します  Opencvのカメラキャリブレーション  を参考に 以下のコードを実装しました まずは Google drive のファイルを使用するためにドライブをマウントします 先ほど撮った画像が入っているフォルダーのパスを指定します 次に チェスボードの格子点を見つけて 表示します チェスボードの大きさによって格子点の数が変わるので 変数として定義しました 格子点を数え cbrow と cbcol に代入します つまり 下のチェスボードの青い点の数と赤い点の数をそれぞれの変数に代入します タイルの数ではないので 注意　↓今回は 格子点が個 青い点の数 と個 赤い点の数 なので 以下のように代入します       print  見つけられませんでした   fname cv destroyAllWindows  実行前と実行した後の比較です しっかりとコーナーがとれていることが分かります カメラキャリブレーションをして どのくらい歪みのパラメータを出します    python  カメラキャリブレーションprint  回転ベクトル   print  並進ベクトル   再投影誤差    カメラ行列  歪み係数       e    e    e    e     e   回転ベクトル            X         Y         Z並進ベクトル            X         Y          Z最後にパラメータを使用して補正し 画像の表示をしていきます     歪補正    画像の切り落とし   結果右の画像が補正された画像です しっかりとまっすぐになっていることが分かります ,7,2023-12-24
21,21,画像処理分野の基礎単語,画像処理,https://qiita.com/SN85l6/items/6dbc1e74ba849d026a48,  この記事の想定読者画像処理を始めて行う人 画像処理についてこれから勉強を始める人    はじめに画像AI研究に関わる中で 最初に何につまづいたというと  用語 でした Webシステムを開発する中では聞いたことのない単語に溢れていて 急に宇宙に連れてこられたのかと思いました みなさんは私みたいに急に宇宙に連れて行かれることないように 地球で勉強しましょう     頻出単語  用語名   意味    矩形検出　  四角形を検出する   射影変換　  斜めになってる写真を正面に向くようにキュッとするときなどに使います    ニ値化   白と黒の色に変換すること    グレースケール   グレーのグラデーションを階調で表したもののこと    ノイズ除去   光の加減などにより どうしても点々と入ってしまうノイズの除去    OCR   光学文字認識    AI OCR   AIを用いた光学文字認識    ガンマ補正   カメラやディスプレイなどの機器によって出力される画像が 人間の目で見たときに適切な明るさや色味になるように補正する     HSV   色相 Hue   彩度 Saturation  明度 Value brightness     パーセントタイル   計測値の分布 ばらつき を小さい数字から大きい数字に並べ変え パーセント表示すること     おわりにこの辺りの単語を知っておけば 話にはついていけるかなと思います 画像AIの分野は 精度との戦いであり 発想の勝負であり楽しくも大変な世界でした 画像AI分野の初心者向けの記事でした ,19,2023-12-23
22,22,OpenCVとSpeechRecognitionで雀魂の音声操作をやってみる,画像処理,https://qiita.com/finalKiba/items/756a3902c2a246b1c5fa,  はじめに雀魂をプレイしてるときにクリックすら億劫になるときがありませんか 私はあります ふんぞり返って声で操作出来たらいいのにと なのでそれを実現するためのプログラムを作ってみることにしました   音声認識   PyAudioまずは音声を入力するところからです マイクからの音声データを順次処理したいのでPyAudioのcallback modeというものを使います こいつは音声データの準備ができたらcallback関数にそいつを渡してくれます  audio open で設定をします 色々項目がありますが特に弄ってみたところで frame per buffer がcallback関数に渡す音声データあたりのサンプル数です   バスガス爆    ばす ガス爆    バスガス爆発    バスがす 爆    ばすがす 爆    バス    ばす    バスが    ばすが    バスガ    初    はつ    発  小さくするとcallbackが呼び出されるまでが早いんですがその分文章がブツ切れになります 麻雀の発声は短いのでとりあえず frame per buffer   int SAMPLERATE      とします 実は先ほどすでに使ってましたが音声データを文字に起こすのにはSpeechRecognitionの reconize google を使います  audio open で指定したcallback関数の中に処理を書いてきます          AudioDataインスタンスを作成 第引数は量子化byte数             以下のif else文の繰り返し                  認識結果を画像認識の方に渡す             elif              繰り返し終わり返ってきた result の中身はこんな感じになっていますので上の処理では transcript だけ取り出して使っています 精度は申し分なさそうですがかなりハキハキ発声する必要があります   alternative      transcript    リーチ    confidence         transcript    ビーチ      transcript    びーち      final   True if文の条件はごちゃごちゃしてますが例えばリーチの場合を例にとりますと transcript の中に一つでも   リーチ   りーち   ビーチ   ピーチ   のいずれかの文字列を部分的に含むものがあれば真になるって感じです msp ちーまん ちーそう ちーぴん とチーが混ざりそうですがチーをするタイミングと牌を切るタイミングは異なるのでチーの判定を後に行えば大丈夫です 辞書型での比較も考えましたが部分一致ですので正規表現書くの大変そうですし速度も知覚できるほどは変わらなそうですからやめました callback関数の呼び出しから条件分岐一番下まで 秒程度でした そのうちほとんどの時間が  画像認識   特徴量マッチング音声に対応した牌を切ってもらうためにその牌が画面のどこにあるかを特徴量マッチングで調べます 今回は動作が速くて拡大縮小 回転などにも強いといわれるAKAZE特徴量を試してみます あえて他の牌と誤認されそうなsでマッチングしてみましたがこの程度なら大丈夫そうかな 次はブラウザサイズを変更して試してみます あれ ratioが低いと表示されるマッチングがなかったので大きくしてみましたが全然関係ないところがマッチングしてました ググってみると先駆者様がいましたが同じく苦しんでいるご様子 う  ん   めんどくさいので  今回は私の環境で動きさえすればいいので拡大縮小はいったん無視します    テンプレートマッチング拡大縮小を見ないのであればテンプレートマッチングでも目的は達成できそうなので今度はそちらも試してみます いい感じです ちゃんとsにマッチしてますね マッチング時間も計測したのですがAKAZEが 秒 テンプレートマッチングが 秒程度でした ということで今回はテンプレートマッチングで実装していきます    スクリーンショットテンプレートマッチングを行う上で当然画面のデータが必要になります まあスクリーンショットとるだけなんですが今回は少しでも速くしようということで dxcam というパッケージを使わせていただきました  pyautogui screenshot と比べてそこそこ速くなります 注意点として camera grab   は前回からフレームに変化がないとき None を返します     実行結果  実装一切手を使わずにプレイしたいので打牌 鳴き リーチ 上がりのすべてを認識させます またQoLのためにツモ切りと鳴きのオンオフも音声入力でできるようにします やることは音声入力に対応した牌や鳴きボタンの画像をスクリーンショットにマッチングさせて座標を割り出し pyautogui でクリックさせるだけです ただしちょっと面倒なことにチーやポンは鳴いた後に鳴く形の選択が必要なことがあります そこで左から何番目を選ぶかを音声で入力し 座標は文字とキャンセルボタンの位置関係が選択肢の数に応じて変化することを基に決定しています だいぶごり押しなのでもう少しスマートな方法があればよかった   無題 png    mjChoiceStr png  ツモ切りはツモ牌が他の手牌から浮いていることを利用します こんな画像でマッチングを取ればツモ牌がマッチするって寸法です   mjPaiTop png  出来上がったコードはこんなです 音声認識のところのelifの羅列だけはほんとにごちゃごちゃしてるんで省略しました  画像の読み込み 画像の読み込み終わり         AudioDataインスタンスを作成 第引数は量子化byte数                click n m   n mは牌種や数字などを区別する            elif                  繰り返し終わり     鳴き選択の場合     それ以外  いざ実行        gif  んーーーーなんか思ってたのと違う 体感時間ですが発声してからコンソールに認識された文字列が表示されるまで秒程度かかっています もっと快適に遊べるようになると思ってたんですが現実は厳しい 問題点を列挙すると  テンポが悪い   認識してもらえるように発声するのがかなり疲れる    これが一番致命的   上と関係してますが短い語句は認識してもらいにくい   赤を区別するのが難しい 一応赤は丸印がついているので区別できるはずだが結構な割合で間違える   mj png  などなどテンポの悪さの原因のほとんどが音声認識周りなのでそこが短縮されればテンポはかなり良くなりそうですが発声が疲れる問題は解決しようがありません 口を動かすのが手を動かすより疲れるなんて当たり前のこともう少し早く気付くべきでした こいつはお蔵入りです 悲しいですが記事のネタぐらいにはなったのでよしとします   終わりに大分長くなってしまいましたがここまで読んでいただきありがとうございました 音声入力はもっと入力回数が少なくてゆったりしたテンポのゲーム向きなのかなといった印象です 脳波で入力できる時代が来るのを待ちます ,1,2023-12-22
23,23,画像セグメンテーションについて調査中,画像処理,https://qiita.com/nonbiri15/items/3d10c52690024df82415,   Segmantation Anything を見てください の実装が いままでの実装の多くを無意味にしています まずは segment anything を見てください 物体のtracking のタスクも segment anything が使える場合には それで十分になってきているかもしれません  ここまで年月追記 ライブラリを調査するためのアドバイス よく使われているカメラの場合には そのカメラでの利用例が開発元あるいは第者によって公開されていることが多い 　それをまず調査する  画像認識関係の場合だと 組み込み対象のデバイスの開発元 例：NVIDIA がさまざまなアプリケーションを提供している 　それらに利用したいライブラリが紹介されていることがある  推論エンジンに提供されているmodel zooにある学習済みのモデルとアルゴリズムを調べてみよう 　例：OpenVino 機械学習分野のフレームワークでの状況を調査する    segmentation の種類  ekECjwJ jpg   引用元  semantic segmentationinstance segmentationpanoptic segmentation   paper with codePaper with code のサイトでは それぞれのタスクに対してSotAのアルゴリズムを紹介している    気にするポイント  セグメンテーションの分類例：走行可能範囲のセグメンテーションあなたの実現したいセグメンテーションは何ですか   セグメンテーションの実行時間  セグメンテーションの解像度  セグメンテーションの精度  学習のしやすいさ  通常カメラでのセグメンテーション   YolovYolovでは 物体検出の他にインスタンスセグメンテーションが追加になっている MS COCO のカテゴリの検出とインスタンスセグメンテーションとができる Youtube  YOLOv COMPLETE Tutorial   Object Detection   Segmentation   Classification    Screenshot from        png   引用元     StereoLabs ZED      How to Use YOLO v with ZED in Python  ZED のカメラを使ってYolovを動作させるためのインスール手続きとかが書かれている       StereoLabs ZED SDK  Image segmentation using yolov with zed i python  セグメンテーション後のマスクを取得する部分には さらに調査が必要そうだ      StereoLabs Mask R CNN  How to Use PyTorch with ZED    image png   引用元  なお Mask R CNN は 画像に存在する物体のクラス名を特定する一般物体検出と 画像内のピクセル単位でクラス分けを行うインスタンスセグメンテーションを同時に行う手法です       D Mask R CNN using the ZED and Pytorch     NVIDIA  image png   引用元     Youtube NVIDIA  Jetson AI Fundamentals   SE   Semantic Segmentation     NVIDIA  BID を使用したステレオ入力における近接セグメンテーション    私たちは DNN の推論に TensorRT を DLA で使用しており GPU とは異なるハードウェアの多様性を提供し フォールト トレランスを向上させつつ 他のタスクを GPU からオフロードしています DLA は Jetson AGX Orin で BID に対して約 fps を提供し  つの DNN で構成され ロボティクス アプリケーションにおいて ms 未満の低レイテンシを提供しています   と NVIDIA のロボティクス プラットフォーム ソフトウェアの副社長である Gordon Grigor は述べています 衝突を防止するためには フレームレートを高くする必要があります しかも ハードウェアの余力を奪わない実装が必要になります  引用元  HarDNet のアルゴリズムを元に再学習している  引用元  segnetは年発表の実装である 年の時点で利用するには 古い実装である そのため 出力画像の解像度も粗く 領域が染み出したりしている    NVIDIA  Free Space Segmentation    image png   引用元     NVIDIA  PeopleSemSegNet Model Card    image png   引用元  V という 初の Tensor コア GPU を使うと fps前後のフレームレートがでているとのこと  引用元  LiDARとカメラ画像とがあるときのmulti modalな手法ですね    yolov にはセグメンテーションがある   image png   引用元       yolovの解説記事 Yolov の使い方   物体検出モデルYOLOvの紹介  動作手順や過去のバージョンとの比較     yolovにもセグメンテーションがある       Yolovでインスタンスセグメンテーション         物体検出 YOLOvまとめ第回 Instance segmentationを実装する       github   image png   引用元  画像セグメンテーションは 物体検出と共通性が高いので 別々のソフトウェアとして実装されるよりは 同一のソフトウェアとして実装される方がうれしい その方が計算の無駄が少なくなるし メモリの管理も楽になるはず  ただ そのなっていくほど 学習データの与え方は難しくなってしまう懸念はある  detectron をTensorRT上で動作させることについての記載がある      MATLAB の場合　 イメージのセグメンテーション    Screenshot from       png   引用元  これらは 深層学習を必要としないセグメンテーション手法である      MATLAB の場合　 Mask R CNN を使用したインスタンス セグメンテーションの実行    image png   引用元  Mask R CNN を学習させる場合の例が記載されている      MATLAB の場合 深層学習を使用したセマンティック セグメンテーション    image png   引用元       MATLAB の場合 Yolov以下のやり取りを見る限り 現状では未対応 年時点  I need Yolov in MATLAB  what is the latest version of YOLO algorithm in MATLAB      Scikit image のimage segmentation  深層学習を必要としない画像のセグメンテーションならば scikit image も候補になる       Image Segmentation  scikit image  Multi Otsu Thresholding    image png   引用元   引用元  深層学習を必要としない分野での画像のセグメンテーションには scikit image に多様な例が示されているので それを参照してみよう 整体組織の顕微鏡写真のセグメンテーションの事例もある 古典的なセグメンテーションの操作の場合には OpenCVに含まれるモルフォロジー操作の関数だけで実装できる場合もあるだろう    OpenVino     openvinoのmodel zoo にあるインスタンスセグメンテーション     OpenVinoでの画像セグメンテーションの解説記事Keras を用いた例が書かれている   image png   引用元     github  panoptic segmentation     Transfomerベースの画像セグメンテーション 引用元  画像セグメンテーションの分野においても Transformerベースの開発が進んでいる 上記のHugging Face の記事を参照のこと   簡単に動作できるようになっているので GPUを持っていないPCで動作させるのもとても簡単   HuggingFaceにはimage segmentationに アルゴリズム 学習対象のデータセットの組み合わせで 多数のモデルがある   HuggingFaceではうれしいことに モデルでの推論が共通になっている   そのため モデルの切り替えがとても楽になっている  個の文献が紹介されている   セグメンテーションであるので ハンドのlandmarkは含まれない   このライブラリだけでは 次の対象物と次の対象物のセグメンテーションが分かるだけ   次の対象物 次の対象物がそれぞれ何であるのかという理解は持たない ,4,2023-12-22
24,24,VertexAI AutoML （画像）を使ってみませんか？#2,画像処理,https://qiita.com/kccs_nobuaki-sakuragi/items/c1725c75ae22a76114b4,こんにちは 京セラコミュニケーションシステム櫻木   kccs nobuaki sakuragi です 最近はGenerative AIが流行りですが 以前からあるGoogle Cloudで提供されているAutoML Visionについて紹介します 　AutoML Visionのサイトを見ると次の様なサービス終了の案内がでています 　AutoML Visionは VertexAI AutoML 画像 へと名前が変わっていますが 同じ機能＋αで機能はVertexAI AutoMLのメニュー内にあります    note alert年月日を過ぎるとAutoML Visionは使用できなくなります 以前のAutoML Visionのすべての機能と新機能は Vertex AIプラットフォームで使用可能です 　名前が新しくなったVertexAI AutoML 旧AutoML Vision メニューを使いながら Google Cloudの画像AIをコードを記述せずモデルトレーニングしたい方向けに複数回にわけ記事を書いてみます 第回目はこちらになります  VertexAI AutoML  画像 を使ってみませんか   googlecloud   Qiita  第回目はGoogle Cloud内で実施できるAutoMLのデータ準備 評価や予測についてまとめた記事になります    note 本記事は 年月日時点で作成した記事です   本記事の対象者  画像AIに興味のある方  Google Cloudの画像AIを触ってみたい方  お試しで画像AIは触ってみたいが サンプルデータが無く 使えない方  Google CloudのVertexAI AutoML画像で何ができるか知りたい方  AutoML Visionから VertexAI AutoMLへ名前が代わり事例を探している方   Google Cloud VertexAI AutoML とは　ドキュメントでは  Vertex AI AutoML  の項目に次のように記載されています    note 機械学習 ML モデルでは トレーニング データを使用して トレーニングされていないデータの結果をモデルが推測する方法を確認します Vertex AI AutoMLを使用すると 提供するトレーニング データに基づいてコード不要のモデルを構築できます VertexAI AutoML 以下AutoML を利用すると最小限のIT技術の知識と操作でモデル作成およびトレーニングを実施する事ができます また本記事では AutoML 画像 分類について説明します    AutoMLのデータ準備について    画像のサイズと種類についてAutoML 画像 でトレーニングできるデータと予測できるデータのファイル形式とファイルサイズが異なります ファイル形式やサイズを意識し画像データを収集する必要があります とくに見落としがちなのは 予測する画像データが MBの制限がある事です その為 予測時に利用する MBの画像ファイルでディスプレイに表示し見分ける事が可能か確認しておく必要があります      トレーニング／予測で利用できる画像種別と容量について          画像ファイル形式    ファイル容量     トレーニング  JPEG GIF PNG BMP ICO   MB    予測データ   JPEG GIF PNG WEBP BMP TIFF ICO    MB  詳細は ドキュメント：分類用の画像トレーニングデータを準備する  を確認してください なお ファイル形式や ファイルサイズを気にしなくて良いパターンは 撮像するカメラや条件がさまざまある場合です 上記に該当する場合は 実際に予測する画像にあわせ 画像も色々なパターンを含め収集する事をオススメします たとえば 撮像するカメラやスマートフォンなどを統一する予定の場合は 画像を集めるところからファイル形式やファイルサイズを決めて置くことという形になります      画像を集める際に気をつけるポイントAutoMLで簡単にAI 機械学習ができるようになっても 実際にあつかうデータの質が大変重要です そこで AutoML 画像 で学習用画像を画像を集める際に注意するポイントを纏めます AIや機械学習はデータの質が大変重要です ここを間違うとデータを集める所を何度も繰り返す必要がでてきて 最終的に AIや機械学習って使えないね  という話になってしまいます なぜ データの質が重要であるかというと AIや機械学習が判断する特徴量も人が見てぱっと判別する物とほとんど同じだからです とくに画像は用意したデータを人がみても短時間で判別できることが重要です その為 本来のラベルや意味と異なるデータ 誤ったデータ ゴミデータ や 判別ができないデータ 特徴量が見えにくい などが含まれていると作成したモデルの精度が下がります その為 画像を集める 学習する前に次のような事を意識できているかチェックしていただくと良いと思います   気をつけるポイント   気をつける背景     現実世界にある物体の写真利用する   AutoML モデルは 現実世界にある物体の写真に対してもっとも効果的に動作します そのため トレーニング用のデータは 現実世界で撮影された写真を使用する      予測を行うデータに近いデータを使用する   ユースケースに低解像度のぼやけた画像が含まれている場合 トレーニング用のデータも低解像度でぼやけた画像から構成する必要があります 一般に 複数の視点 解像度 背景を持つトレーニング画像を用意することも検討します      人間が割り当てることができるラベルを使用する   AutoML モデルは 人間が割り当てることができないラベルを予測することはできません 人が画像を   秒間見てラベル判定判別できなければ モデルもトレーニングすることはできません      ラベルごとに十分な数の画像を使用する   ラベルごとに約   個のトレーニング画像をオススメします ラベルあたりの最小数は  です 複数のラベルを使用してモデルをトレーニングする場合は 各ラベル毎に多くのサンプルが必要になります 複数ラベルを使う場合のスコアは 解釈が難しくなります      ラベルのバランスを考慮する   モデルは もっとも一般的なラベルの画像数が もっとも一般的でないラベルの画像数よりも最大で  倍存在する場合に最適に動作します 非常に低い頻度のラベルは削除することをオススメします      None of the above ラベルを使用する   None of the above ラベルと 定義されたラベルのいずれとも一致しない画像を含めることを検討してください たとえば 花のデータセットの場合 ラベルが付けられた品種以外の花の画像を含め それらに None of the above のラベルを付けます      note　私達が製造業の外観検査で利用する場合は 基本的にトレーニングデータと予測データは同じファイル形式 データ容量も同じにしています 理由は 検査画像は基本的に画像を撮像した条件やカメラが同じである為です None of the aboveはモデル作成したけどあまり精度がでない時など 画像分類するには他の物に比べ少ない画像をつにまとめると精度が上がる場合がありました 外観検査では白黒つけにく画像や年に数回しかでない不良画像などもあり そういった場合にNone of the aboveを使っていました     データの分割について　一般的な機械学習のフローではトレーニング前にデータを トレーニング 検証 テスト用分割する必要があります 分割する仕組みをPythonなどで作っておくと手間ではありませんが 初心者であったり サクッと精度を観たいという人には手間がかかるのが現状です 　Google CloudのAutoMLでは自動的に学習 評価 テストの画像に分割される為 学習データの分割方法や画像に関する知識がなくてもトレーニングに進む事ができこの部分が簡略化されます 　もちろん手動で分割する事もできますので モデルを精度向上 反復修正 をする場合などは一般的な機械学習のフローと同じ手順で進められます    AutoMLのモデルの評価と予測の特徴についてAutoMLの評価と予測がGoogle Cloud上で実施できるのが特徴です 専門知識がなくてもトレーニングとモデル作成 評価までできます 画像を準備すれば 画像モデルをすぐに評価までする事ができるのが特徴です AutoMLを利用すると データを準備し トレーニングをする その後 実際のデータについて詳しい専門家にヒアリングしモデル精度を上げる事に注力する事ができます    AutoMLで作成したモデル評価についてモデル評価指標とは テストセットに対するモデルのパフォーマンスを定量的に測定する指標です 評価指標をどのように解釈し 使用するかは ビジネスニーズや どのような問題を解決するかによって異なります たとえば 偽陽性の許容範囲が偽陰性の許容範囲よりも低い場合もあれば その逆の場合もあります このような質問に対する答えは どの指標を重視するかによって変わります AutoML 画像 分類 では 適合率 再現率 信頼度しきい値など さまざまな評価指標が提供されます また トレーニング後 作成したモデルの評価がGoogle Cloudのコンソールから見る事ができます とくに混同行列を見ると ラベル毎の精度状況がひと目でわかります       images  png    png  代表的な評価指標は 以下のとおりで Google Console上でもみる事ができます    評価指標   定義   例       適合率     モデルによって生成された分類予測のうち正しい分類であった割合    モデルが猫と犬を分類するタスクで 適合率が   の場合 モデルは猫と犬の画像を正しく分類する割合が   であることを示します        再現率     このクラスの予測のうち モデルが正しく予測した割合    モデルが猫と犬を分類するタスクで 再現率が   の場合 モデルは猫の画像を正しく分類する割合が   であることを示します        混同行列     モデルによる予測のうち正しい予測の頻度を示します    混同行列は 行が実際のクラスであり 列が予測されたクラスを示します 正しい予測は 行と列が同じセルに存在します       noteこれらの指標は モデルのパフォーマンスを評価する際に さまざまな観点から考慮する必要があります より詳しく知りたい場合は  Googleが無償で提供しているMachine Learning　基礎コースまた 混同行列の項目数のボタンをスライドすると割合で表示されていた物が画像枚数で表示できます ラベル毎に画像枚数が違う場合は この赤枠のボタンをスライドすることで 枚数単位で精度の良し悪しを判断する事ができます 左のDaisyをTulipsに誤っている所は と表示されていますが 枚数にすると枚間違っている事がわかります       images   png     png        images   png     png  精度を上げていく課程をGoogleCloudでは モデルの反復修正という言葉をつかっています モデルの反復修正について詳しく知りたい場合はドキュメントの モデルの反復修正  を確認してください    AutoMLで作成したモデル予測についてGoogle Cloud内で作成し評価が終わった学習モデルは Google Cloud内でオンライン予測とバッチ予測を行えます 作成したモデルをオンライン バッチですぐに試すことができます オンライン予測とバッチ予測の違いは     いつ どのように予測を行うか    が使い分けをします 使い方は モデル作成後にも選択できる為 当初はオンライを考えてたが 夜間にチェックしてもいいかもとバッチ予測へ変更したり オンラインとバッチモデルを併用するなどする事もできます オンライン予測とバッチ予測の使い分けについてです     オンライン予測   は リアルタイムに予測を行います アプリケーションの入力に応じて すぐに予測結果を返す必要がある場合に使用します たとえば ユーザーの行動を分析して 次のオススメ商品を表示するなどの場合に使用されます   本来はプログラムを作成しオンライン予測をする事が多いですが Google CloudのConsoleからも実施できますので初心者でも簡単に使うことができます   オンライン予測をGoogle Cloud Consoleから実施した例        images  png    png      バッチ予測  は 一度に大量のデータを処理して予測を行います すぐに予測結果を返す必要がなく 大量のデータを処理する必要がある場合に使用します たとえば 画像認識で 画像データの中から特定の物体を検出するなどの場合に使用されます      オンライン予測とバッチ予測の使い分け  項目   オンライン予測   バッチ予測     リクエスト   同期   非同期     モデルの配置   エンドポイントにデプロイする   モデルソースから直接リクエストする     データ処理    つのデータに対して予測を行う   複数のデータに対して同時に予測を行う     使用例   リアルタイムで予測を行う必要がある場合   大量のデータを一括で処理する場合       具体的な利用シーン        ユーザーの行動を分析して 次のオススメ商品を表示する　  商品の在庫を予測する    最後に本記事ではGoogle CloudのVertexAI AutoML 画像 のデータ準備 評価 予測についてまとめました 次回以降は 実際の操作について記事したいと思います    note info本記事は年月日に作成しております よって 引用している文章などはこの時点での最新となります ご了承ください   おしらせ弊社X 旧：Twitter では Qiita投稿に関する情報や各種セミナー情報をお届けしております 情報収集や学びの場を求める皆さん ぜひフォローしていただき 最新情報を手に入れてください grinning ,8,2023-12-22
25,25,【python入門】OpenCVを使うその２　画像にモザイクをかける,画像処理,https://qiita.com/Aruhimanahito/items/a65649a566c6bd4ae8e0,この記事は   完走したい 楽しくいろいろやる Advent Calendar   の日目です   画像処理がしたいpythonに画像が入れられることは確認できたので 画像の中の顔を認識できるかやってみます 画像処理もします   画像から顔と目を検出するぬいぐるみの顔は検知できませんでした なので 余っていた証明写真を使いました ここのサイトを参考に こんな感じで書いたら ちゃんと顔と目を認識してくれました   本題顔と目を認識できたので モザイクをかけていきます モザイクの仕方が乗ってたので参考しました 最終的に こうなりました眼にモザイクを書けようとしたけど まだ勉強が足りませんでした   まとめ参考,0,2023-12-22
26,26,ShaderGraphで2つのテクスチャーを合成する【Unity、ShaderGraph、画像処理】,画像処理,https://qiita.com/generosity_honda/items/2c5b75eec1be26ccf294,  ■ 概要つのテクスチャー 画像 を合成する手順  を紹介します この記事を書くに至った経緯として  つの異なるサイズのテクスチャーを合成して枚の画像ファイルとして保存する機能 を実装する機会があったのですが  イメージこんな感じです  実装の候補としてTextureDを使ってピクセルの配列をfor文で回して色を計算して みたいなCPUプログラム側での実装や ゲーム画面に画像を並べてそれをスクショして保存する などがありましたがどれもちょっと イケてない なぁ…といった感じだったので Shaderを使って実装してみました という経緯になってます とりわけ ShaderGraph で実装している記事が見当たらなかったので今回は ShaderGraph を使って実装しました   ■ 環境Unityバージョン　：　Unity  fプロジェクトテンプレート　：　D URP   今回はShaderGraphの機能が標準で搭載されている     URP レンダリングパイプラインのプロジェクトで解説していきます   ※ビルトインのレンダリングパイプラインでもパッケージを後からインストールしてShaderGraphが使えます  その手順はこちらの記事が参考になります     ■ 実装実装の結果から 見た目はこんな感じ スライダーを動かして 合成するテクスチャーのUVを調整できるようにしました   result gif  ShaderGraphファイルの全体像はこちら   スクリーンショット     png      　プロパティ①メインテクスチャー  合成される側のテクスチャー ②サブテクスチャー  合成する側のテクスチャー ③UVのU 横軸 の開始位置  〜 ④UVのU 横軸 の終了位置  〜 ⑤UVのV 縦軸 の開始位置  〜 ⑥UVのV 横軸 の終了位置  〜 を外部から設定可能にするためにプロパティとして定義     　合成範囲のスケールを算出  スクリーンショット     png      　合成するテクスチャーのUV座標を算出  スクリーンショット     png      　Stepノードを使って合成範囲かどうかを判定  スクリーンショット     png  if文で表現するとこんな感じ            範囲内であれば合成する側のテクスチャーのカラーを適用したい         col   subTexture color     　適用するテクスチャーのカラーを決定  合成するテクスチャーのUV座標を算出 で算出されたUV座標を サブテクスチャーのUVに繋ぐ   Stepノードを使って合成範囲かどうかを判定 の判定結果をもとにBranchノードで適用するテクスチャーのカラーの出力を決定する   スクリーンショット     png    ■ 最後に  感想 基本的なShaderGraphの使い方は省略させてもらいましたがこんな感じでつの異なるテクスチャーを合成することができました 今回の記事ではつのテクスチャーでしたが 工夫すれば複数枚のテクスチャーを合成することも可能です また  シェーダー マテリアル で加工した状態のものをTextureDにコピー  すれば その テクスチャーを画像データとして保存する  こともできます 最後までご覧いただき ありがとうございました ,5,2023-12-21
27,27,Fisheye 画像のデータセット,画像処理,https://qiita.com/nonbiri15/items/5684d74d86ba94d87882,Fisheye  魚眼 でのアルゴリズムを評価しようとすると 目的にふさわしいデータセットを見つける もしくは作ることから始める 目的にふさわしいデータセットがあれば幸いだ それらが どのように撮影しているのかを学ぶのは役に立つ そして 自分のユースケースでのデータセットを作る際には参考にできる データセットの詳細 ライセンスの条件などは 各自調査してほしい  引用元      LMS Fisheye Data Set   A data set providing synthetic and real world fisheye video sequences    Screenshot from       png   引用元   引用元   引用元   引用元  撮影環境と撮影装置  Screenshot from       png  撮影画像 引用元     Ricoh Theta の全天球画像 動画RichoのThetaは つの魚眼カメラによって全天球の動画が取得できるデバイスだ それらで撮影された画像は大量に存在する アノテーションが存在しないものの 参考にすることも視野に入れておいていい  　参考元   omni画像のデータセットのリスト,0,2023-12-21
28,28,画像の綺麗さを判定するNIMAスコアを試してみた,画像処理,https://qiita.com/Hirata-Masato/items/e31145993f56ce6bfd6e,このエントリーは  一休 comのカレンダー   Advent Calendar    Qiita   の日目の記事です   はじめに 一休 com  では 施設を検索したリスト画面で画像を大きく見せるつくりをしています 施設が登録した画像を表示するのですが その選定には任意性があります 中には画質が悪いせいで 実際の施設は良いのに予約してくれないといったこともあるかもしれません というわけで 施設画像の画質が良いことはある程度大切になってきますが 定量的に計算するのは難しいです 今回はその計算が行える googleが発表した ResNetベースの画質評価モデル  を利用しました   コード主要ライブラリのバージョンpythonのインポート モデルの定義は以下で行います       レスポンスが正常かどうかをチェック下のように 複数画像URLに対して計算を実行できます   スコア用意した画像だとこんな感じのスコアになりました 全部Stable diffusionの Juggernaut XL  を使って生成しています 施設ページにありそうな画像をイメージしています NIMAは 最低   最高 でスコアリングされます 大体綺麗なのでそこまで差は出ていません 壮大な自然系の画像だとスコアが高くなり 背景がボケてる画像だと低めに出る傾向にあります  スコア      スコア      スコア         image png       image png   スコア      スコア      スコア         image png       image png  画質を悪くしてみます 拡大してみると分かりやすいです スコア台辺りから厳しくなってきますね  スコア      スコア      スコア         image png      image png   一休 com  の施設画像万枚について実際に計算してみたところ 良い画像でも意外とスコアが上がらないケースが散見されましたが スコア 以下は概ね悪いことが分かりました 検索時に提示する画像を選定する一つの基準になりそうです ,3,2023-12-21
29,29,【CNNより有能？】metric learningによる異常検知を試してみた,画像処理,https://qiita.com/takuya66520126/items/ddb271b57b098dcfe81b,  サマリ  metric learningとは 関連する画像同士における特徴量表現の距離を意図的に近く集合させることが可能な学習モデルであり サンプルが少ないときや 未知クラスがあるときに有効な手法である   背景 目的  東京のとあるデータ分析会社のデータサイエンティスト職として従事   社内メンバーで 日本酒コンペ  にチーム参加した際 metric learning  距離学習 を用いた画像検索問題に出会う   銅メダルをチームで取得できたが metric learningについての理解が浅かったので知識の整理のためにまとめたい  コンペの話については年明け以降に 弊社テックブログ  にて掲載予定    qiitaにてmetric learningについての過去記事を調査すると 異常検知との組み合わせた検証記事が多かったのでそれらを参考に 今回は異常検知×metric learningの検証を行った   用語   metric learningとは   metric learningは 画像処理モデルの一つとして利用されている学習手法   特徴空間内におけるデータ間の距離や類似性を適切に学習するため 分類モデルなどによって画像データからベクトルを抽出し 関連する画像同士は意図的にベクトルの距離が近く 関連しない画像同士は距離が遠くなるようデータ間の距離が計算されような学習を実行   定番のクラス分類の学習であれば サンプル間の距離を考慮せず全結合層で分離可能な separable 特徴量になるよう学習してしまうが metric learningだとサンプル間の距離が考慮されるので識別的な discriminative 特徴量を得ることができる   image png    上記の経緯から metric learningは各クラスのサンプルが少ないときや 未知クラスがあるときでも十分な性能を発揮できることが強みであり 顔認識や異常検知 画像検索タスクなどで用いられている   今回は metric learningの中でも実装が手軽なArcFaceを用いて異常検知用データにて精度を検証   ArcFaceではソフトマックス損失関数を拡張して同一クラス間の分散を小さくする工夫を追加することで 同一クラスの距離が近くなるように学習させることが可能 ArcFaceの損失関数についての詳細は こちら  の記事の説明が分かりやすい    異常検知とは   異常値 Anomaly とは あるデータセットにおける予期せぬ変化 または予期されるパターンからの逸脱を指す   異常検知とは それらの異常値を検出し警告するために使用される技術    手法   データセット   実験条件 こちら  を参考に学習 評価を実施     評価対象  ①Conventional   距離学習を利用しない通常のCNNの場合  ②ArcFace  ①②どちらもResNetをベースにモデル構造を定義   ②ArcFaceに関しては最終層に距離学習を行うスクリプトを追加し 出力されるembeddingsは次元とした     学習手順  学習時に際しては 異常 画像を排除し  正常 画像セットのみを利用     評価手順  評価用画像は test データ全量を用いた   すべての評価用画像を 学習済みモデル Conventional  ArcFace に与えて次元のembeddingsを得る   評価用画像のembeddingsと学習用画像  正常 のみ画像セット のembeddingsにおける距離を計算 ※距離計算はcosine類似度により算出   評価用画像jごとに すべての学習用画像  正常 のみ画像セット との距離を計算し そのうち一番短い距離の値を 評価用画像jの正解までの距離 distance j とした   モデルの精度はAUCで評価 評価用画像の内 正常を   異常を  でラベリング   評価用画像の正解までの距離指標  distance j  をもとに AUCを算出   結果  精度評価においてArcFaceがConventionalモデルの精度を上回る結果に       学習条件   AUC       Conventional        あとがき  今回の検証スクリプトについては こちら     今回 metric learningを用いたことで異常検知に関する精度の向上が見られたが 従来の画像分類モデルの方が精度が高い結果になっているケースも他の人の検証結果で見かけたので 従来モデルより精度劣化するユースケースやその理由についても今後理解できるようになりたい   metric learning手法は今回採用したArcFace以外にも複数あるが 実装が簡単な点で気軽に試せるのが良かった 今後のデータ分析コンペで上手く活用できるよう研鑚を積みたい   参考にした記事   Kaggleに挑む深層学習プログラミングの極意 小嵜耕平／秋葉拓哉／林孝紀／石原祥太郎 著   ,6,2023-12-19
30,30,【C言語】平均化フィルタによるPGM画像の平滑化,画像処理,https://qiita.com/Santonn/items/204df5fea5b305939b19,   note alertコードに問題があります．修正をお待ちください．  はじめにC言語の学習として グレースケール画像に対して平均化フィルタによる平滑化処理を行う課題が出た 平均化フィルタのプログラムは 他のフィルタ処理にも応用が利くため残しておきたい．  平均化フィルタとは平均化フィルタとは 周囲の画素の平均をとるフィルタのことである．画素の値を周囲の画素の平均にすることによって 周囲の画素との値のギャップが減り 結果的に画像がぼやけたような画像になる．画像  A    a  ij    から画像  B    b  ij    が得られるとすると    で囲われた部分が 周囲の画素である．   コード C言語で平均化フィルタを実現する以下は 自分なりに作成した最終版である いただいたコメントを参考に修正しました     c  averagingFilter c      画像を読み込む処理     画像の画素は int型次元配列image ySize  xSize で参照できるものとする．                  周囲の画素を左下から反時計回りにチェック                  参照する値が 画素の座標として取りうる値かチェック          行目の計算が終わったときのみスキップされる              行手前に  tempに格納しておいた計算結果を代入              最終行は計算結果をすぐ代入          最終行はこの処理は要らないためスキップ      画像を出力する処理   説明一行の端から端まで計算を行い 終わったら次の行へ   というプログラムになっている．どの画素に対しても周囲を参照するように書かれているため 取りうる値かどうかのチェックが必要．例えば 左上の画素 image     の周囲には画素が存在しない部分がある．そこを参照しようとするとエラーが起こる．平均の計算には  元の値  を用いるため 行目の計算が終わった段階で行目に結果を代入してしまうと 行目の計算で行目を参照する際に元の値が参照できなくなる．それを防ぐために  n 行目の計算結果は  n  行目の計算が終わった段階で代入されるようになっている．最終行はこの処理では行えないため 別で記述している．このプログラムでは 画素が存在しない部分は加算しない   とみなす ようにしているが そもそも画像の端点は計算を行わないというやり方や 周りに同じ画像を並べた画像に対して処理を行い 最後に真ん中の画像を参照することで 画素が存在しない部分を補う方法もあるらしい．平均化フィルタ以外にも 周囲の画素の値を用いて計算を行うフィルタ処理はいくつかある  例えば 平均化フィルタ以外の平滑化フィルタ    や エッジ検出にも用いられる．それらのプログラムは このプログラムの計算部分を変えるだけで実現することができる．   次元配列に格納ではダメなのかimage ySize  xSize と同じ大きさの次元配列をもう一つ定義して そこに計算結果を代入していくのではダメなのか．結論から言うと ダメではない．ただ ローカル配列として定義しようとするとスタック オーバーフローとなる．課題で与えられたPGM画像は×で ローカル配列としてつ確保するには大きすぎたため コアダンプが出力された．課題に取り組む段階でこのコアダンプの原因が分からず悩まされた．そのため 次元配列を用いて無理やり実現することにした．次元配列を用いたプログラムは 計算結果を格納するだけのシンプルなコードである．しかし さんざん苦しんで作ったのであえて次元配列を用いた方を最終版として残したい．  関連pタイル法による値化にも取り組みました      ガウシアンフィルタ メディアンフィルタ等    ,0,2023-12-19
31,31,SAR干渉画像が欲しい男の奮闘記,画像処理,https://qiita.com/nemiko007/items/a36a843a72fda37df92f,  はじめに本記事は SAR干渉画像の入手方法をまとめた記事です この画像が欲しい方にとって役立つ記事だと思います   SAR干渉画像の見方以下の記事を参照ください   SAR干渉画像の入手方法最終的に以下のサイトから得ることにしました    INSAR Browser国立研究開発法人産業技術総合研究所 AIST が運営するサイトであり  SIGMA SAR を使った解析結果と衛星データがダウンロード出来ます ただし PALSARでのものしかないので最近のデータは入手できない難点があります    TelluSARさくらインターネットが運営する衛星画像閲覧サービスです PALSAR PALSAR ASNARO に対応しており 干渉画像の他にも散乱強度のデータも見ることができます 使い方は以下の記事を参照ください    COMET LiCS Sentinel  InSAR portalSentinel のSAR干渉画像のデータベースです 世界中の干渉画像を見ることができます   SAR干渉画像の活用例以上のデータを使い 簡単に地図を作ってみました    富士山における地表面変動の検出  fujiSAR jpeg  上のSAR干渉画像は富士山の地表面変動を解析したものです  Sentinel  南方軌道右 上の地図を見ると中心に渦ができていることがわかります 先ほどの SAR干渉画像の見方のサイトを参照すると富士山はおよそcm衛星から西向きの変動 もしくは沈降が見られると考えられます   まとめ以上 SAR干渉画像の入手方法をまとめました 研究などに使う際は自己責任でお願いします ,0,2023-12-18
32,32,マイクロインタラクションで仕事探しを楽しくする,画像処理,https://qiita.com/yusuke_saito/items/0553ddcf495bac171758,この記事は スタンバイ Advent Calendar の日目の記事です   はじめにこんにちは 株式会社スタンバイでデザイナーをしている齋藤です 求人検索サービス スタンバイ のネイティブアプリを中心に 幅広くUI UXを考える仕事をしています この記事では 他の施策に追われて後回しになりがちな UI操作時のインタラクション について デザインから実装までのコミュニケーションで困りそうな点を想定しつつ 実現までの道のりを書いていきます  おもしろポイントは 前半の イージング基礎知識   イージングまとめ と 後半で SwiftUIのspringアニメーションをOpenCVでkeyframeに変換する異常ノウハウ   複雑なspringアニメーションをキーフレームに変換する が出てくるところです   仕事探しは面倒で大変突然ですが 仕事を探している時って 自分も含めて 楽しくない場面が多いですよね   希望の仕事が見つからない cry   そもそも探すことが面倒 angry   応募しても採用されるか不安 fearful 私たちも 検索精度 求人品質 UXなど様々な改善に取り組んでるものの 基本的な体験としてはポジティブな気持ちになりにくい そんな仕事探しが少しでも前向きなものになるよう 操作自体を楽しくできないか というところから マイクロインタクションをきちんとデザインしたいと以前から考えていました   今回やること例えば お気に入りボタンを押した時のアニメーション これをもっと  良く  していきます   期待する動きを実装できる仕様に落とすシンプルなアニメーションなら  duration   s easing  easeOutで背景色を変更 のような指定をすればOKですが  ボタンを押した時 なんかピョコンと楽しい感じの動きにしたい  くらいの理解度の場合 そもそも今の仕組みで実装できるのか できる場合はどのような指定が必要なのかを明確な仕様に落とす必要があります    エンジニアに相談実装のことは分からないけどイメージはある という場合は 近い動きのアプリなどを見せつつ希望を伝えて まるっと調査から依頼するのも一つの方法かなと思います この場合   エンジニアがアニメーション実装に詳しい場合は 一発で期待する動きになる一方で   イメージと違った場合 どのように修正依頼したら良いかデザイナーも分からないという可能性があります 大抵は 何往復か動きの調整を経て完成にすると思うのですが  定量的な効果の小さい案件に工数をかけられない とか 依頼時のコミュニケーションがスムーズにいかない予感がする とか 様々な理由で やっぱ辞め  となることもあるかもしれません いや でもやりたいんだよ オイ ……ということで まあ 勉強するか〜 基本的な仕組みを理解することで 無理なく実装できるアニメーションを自力で考え 依頼できる状態を目指しましょう    仕組みを理解する昨今 検索すれば大抵の情報は出てきます アニメーションの基礎として イージングはざっくり理解しておくと便利です     イージングまとめ  イージング＝値をAからBまでどんな速度で変えるかを数式で表現したもの  イージングの種類を変えると色々な動きを実現できる  単純な形状のイージングはCSSのtransitionで指定できる  複雑な形状のイージングもCSSのkeyframeで近似的に指定できる  自然に見えやすいイージングの使い分け    easeOut  ボタンの色などユーザの操作を受けたインタクション    easeIn  止まっているものが動き出し画面外に出ていく    easeInOut  止まっているものが動き出し別の座標に移動する    ease  最初に弱いeaseIn 後半強めにeaseOutのような動き CSSにある  アニメーションの時間 duration が長いとユーザの操作を阻害するので 短めにすると程よい感じになる  〜 sくらい   迷ったときは現実空間の物体の振る舞いを参考にすると自然な動きになる  UI操作に過剰なインタラクションは不要この辺りの考え方は iOSやAndroidのネイティブ実装でも共通しているので UIアニメーションは基本的にこの範囲でイージングと時間を指定すれば必要十分でしょう    でもまだ思ってたのと違う イージングはわかったけど そのピョコンじゃないんだよな〜 という時は何らか実装的な頑張りが必要で 大抵はイージングで対応した方が絶対に合理的です が 場合によってフレームワークがいい感じのアニメーションを関数として持っていることもあります 例えば  SwiftUI アニメーション ばね で検索してみると iOSにはSpring animationという関数があることが分かります WWDCでの解説セッションが動画で公開されていました    動かしてみるChatGPTにコードを書いてもらって  SwiftUIのPlaygroundで角の画面にpxの黒い正方形を配置します 白背景の画面をタップするとspringアニメーションで正方形のScaleをと でトグルします springには適度な値で引数を指定してください   いいピョコンの気配があります 色々と数値を変更しながら関数を実行すると パラメータと動きの関係が分かるデザイナーの誕生です だだし こうして生成したコードがそのまま使えることはほぼ無いので 一個前のステップで実装方法をエンジニアと相談しましょう ここから諸々がんばって 期待する動きを実現します できました   複雑なspringアニメーションをキーフレームに変換するiOSがいい感じになったところで 他の環境でも同じ動きをつけたいですよね が そんな関数はありません 残念 さて ここでイージングの説明を思い出してください   複雑な形状のイージングもCSSのkeyframeで近似的に指定できるやりましょう    人力で頑張る場合方針としては scaleの増減が切り替わるカーブの山と谷をキーフレームと考えれば良さそうです   変換したいアニメーションを動画でキャプチャする      動かす図形は大きければ大きいほどアニメーションの誤差が減ります  動画をフレームずつ進めて 動きが止まる山や谷の経過時間をメモしつつ静止画としてFigmaなどにコピペ  コピペしたフレームの四角をトレースし px数からscaleを計算する  サイズが最後に変わった時間と最初に変わった時間の差をアニメーション全体のdurationとする力技ですが これが一番簡単です 最初のキーフレームはanimation timing functionをease それ以外はease in outにしておくと良さそう    pythonで自動化ただ 人力でやるの面倒なんですよね 仕組みが分かる単純作業は人間の仕事じゃないので 機械にやってもらいましょう     Scaleアニメーションする正方形の動画を分析し キーフレームを抽出する     return  時間 スケール 方向を含むタプルのリスト      フレームレートの取得      初期設定          グレースケールに変換          二値化          輪郭を検出              最大の輪郭を取得              方向を決定 増加 減少 変化なし       最後のスケール変更を追加     same のキーフレームを後のフレームに統合し 連続する同方向のキーフレームを後のフレームに統合する      param results  時間 スケール 方向を含むタプルのリスト     return  統合されたキーフレームを含む更新されたリスト       same のキーフレームを統合            i        次のキーフレームもスキップ              同じ方向が連続する場合はスキップ              方向が切り替わる時には後の要素を残す            i        次の要素へ進む      リストの最後の要素を確認して追加 方向が変わらない場合     与えられたデータからCSSのキーフレームを生成する     param results  時間 スケール 方向を含むタプルのリスト     return  生成されたCSSキーフレームと  animate クラスの定義を含む文字列          最初のキーフレームにはease それ以外にはease in outを適用input movは 先ほどの黒い正方形の動きをキャプチャしたものから 必要な所をQuickTimeで切り出して使います 良さそうな気がする 厳密には ばねの反発は徐々に減衰するので easeIn強めeaseOut弱めのカーブをcubic bezierで書くとさらに良い可能性がある   お気に入りボタンはどうなったの そういえば スタンバイアプリのお気に入りボタンを押した時のアニメーション こちら 動きを改良したものが年月くらいにリリースされている予定です よろしければインストールして使ってみていただけると嬉しいです ,5,2023-12-18
33,33,人物全身画像の逆レンダリングを動かしてGUIを実装してみた,画像処理,https://qiita.com/ShunTatsukawa/items/12cfd2108556257cb580,この記事は筑波大学の金森教授   にポートフォリオとしての使用許可を頂き執筆しております   はじめに今回は筑波大学の研究である   光の遮蔽を考慮した人物全身画像の逆レンダリング    のソースコードが公開されていたので 実際に動かしてみることにしました また ただ動かすだけではすぐに終わってしまうので光源位置をマウスで操作したり 光源の色を変えられるようなGUIを実装してみます   目次   再照明ってなによ    Chapter    実際に動かしてみる   Chapter    光源色をいじれるようにしてみる   Chapter    光源の位置を動かせるようにしてみる   Chapter    参考文献   reference    謝辞   shaji   再照明ってなによ 再照明とは ある照明環境下で撮影された物体を 他の照明環境下に持っていった場合に どのような陰影がつくかシミュレーションする技術のことです relightingとも言います この技術は主にコンポジット 合成 に対して役に立つことが期待されています 従来 人物に対する再照明は顔のみを対象とすることが多かったのですが 筑波大学の研究では球面調和関数に可視関数を導入することで全身画像に対しても精度の高い再照明を実現しています 細かい説明については以下のサイトでされていますので興味がある方は読んでみてください   実際に動かしてみる まず 論文の プロジェクトページ    に飛んで光源データとPytorch版の訓練済みモデルとソースコードをダウンロードします  モデルは右クリックして 名前を付けてリンク先を保存 で保存できるよ 次にtest with photos pyで光伝達マップや光源データなどを出力する必要があります photo inputsフォルダに再照明したい画像とそのマスク画像を入れれば 好きな画像で再照明を行うこともできます 以下は 元から入っていた画像で推論を行った結果です   relighting png  この結果をもとにrelight pyで再照明結果をmp出力します 出力結果がこちらです ちなみに モデルをトレーニングしたいときはtrain pyでトレーニングできます トレーニング用とテスト用のライトデータは プロジェクトページ    からダウンロードできます   光源色をいじれるようにしてみる再照明や光源データの仕組みを学ぶついでに リアルタイムで光源色を変えたり光源位置を変えたりできるようにしたいと思います まずは光源色変化です 光源データを覗いてみたところ 列のnumpy配列でRGBを表現しているようでした そこで各配列に係数をかけて正規化し 色をコントロールしてみます ここでsh rot関数は球面調和関数の回転を行っているようです 回転行列Rと光源配列を引数として球面調和関数の係数を返します 次にUIにはtkinterのcolorchooserを使用します ユーザーが選択した色から先ほどの各配列にかける係数を決定します   光源の位置を動かせるようにしてみる光源周分の再照明結果をあらかじめ読み込んで tkinterのscaleと呼ばれるスライダーUIで光源角度を調整できるようにします 結果は以下のようになりました   relighting gif  いい感じにできたと思います 再照明についても 論文を読みながらの実装だったので少し理解できました 余談ですが山や谷などの地形画像に対しての再照明を卒論テーマにしようと思っており 今回の実装を参考に色々試行錯誤しています 今回はデータセット作成などについては取り組めなかったので また機会があったら触れてみたいですね   参考文献   光の遮蔽を考慮した人物全身画像の逆レンダリング     GPUコラム GPUが加速するテクノロジー AI CG研究     謝辞本記事の参考元となる論文の著者である筑波大学の金森教授にポートフォリオとしての掲載許可をいただきました 厚く御礼申し上げます ,0,2023-12-16
34,34,バイナリを読みながらJPEG画像が破損している原因を探る,画像処理,https://qiita.com/d_ogawa/items/e5b665a2ee271543e2b3,この記事は レコチョク Advent Calendar   の日目の記事となります   この記事について弊社レコチョクでバックエンドエンジニアをしています小河です この記事では 破損したJPEG画像をバイナリベースで解析し なぜ破損しているのかを探ります その中でJPEG画像がどのように構成されているか どのように作られているかも触れていくので JPEGを理解する助けにもなれるかと思います 前提知識として以下のものが必要になるのでご留意ください    必須な知識  n進数の概念  ビット バイトの概念   あった方が良い知識  JPEGにおける直流成分 DC成分  交流成分 AC成分  DCT 離散コサイン変換  量子化 DQTセグメントの概念について      以前書いた JPEG画像の 品質 は何が司っているのか   レコチョクのエンジニアブログ  を読んでもらえるとインプットできます  ハフマン符号  目次  前段  問題の画像ファイルを観察する    メタ情報を見る    見た目上の特徴    画像ビューワー間での見え方の違い    総括  大雑把にバイナリベースで画像を観察する  JPEG形式に圧縮されるまでの流れ    ①YCbCr形式への変換    ②ピクセル x ピクセルのブロックに分ける    ③DCT 離散コサイン変換     ④量子化    ⑤エントロピー符号化  試しにイメージデータを見てみる  DHTセグメントについて    概要    DHTセグメントは何を表現しているのか    DC成分に紐づくDHTセグメント    AC成分に紐づくDHTセグメント    DHTセグメントの解析結果  イメージデータを読み解く    問題の画像のイメージデータについて    改ざん前のイメージデータについて  なぜ画像がおかしくなっていたのか      左上の市松模様のようなノイズが何個か続いているのはなぜか     元画像と比べて全体的な色味が赤くなっているのはなぜか   最後に  前段レコチョクでは展開している各サービス内で 画像を表示する必要があります アルバムのジャケット写真が良い例かと思います ちなみに弊社が展開している  レコチョク   だとこんな感じでジャケット写真を表示します↓   image  png    このアルバムだと 未来未来 が一番好きです そのような画像を表示するために フロントエンド 各サービス からのリクエストに応じて 特定の画像を返すシステムをバックエンド側に持っています パラメータに応じて リサイズ処理も行います このシステムは弊社の中では 画像サーバー と呼ばれているため 以降もその名前で扱います 私はこの画像サーバーの保守運用を行なっているのですが あるサービス担当者からこのような問い合わせがきました  画像サーバーから取得したあるジャケット写真の見た目がおかしい 実際のジャケット写真はお見せできないので 同事象を意図的に引き起こした画像をご覧ください   image  png  本来はこうあるべきでした   image  png  原因を調査した結果 画像サーバー内のリサイズ処理に使っているモジュールに異常があると特定できました そのモジュールのバージョンを上げるとこの事象は発生しなくなったためです ただ この画像ファイルには一体何が起こっていたのかが気になりました 後学のために 問題の画像ファイルについて詳しく調べてみることにしました   問題の画像ファイルを観察するひとまず 問題の画像ファイルについてざっと特徴を見ていきます    メタ情報を見るそもそもですが この画像ファイルはJPEG形式のものです したがって  file コマンドでJPEGとして認識されるか見てみると   認識されているようです このファイルのメタ情報を見てみたいので  ExifTool  の結果を見てみます 特に際立ったものはありません    見た目上の特徴改めて問題の画像を見てみると このような特徴があります   左上の市松模様のようなノイズが何個か続いている  元画像と比べて 全体的な色味が赤っぽくなっている  image  png     画像ビューワー間での見え方の違いこの画像ですが 実は画像ビューワーによって表示のされ方が違います macosのプレビューだとこのように 最初は表示されていますが 途中からは灰色で埋められています   image  png  上記の結果から 以下のような仮説が立てられます   問題の画像にはJPEG形式として見た時に 一部不正な箇所がある  ビューワーやビューワーが利用しているJPEGのデコーダーによって画像を読み込むための方法が異なる      前者の全て表示されるビューワーの場合 不正な箇所があっても無理やり読んでしまう仕様になっている      後者のビューワーの場合 不正な箇所があった場合 その時点で表示をやめてしまう   総括  メタ情報 Exif には問題はない可能性が高い   JPEG画像として見た時に 不正な箇所がある可能性が高いしたがって メタ情報以外の部分について問題がありそうです 少し大変ですが バイナリベースで問題の画像を観察してみることにします   大雑把にバイナリベースで画像を観察するただただ進ダンプを見ても辛いので  fq  で問題の画像を見てみます 結果を以下に記載します 右端の列にどのような情報が含まれているかが表示されます 結論 ここに表示されているものについては違和感がありません JPEGはいくつかの  セグメント  という部分に分かれているのですが 必ず入っていなければならないセグメントが含まれているからです これは先ほどの fq の実行結果のうち 右端の列の先頭を切り出したものです        以下略 ここに見えている範囲では SOI や APP というセグメントが含まれています ここは SOI セグメントが存在していることを示しています ちなみにこのセグメントは JPEGファイルの始まりを示す役割があります           marker      prefix  raw bits  valid       code   soi      Start of image  ← SOIという名前がありますよね    ここは APP セグメントが存在していて かつそこにどういった情報が入っているかを示しています ちなみにこのセグメントは JFIFという規格で設定されたメタ情報を含みます 先ほど取り上げたセグメントも含めて 下記のようなセグメントが画像に含まれていました 必須とされるセグメントは全て含まれていました ※ 各セグメントごとに説明は書いていますが よく分からなければ読み飛ばしてもらって構いません  SOI Start Of Image     固定で xFF xD が設定されていて JPEGファイルの始まりを指します   APP Application     JFIFと呼ばれるメタ情報です JFIFはExifの前身ですが この 画像のようにExif APP と同居している時もままあります   APP Application     実はExifの情報はこのセグメントに含まれています   DQT Define Quantization Table     JPEGは圧縮の際に量子化 Quantization という過程を挟みます その時にどの程度の量子化を行うかを定義しています     以前に書いた記事内で 大雑把にですが解説しています  JPEG画像の 品質 は何が司っているのか   レコチョクのエンジニアブログ     SOF Start Of Frame     JPEGファイルの種類や画像サイズといった基本的な情報が含まれています     DQTをどのように適用するかという情報も入っています   DHT Define Huffman Table     DQTによって量子化されたデータは最終的にハフマン符号に変換されます どのような法則で符号化されるのかが示されています   SOS Start Of Scan     この後にイメージデータ ハフマン符号化された画像そのものを表すデータ が続くことを示します     イメージデータについての設定情報も含まれます     固定で xFF xD が設定されていて JPEGファイルの終わりを指します 先述したように 必要なセグメントは含まれているようなので JPEGの構造がおかしいという可能性は低めだとわかってきました そうすると   イメージデータ 符号化された画像そのものを表すデータ が怪しい気がしてきます   問題の画像ファイルのイメージデータを自力でデコードして調べてみることにします ただ デコードするためにいくらかJPEGについての知識をインプットする必要がありますので しばらくお付き合いください      JPEG形式に圧縮されるまでの流れ例外もありますが 大体は以下のフローで圧縮が行われます   スクリーンショット       png     ①YCbCr形式への変換色を表現する方式としてRGB方式があまりに有名ですが JPEGの場合は内部的に  YCbCr  形式を使います RGBはR→赤 G→緑 B→青のつの要素で特定の色を表現します その一方でYCbCrは  Y→輝度 Cb→青み Cr→赤み  で色を表現します ちなみに CbとCrは一般に  色差  と総称されます なぜYCbCr方式が良いのかというと 圧縮に有利だからです  人間の視覚は輝度 Y の変化には敏感だが 色 Cb Cr の変化には比較的鈍感だ という性質があります したがって 輝度 Y の成分より 色 Cb Cr の成分をより強く圧縮することが可能になります 以降の処理はY Cb Crというつの成分に対して それぞれ別々に処理が行われます    ②ピクセル x ピクセルのブロックに分けるY Cb Crというつの成分に対して それぞれ  ピクセル x ピクセル  のブロックに分けます 以降の処理はこのxのブロックごとに それぞれ別々に処理が行われます    ③DCT 離散コサイン変換 xのブロックごとに  DCT 離散コサイン変換   を行います DCTを行うことで   画像を周波数成分へ変換する  ことができます 周波数成分  と思われる方は こちらの記事で説明していますので参照ください→  JPEG画像の 品質 は何が司っているのか   レコチョクのエンジニアブログ  ひとまずこの記事では 以下のことが把握できていれば問題ありません   DCTを行うと   周波数成分へ変換される    周波数成分には 全体的な色味を司る  DC成分  と 主に色の変化を司る  AC成分  に分けられる  一つのxのブロックごとに  つのDC成分  と  個のAC成分  が生み出される   ④量子化DCTによって生成された周波数成分に対して   量子化  を行います 量子化についてもこちらの記事で解説しています→  JPEG画像の 品質 は何が司っているのか   レコチョクのエンジニアブログ  ひとまずこの記事では 以下のことが把握できていれば問題ありません   量子化によって 周波数成分を示すデータの精度が落とされる 圧縮される    ⑤エントロピー符号化量子化されたデータを  符号化  します 符号化にはほとんどの場合   ハフマン符号  が使われます  算術符号が使われることも稀にあるようです こうしてできた符号は SOS セグメントの後に  イメージデータ  として配置されます イメージデータのイメージ図を以下のように示します 図で示したように   つ目のブロックのY 輝度  つ目のブロックのCr 色差  つ目のブロックのCb 色差  つ目のブロックのY 輝度      という順番にデータが配置されます   image  png  上の図ではデータの内容を適当に  という感じで表現していましたが この正体は周波数成分です  ③で周波数成分へ変換されているので   周波数成分はつのDC成分と個のAC成分に分けられる  訳ですが それらはこのように配置されています   DC成分が先頭に配置され そこからAC成分がずらずらと並んでいきます     image  png  また それぞれのDC成分やAC成分は    の数値で表されます   DC成分についてだけは 前のブロックの数値からの差分を表現  しています つ目のブロックのDC成分だけは実際の数値を表現します つ目以降のブロックについては ひとつ前のブロックのDC成分からの差分値を表現します   image  png  例    何個目のブロックか   前のブロックの値との差分   値   の場合   →→→→  という値が記録されます この差分値だけで表現する方法を  差分符号化  と呼びます 差分符号化は 表現したいデータの値が似通っている時にはデータ量を少なくできる 圧縮できる という利点があります 一方で   データの改ざんにはめっぽう弱い  という弱点があります 例えば つめのブロックのDC成分に 以下のような改ざんがあったとします   何個目のブロックか   前のブロックの値との差分   値     何個目のブロックか   前のブロックの値との差分   値                             改ざんされている            改ざんによって 改ざんされたブロック以降の値が根こそぎおかしくなってしまいます 繰り返しますが   差分符号化が使われているのはDC成分だけ  で AC成分は値そのものをイメージデータに記録します   試しにイメージデータを見てみるある程度知識をインプットできたと思いますので 問題の画像の実際のイメージデータを見てみましょう つめのブロックのY 輝度 の部分だけ取り出してみました 何も分かりません   どういったルールで符号化されているかわからないためです 符号化のルールについては DHT セグメントで定義されていますので その内容を見ていくことにします   DHTセグメントについて   概要DHTセグメントは  符号化するにあたって必要な情報が格納  されています 具体的には  どのような符号がどのようなデータと対応しているか   ということを定義しています また DHTセグメントは普通 複数存在します 問題の画像ファイルのDHTセグメントの部分だけ抜き出してきましたが つあります これは 下記の  つの成分ごとに違ったDHTが割り当てられている  からです   輝度 Y のDC成分  輝度 Y のAC成分  色差 Cb Cr のDC成分  色差 Cb Cr のAC成分どのDHTセグメントがどの成分のものなのかは DHTセグメントの後に続くSOSセグメントで定義されています 詳細は省きますが SOSセグメントの内容を読み解くと以下のような対応関係があることがわかります   輝度 Y のDC成分 → つめのDHTセグメント  輝度 Y のAC成分 → つ目のDHTセグメント  色差 Cb Cr のDC成分 → つ目のDHTセグメント  色差 Cb Cr のAC成分 → つ目のDHTセグメント輝度 Y のDC成分を符号化 複号する時にはつめのDHTセグメントに定義されたルールを参照する必要があるということです 他の成分についても それぞれに対応するDHTセグメントを参照して符号化 復号を行います    DHTセグメントは何を表現しているのかDHTセグメントは主に  ハフマン符号とその後に続くデータのビット数  との関係を定義しています また   DC成分に対応するDHTセグメントと AC成分に対応するDHTセグメントとでは内容が異なります   まずは比較的単純なDC成分に対応するDHTセグメントの内容を見ていきます     DC成分に紐づくDHTセグメント実際に 輝度のDC成分に紐づくDHTセグメントの内容を見てみます                                                                                                                  markerxd                ff                                                                                      prefix  raw bits  valid xd                   c                                                                                   code   dht      Define Huffman table s  xd                                                                                                    length  xd                                                                          data  raw bitsxf                                                                                  詳細は省きますが 上記の内容は以下のような内容を示しています   ハフマン符号   その後に続くデータのビット数  例えばですが                 というビット列があったとします 先頭から読み進めていくと  というビット列が存在することがわかります この  というビット列は 上の表の一番上の行のハフマン符号を合致します 表によれば   の後はビットのデータが続きます   の後のビットを切り出すと  です これはハフマン符号ではなく実データを示すので 進数に変換すればデータ値が導けます ということで   というビット列は   というデータ値を示します 以上のことから分かるように   ハフマン符号はデータ値そのものを示しているのではなく データ長を示します   ちなみに  JPEGが圧縮されるまでの流れ の項で示したように DC成分→AC成分つ目→AC成分つ目    という順番でデータが並んでいきます したがって つのDC成分の値を読み取ったら その後に続く個のAC成分を読み取る必要があります     AC成分に紐づくDHTセグメント今度はAC成分に紐づくDHTセグメントを見てみます                                                                                                                  markerx                ff                                                                                      prefix  raw bits  valid x                   c                                                                                   code   dht      Define Huffman table s  x                       e                                                                             length  x                                                                          data  raw bitsx                        c             Qaq       x  b d f f                                                          A                        詳細は省きますが 上記の内容は以下のような内容を示しています   ハフマン符号   その前にの成分がいくつ続くか   その後に続くデータのビット数                                                  ブロックの終端までが続く EOB   DC成分のDHTセグメントと同じく ハフマン符号がデータ長を示している ということは変わりません ただ 以下の点が異なります   ハフマン符号がデータ長だけではなく   その前にの成分がいくつ続くかも示すようになっている       ZRL Zero Run Length という概念も追加されていますが 本記事では扱いません以下のビット列をデコードしながら説明していきます 先頭から読み進めると  というハフマン符号が見つかります  上の表の行目 このハフマン符号の後にはビットのデータが続きます そのデータは  です   という進数を進数に変換すると   そのままです になります したがって AC成分のつ目の成分の値は  ということがわかります AC成分はあと個あるはずなので続けて読み取っていきます   まで読み取ったのでビット目から読み取ります すると   というハフマン符号が見つかります  上の表の行目 このハフマン符号の後にはビットのデータが続きます そのデータは  です   という進数を進数に変換すると  になります したがって つ目のAC成分の値は          ではないです つ目のAC成分ではなく つ目の成分の値が  です つ目とつ目のAC成分は 　  です なぜなら ハフマン符号  は その前に値が  の成分がつ続くからです 上の表の行目を参照  残りのビット列を読み進めると  というハフマン符号が見つかります これは上の表によると EOB End Of Block  と紐づいています これは ブロックの最後までずっと 値が  の成分が続くということを示します したがって つ目から個目のAC成分は全て  になります 総括すると   というビット列は  つ目のAC成分 →     つ目とつ目のAC成分 →     つ目のAC成分 →     つ目 個目のAC成分 →   という意味を持つことになります    DHTセグメントの解析結果つのDHTセグメントの中身を解析すると 下記のような結果となります  一応載せてあるだけなので読み飛ばしてください 輝度のDC成分を表現するときのハフマン符号 つ目のDHTにて定義   ハフマン符号   その後に続くデータのビット数  輝度のAC成分を表現するときのハフマン符号 つ目のDHTにて定義   ハフマン符号   その前にの成分がいくつ続くか   その後に続くデータのビット数                                               ブロックの終端までが続く EOB   色差のDC成分を表現するときのハフマン符号 つ目のDHTにて定義   ハフマン符号   その後に続くデータのビット数  色差のAC成分を表現するときのハフマン符号 つ目のDHTにて定義   ハフマン符号   その前にの成分がいくつ続くか   その後に続くデータのビット数                                                  ブロックの終端までが続く EOB     イメージデータを読み解く   問題の画像のイメージデータについてでは 今度こそイメージデータを読み取るための知識が揃ったのでやっていきます ブロック目の部分のイメージデータを読み取ります この部分です↓  image  png  ブロック目のイメージデータは以下の通りです DHTセグメントの内容と照らし合わせながら解析していくと 下記のことがわかります つ目のブロックのY 輝度   成分            ハフマン符号   その前にの成分がいくつ続くか   成分の値 進数    成分の値 進数   続いて色差 Cr Cb もまとめて読み取ります つ目のブロックのCr 色差   成分           ハフマン符号   その前にの成分がいくつ続くか   成分の値 進数    成分の値 進数   つ目のブロックのCb 色差   成分            ハフマン符号   その前にの成分がいくつ続くか   成分の値 進数    成分の値 進数      改ざん前のイメージデータについて急ですが もう一度イメージデータのビット列を見てください ↑のビット列は     で囲った部分を意図的に改ざんして作りました 改ざん前のイメージデータはこんな感じでした ちなみに 改ざん前のイメージデータはこのように綺麗な画像になります   image  png  ということは 画像がおかしくなった原因として 改ざんした部分が関わっていそうだということが分かります なぜ画像がおかしくなったかを調べるためにも 改ざん前の画像についても同じように解析していきます そうすると   改ざん後の画像とは解釈が異なっている  ことがわかります つ目のブロックのY 輝度   成分            ハフマン符号         その前にの成分がいくつ続くか   成分の値 進数    成分の値 進数       AC          変更前の画像と同じ   同じ                            同じ              同じ               AC   個目のAC成分 について注目してください 改ざん後の画像では  という値でしたが 改ざん前の画像では EOB ブロックの最後まで  に変わっています こうなっているせいで 酷いことが起きています 改ざん後の画像では AC  は普通の値なので 次は AC    AC     と続いていきます 一方で 改ざん前の画像では AC  が EOB なので 次に続くビット列はCr 色差 のものになります したがって   ビット列のうち一部しか変わっていないのに それ以降のビット列の解釈に差が生まれていました   こういったイメージです↓ビット列 値は適当  → 改ざん前の画像   →        Y 輝度       Cr 色差    Cb 色差    改ざん後の画像   →           Y 輝度             Cr 色差      Cb 色差    実際に  それ以降に続くCr 色差  Cb 色差 の値も 修正前のものに比べると大きく変化している  ことがわかります つ目のブロックのCr 色差   成分           ハフマン符号   その前にの成分がいくつ続くか   成分の値 進数    成分の値 進数   つ目のブロックのCb 色差   成分            ハフマン符号   その前にの成分がいくつ続くか   成分の値 進数    成分の値 進数     なぜ画像がおかしくなっていたのか 以上の観察から なぜ画像がおかしくなっていたのかを考察します 改めて 問題の画像を載せておきます   image  png  この画像には以下のような特徴がありました     左上の市松模様のようなノイズが何個か続いている    元画像と比べて 全体的な色味が赤っぽくなっている   左上の市松模様のようなノイズが何個か続いているのはなぜか   つ目のブロックの輝度に対応するイメージデータの一部が 本来あるべきものと異なっていた  ためです これにより   つ目のブロックの色差 およびつ目以降のブロックの解釈の仕方も連鎖的におかしくなっています   どこかのタイミングで帳尻が合ったために 途中からノイズがなくなったと推測できます  ノイズがなくなった以降のブロックは正常にデータを解釈できていると推測できる    元画像と比べて全体的な色味が赤くなっているのはなぜか これも つ目のブロックの輝度に対応するイメージデータの一部が 本来あるべきものと異なっていたためです また ノイズがなくなった後も赤くなっているのは   DC成分の符号化方法である 差分符号化 のため  です  ⑤エントロピー符号化 の項でこのように説明していました   この差分値だけで表現する方法を  差分符号化  と呼びます   差分符号化は 表現したいデータの値が似通っている時にはデータ量を少なくできる 圧縮できる という利点があります   一方で   データの改ざんにはめっぽう弱い  という弱点があります つめのブロックでデータの変化が起きていたのが原因で それ以降のDC成分の値もおかしくなってしまいました ノイズが消えた後のブロックについては 正常にデータを解釈できていると思われますが それ以前のDC成分の値がおかしかったために その影響を受けてしまっています 差分符号化の データの改ざんにはめっぽう弱い という弱点を突かれた形になります また DC成分は全体的な色味を左右します こういうこともあり 全体的な色味が赤くなってしまっていました   最後に最後まで読んでいただき ありがとうございます レコチョクでは ソフトウェア開発だけでない幅広い技術を経験する環境でチャレンジしたいメンバーを募集しています レコチョクに興味をお持ちの方は  レコチョクの採用ページ  をぜひご覧ください 明日の  レコチョク Advent Calendar    は日目 Android Graphics Shading Languageって何  です お楽しみに この記事はレコチョクのエンジニアブログの記事を転載したものとなります  ,3,2023-12-15
35,35,【C言語】pタイル法によるPGM画像の2値化,画像処理,https://qiita.com/Santonn/items/17fd15e720586b1eaa8a,  はじめにC言語の学習として グレースケール画像に対して値化処理を行う課題が出た もしかしたら 同じ課題に直面する人がいるかもしれないため共有したい．  pタイル法とはpタイル法とは   対象の面積が全体の面積のp となるように閾 しきい 値を設定する 値化方法である つまり 対象を真っ白として 画像を真っ白と真っ黒で構成したいならば    \\  真っ白の面積 \\    \\  真っ黒の面積 \\    p     p  となるような閾値を設定し 値化処理をおこなう．   コード C言語でpタイル法を実現する以下は 自分なりに作成した最終版である．   note warnpTileMethod cでは pがパーセントではなく 割合になっています．p  ならば  となります．   c  pTileMethod c      画像を読み込む処理      画像の画素は int型次元配列image ySize  xSize で参照できるものとする．       の各値が出た回数をカウント      降順に  出た回数の総和をとる          総和が全体画素数のp割以上になったら そこをしきい値とする．      しきい値以上かそうでないかで値化する．      画像を出力する処理  関連画像の平滑化にも取り組みました ,0,2023-12-15
36,36,MATLABでPhotoShopの深度ぼかしを再現する。,画像処理,https://qiita.com/ShunTatsukawa/items/c0ad971232b2069bc6e7,  はじめにだいぶ前の話なんですが 大学の授業でMATLABを触ったので 話題のAIと組み合わせてPhotoShopの深度ぼかしフィルターを再現することにしました 今回は画像とその深度マップ 焦点距離を入力とし その距離に応じてフィルタの強弱を変化させることで 実際の一眼レフカメラで撮影したような画像を生成するフィルタの実装を目指します   目次   そもそも深度ぼかしって    Chapter    ガウシアンぼかしとレンズのボケの違い   Chapter    ボケの実装   Chapter    焦点距離の実装   Chapter    実装結果   Chapter    参考文献   reference   そもそも深度ぼかしって PhotoShopの 深度ぼかし 機能では AIによって入力画像の深度を推定し まるで一眼レフで撮影したような写真に加工することができます 恐らくぼかしの強度を 画素の深度によって変えているんだろうとおもいます   deep blur s png   Photoshopで立体的なぼかし加工を行う方法    ガウシアンぼかしとレンズのボケの違いさて まずボケの実装をしましょう MATLABのimgaussfilt関数でボカせばいいと思うかもしれませんがそんなに単純な問題でもありません 下の画像から分かる通り レンズによってできるボケは明るい部分が暗い部分を侵食するようにボケていることが分かります   レンズブラー png  つまり 輝度の高い部分から優先的に平均化フィルタやガウシアンフィルタといった畳み込み処理を行う必要があります これはレンズが対数的な特性を持っていることに由来しています レンズ特性については天下のCanon様の記事で詳しいことが説明されているので気になった方は見てみてください   CANON LOG TECHNICAL SITE    ボケの実装上で述べたようなレンズ特性を再現するには   入力画像に対して指数変換を行う  指数変換した画像に対して畳み込み処理を行う  畳み込み処理後の画像に対して対数変換するという手順を踏む必要があります       指数変換X を入力配列とすると指数変換後の配列X  は    mathX    X  k となります kはフィルムの特性など 撮影画像情報がもつ所定の非線形性で決めなければなりません k     の範囲で調整することで実際のレンズで撮影した画像と似たような結果が得られました    MATLABI imread  matlab png   Id double I  k     R   k  Id       G   k  Id       B   k  Id             畳み込み処理の画像に対して畳み込み処理をします 今回はより畳み込み処理後に画素付近の情報を残すことのできるガウシアンフィルタを採用しました Matlabの標準関数を使ってもいいのですが勉強のために自作しました 適当に記述すると黒い枠のようなものが出来上がってしまうため 今回は画像サイズからはみ出さない範囲で処理を行うよう記述します  画像サイズからはみ出さない範囲で処理      対数変換次にの画像に対して対数変換をします 指数変換で用いた式に対して逆関数を求め 底の変換公式より   mathX    \frac log X   log k  となります  ブラーをかけた画像    出力結果ここで 定番のlennaの写真を用いて出力結果を確認してみましょう 左：入力画像　右：出力画像 σ    lenna比較 png  綺麗にボケていますね これだけでは本当に輝度の高い部分から畳み込まれているかわからないのでMATLABのimgaussfilt関数と比べてみました 左：ガウシアンブラー σ  右：提案手法での出力画像 σ    gauss比較 png  比べると分かりやすいですが 確かに明るい部分が暗い部分を侵食してボケています   焦点距離の実装さて 焦点距離の実装はどうやるのかというと まず入力画像の深度マップを用意します 今回は単眼深度推定モデルとして有名なMiDaSを使用しました 以下のサイトを参考に 入力画像の深度マップを出力します    MiDaS 深度推定モデルをいろんなモノで試してみる  そして出力した深度マップを深度ごと分解して値化し 多層マスクを作成します このマスクに対して異なる強度のボケ画像を適用し 最後に合成するわけです 分かりずらいかもしれないので図を用意しました   手法図 jpg  図のように 例えば深度が奥へ行くほど強い強度のフィルタを適用するようにすれば 奥がボケて手前に焦点が合ったような画像が作れるわけです  なんで手間が奥になってるんだよ 分かりずらい図だな  実際のプログラムではユーザーが指定した焦点距離から離れれば離れるほどボケるようなアルゴリズムにしました    MATLAB 多層マスク作成 指定した焦点距離より手前の処理 指定した焦点距離より奥側の処理  実装結果rが焦点距離で ユーザー側から指定できるようになっています 図のように rを変えることで焦点の合っている物体が 円錐→球→立方体と変わっているのが分かるかと思います   焦点距離比較 png  今回は簡易的な実装なので層にしましたが さらに多層化することでより自然な焦点ぼかしが実装できるでしょう ひとまず MATLAB初心者の実装としては満足な出来でした   参考文献    CANON LOG TECHNICAL SITE     デジカメ画像をキレイにボカそうアルゴリズム編      MiDaS 深度推定モデルをいろんなモノで試してみる     Photoshopで立体的なぼかし加工を行う方法  ,4,2023-12-15
37,37,ImageMagick: 画像をスクリプトで作成,画像処理,https://qiita.com/ekzemplaro/items/4cef39398d0bc08f1caa,テストに必要な　アスペクト比   の画像をスクリプトで作成する方法です 次のような画像が作成されます ,0,2023-12-14
38,38,【JavaScript】メディアンカットの実装例,画像処理,https://qiita.com/mogamoga1337/items/a22bd3c697bb1458f82d,   サンプル   コードメディアンカット部分のみ記載する また スマホのメモリ不足でクラッシュしないように実装を意識している 最後にソートして色情報の配列を返しているのは自分のアプリの都合 しなくてもいい    js   引数はImageDataオブジェクトであること    注意 引数のimageDataはメディアンカットで減色処理されます            透明は無視する           最も要素が多い色空間を選択           色空間のRGBの各要素の最大最小を求める           単色は分割する意味がない           RGBで濃度差が大きい要素を求める           中央値で分割する       分割された色空間の平均値を求める   チラ裏結構減色しても分からないもんだな 面白い    参考にした記事,0,2023-12-13
39,39,マルチカメラモジュールについて調査中,画像処理,https://qiita.com/nonbiri15/items/bd95a48fcdda6e91f7b2,システムによっては 複数台のカメラを接続する必要がある そのために 次の条件が必要になる  複数台のカメラを接続できる電気的なインタフェースを持つこと  その複数台の電気的なインタフェースから信号を読み取れること  そのOSでカメラドライバが認識されること  実際に画像が取得できること通常は こういった部分をすでに用意されている環境でカメラを使っているので 自分でカメラシステムを構築しない限り考えなくてすむ     複数台のカメラシステムを組むときに考えなくてはならなくなること  それらのカメラは 常時カメラ画像を取得しなければならないものであるかどうか   カメラの画像の画像サイズ フレームレートはいくらであるのか   カメラでの転送データはどのようなデータ形式なのか   複数台のカメラは同期している必要があるのかどうか   遅延はどのようであるのか  タイムスタンプは入っているのかどうか   リアルタイム性はどの程度必要か       リアルタイム性を必要としないほどシステムは構築しやすい   複数カメラの画像を独立に処理できるか       複数カメラの画像を独立に処理できるときは システムの構築が楽になる       例：       起こりがちな問題点  画像サイズとデータ定式 フレームレートの組み合わせでは そのインタフェースで規定されている転送レートを超えてしまう       対策例：YUVなどのデータ形式を用いる       対策例：jpegなどの圧縮済みデータを転送する  エンコード デコード処理が加わることで レイテンシが増える    jpeg画像への圧縮にすると エンコード デコードの時間だけ遅くなってしまう       他にも 圧縮ノイズが画像認識 画像処理の際に悪さすることがある   同期していないカメラ画像は ステレオカメラ処理ができない       だから 市販されているステレオカメラは同期がとれるようになっている       同期されていないカメラからは 整合性のある解釈ができないことがある   遅延は リアルタイム性を必要とするアプリケーションにおいて悪さをする       遅延がどの程度発生しうるのかのワーストケースを見積もること   タイムスタンプの入っていない画像 それに基づく検出結果は 判断に使えない場合がある       リアルタイムの動作を期待するものの場合 必ずタイムスタンプを利用すること   データのコピーが発生しまくることが システムを遅くする       画像データの転送にCPUを介在させないこと      複数台のカメラシステムを組むときのCPUボード側の問題点  複数台のカメラを受け取れるインタフェースを持っているか データ転送を受けきれるかは CPUボード側の問題でもある   そのため CPUボード側の問題を切り離して マルチカメラ側だけを議論しても価値を持たない   Screenshot from       png   引用元    つのカメラモジュールを接続できる例  GMSL カメラである   同軸ケーブルの長さを伸ばせる なので 標準ではcmしか引き伸ばしできない      複数台のカメラから物体検出をする例    画像を別々に処理できる場合なら案：　複数のRaspberryPiによる接続と検出処理  検出結果だけをRaspberryPiから送信する   メインのボードでそれらを受け取って 結果を統合する 利点：  カメラの受け口をRaspberryPiの数だけ増やせる   受信したカメラ画像をそれぞれのRaspberryPiで処理できるので負荷を分散できる   RaspberryPiとメインのボードはEthernetで接続すればよいので 接続が簡単になる   送信する内容が検出結果だけになるので 処理が簡単になる 欠点：  入力数に応じてRaspberryPiの台数が増える    参考複数のビデオストリームの場合  複数台のカメラからの人検出は監視カメラのモニタリングシステムなどで実現されている   必要に応じてそういったものを参考にすること   NVIDIAのソリューションによっては台のPCもしくは台のJetson で 複数カメラに対して同時に物体検出を動作させている例がある    車載分野のマルチカメラの利用例  Screenshot from       png   引用元  運転席よりもさらに前方の位置から度の視野のカメラを利用することで 見通しの悪い場所での安全を確保しやすくなります    ステレオ計測のマルチカメラ化  ステレオ計測だと欠損点が多数生じてしまうので それを補うようにマルチカメラ化のアプローチが存在する       マルチカメラシステム ―  個から  個のカメラを使ったシステム         マルチカメラ D センシングによる  度ステレオビジョンのプロトタイプを開発    Screenshot from       png   引用元  車載の広角カメラの組み合わせで 各方位へのステレオ計測を可能にしている      多視点カメラつの対象物に対して複数の視点から撮影するのは 多視点カメラと呼ばれる 多視点カメラというキーワードで検索してください   Jetsonを利用したJetson プラットフォーム NRU V Series  x GMSL車載カメラおよびGイーサネット対応NVIDIA Jetson AGX Xavier エッジ AIプラットフォーム  マルチビューステレオ multiview stereo というキーワードも使われます     multi view stereo のGithub上のリポジトリのリスト Starの多い順   上記のリンクをご覧ください pdf  UAV空撮画像における次元モデリング SfM MVS ソフトウェアの形状特性比較  pdf  ②次元空間モデリングシステムの構築 －廉価に次元空間をモデル化する技術の開発    ドローンの分野は SfM  MVSの両方が用いられる分野だ 次のように書かれている これらの画像から 点群と それぞれの視点位置とを算出する   image png     車載関係の分野でのマルチビューの記事 Teslaはカメラを使ってどのように世界を認識しているか  この記事は 参考になりました Transformerベースの物体検出が マルチカメラでどのように扱われているのかを概観を知ることができます ,1,2023-12-13
40,40,M12マウントのカメラモジュールを調査中,画像処理,https://qiita.com/nonbiri15/items/2ce677dad83d5bd4a368,画像認識のシステムを作る際に カメラモジュールを選ぶことがある そのための情報を探すヒントをMマウントのカメラモジュールについてメモしようと思う    Mマウント Sマウント Mマウントは 小型のレンズを取り付けるためのマウントです MマウントのカメラモジュールとMマウントのレンズとの組み合わせでは 視野角の異なるカメラを組み上げることができます      取り付け上の注意点  レンズの焦点距離によって レンズのホルダーの適度な長さが変わってくる そのため ホルダーをカットしたり ホルダーを取り替える必要があることがある   Mマウントのカメラモジュールの場合 固定焦点であることが多い   レンズのピント調整の手順を標準化すること   ピント合わせができたら レンズが動かないように接着剤で固定する    調査方法M camera などのキーワードで検索すると Mのカメラモジュールやレンズのサイトが多数出てくるので それをヒントにサイトにたどり着いてほしい    Mのカメラモジュールの着目ポイント  使用しているイメージセンサ      イメージセンサの大きさ      イメージセンサの画素数      イメージセンサのpixelの大きさ      イメージセンサの感度  固定焦点かオートフォーカス可能かどうか      圧倒的に多数は固定焦点   イメージセンサの大きさとレンズのイメージサークル      レンズを通過した光は 像面にイメージサークルを作る       そのイメージサークルの範囲だけが 意味のある画像範囲を作る       通常のカメラでは イメージサークルが センサ面をカバーするように作る       魚眼カメラ 全方位カメラなどの超広角のカメラでは イメージサークルがセンサー面をカバーしきらなくなったり イメージサークルがセンサ面の範囲に収まるようになったりする       イメージセンサとレンズの組み合わせの選択は 必要な解像度が出るかどうかの性能の見極めの重要なポイントとなる   カメラの照度特性      利用環境のなかで十分な輝度値 SN比が得られるかを確認すること       イメージセンサの感度特性が重要になる  低照度環境ではどう考えるか      SN比が不足する場合には  画素数を減らして つのpixelサイズの大きなイメージセンサを使うという方法がある    カメラモジュールのインタフェース     MIPI CSI  カメラのモジュールの下位のインターフェースであることが多い   カメラモジュールのISP image signal processor  のインタフェースを使ってカメラの制御 データの取得をすることとなる    CSI    MIPI規格編       MIPI CSIインタフェースの動作確認参考記事を示します    マイコンでカメラモジュールから直接画像を取り込む方法     Ultra Linux で MIPI カメラから画像を取得する  解説編      Raspberry Piとカメラモジュールを接続する     Jetson NanoにCSI カメラを繋いでみたー         USB インタフェースさらにUSBというキーワードを追加して検索してみてください      GMSL インタフェースさらにGMSLというキーワードを追加して検索してみてください    解説記事 組み込みカメラ用 M レンズを選ぶための究極の方法を教えます      参考記事   M Sマウント のレンズについて調査中  ,0,2023-12-12
42,42,異物検出と短波赤外カメラ・紫外カメラ,画像処理,https://qiita.com/nonbiri15/items/37aa4972b94d1c2a6dd6,異物検出は 何かと期待されている分野だと思う    主旨  短波赤外線もしくは紫外光を用いたカメラがある   そのようなカメラを用いたときには 異物検出が簡単になる  場合がある     そのような事例の調査結果をメモする    なぜ RGBカメラ以外を用いるのか次の図は モンシロチョウの可視光画像と紫外線画像である   image png   引用元  モンシロチョウのオスは紫外光で暗く見える モンシロチョウは紫外光が見えるので モンシロチョウにはモンシロチョウのオスとメスとの違いが見えている そのような違いがあるときに 機械学習でのモンシロチョウのオス メスの区別は簡単になる 機械学習で分類可能なものは 取得したデータに分類のヒントになる特徴があるときだ その特徴が明確であるときは分類できるが そのような区別しやすい特徴がないときにはなすすべもない 材料は その吸収 反射特性が波長域によって異なる そのため RGB以外の波長でのカメラ画像が 異物を検出しやすい  ことがある   そのため 異物検出には短波赤外カメラ 紫外カメラなどカメラや マルチスペクトルカメラが使われることがある 解説記事  近赤外線 NIR と短波赤外線 SWIR の特徴と使い分け  短波赤外線 nm以上nm未満の波長域 は水に吸収される    NIRカメラの場合pdf  NIRイメージング技術    image png    写真 パンの乾燥状態の撮像 ①は数時間保存したものを撮影①は数時間保存したもの ②は購入時 パッケージから出したばかりのもの 可視光では判別でない状態でも識別可能となります  引用元    Screenshot from       png   引用元     短波赤外線カメラ SWIR カメラ の場合  image png   引用元    image png   引用元     紫外カメラ UV カメラ の場合  Screenshot from       png   引用元     ハイパースペクトルカメラ  Screenshot from       png   引用元     普及型分光カメラ 普及型分光カメラ    Screenshot from       png     異物検出の機械学習に着手する方には  既存の異物検出の事例を元に 案件の異物検出にヒントを探してみてください   SWIRカメラ NIRカメラ UVカメラなどのカメラを検索してみてください   また それらの波長域のレンズ 照明なども調べるとよいでしょう   SWIRと異物検出というキーワードで検索してみてください    異物検出システムの運用時の留意点以下の見解は この分野では素人の見解です   現場でのシステムの利用者が対応しやすいように作る   まだ学習させていない種類の異物が発生したときに 見逃しが少ないように作る   例：おこげ検出の食品検出であっても 金属の異物混入を求められるかもしれない   生産品目や異物の要因が変動することが考えられるので それに対して現場で適応できるように作ること   生産を年 年と続けていくためには 異物検出システムも入れ替えなくてはならない   システムを作ったときの学習用 評価用のデータを残しておくこと   システムをブラックボックスにしないこと   AIというアルゴリズムは存在しない なんでもかんでもAIという言葉で説明しようとする業者は 自分たちで運用していく現場との相性が悪いかもしれない  個人的見解     注意  ここで紹介している技術の中には特許が成立しているものがあるかもしれません     参考になる展示会 画像センシング展   国際画像機器展  ,0,2023-12-09
43,43,【YOLOv8で学習→物体検出】楽に学習データを用意して好きなものを検出してみよう,画像処理,https://qiita.com/ysv/items/2bc7fe4f927fa2c10156,  はじめに学習済みのモデルを使ってサクッと物体検出をするということと 自分で任意の内容を学習させて物体検出をするということには心理的にまあまあなハードルがあると思っています 学習データの準備が面倒そう 学習させること自体もめんどそうだなと思います 自分が触っていた物体検出の仕組みが用意している学習済みモデルで検出できないものを検出しないといけないことになり 重い腰を上げて一番楽そうな方法を模索したのでその結果を記事にまとめました 学習させないといけなくなったけど 楽したいぜ という人には意味のある内容かと思います 詳しい人は読まなくていい内容です    使用する物体検出のアルゴリズム モデル日本語のドキュメントがあったりPyTorchでサクッと動かせたりと準備が楽そうだったので YOLOv  を使うことにしました  実際楽でした    手順 クイックスタート  に従い準備を進めていきます と言ってもpip一回ですが       パッケージのインストール  環境準備が面倒という人は Google Colab  からどぞ    余談ですがYOLOはバージョンごとに作者がバラバラなので全部YOLOって言っていいのかみたいな議論もあるのですが vについてはUltralyticsという会社がリリースしたのでパッケージ名もこれになってます       早速使ってみるとりあえず動くか試してみましょう yolo predict model yolovn pt source   imgsz 前述のコマンドを実行するとこんな画像が保存されます ちゃんと動きました   zidane jpg  ちなみにこちらが学習済みの内容 個のオブジェクトがあります です 検出したいものがこの中にあるなら追加で学習させる作業はやらなくてOKです     person    bicycle    car    motorcycle    airplane    bus    train    truck    boat    traffic light    fire hydrant    stop sign    parking meter    bench    bird    cat    dog    horse    sheep    cow    elephant    bear    zebra    giraffe    backpack    umbrella    handbag    tie    suitcase    frisbee    skis    snowboard    sports ball    kite    baseball bat    baseball glove    skateboard    surfboard    tennis racket    bottle    wine glass    cup    fork    knife    spoon    bowl    banana    apple    sandwich    orange    broccoli    carrot    hot dog    pizza    donut    cake    chair    couch    potted plant    bed    dining table    toilet    tv    laptop    mouse    remote    keyboard    cell phone    microwave    oven    toaster    sink    refrigerator    book    clock    vase    scissors    teddy bear    hair drier    toothbrush      学習させる今回諸事情によりバーコードを検出させる必要がありました これは事前学習されていませんので自分でどうにかする必要があります この記事の説明はバーコードで進みますがこちらを自身の検出したいものに置き換えて進めてください ちなみに 学習に関するドキュメントはこちら→ Model Training with Ultralytics YOLO          学習方法の確認ドキュメントを見るとわかりますが つのCPU or GPUを使うのか 複数GPUを使うのかなど どんなハードウェアを使うかで微妙にやり方が変わります 私はMのMacbookを使っているのでその例を書きます   ベースとするモデルmodel   YOLO  yolovn pt    M macのGPUを使ってモデルを学習trainの引数を説明します  data   これは学習に使用するデータセットの設定ファイルへのパスです あとで出てきます  epochs   エポック数です 要は学習を何回繰り返すか指定します  imgsz   入力画像のサイズの指定です xピクセルの画像サイズを使うということです  device   学習に使用するデバイスの指定です  mps  つまりAppleのMetal Performance Shaderフレームワークを使うぞ という指定をしています M使うぞーということです このプログラムを動かせば学習開始できるわけですがその前に学習用のデータを準備しないとです       　学習データの調達通常学習データを準備するとなると 種類の画像のセットを用意する必要があるかと思います ※割合はイメージです   Train 学習用データセット   全データセットの大部分を占める   程度  モデルはこの画像群からパターンを学習する    Valid 検証用データセット   全体の一部を占める   程度  トレーニング中にモデルの性能を評価するための画像群 モデルのトレーニングが適切に進行してるか確認する    Test テスト用データセット   全体の一部を占める   程度  トレーニング中には使用されず モデルの最終評価に使用する 未知のデータに対してうまく動作するか確認する これらを何百枚 何千枚と自分で用意してひたすらラベリングを…というのは ちょっと大変なので公開されているものを使うことにしました  roboFlow  というモデル構築のプラットフォームがあるのですが その中に Roboflow Universe  という オープンソースのデータセットを公開してくれている大変便利なページがあります ここで検出したいものを探してみてください 私はバーコードを検出する必要があるのでバーコードのデータセットを探してみました   スクリーンショット       png  便利なのはフォーマットも選べることですね 今回はYOLOvですがそれ以外にも色々な形式を選択できます  ダウンロードにはアカウント登録が必要です ちなみに Try Pre Trained Model という選択肢もあり…  スクリーンショット       png  ↓こんな感じでソースコードのサンプルもあり API経由で学習済みのモデルをすぐ使うことができます 使いたいだけで学習方法興味ないです という人はこの記事はここまでで良いと思います 笑学習させてみるぞ という方はYOLOvのフォーマットを選んでダウンロードしましょう   スクリーンショット       png  ダウンロードしてみた中身です data yamlが先ほどちょっと出てきたデータセットの設定ファイルです   スクリーンショット       png   test train valid それぞれのフォルダに画像ファイルとラベルのファイル 矩形領域を表したもの が格納されてます       　学習の実行データセットが調達できたとので学習を実行してみましょう   ベースとするモデルmodel   YOLO  yolovn pt    M macのGPUを使ってモデルを学習学習が終わると  runs runs detect train weights  というフォルダに last pt と best pt という二つのファイルが生成されます これらが学習を終えたモデルになります last ptは学習の最後に保存されたモデルの重み best ptは学習中 最もパフォーマンスが良かった重みのファイルです       動作確認早速動かしてみます 机の上にあった本の写真で推論させてみたところ無事バーコードが検出されました   スクリーンショット       png    まとめデータセットの準備が大変そうでとっつきにくかったのですが 公開されているデータセットが世の中にはroboFlowに限らず色々とあるためそこまでハードルを感じる必要もないなと思いました   そもそも自分で学習させなくても APIが結構ありますが   ,20,2023-12-08
44,44,AIと機械学習、画像認識のまとめ,画像処理,https://qiita.com/yuki_H_Web/items/49240622b463317c419b,これは  フェンリル デザインとテクノロジー Advent Calendar    日目の記事です 年にフェンリルに入社したウェブエンジニアのyukiです 昨年まで年ほど画像認識系の会社にインターンを通じて関わっていました そこでは主にデータ収集 加工から学習 結果の比較検討までを担当しました 機械学習の業界から離れ ウェブエンジニアとして出発した今だからこそ知識を再整理したいと思い 今回の記事では最近話題の人工知能と機械学習の違い 画像認識の種類などについて書きます なお 中身のアルゴリズムの構築等は専門的に行っていないため 正しい知識と異なる点があるとは思いますがご了承ください   AI 人工知能 と機械学習の違い   AI 人工知能   人工知能は 通常は人間の知性を必要とするような方法で または人が分析できないような規模のデータを扱う方法で 推論 学習 行動を行えるコンピュータとマシンの構築に関する科学分野です  人工知能 AI とは  より引用    人工知能  は多くの場合 それ自体がシステムと考えられていますが 複雑な問題を解決するために推論 学習 および行動できるようにするために システムに実装された一連のテクノロジーです  人工知能 AI と機械学習 ML の比較  より引用  要約すると AIは単一の機能を実現するものではなく 幅広いさまざまな事象に対して人が扱えない規模のデータ等を用いて解決方法を導き出すものです 今話題の ChatGPTなどの大規模言語モデルは 人工知能に当たると思います    機械学習   機械学習  は 人工知能 AI のサブセットで 機械またはシステムが経験から学習して改善を自動的に行えるようにします 明示的なプログラミングの代わりに 機械学習はアルゴリズムを使用して大量のデータを分析し 分析情報から学習してから 情報に基づいた意思決定を行います  機械学習アルゴリズムでは 時間の経過とともにパフォーマンスが向上し より多くのデータが公開されるようになります 機械学習モデルは 出力データ つまりトレーニング データに対してアルゴリズムを実行することでプログラムが学習するものです データ使用量が多いほど モデルの精度が向上します  人工知能 AI と機械学習 ML の比較  より引用  要約すると 機械学習は人工知能の一部であり システムが自律してデータを学習 改善し 分析結果から情報に基づいた意思決定を行うものです 私が関わった画像認識というジャンルは こちらの機械学習の一部になります   画像認識画像認識の種類について大まかに紹介します    Classificationつ目は Classification 画像分類 です こちらは枚の画像がどのようなものに当てはまるのかを推測するものです 例えば AかBかを自動で判別させたいという時に使えるものになっています 工場では異物の混入の検知などで使われています 主なモデルとして ResNetやMobileNetなどが有名です どちらかというと少し古いイメージがあります    Object Detectionつ目は Object Detection 物体検知 です こちらは枚の画像の中に事前に学習した特定の要素を検出し それをバウンディングボックスとクラス名 信頼度を推測するものです   yolo jpeg    より引用 上記の画像はObject Detection segmentationの結果が出力されています 赤色と緑色の線で囲った四角のクラス名 Person  tie と信頼度         が出力されているものになります 主なモデルとしてYolo you only look once などが有名です 実際の事例としては 交通量調査の車両のカウントや工場等の異常検出 顧客分析 人流検知などがあります Object Detectionをメインで扱っていたため この画像を見ると懐かしい気持ちになります    Segmentationつ目は Segmentationです   yolo jpeg    より引用 上記の画像の人物が赤色の線で囲った領域で重ねられている部分があるものがSegmentationの出力結果です SegmentationはObject Detectionとは違って出力結果がより細かく物体を特定して出力されていることが分かると思います こちらの主なモデルとしてMask R CNNがあります 実際の事例としては 医療分野でのMRIのような病気検知などで使われることがあります 改めてSegmentationについて調べてみると 現在は Semantic Segmentation  Instant Segmentation Panoptic Segmentationの種類があります 中には個別の領域にそれぞれIDが振られ 認識できるようなものもできているようです ReID Person Re Identification と呼ばれる人認識の分野では 背景等のノイズ除去のために使用することなどがありました    感想AIと機械学習 特に画像認識の分野の知識の再整理をしました 振り返るにあたって さまざまなリポジトリや記事を見ましたが 業務で使っていた時よりもどんどん技術が進んでいるなという印象を持ちました この記事ではあまり書いていませんが 自分の好きな分野であるReIDでは 生成AIほどの急成長はないものの さまざまなアプローチから衣服交換問題や前から見る姿と後ろ姿の違い等の課題を解決していく動きがありました 今後 一気に急進化する可能性があるため 今後ともしっかりチェックしていこうと思います 参考資料： 人工知能 AI とは   人工知能 AI と機械学習 ML の比較   Yolov公式リポジトリ   画像分類 物体検出 セグメンテーションの比較  ,2,2023-12-08
45,45,【MATLAB】モザイクアートを手実装する（モノクロ画像編）,画像処理,https://qiita.com/dokagui_tairan/items/69d9a4d0422fb481f37a,  はじめに matlab で画像処理の練習としてモザイクアートの実装を行ってみたいと思う．既存の実装例をググらずにノー知識で実装を行うため 処理が稚拙だとか滅茶苦茶になっているとか色々あると思うがご容赦願いたい．タイトルにもある通り処理対象はグレースケール画像のみとする．こちらはカラー画像よりグレースケール画像の方が実装の難易度が低いと考えたためである．いい感じに行けたら次はカラー画像に対しても実装を行いたい．なお画像処理を行うが  matlab の Image Processing Toolbox は持っていないので あくまで matlab 標準の関数のみを用いて実装を行う．  実装   対象となる画像の指定   matlab  対象の画像を指定  ファイルのフルパスを作成file   fullfile p  f    ファイルを読み込むrgbImage   imread file    オリジナルの画像を表示figure  imshow rgbImage    グレースケール変換grayImage   rgbgray rgbImage    グレースケール変換後の画像を表示figure  imshow grayImage  画像についてはいらすとやの  踊るサンタクロースとトナカイのイラスト   を使用した．前述の通りグレースケール画像に対して処理を行うため 変換してしまう．    モザイク処理   matlab  モザイク分割widthN       横方向の分割数を指定する  縮小率の設定によってはwidthの値が大きくなったり小さくなったりするので再定義する          画像サブセットの輝度の平均からモザイク処理を行うforループ内の画像サブセットの座標を取り出す処理は  偶然にもとづくプログラミング手法  を用いて実装を行っている．モザイク処理を行った．これで読み込むデータの下準備は完了である．   画像データセットの検索モザイク処理を行った画像の各ブロックに対して 画像データセットから近しい色の画像を探してきて当てはめていく処理を行う．    使用するデータセットいい感じのデータセットが無かったので とりあえず取り急ぎパソコンに大量に保存されていた   お兄ちゃんはおしまい    のスクショを使用した．枚も画像があったので十分だと思われる．    検索処理の実装  素材を検索pwd   pwd    カレントディレクトリのバッファcd p     ディレクトリを移動cd pwd     ディレクトリを戻す       × のみ処理      データが多すぎる場合は途中で打ち切ってもいい  検索結果の可視化構造体 im を定義し  im value に画像データの平均輝度 uint 諧調   im file に画像データのフルパスを格納する．それを平均輝度 に加算した値  輝度は だが matlab のインデックスはスタートなので にする をインデックスとして image table の中に cell として入れ子で格納する．最後に結果を image table mat として保存する．次に使う場合には load image table mat とすればロードできる．  fig png  データセットの輝度の分布はこんな感じ．輝度以下とか以上のデータが少ないけどまぁいいでしょう．  モザイクアート作成          ドンピシャの輝度のデータが個も無かった場合              近しい輝度のデータを選ぶ              データが見つからなかったら処理をやめて次のピクセルに行く          画像を縮小雑な実装 　工夫した点は もしドンピシャの輝度のデータが無かった場合にはその近辺の輝度のデータを探してくるのと 同じ輝度データのファイルが複数存在した場合にはランダムに選択するくらいかな．なお モザイクの一つ一つにつき毎回画像データをリサイズしているため 一回リサイズした画像はどこかにバッファしておくと処理が高速になるだろう．ほかにも色々粗があってさすがにアレなので誰かリファクタリングしてくれ．志の高い勇者を求む．   結果  output png  結果はこんな感じ．なんとなく上手く行っているように思う．  output png  サンタの顔部分を拡大するとこんな感じで 確かにおにまいのスクショで構成されていることが分かる．真っ白い部分はデータが無かった部分だ．真っ白に近いデータは見つからなかったのだと思われる．   他の画像で作成  g png    g png  水星の魔女でやるとこんな感じ．戦争作品のモザイクアートを日常系作品のスクショで行うという  非常にメッセージ性の強い作品  ができた 大嘘 ．  g png  ちなみに目の部分を拡大するとこんな感じ．  まとめ今回は matlab で画像処理の練習としてモザイクアートの作成を行った．なんとなーく実装はできたように思う．次はカラー画像に対して同様に行いたいが どのように行ったら良いのか見当もつかない．また 今回初めてアドベントカレンダーに参加してみた．締め切りがある中で記事を執筆する必要があり スケジュール管理がガバったためお見せ出来ないようなコードを公開せざるを得なかった．まぁぶっちゃけ仕事でもこのくらいのクオリティで仕上げてることも多いので良いか よくない ．,9,2023-12-06
46,46,VertexAI AutoML （画像）を使ってみませんか？,画像処理,https://qiita.com/kccs_nobuaki-sakuragi/items/01cf72e38e0eb096557d,   note弊社X  では Qiita投稿に関する情報や各種セミナー情報をお届けしております 情報収集や学びの場を求める方々にぜひフォローしていただき 最新情報を手に入れてください 是非フォローお願いいたします こんにちは 京セラコミュニケーションシステム櫻木   kccs nobuaki sakuragi です 最近はGenerative AIが流行りですが 以前からあるGoogle Cloudで提供されているAutoML Visionについて紹介します しかしながら AutoML Visionのサイトを見ると次の様なサービス終了の案内がでています       images  png    png  安心してください 同じ機能＋αで機能はVertexAI AutoMLのメニュー内にあります    note alert年月日を過ぎるとGoogle Cloudで使用できなくなります 以前のAutoML Visionのすべての機能と新機能は Vertex AIプラットフォームで使用可能です AutoML Visionは VertexAI AutoML 画像 へと名前が変わっています 今回は 名前が新しくなったVertexAI AutoML 旧AutoML Vision メニューを使いながら Google Cloudの画像AIをコードを記述せずモデルトレーニングしたい方向けに複数回にわけ記事を書いてみます 第回目の記事は VertexAI AutoML 画像 の特徴をまとめた記事となります    note 年月日時点で作成した記事です   本記事の対象者  画像AIに興味のある方  Google Cloudの画像AIを触ってみたい方  お試しで画像AIは触ってみたいが サンプルデータが無く 使えない方  Google CloudのVertexAI AutoML画像で何ができるか知りたい方  AutoML Visionから VertexAI AutoMLへ名前が代わり事例を探している方   Google Cloud VertexAI AutoML とは　ドキュメントでは  Vertex AI AutoML  項目に次のように記載されています    note 機械学習 ML モデルでは トレーニング データを使用して トレーニングされていないデータの結果をモデルが推測する方法を確認します Vertex AI AutoMLを使用すると 提供するトレーニング データに基づいてコード不要のモデルを構築できます VertexAI AutoML 以下AutoML を利用すると最小限のIT技術の知識と操作でモデルを作成およびAIモデルのトレーニングを実施する事ができます    AutoMLで作成可能なモデルAutoMLを利用して構築できるモデルはつのタイプがあります 構築できるモデルは 所有するデータの種類によって 作成可能なモデルが変わってきます    データの種類       サポートされる目標 推論パターン       画像データ       分類 オブジェクト検出      映像データ       アクションの認識 分類 オブジェクトの追跡      テキストデータ    分類 エンティティ抽出 感情分析        表形式のデータ    分類／回帰 予測     AutoMLを利用すると Googleが用意したモデルを使い モデルを作成 精度の評価 活用までをEnd to Endでする事が可能になります 利用者は データを用意するだけ モデルの評価とモデル精度に注力する事ができます 今回は AIの中でも良し悪しがわかりやすい AutoMLにある 画像データの分類を紹介します    Vertex AI  画像 の特徴VertexAIを使用すると 画像ベースのモデルをコードなしで作成でき 学習と評価が簡単に行えます     なぜ VertexAI AutoML 画像 を利用するのか 機械学習やAIの初心者 中級者機械学習を活用 評価をするのに 時間短縮したい方は絶対に活用したほうがよいです オススメの理由は次の点になります        機械学習の環境構築が不要  Pythonのバージョンやライブラリ データセットの保存先などを意識せず始められます 広範囲の技術を必要としない為 殆の方は 運用設計工数も下げることができます        作成したモデルは別の場所でも利用可能  作成したモデルはGoogeCloud以外のさまざまな場所で簡単に利用できます   サーバやPCはもちろん Andorid端末やiPhone端末に組み込むこともできますし アプリを組めばカメラと連動させることも可能です 事例 AutoML 画像 で作成した物体検知モデルをGoogleGlass Android OS で使う事もできます   GoogleGlassで動かした時のデモ画像　      images  png    png    ※GoogleGlassは年月日以降は販売終了しています  Glass Enterprise Edition の発表に関するよくある質問   Glass Enterprise Edition ヘルプ  事例 Google Cloud Vertex AIを利用した外観検査AIシステムの導入この動画は弊社で実施した VertexAI AutoMLVisonを活用した外観検査事例の動画です このように 作成したモデルは GoogleCloudの垣根を超えて利用できます 上記の事例以外でもオンプレミスのサーバやEdge RaspberryPiやAndroid iPhone  別のクラウドでも活用する事ができるのが特徴です        説明可能なAIが利用可能 AutoML Edgeのみ 　機械学習のプロジェクトの困難な所のつに このモデルはどこを見て 答え 予測 推論 しているのか  を説明する事です  いろいろな意見があるとおもいますがあくまでも私見です  その為 機械学習のモデルを可視化する為に活用するのがExplainable AIです 機械学習の活用が進んでいくと 機械学習モデルの精度を評価する為 説明可能なAI Expenable AI の重要度が増してきています その 説明可能なAI Expenable AI がGoogeCloudのAutoMLも利用可能です  Vertex Explainable AI の概要     Vertex AI     Google Cloud  たとえば 物体検知の場合はその物の座標がわかりますが 分類の場合どこを見て判断しているのかがわかりません AutoML 画像 ではEdgeモデルのみになりますが ExpenableAIを利用できます また AutoML 表形式 でも分類 回帰モデルで利用可能です AutoMLを活用すると 最低限の知識で機械学習が試し モデルの評価もGoogleCloud機能だけで完結できる特徴があります    AutoML を活用する際のステップAutoMLモデルのトレーニングと利用のステップは データ型や目的 推論パターン に関係なく同じです VertexAI AutoMLを使うメリットは データセットの作成から評価までをGoogeCloud内で完結できます その為 機械学習でよく問題になる実行環境の構築が不要です VertexAIのメリットはデータがあれば画像AIモデルが作成できる事です これにより データ収集やモデルの評価に時間を割り当てる事に注力できます 画像AIの導入STEPを一般的な手法とAutoML 画像 にわけ表にしてみました    STEP      一般的なSTEP概要    一般的な機械学習環境 自身のPC／サーバC の場合   VertexAI AutoMLを利用する場合 　         トレーニング データを準備する    各自で準備しPC サーバ内のHDDへ保存　  各自準備しGoogle Cloudへ保存        データセットを作成します   PC／サーバ内で作成　  Google Cloud上で作成       モデルをトレーニングする   PC／サーバ内　   Google Cloud上でトレーニング        モデルの評価と反復処理を行う   PC／サーバ内　   Google Cloud上で再評価       モデルから予測を取得する   PC／サーバ内 任意のクラウドなど　   Google Cloud or オンプレミスサーバ or Edgeデバイス       予測結果の良し悪しを判断する   PC／サーバ内 任意のクラウドなど　  GoogleCloud内 PC／サーバ GoogeCloud以外のクラウドなど  ※ここでのEdgeデバイスは RaspberryPiやAndorid端末／iPhone端末などを指します 画像からの物体検出や分類であれば PC サーバで実施できることはVertexAI AutoML 画像 で実施する事が可能です    AutoML 画像 で作成可能なモデル形式について基本的に GoogeCloud内でのみ利用する場合は モデル形式を意識する必要はありません モデル形式を意識する必要があるのは GoogeCloud内でもGCEや個別にコンテナで稼働したい場合 Android OSやiPhoneなどで利用する場合のみになります   利用可能なファイル形式　  モデルの種類           分類 マルチラベル   オブジェクト検出   TF Lite   ○   ○　 　○   Edge TPU TF Lite   ○   ○　 　×   コンテナ   ○   ○　 　○   Core ML   ○   ○　 　×   Tensorflow js   ○   ○　 　○  次のような利用シーンからモデル形式を選定してください    利用可能なファイル形式       利用シーン     TF Lite    エッジまたはモバイル デバイスでモデルを実行する場合    Edge TPU TF Lite    Edge TPU デバイスでモデルを実行する場合    コンテナ    Docker コンテナで実行するため モデルを TF Saveモデルを実行する場合    Core ML    iOS デバイスや macOS デバイスでモデルを実行する場合   Tensorflow js    ブラウザと Node js でモデルを実行する場合  この様に AutoML 画像 のモデルはさまざまな利用シーンで活用する事ができます モデルのエクスポート方法の詳細は  AutoML Edge モデルをエクスポートする     Vertex AI     Google Cloud  を参照してください AutoML Edgeモデルを指定すると 活用範囲がGoogeCloud内からさまざまな所に広がっていきます   最後に本記事ではGoogle CloudのVertexAI AutoML 画像 の特徴を中心に紹介しました 次回以降は モデルの評価や予測について説明したいと思います    note info本記事は年月に作成しております よって 引用している文章などはこの時点での最新となります ご了承ください ,12,2023-12-05
48,48,Viteの最小限設定,画像処理,https://qiita.com/oym/items/21f595da7305efa06b58,   vite config tsを作成するルートディレクトリに作成する   Sassを使用する   sh  npm i   save dev sass   ベンダープレフィックスを使用する   sh  npm i  D postcss autoprefixerルートディレクトリにpostcss config jsを作成して 以下を記述   画像を圧縮するビルド時に画像を適切なサイズに自動的に圧縮する optimizationLevelやqualityなどのプロパティの値を変更することで 圧縮率を設定できる    ソースマップを有効にする   サブディレクトリにビルドする場合   もしくは 明示的にも指定可能      aboutにデプロイする場合  base    about  小さい規模のプロジェクトであれば 上記の設定だけで十分対応できる ,1,2023-12-05
49,49,初心者でも大丈夫！GitHub Copilot Chatで始めるKaggle画像系コンペ,画像処理,https://qiita.com/kmizunoster/items/fde9c4b3db082c5be05f,この記事は  Safie Engineers  Blog  Advent Calendar    の日目の記事です    はじめに セーフィー株式会社   で画像認識AIの開発エンジニアをしている水野です 年月日付で現職にジョインしたのですが 本業で語れるネタがまだ無いので趣味でたまに参加している    Kaggle   と流行りの生成AIを絡めた記事を投稿します KaggleについてはKaggle Competitions Masterのランクを保持していますので コンペに関する知見も織り交ぜながらお話しできればと思います 早速ですが このような悩みをお持ちの方はいないでしょうか   Kaggleに登録し 定番の タイタニックコンペ  を終えて 次は画像系コンペに参加してみたいが どのように進めたらいいのか分からない  公開Notebookをベースラインとして作業をしているが コードの意味が理解できない  自分でベースラインNotebookを作りたいが 作り方が分からないご安心ください これらは全部    GitHub Copilot Chat   で解決できます 以降の記事では Kaggleの画像系コンペにおけるGitHub Copilot Chatの具体的な活用方法について説明します ただし今回試行錯誤する中でGitHub Copilot Chatの性能を引き出すコツ 制約事項 イマイチな点も見えてきました 現状は完全に人間を代替するものではなく あくまでコーディング作業を補助してくれるツールであることを忘れず 上手く活用していきましょう    背景各論に入る前に そもそもなぜ表題のテーマで記事を書こうと思ったのか軽く説明させてください     なぜ画像系コンペがターゲットなのか まず単純に私自身が画像処理 認識技術が大好きであり もっとたくさんの方にその面白さを知ってほしいという思いがあります Kaggleの画像系コンペでは多種多様なデータに触れられる機会があり面白いわけですが 他のデータ形式のコンペと比較して画像系コンペの参加者数は少ない傾向にあります これにはいくつかの要因が考えられますが つに画像系コンペの取り組み方に関する情報が少ないことが挙げられると思います このような課題意識を背景として 前職在籍時に  Kaggleスコアアップセミナー 画像系コンペ入門   というセミナーを企画し 画像系コンペ入門者向けに基本的な取り組み方について説明しました このセミナーによって画像系コンペに対する一定のハードルは下げられたと信じていますが 近頃話題の生成AIを上手く活用出来ればさらにハードルを下げられる または高く跳べるようになる のでは と思っています     GitHub Copilot Chat生成AIと言えば世の中一般的にはChatGPTの方がホットかもしれませんが ソフトウェア開発に関わるエンジニアにとってはGitHub Copilotの方が日々手元で使う開発環境とシームレスに連携が出来るため恩恵が大きいのではないかと感じています また年月にビジネス版が公開されて以来 導入する企業数も増えており お世話になっている方も多いのではないかと推察します そんな中ビジネス版ユーザには年月に 個人版ユーザには同月にGitHub Copilot Chatのbeta版が解禁されました GitHub Copilot Chatは対話型のコーディング支援ツールで GitHubの レポート  では検証に参加した開発者の が GitHub Copilot Chatを使うことで集中力が増し イライラが減り コーディングがさらに楽しくなったため フロー状態が維持されたと報告しています これはまだChatの存在しなかった年の調査の結果    よりも高い数値となっていて Chatインタフェースの有用性を示しているのかもしれません GitHub Copilot Chatについて使ってみたという記事や基本的な機能の解説記事は見つかるものの より実践的な内容の記事はそこまで多くない印象でした そこで今回はKaggleで活用できるのかどうかを調べてみようと思い立ちました    note warn本記事は執筆時点 年月末 のbeta版ツールをベースにしています この記事を読まれる時期によってはここで説明した方法が通用しなくなったり 出力結果が全く異なるものになる可能性があります 適宜公式の最新情報をご確認ください    基本編    GitHub Copilot Chatのセットアップ  GitHub Copilot Chatを使用するためにはGitHubのアカウントとGitHub Copilotのライセンス  ビジネス版 or 個人版 が必要です      本記事では個人版アカウントで検証しています  VS Codeまたは対応開発環境 JetBrains IDEsやVisual Studio をインストールしGitHub Copilotをセットアップ      VS Codeでのセットアップ方法はこちら   Use GitHub Copilot to enhance your coding with AI        本記事ではWindows WSL環境上で立ち上げたVS Codeを前提として説明します    GitHub Copilot Chatの使い方  VS Code内で以下いずれかの方法でChat UIを呼び出します  テキストボックスにプロンプトを入力しSubmit   Enter キーまたは紙ヒコーキのボタンを押下  すると 回答が得られます    GitHub Copilot Chatにはプログラミング関連であれば何でも質問することができます    一般的な質問を投げた場合には  Sorry  but I can only assist with programming related questions   とあしらわれてしまいます  Chat view内で Shift Enter を入力することでプロンプトを複数行にわたって入力することができます  スラッシュコマンドを使用することで様々な機能を呼び出すことができます    スラッシュコマンド今回の記事で利用するコマンドは以下の通りです その他のコマンドについての説明は  Use GitHub Copilot to enhance your coding with AI  をご参照ください    実践編ここからは冒頭で挙げたつの課題に対して それぞれ具体的な事例を交えながらGitHub Copilot Chatが具体的にどのように利用できるのか説明します     サマリ実践編はそれなりにボリュームのある説明になっているので まず最初に全体像のサマリをお伝えしておきます 時間の無い方はここを読むだけでも要点が掴めるかと思います  課題 解決策 やり方  画像系コンペの進め方が分からない コンペに関する質問をする  Chat viewを開き 聞きたい内容を質問する さらに深堀したい内容があれば追加で質問する  コードの意味が理解できない 既存コードの内容を説明してもらう  説明して欲しいコードを選択状態にし Inline chatを開き explain コマンドを実行する 好みに合わせて説明を生成する方法を指示する 日本語で説明してもらう 五歳児にも分かるようにしてもらう etc    ベースラインNotebookの作り方が分からない 要件を伝えてNotebookを生成してもらう  Chat viewを開き 作りたい処理の内容を記載し  newNotebook コマンドを実行する 新しい処理を追加する場合は新規コードセルでInline chatを開き 要件を伝えて生成してもらう       コンペに関する質問をするGitHub Copilot Chatにはプログラミング関連であれば何でも質問することができます 以下ではいくつかの例を紹介します      コンペの進め方を聞いてみる定番の MNISTコンペ  に関して質問してみました さすがによく使われるデータなだけあって具体的な手順についても上手く回答できているようです   image png  続いて定番ではないタスクとして私も参加した DFLコンペ  に関しても聞いてみました 単純に質問すると回答を拒否されるようです   image png  そこでMNISTのコンペに関する質問を回してから 同じ質問をしてみました そうすると回答を得られました しかしこのコンペはサッカーの試合動画からある特定のアクションが発生したタイミングを検知するというコンペなのですが 出力された手順を見るからにテーブルデータコンペと勘違いしているようです   image png  具体的なコンテキストを与えるようにプロンプトを修正して 再度質問しました 今度は的を射た回答を返しているようです   image png       手順をさらに具体化する前項の質問でコンペに取り組む際のおおまかな手順は分かりましたが まだ手を動かせるレベルにはなっていないと思います 各項目を具体的にどのように進めればよいのか追加で確認してみます 以下はデータの理解について聞いてみた例ですが 一般的にどのような考え方で行われるのかの説明とサンプルコードが示されました 他の手順についてもステップずつ確認していけば 一通りの手順を具体化できると思います   image png       コンペのデータセットをダウンロードする方法を聞いてみる次に以前のセミナーで題材にした  Plant Pathology    FGVC   について データのダウンロード方法を質問しました Kaggleプラットフォーム上と同じ配置になるように   kaggle input  以下にデータを配置するという情報を伝えています  dataset path    dataset path   必ずしもKaggleプラットフォーム上と同じ配置にする必要はありませんが 同じにしておくと自分で作成したNotebookのみならず公開Notebookもそのまま動かせる等メリットが多いので 合わせておくと便利です Kaggleのアカウント作成に始まり コンペへの参加  Kaggle API  を利用するための設定 Kaggle APIを用いたデータセットのダウンロード方法  kaggle api まで事細かに教えてもらえました 強いて言えば最後のunzipコマンドがフラットに解凍してしまうのがイマイチで コンペ名を指定して解凍するコマンド  unzip  kaggle input plant pathology  fgvc zip  d  kaggle input plant pathology  fgvc  になっていれば完璧でした   kaggle api   基本的にKaggleとのデータのやり取りはブラウザ経由でも可能となっていまずが Kaggle APIを使いこなすと様々な処理の自動化等便利な場面があるため まだ使われていない方はこの機会に使えるようにしておくと良いと思います   image png       公開Notebookを動かす方法を聞いてみるプログラミング関係の質問と言えるのか微妙な質問もしてみました 公開Notebookの動かし方についてですが 回答を拒否されることなく完璧な回答が返ってきました GitHub Copilot Chatの内部でどのようなトピック判定処理がされているのかは分からないのですが Kaggle関係の質問であればだいたいプログラミング関係の質問と判定されるのかもしれません   image png       長さ制限で上手く回答が生成されないケースへの対処法いくつか質問を試す中で  Sorry  the response hit the length limit  Please rephrase your prompt  と表示され 回答が途中で終わってしまう状況が発生しました   image png  このようなケースでは以下のいずれかの方法で対応することが可能でした   英語で質問してみる  Chatの履歴を削除して再度質問するChatGPTでも日本語で回答が得られない場合に英語で確認するのはベストプラクティスだと思いますが GitHub Copilot Chatにおいても英語で同じ質問をした場合は最後まで回答が得られました 詳細は不明ですが 日本語では長さ判定の制限にかかりやすくなっているのかもしれません   image png  Chatの履歴を削除して再度質問した場合 最後まで回答は得られましたが  私はAIプログラミングアシスタントであり 具体的な戦略を提供することはできませんが という前置きが入るようになり 急にそっけなくなりました また回答の質もやや悪いように感じます Chat履歴というコンテキスト情報があるかどうかで回答の質が変わるようです   image png        既存コードの内容を説明してもらうKaggleでは各コンペのフォーラム上での情報交換が活発に行われていて EDA 探索的データ分析 やモデルの学習 推論を実行できるNotebookを公開してくれる参加者が多いです 特に初心者の内はそのような公開Notebookをベースにして改善作業をすることが多くなると思いますが そこで問題になってくるのが   第三者が書いたコードを理解すること   です 丁寧にコメントを書いてくれているNotebookも中にはありますが 全部が全部そういうわけではないですし 日本人にとっては言語の壁もあります Kaggle上では基本的に英語でやり取りされます       公開Notebookの内容を日本語で説明してもらうそこでGitHub Copilot Chatにコードの内容を日本語で説明してもらいましょう DFLコンペの精度評価指標を説明したNotebook  Competition Metric   DFL Event Detection AP   Kaggle  を開き つ目のコードセル全体を選択した状態でexplainコマンドを実行しました 全体で行もあるコードセルなので全体の概要情報のみが出力されていますが これがパッと出てくるだけでもコードの理解のしやすさが変わってくると思います 微妙な違いではあるのですが    workspace  explain 日本語でコードの内容を説明してください   等と少し具体的に指示してあげると上手くいくようでした 上手く回答が生成されない場合はあきらめずにプロンプトを修正して再度質問してみてください      詳しく知りたい箇所を説明してもらうさらに詳細に説明してほしい場合は 該当箇所を選択状態にして Ctrl I   Macは Cmd I   で Inline chat  を開き  explain コマンドを実行するという流れになります  event detection ap 関数にはPandasを駆使したコードがいくつかあるので詳細に説明してもらいましょう 対象のコードは以下で 評価動画中の評価対象区間を算出する処理です 実行してみると 一つ一つの関数について説明が出てきました そもそもPandasに馴染みの無い方がこれだけで完全に理解するのは難しいかもしれませんが さらに詳細に調べる際のとっかかりにもなるのではと思われます   image png       五歳児にも分かるように説明してもらう説明をさらに噛み砕きたい場合は 説明の仕方に注文を付けることも有効です 上で説明した評価対象区間算出コードを五歳児にも分かるように説明してもらいました お昼寝という五歳児にもお馴染みの概念を使いながら 頑張って回答を生成してくれています   image png        要件を伝えてNotebookを生成してもらう   note info本節の内容はWindows WSL環境上に Kaggle dockerコンテナ  を立ち上げて動作確認をしています 使用したDocker imageのバージョンは  です 作りたい処理の内容を Chat view  に記載し newNotebook コマンドを実行することでNotebookを生成させることが出来ます 以下ではデータの扱いやすさの都合でMNISTコンペを例に説明しますが 他のコンペでも適用可能な方法ですので 参加したいコンペに応じてプロンプトを修正してください      MNISTコンペのEDA Notebookを作成してもらう例コンペに取り組む際はベースラインモデルを作成する前にデータセットに関して理解しておくことが重要です 与えられたデータセットについて 様々な観点で調査することをEDA 探索的データ分析 と言いますが ここでは実際にEDAを実行するNotebookを作成してもらいます プロンプトおよび実行結果は以下の通りです MNISTは画像識別の定番データセットなのであまり情報を与えなくても問題は無さそうですが ここでは初見のデータセットのつもりでどのようなタスクなのかの情報を与えています 回答としてはまず生成されるNotebookのアウトラインが表示されますので 構成に問題が無いかどうか確認します もし問題があればプロンプトを修正して再度質問します 回答の最後に表示されている Create Notebook をクリックすると実際にNotebookが生成され始めます   image png   Ctrl Shift U   Macは Cmd Shift U   でOutputを表示し TaskでGitHub Copilot Chatを選択すると以下のようなログが確認できます つのコードセルを生成する際にgpt モデルにリクエストを投げてコードの補間を実行しているようです これが生成されるコードセル数分繰り返されて最終的なNotebookが完成します   image png        エラーの修正方法を確認する生成されたNotebookをそのまま動かすとラベルの分布の確認のコードで点だけエラーが出ました   image png  エラーが出た場合はエラーメッセージを Chat view  に貼り付けて修正方法を確認できます ちゃんとコンテキスト情報を読み取った上でコードを生成してくれているようです   image png  言われた通りに修正し再実行してみると問題無くグラフが表示されました   image png        一部コードの改善方法を提案してもらう以下はデータセット中の画像を視覚化するコードです エラー無く描画されていて問題無さそうですが 画像とセットになっているラベルも表示したいところです   image png  このようなケースでは該当ソースコード部分を選択して  Ctrl I   Macは Cmd I   で Inline chat  を呼び出して どのように修正したいのかを入力しSubmitすることで 修正後のコードを提案してもらえます この際にコマンド無しで実行するか  fix コマンドを付けて実行するかで出力結果がやや異なるようです   コマンド無しで実行した場合      labelの描画処理が追加されていますが コメントは修正されていません        スクリーンショット     png     fix コマンドを付けて実行した場合      Split diff viewで表示されていて  split view  部分的に修正を適用することも可能です      labelの描画処理が追加されていて コメントも修正されています        スクリーンショット     png    unified view   コマンド無しで他の修正を何回か試したところ Split diff viewで表示されることもありましたので必ずしもUnified diff viewで表示されるわけではなさそうです   split view    fix コマンド付けて他の修正を何回か試したところ Unified diff viewで表示されることもありましたので必ずしもSplit diff viewで表示されるわけではなさそうです いずれの修正にせよ意図通りに描画されるようになりました 他のNotebookでも修正を試しましたが 現状は fix コマンドを付けて実行した方が修正の精度が高く感じたので 特別な理由が無い限り fix コマンドを利用するのが良さそうです   image png       MNISTコンペのベースラインモデルをPytorch lightningで作成してもらう例EDAが完了したら次にベースラインモデルを作成します 今回は Pytorch lightning  を使用してNotebookを作成してもらいます   image png  生成されたNotebookには以下の問題がありましたので 動作させるためには修正が必要でした 修正はEDA Notebookを作成した時と同様にGitHub Copilot Chatとの対話的なやり取りで実施しました   そのまま実行するとエラーになる記述が残ってしまう      以下が例ですが おそらくPytorch Lightningが存在しなければinstallするということをしたかったのだと推察されますが コードに置換できずにそのままになったようです        image png    今回のKaggle dockerコンテナではPytorch Lightningのversion   がインストールされていたが 出力されたコードは 系に対応していなかった      Pytorch Lightning  は年月にリリースされたばかりなので 学習データの大半はそれ以前のものだと思われるので仕方ない感じもします      これはプロンプトを与える際に具体的なバージョンを指定してあげれば解決するかもしれません   nn Linear の入力チャンネル数が正しくない        image png        生成されたのはx入力でmax poolingが回のモデルでしたので 正しくは    ですが    になっていました  in ch   in ch   ResNet等のよく使われるアーキテクチャがx入力の時の最後のconv出力がxなので何かしら影響を与えているのかもしれません上記の通り 生成されたNotebookそのままではいくつか問題が残ることが分かりましたが 今回の例では分程度で生成が完了しており たたき台のNotebookとしては十分と言えるのではないでしょうか       新しい処理を追加してもらう生成されたNotebookに機能が不足している場合は追加するよう依頼できます ベースラインNotebookにエラー分析処理のつとして混同行列 Confusion matrix を計算する処理を追加してみます Notebook上で新規Codeセルを追加して  Ctrl I   Macは Cmd I   でInline chatを立ち上げ 生成して欲しい処理を指示します すると一瞬で以下のようにコードが提案されました Acceptを押して実行すると正常に混同行列が描画されました   image png    上記コードで生成された混同行列        image png     今回触ってみて分かったこと    性能を引き出すコツ  プロンプトを可能な限り具体的にする      たったワードの違いで回答の質が大きく変わる場合があります      ChatGPTで培われたプロンプトエンジニアリングの技術は同様に有効と思われるので それらを参考にするのも良いでしょう  適切なコンテキスト情報を与える      Chat履歴や開いているファイル等のコンテキスト情報によって回答が変化しますので 得たい回答につながるコンテキスト情報を与えられているかどうか注意しましょう    制約事項   Sorry  the response hit the length limit  Please rephrase your prompt  が表示されて回答の生成が中断されることがある      Copilot Chatの回答が一定の長さを越える時に発生しました      現状は以下のいずれかの方法で対応可能と思われます          英語で質問する          Chat履歴を消してから再度質問する          回答が短くなるように質問のスコープを限定する      プロンプトを修正することで解決できるかもしれません    イマイチな点  同じ質問をしても回答してくれる場合と回答してくれない場合がある      GitHub Copilot Chatは持っているコンテキスト情報によって回答の質が変わるというのは理解が出来るのですが 回答するかどうかの判定も行われてしまうようです   newNotebook コマンドで生成されたコードがPythonの文法的に不完全な場合がある      コンテキスト情報が不足していることによるエラーは仕方がない面もありますが さすがにPythonで許容されない記述をそのまま残すのは頂けないと思いました  最新のモデルやライブラリに対応したコードを出力してくれない      機械学習界隈は進化の速度が速い分野なので新しい技術にすぐに対応して欲しいところですが 新しいものほど学習データは少ないので難しいのかもしれません      最近話題の Cursor   The AI first Code Editor   ではユーザ側で好きなデータを学習させられる仕組みがあるようですので Copilotにも今後そのような仕組みが追加されることを期待しています   まとめKaggleの画像系コンペにGitHub Copilot Chatを活用する方法について 大きくつのトピックを説明しました   コンペに関する質問をする      本記事で述べた質問以外にも色々試してみた感じでは プログラミング関係の質問かやや怪しい質問 例えばKaggleプラットフォーム上での操作方法等 にも答えてくれるようでしたので Kaggleコンペに取り組む中で分からないことはだいたい何でも聞いて良さそうです  既存コードの内容を説明してもらう      やや複雑な公開Notebookについて説明してもらいました 分からない部分を対話的に深堀していけるので 自分で一つ一つGoogle検索して意味を調べながら理解するよりも圧倒的に効率良く作業を進められると思います  要件を伝えてNotebookを生成してもらう      シンプルなコンペについて要件を伝えてNotebookを自動生成してもらいました 現状では一発で動作するものを得られる可能性は低いですが たたき台としては十分に使えるものですので 何から始めたらよいのか分からない入門者にとってこれほど便利なものは無いのではないかと考えています上記の通り現状は制約事項もあり まだまだ改良の余地ありという感じですが ツールの特性を理解して上手く利用すれば コーディング支援ツールとして十二分の働きをしてくれると思います まだ画像系コンペに参加したことがない方は GitHub Copilot Chatの力を借りて ぜひこの機会にチャレンジしてみてくださいね ,8,2023-12-03
50,50,Amazon Textract による「画像からの表データ読込」をブラウザから試す,画像処理,https://qiita.com/RyoWakabayashi/items/b7bd7079d20326d9e4be,   はじめにAWS は様々な AI サービスを提供していますそして それらのサービスはブラウザから簡単に試せるようになっています  サービス      機能                       お試し記事       Elixir 実装例    Transcribe   音声の文字起こし 音声認識    参照       参照      Polly        文書の読み上げ 音声合成      参照      参照      Lex          チャットボット                参照     Coming Soon       Translate    翻訳                         参照      参照      Rekognition  画像認識 参照       参照      Textract     画像からの表データ読込        参照      参照      Bedrock      生成AIによるチャット 画像生成   参照      参照    本記事では AWS の文字認識サービス Textract をブラウザから試してみます   Amazon Textract とはAmazon Textract　は AI モデルによって画像からテキストを読み取る いわゆる OCR Optical Character Recognition  のようなサービスですしかし 単なる文字認識にとどまらず 文書の構造を理解し 表データなどをそのまま使いやすい形式で返してくれます    Amazon Textract の使い途 ユースケース Amazon Textract は 年月現在   つの言語 国に対応していますが 日本語には対応していません  英語  スペイン語  イタリア語  ポルトガル語  フランス語  ドイツ語従って 日本で使う場合は数字などに対してのみ使用しましょう入力形式は以下のものをサポートしています  PNG  JPEG  TIFF  PDFAmazon Textract は以下のような用途に使用可能です  印刷された英語文書の電子化  印刷された数値データの電子化  Fax やアンケートなど 手書き文字の自動読取 後続処理への自動取込    Amazon Textract の料金AWS の料金は基本的に従量課金制ですまた 料金はリージョン サービスを提供しているサーバーが設置されている地域 によって変動しますTextract は日本語をサポートしていないこともあり 東京リージョンでは使用できませんバージニア北部リージョンを使った場合 以下のような料金になります  機能                     料金                 通常の文字認識                   ページ    署名                           ページ    レイアウト                       ページ    クエリ                          ページ    テーブル                        ページ    フォーム                         ページ    クエリ   テーブル                 ページ    クエリ   フォーム                  ページ    テーブル   フォーム                ページ    テーブル   フォーム   クエリ         ページ  ただし  AWS のサービスの多くには無料利用枠が存在します  Amazon Textract は AWS 無料利用枠に含まれており 無料で使用開始できます 無料利用枠は  か月間有効で 新規に AWS をご利用になるお客様は 最大で次の分析が可能です   Detect Document Text API    ページ 月  Analyze Document API     ページ 月  署名のみを使用する場合   フォーム テーブル およびレイアウトの機能を使用する場合  ページ 月  クエリ フォーム   クエリ テーブル   クエリ フォーム   テーブル   クエリ それぞれ  ページ 月無料利用期間中の場合  本記事で実行する程度の処理であれば無料範囲内に収まります   アカウント作成Amazon Textract を使うためには  AWS のアカウント作成が必要です以下の公式手順に従って アカウントを作成してください   note warnクレジットカードと電話番号が必要になります予めご用意ください   ブラウザからの文書分析    Amazon Textract への移動AWS のマネジメントコンソールにログインすると 以下のような画面が表示されます  スクリーンショット       png  この画面から最近使ったサービスなどにアクセスできるようになっています上メニューにある検索ボックスに  tex  と入力してみましょう  スクリーンショット       png  サービスの候補として Amazon Textract が表示されますAmazon Textract をクリックしましょうもし東京リージョンなど サポートされていないリージョンで AWS のページを開いていた場合 以下のような画面が表示されます  スクリーンショット       png  米国東部 バージニア北部 をクリックし リージョンを移動しましょう   note warn他のサービスを使うとき 東京リージョンに戻るのを忘れないようにしましょうリージョンは上部メニューの右側 リージョン名をクリックすると変更可能です  スクリーンショット       png  リージョンを選択すると 以下のような画面が表示されます  スクリーンショット       png      生のテキスト左メニューの 文書を分析する をクリックすると 以下のような画面が表示されます  スクリーンショット       png  このデモでは 左側の給与明細の画像を分析し 右側に認識できたテキストを表示しています左側画像内の各テキストを囲っている四角形は Textract がテキストを認識できた位置情報です右側の各テキストをクリックすると 左側の該当箇所の色が変化します  スクリーンショット       png  実のところ これだけのことであれば Rekognition で十分です例えば画像のこの箇所  スクリーンショット       png  Textract は以下のように認識しています  CO  FILE DEPT  CLOCK NUMBER  ABC    しかし 実際にこのデータを使う場合 本来は以下のような紐付きが欲しいはずです  CO   ABC  FILE    DEPT     CLOCK    NUMBER  画像上のテキストに対して 単なる文字認識ではなく このような意味的な紐付き 構造を理解できるのが Textract の強みです    レイアウト右側 結果の上にある レイアウト タブをクリックしてみましょう  スクリーンショット       png  各テキストが文書内で 構造上どういう意味合いになっているか が分析されています先ほどの画像先頭の箇所は ヘッダー として認識されています少し結果を下っていくと テーブルなども正しく認識されています  スクリーンショット       png  単に ヘッダーに何が書いてあるか や どのあたりに表があるか だけであれば この レイアウト で比較的安く分析可能です結果は右上 結果をダウンロードする から CSV 形式で取得可能ですダウンロードした ZIP を展開して   layout csv  を開きます   csvs ページ番号  レイアウト  テキスト  Reading Order  Confidence score    Layout    図            ヘッダー    CO  FILE DEPT  CLOCK NUMBER ABC              テキスト    ANY COMPANY CORP   ANY AVENUE ANYTOWN USA          セクションヘッダー    Earnings Statement      Textract 自体は日本語に対応していませんが ユーザーが日本語を使っている場合 デモの CSV もちゃんと日本語にしてくれているのは親切ですね各値の先頭     は おそらく Excel が勝手に形式を変えるのを回避するためでしょうmacOS の場合  Numbers で開くと以下のような表示になります Windows なら Excel で開きましょう   スクリーンショット       png  右端列の Confidence score    Layout  は結果の確信度 AI がどれくらい答えに自信があるか を表します    フォーム フォーム タブをクリックしてみましょう  スクリーンショット       png   フォーム の場合 項目と値が紐づいて認識されていますただし あくまでも項目と値が  対  の場合についてのみ分析できており テーブルの場合は認識できていませんダウンロードした結果から  keyValues csv  を開いてみましょう  スクリーンショット       png  こちらは残念ながら日本語になっていませんが   Key  が項目で  Value  が値ですそれぞれに  Confidence Score  がありますが このデモ上ではどちらも同じ値になっていますね    テーブル テーブル タブをクリックすると テーブル毎の分析結果が表示されます  スクリーンショット       png  先頭のテーブルについて 正しくセル毎に分割できている事がわかります 現在閲覧中のテーブル から  をクリックします  スクリーンショット       png  テーブルのタイトル ヘッダーが認識できており 行毎に高さが違っても正しく認識できていますダウンロードしたファイルでは テーブルの結果は table  csv から table  csv に分かれています  スクリーンショット       png      クエリ クエリ タブでは 画像に対して直接質問を投げる事ができます クエリを入力してドキュメント内を検索 というところに what is the regular hourly rate  と入力してから クエリを送信 ボタンをクリックしますすると 以下のように結果が正しく取得できます  スクリーンショット       png      JSON 形式ダウンロードしたファイルの中に  analyzeDocResponse json  というファイルがあるので開いてみましょう   json   DocumentMetadata         Pages      Blocks           BlockType    PAGE         Geometry             BoundingBox               Width               Height                Left               Top             Polygon                 X                 Y                 X                 Y                 X                 Y                X                 Y   実際にプログラムから Textract を使う場合 このような形式で分析結果が取得されますElixir というプログラミング言語での実装例はこちら   まとめAmazon Textract によって 単なる OCR ではなく 意味や構造も含めた分析結果を得る事ができましたただし 日本語には対応していないため あくまでも英語文書や英数字だけの文書を対象として使用しましょうまた 機能によって料金が異なるため やりたいことに合わせて選択しましょう,7,2023-12-02
51,51,ソシャゲのストーリーをテキスト化して布教したい！【画像認識】,画像処理,https://qiita.com/tamazoo/items/5fe3f5e80f5c0a35a63c,  初めに昨今のソシャゲはゲーム内容だけでなくストーリーや世界観なども凝ったものが多くなってきた印象があります かくいう私も プロジェクトセカイ 以下プロセカ というゲームのストーリーにハマったオタクです 面白いコンテンツがあったら友達に布教したくなるのがオタクのサガ さっそく友達に布教しようと思ったところ  ゲームや動画でストーリーを読むのは時間がかかる 文字媒体なら読むかも  と言われましたが 公式から台本なども特に提供していなかったためゲーム画面からセリフをテキスト化するコードを作成することにしました   動作解説  遥杏 gif     動作手順  動画を読み込ませる   キャラ名ウィンドウ と セリフウィンドウ のそれぞれの領域をマウスで選択する  一定フレーム毎に選択した領域内の文字を読み込む  セリフが次に移ったタイミングで キャラ名 と セリフ を出力する  コードの解説コード全文は こちら  で公開しています    読み取り範囲領域の選択読み取り範囲の選択には OpenCVライブラリの selectROI   を用いました  selectROI   の引数に画像データを渡すとGUI上で画面領域が選択でき 選択した領域のx座標 y座標 幅 高さが引数として返されます これを用いて キャラ名ウィンドウ と セリフウィンドウ のそれぞれの読み取り範囲を指定します   image png     画像加工   python main py  二値変換  ビット反転処理  拡大処理セリフウィンドウはそのままでもうまく画像認識できたのですが キャラ名ウィンドウはなかなか読み取りができなかったため事前に画像に加工をかけることにしました tesseract ocrは白背景に黒文字でないと読み取り精度が下がるため 指定した画像領域を二値変換した後ビット反転処理をかけてあげる必要があります   オリジナル   二値変換   ビット反転処理  他にも キャラ名ウィンドウは小さすぎてうまく画像認識が出来なかったため元の画像を倍に引き伸ばす処理も入れました    画像認識これを用いるにはTesseractのインストールをする必要があります 基本的な使い方は  image to string   に ndarray 型の画像データと言語とオプションを指定してあげると画像から認識した文字を返してくれます オプションではそれぞれ以下の指定を行っています    psm  読み込ませるテキストの形式を指定 縦書きか横書きか 複数行なのか 等    oem  学習方法を指定 認識速度や精度に影響あり    name whitelist txt  今回はホワイトリストを読み込ませるために使用    ホワイトリスト登場するキャラはある程度決まっているので ホワイトリスト機能を用いて主要キャラの名前に含まれる文字をそれぞれ指定しました これにより キャラ名を誤認識することがほとんどなくなりました    テキスト起こしのタイミング  前のセリフとの一致率が低くなったタイミングで書き出し処理一定フレームごとに読み取ったテキストを読み込んでいては セリフが流れている途中のテキストも読み込んでしまいます ただ セリフが最後まで表示されたときのサインが見当たらなかったため 一定フレーム前の画像とテキストの比較をして類似度が低かった場合にテキストとして書き出す方針にしました 例として以下のような場合を考えてみます    plaintext セリフ途中のテキスト例前： これ 今： これはペンで これはセリフが流れている最中のテキストを受け取った例です この場合は類似度が高くなるため テキスト書き出しは行いません 書き出し対象となるのは以下のような場合です    plaintext セリフが切り替わったテキスト例前： これはペンですか  今： はい それは セリフが切り替わった際はテキストの一致度が大幅に下がるため これをトリガーとしてテキスト書き出しを行います この類似度の算出には difflib ライブラリの SequenceMatcher   を使用しました 取得するフレームの間隔やゲーム内のセリフの流れる速度によると思いますが 今回は あたりを閾値にすると良い感じにセリフの切り替わりを感知できました   出力結果   plaintext output txt杏遥はこのへんじゃ服とか買わなそうだよね 意外とちゃんと女の子っぽい服 着てるし遥意外と 杏あはは だって小学校とか中学校の途中までは どっちかが足速いとか どっちが先に給食食べ終わるとかぐくだらないことでいろいろ競い合ってきだただじゃん 遥そうね 勉強以外はいい勝負だったね杏うっ……やり返したつもりう ーー抽に本兆のことを言っ誠記けだだよav導  論   なかなかの精度で読み取ることが出来ました ただ 実用化するには以下のような点が課題だと考えています   そのまま台本として使うには テキストの読み取り精度に不安が残る        ぐく だらないことで   合ってき だただ じゃん  など  場面の切り替えなどのときに背景絵を文字として認識してしまう      文字化けを起こしているような箇所はこれのせいなので 現状としては画像認識で読み取ったテキストをベースに目視で修正といった運用になるかと思います 誤っている箇所の自動検知などもできれば面白そうですね   終わりに画像認識系の技術に少し興味があったので 趣味と実益を兼ねたコードが書けて楽しかったです この記事を通して コンテンツの布教に取り憑かれたオタクの熱意を感じてもらえれば幸いです また 友人への布教結果は後に追記しようと思っています ,12,2023-12-01
54,54,工業用カメラを調査中,画像処理,https://qiita.com/nonbiri15/items/ddfefa20cb36aba9ad59,高解像度で明るいカメラを利用することを考えると 工業用カメラが視野に入ってくる 工業用カメラの場合 通常のUSBカメラと違ってくる部分が多いので その違いについてメモする     注意：  このメモでは 網羅的な調査ができていません   たまたま気づいたものを記載しているにすぎません   マシンビジョンでの性能を確保するには カメラ本体 レンズ 照明などのハードウェアの組み合わせ レイアウトなどが重要になります   どのようなアルゴリズムでどのような画像認識をするのかによって 最適な組み合わせは変わります    グローバルシャッターであること グローバルシャッターとは イメージ センサーのグローバル シャッター モードでは 画像取得のたびに センサーの全ピクセルがプログラムされた露光時間だけ同時に露光を開始し 露光を停止できます 露光時間の終了後 ピクセル データの読み出しが開始され すべてのピクセル データが読み取られるまで行ごとに読み出されます グローバル シャッター カメラのこのメカニズムにより ぐらつきや傾きのない 歪みのない画像が生成されます グローバル シャッター センサーは通常 高速で移動する物体を撮影するために使用されます  引用元  工業用カメラでないカメラの場合には ローリングシャッター方式のものとグローバルシャッター方式のものとがありますが マシンビジョン目的の場合には 工業用カメラの場合には グローバルシャッターであることが必須となります  もちろん ローリングシャッター方式で十分な用途もあって そのような組み込み用のカメラモジュールも多数ありますが ここでは 計測を意識したカメラについて述べます     カメラのマウント　Cマウント産業カメラの場合 カメラの光学系を付け替えられるものがあります そのカメラ光学系のマウントの一つが　Cマウントです 以下の記事は Cマウントの説明をしてくれています    大きなイメージセンサCマウントのカメラでは 廉価なカメラで多いMマウントのカメラと違って大きいため image circle が大きくなりやすいです そのため 大きなイメージセンサを使うことになります      カメラ光学系の選択C Mountのカメラでは それに適合するレンズを選択することができる       Edmund Optics      MiSUMi       C CSマウントカメラ用タムロンレンズ　   高解像度 高フレームレート 高帯域高解像度 高フレームレートのため通信量が増えるので 帯域が十分に確保されるインタフェースを要求する   ケーブルが抜けるトラブルをきらう そのため 素のUSB端子を使うことはない 少ない      インターフェス各種GigE インターフェースがそのつ GigEVision ハードウェアインターフェース標準規格  GigE カメラのインストールの説明の例記事  USBとGigEの違いとメリット デメリット     USBVision   USBVision ハードウェアインターフェース標準規格     GMSLカメラ GMSLカメラとイーサネットカメラの詳細比較      GMSLのカメラの例     ティアフォー  GMSLインタフェース対応 HDRカメラ C  車載用途として開発されたカメラです カメラモジュールが該当のページに書いてあるようにコンパクトなパッケージになっています 視野角も種類用意されています   GMSL USB変換キットをリリース予定とあるので それが販売されれば USBのインタフェースで接続できるようになる  CameraLink ハードウェアインターフェース標準規格     CoaXPress   カメラ CoaXPress ハードウェアインターフェース標準規格  これらのインタフェースでは まずケーブルを受ける側のコネクタのある基板が必要になります ですから LinuxPCやwindowsPCをそのまま画像信号を受ける側には使えません これらのインタフェースを使うシステムを開発する場合には しかるべきサポートが受けられる業者の助けのもとで開発するべきでしょう    カメラのインタフェースと最大伝送距離カメラモジュールからのインタフェースと接続先のボードのインタフェースとの間の伝送距離によって 利用できる方式は制限が出てくる そういったことから 選択できる工業用カメラについて絞られる USB    最大伝送距離は m   画像 動画の形式の留意点  画像フォーマットによって 輝度のbit幅が失われることがあるので注意がいる   UINTの場合だと までの階調しか得られない   YUVなどのフォーマットの場合もある   bit グレースケール PNGファイルというのもある   各自調べること   一つのシステムの中でのjpeg エンコードとjpeg デコードを行うことを避けたい    複数カメラの同期  複数カメラで 同時に画像を取得したい場合には カメラの同期が重要になる   多点カメラで 市販のステレオ計測以上のことを実行しようとするならば 複数のカメラを同期させることが必要になる カメラの同期についての解説記事 外観検査用AIスマートカメラ      製品の例参考情報 日本インダストリアルイメージング協会     センサーフュージョン   工業用カメラの調査に有用なサイト,1,2023-12-01
55,55,500KB以上の写真にリサイズするツール,画像処理,https://qiita.com/bellx2/items/aa48907c29b5050b1163, KB以上の写真を送ってください と言われたけど そもそもスマホで撮影した元画像がKB未満で困っているという謎の話を聞いて即席で画像サイズを 無駄に 大きくするツールを作りました   ※画質が向上するわけではありません      使い方下記URLからブラウザーで利用できます ※URLをoverkbに変更しました    画像をアップロードすると quality で保存しそれでも足りない場合は  づづリサイズしていきファイルサイズがKBを超えたところで確定します ボタンをクリックするとJPEG画像がダウンロードされます プレビューとしてPNG画像が表示されています 画像長押しで保存できます PNGは非圧縮なので確実にKBより大きくなっています  Streamlit Community Cloud   環境で動いているのとtempfileを使って適時開放しているので画像ファイルは誰も見れません 無料プランなので利用が少ないと止まっているかもしれません    ソースコード興味のある方は以下よりどうぞ    技術面のメモ  PillowのJPEGのqualityは ですがにすることで圧縮を一部使わない方式で保存されます ロスレスでは無い     参考    プレビューにJPEG画像を使用するとサーバー側で最適化され ブラウザーで保存すると小さくなってしまう問題があったのでPNGにした    ぶつぶつ写真を高解像度の元ファイルで送ってくれの意味だとは思うのですが 最近はスマホ撮影も多く高解像度で圧縮が良く効いているため 画像KBもありませんよね    PCと画像編集ソフトでできますが そんな環境も知識も無くて困っている人のために作ってみました 画質も向上しないし保存領域の無駄なので こんなツールが役に立たなければ良いのですが    笑 ,0,2023-11-29
56,56,AIではない選択 - 過去を見つめ、未来を変える画像処理,画像処理,https://qiita.com/yuitomo/items/670eadcc7923a85b3e3b,  AIではない選択   過去を見つめ 未来を変える画像処理   はじめにこの記事は この記事はNTTテクノクロス Advent Calendar の日目です こんにちは NTTテクノクロスの広瀬です 医療関連やメディア処理を中心としたデータサイエンスの研究開発や 講演や講師活動を通じてPythonの啓蒙活動などをしています さて ディープラーニングと大規模言語モデル LLM の進化は画像処理技術の進化とも密接に関連しています その中でも近年の   CLIP Contrastive Language Image Pre Training    のような技術は 画像とテキストを組み合わせた強力なモデルを生み出し Computer Visionの世界を新たな次元に引き上げました この先進的な技術は フレームワークやAPIの整備が短期間で進み 公開とほぼ同時に利用可能になったことで ディープラーニングから画像処理を学び始めた技術者であっても非常に高性能なシステムを構築することができるようになっています さらに一歩先の技術者を目指すにあたって 画像処理の基本的な概念を身につけることで より複雑なタスクの解決のための洞察が得られるのではないでしょうか そこでこの記事では AI 機械学習 ではなくルールベースの画像処理手法はどのようなことをしていくのかに触れていきたいと思います ルールベース手法は 教師データが不要というメリットが有り 結果の解釈性や厳密性が求められる場合などにおいては第一の選択肢となります さらに ディープラーニングと対象的に特徴量設計を手動で行う必要があることから データがどのような特徴を持っているかを探索する際には 基礎技術を身に着けておくことがアイディアの幅を広げる術のつになるのではと思います    物体検出 ペンと鉛筆を見つける 処理対象として 机の上に  ペンと鉛筆  を転がしてみました 機械学習やディープラーニングを使わずにこのペンと鉛筆を見つけるにはどうすればよいでしょうか  サインペンと鉛筆をどう見つける  今回は背景が格子状なのと色味が特徴的なので   色を特徴  としていきましょう やや無理やりですが 今回はこれでルールを作っていきます このように ディープラーニングを使用しない場合は何を特徴として判断していくのかを 人間が考えて設定する必要があります 手間がかかる反面 なぜそれをペンだと思ったのか なぜ間違えたのか が理解しやすいのがポイントですね     赤色とは では ルールベースの手法なので を定義していきましょう 赤っぽさとは何でしょうか 画像データは基本的にRGBのつが  bit の情報を持つように表現されています この組み合わせがどうなったときに赤と呼びましょうか ちょっと複雑なルールになりそうですね  どうなったら赤と判定する      色相画像処理においてRGBで色を扱うのは非常に難しいので HSVの色空間で扱うようにします RGB色空間では 色は赤 Red  緑 Green  青 Blue のつの基本色の組み合わせによって作られます しかし RGBでは色と明るさが密接に結びついており 色の調整が直感的でなく 特定の色の範囲を指定するのが難しいことがあります たとえば RGB値を均等に変更しても 色の変化が均等に見えない場合があります これに対して HSV色空間は 色相 Hue    彩度 Saturation    明度 Value  のつの軸で色を表現します HSV色空間の大きな利点は 色を直感的に指定できることです たとえば  赤色の範囲 を指定する場合 単に色相の特定の範囲 例：°から° を指定するだけで済みます このように HSV色空間は色の調整や特定の色の範囲を選択する際に非常に便利です  色相と定義する色の範囲 改めて ↑のようなルールを設定します     実装編     サインペンの検出では 実際に検出を行ってみましょう ここまで赤色の話をしてきましたが まずは今回のデータでは比較的簡単なからトライします   表示用    指定された色閾値に基づいて画像から特定の色範囲のマスクを作成し マスク内の小さな隙間や穴を埋める関数      return  作成されたマスク画像 白黒       画像をHSV色空間に変換      指定された色範囲に基づいてマスクを作成      モルフォロジー変換 防縮処理 を適用してマスク内の隙間を埋める  色相 までの範囲を青色と設定 サインペンのマスク画像 非常に単純な処理ですが いい感じに抜けましたね まだ ピクセルずつ青色っぽいかどうかを判断した結果 でしかないので 続いてどの座標にあるかを見ていきます これにはBlob処理 検出 を行います Blobと呼ばれる画像内で互いに接続している同色のピクセル群を見つける処理になります サインペンを表す白いマスクの塊が 画像中の度の座標にあるかを見つけていきます     マスク画像から輪郭を検出し それらを結合して一つのバウンディングボックスを作成し     元の画像に描画する関数      param image  入力画像 BGRまたはRGB形式      param mask  マスク画像 白黒      param color  バウンディングボックスの色 BGR形式の色指定      return  バウンディングボックスが描画された画像    マスク画像から輪郭を検出し すべての輪郭を結合して一つの大きなバウンディングボックスを作成します     このバウンディングボックスは 元の画像に指定された色で描画されます       マスクから輪郭を検出      全ての輪郭を結合して一つの大きなバウンディングボックスを作成      バウンディングボックスを描画Blobを検出してそれを囲うバウンディングボックスを描画しました  画像はわかりやすく輪郭線も書いてあります たったこれだけの処理で 画像からサインペンの位置を見つけることができました  サインペンの検出結果      鉛筆の検出赤色の鉛筆もやることは同じです 赤色の場合は 両端 度と度 が繋がっているのでそれぞれの端の色を指定して マスクのorで結合します バーコードの白い部分や鉛筆の硬さを表す部分の黒などがあるので サインペンと違ってやや検出が甘いですね ラベル付けの精度を求められるケースでは厳しいですが 鉛筆の位置を取りたいだけなら十分でしょう  鉛筆のマスク画像     最終結果機械学習を使用せず 赤い鉛筆と青いサインペンを検出することができました 今回は色を使用したルールベース手法を使用しましたが 細長い物体なので形状や面積などを特徴として検出することも出来そうですね この様に この物体を見つけるためにどのような特徴を使ってルール設定をしていくか考えていくのが 難しくもあり楽しい部分でもあります  検出された最終結果    おわりにディープラーニングから画像処理を始めた方にとっては 色相という考え方は新しいものだったのではないでしょうか ディープラーニングなどの先端技術は 複雑なタスクに対して驚異的な性能を示す可能性を持っています しかし 基本的かつ古典的な手法も ケースによっては効率的かつ効果的な解決策をもたらすこともあります 技術選択においては 目的と状況に応じた適切なバランスを見極めることが重要です 単純な手法が適切な場合もあれば 複雑な技術が必要な場合もあります AI時代の技術力のポイントは 手段としての技術の選択ではなく 目的に合致した解決策を選ぶことができることなのだと思います 技術力の新しさはそれ自体が価値となるべきではなく どの様にそれを活用して価値を産み出すかが鍵となるのではないでしょうか 引き続きNTTテクノクロスアドベントカレンダーをお楽しみください ,11,2023-11-27
57,57,Amazon Textractで色の実験をしてみた,画像処理,https://qiita.com/JamyJamy/items/c76291363616ee794390,   はじめにpythonで画像から文字を抽出する処理をしていたのですが 色や複数の文字の大きさによって読み取れないものがありました そこで Amazon Textractを使用して実験してみることにしました サンプル画像は楽天証券の登録銘柄です    Pythonでの結果    サンプルコード※画像ファイルはコードと同じディレクトリに置いています  画像から文字を抽出  抽出された文字列を表示print text     実験結果  黒背景      一応全文取得できる      日本語は怪しい      還記のように数字が変換されている箇所もある  白背景      数字が銘柄コードしか取れていない      日本語は怪しい  白黒      一応全文取得できている      日本語は怪しい        を   のように変換したり  が漢字になってしまっている箇所もある     サンプル画像 黒背景      出力結果     サンプル画像 白背景      出力結果  スクリーンショット       png       サンプル画像 白黒画像      出力結果  スクリーンショット       png    スクリーンショット       png     Textractでの結果    設定など  サンプルドキュメントは フォーム を選択  データ出力は レイアウト  フォーム  テーブル のつを選択    実験結果  黒背景      一応全文取得できる      日本語非対応なので 日本語がTERYなどの英語になっている部分がある      フォームで読み込めていない部分がある      テーブルの分割も不自然  白背景      一応全文取得できる      日本語が英語になったりはしていない      漢字の隣にあるPなどを読み取れていないものもある      フォームで読み込めていない部分が多々ある      テーブルは黒背景より規則正しいが 完璧ではない  白黒画像      一応全文取得できる      日本語が英語になったりはしていない      漢字の隣にあるPなどを読み取れていないものもある      フォームとして取得できていない箇所が多い      テーブルは綺麗に抽出できている 文字の読み取りは完璧ではない      サンプル画像 黒背景   スクリーンショット       png    スクリーンショット       png    スクリーンショット       png       サンプル画像 白背景   スクリーンショット       png    スクリーンショット       png    スクリーンショット       png       サンプル画像 白黒画像   スクリーンショット       png    スクリーンショット       png    スクリーンショット       png     結果Python Textractそれぞれに長所と短所がありました また AIを使ったからといって完全に色や形が共存する画像から テキストを完璧に抽出できるわけではないということがわかりました  今回の実験だと 白背景をpythonで取得するのが 文字列だけであれば番精度が高そうです Pythonのライブラリをもっと駆使したり Textractのドキュメントは種類がつあったので これらを横断的に実験してみたら もう少し精度の高い抽出ができるかもしれません ,5,2023-11-24
58,58,うなづき検出,画像処理,https://qiita.com/nonbiri15/items/474a9f0746ca992ed6db,人にとって重要な認識技術は 人に関する認識です 人検出 頭部検出 顔検出 顔landmark などの検出 人のpose推定 head pose の推定 hand poseの推定などがあります    うなづき検出が重要な理由  人についての理解を持たせようとしたら 人の動作の意味を解釈する必要があります   そのつが うなづき動作です   同意 不同意の区別をするために 首を縦にふる 横にふる動作です   この動作が どれくらい世界的に共通性があるのかどうかを私は知りません   それでも 私が属している文化圏の中では 同意 不同意のジェスチャーとしてうなづきがあります    head pose推定 またはlandmark検出 とうなづき検出とのあいだ  head pose 推定は 最近の顔検出ライブラリに含まれることが多くなっています   顔向きが重要な理由：      正面顔でない横向きの顔は 顔照合の対象から除外したい       顔をどちらに向けていているのかで 何に関心を持っているのかがわかる           よそ見 居眠りの検出      カメラの側 ロボットのカメラの場合には ロボットの側 にその人が注意を払っているのかがわかる   landmark 検出も顔検出ライブラリで重要な出力として含まれていることも増えています       点という少ないlandmarkからもっと多いlandmarkの場合の両方があります       landmarkが重要な理由：          顔照合のための顔の正規化をするのに重要          顔の向きを判定するために顔のlandmarkが情報源として重要          口を空いているのかなどの判定材料          顔画像への加工のヒント 例：　口紅を塗る アバターの表情を作るうえで 元となる顔の表情を取得する    顔向きと頭部のジェスチャとの違い      頭部のジェスチャを見極めるには 顔向き head pose の変化を見極めること      単にhead pose が各フレームでわかったからといって すぐにhead のジェスチャーを判定できるわけでない    うなづき nodding  検出   以下の実装を見つけたNodding Pigeon ライブラリは 短いビデオで頭のジェスチャーを検出するための事前トレーニング済みモデルと単純な推論 API を提供します  内部では ランドマーク機能を収集するために Google MediaPipe を使用します 以下のコードは README mdに書かれているサンプルコードです以下の値は ジェスチャーの判定結果の例です この実装では 顔landmark を利用していない 代わりにOptical flow を用いている    そのうなづき検出は 実用になるものですか  ここにあるようなOpen Source  Open Modelでの実装で あなたのユースケースでは 十分な結果がでますか     ありがちな状況 起こるかもしれないこと ：  簡単な状況ではうまくいっても 実ケースの状況ではなぜかうまくいかない   人によって うなづきの動作の大きさが異なり そのためにうなづきが無視される   うなづき以外の動作まで うなづきと判定してしまう   顔が正面顔ではうまくいくが 度横向きになったとたんにうまくいかない   マスクをしている状況だとうまくいかない   複数の顔が同時に見えていて しかも顔がすぐ隣にあるとうまくいかない   帽子をかぶっているとうまくいなかい   サングラスをかぶっているとうまくいかない     実用に耐えるかどうかテストしよう   これらの問題に対して言えることは testを書こうということだ   ユースケースに対するテストは Open Source 商用ライブラリのどちらでも必要だ   testに対してうまくいくことを示すことが うまくいくことを示す一番の方法だ    付記： 顔向き推定について調査中  を改訂しました mediapipeを用いた実装などについて追記しています うなづき検出は 顔のlandmark 検出の派生用途になります そのため 顔のlandmarkを検出する各種ライブラリの最新の動向を気にかけてください ,1,2023-11-24
59,59,ブラウザで簡単画像リサイズ！Jimpライブラリの活用ガイド,画像処理,https://qiita.com/blue_islands/items/3ad72c5de8b12bf4faea,JimpをWebアプリケーションで使用するための手順をまとめます Jimpは純粋なJavaScriptで書かれた画像処理ライブラリで Node js環境だけでなくブラウザ環境でも使用できます 以下の手順では ウェブページで画像をアップロードし Jimpを使って画像を処理し 最終的な結果を表示する一連の流れを説明します     Jimpのブラウザ版を読み込むウェブページにJimpのブラウザ版を読み込みます CDNを介して簡単に追加できます    html    画像アップロードのためのHTMLを準備するユーザーが画像をアップロードできるように   タグを含むHTMLを用意します    html    画像アップロードのイベントハンドラを設定するJavaScriptを使用して 画像ファイルのアップロードイベントを処理するイベントハンドラを設定します                    ここで画像の処理を行う             catch err                     console error err      Jimpを使用して画像を処理する画像をリサイズ 圧縮 色調整など 必要な処理を行います            例  画像をリサイズし 品質を に設定    処理した画像を表示する画像処理が完了したら 結果をウェブページに表示します 上記の例では  getBase メソッドを使用して画像をBaseエンコードし   タグの src 属性に設定しています   GitHubリンクJimpの詳細や追加情報については GitHubの公式リポジトリを参照してください  Jimp on GitHub  JimpのGitHubページには APIドキュメント 使用例 コントリビューションガイドラインなどが含まれています また 問題が発生した場合のサポートや コミュニティによる議論もここで行われています ,1,2023-11-23
60,60,Railsで画像を保存する際のassets配下とpublic配下の違い,画像処理,https://qiita.com/soraa24926/items/dd50c4ca4ae8289f1bdc,  Railsで画像を保存する際のassets配下とpublic配下の違いRailsアプリケーションにおけるアセット管理は 画像 スタイルシート JavaScriptなどの静的ファイルを効率的に扱うための重要な部分です 特に画像ファイルの配置には  assets ディレクトリと public ディレクトリのつの主要な選択肢があります これらの違いを理解することは Rails開発において重要です この記事では  assets ディレクトリと public ディレクトリの特徴と使い分けについて詳しく解説します    assets配下に画像を保存する    assetsディレクトリの役割     概要 assets ディレクトリは Railsアセットパイプラインの一部として機能します アセットパイプラインは JavaScriptファイル スタイルシート 画像などのアセットを効率的に管理し 配信するための仕組みです      特徴    プリコンパイル     assets ディレクトリに置かれた画像は 本番環境へのデプロイ時にプリコンパイルされます これにより ファイル名にユニークなフィンガープリントが付与され キャッシュ管理が容易になります     圧縮 最適化    画像ファイルは パフォーマンス向上のために自動的に圧縮されます      使用例   public配下に画像を保存する    publicディレクトリの役割     概要 public ディレクトリは Railsアプリケーションのルートに直接アクセス可能な静的ファイルを配置する場所です アセットパイプラインを経由せずに直接参照されます      特徴    直接アクセス    ブラウザから直接URLを指定してアクセスできます     プリコンパイル不要    プリコンパイルのプロセスを経ずに そのままの形で配信されます     エラーページなどの配置    カスタムエラーページなど アプリケーション外部からのアクセスに必要なファイルを配置するのに適しています      使用例   どちらを使うべきか    シナリオに応じた選択    アセットパイプラインの恩恵を受けたい場合    JavaScriptやCSSと一緒に画像も管理し 圧縮やフィンガープリントなどの機能を活用したい場合は  assets ディレクトリを利用します     直接アクセスが必要な場合    エラーページやメールテンプレートなど アプリケーションの外部から直接参照される画像は public ディレクトリに配置します まとめRailsにおける assets ディレクトリと public ディレクトリは それぞれ異なる目的と機能を持っています  assets ディレクトリはアセットパイプラインを通じた管理と最適化のために  public ディレクトリは直接アクセスが必要な静的ファイルのために使用します 適切なディレクトリを選択することで Railsアプリケーションの効率的なアセット管理とパフォーマンス向上が可能になります ,1,2023-11-23
64,64,PostmanでGPT-V(Vision) APIを呼び出して、伝票の読み取りをしてもらった,画像処理,https://qiita.com/yo-nagase/items/6d77fafc3c65c4b3be82,先日のOpenAI dev dayにて ついにGPT VをAPIから呼び出すことができるようになりました 今日はこのAPIを呼び出して レシートに記載されている情報を読み取ってJSONで返してもらってみようと思います    APIの仕様を確認する対象のAPIのドキュメントは以下にあります ドキュメントによると 以下のような形でリクエストを投げると結果が得られるとあります  image url のところを見るとわかる通り この例ではインターネット上の画像を読み取り 結果を返すサンプルとなっています では インターネット上に存在しない 手元やアプリケーションサーバ上にあるイメージを読み取ってもらうようにするにはどうすれば良いのでしょうか ドキュメントのもう少し下の方を確認すると ありました base形式で画像の情報を渡すことで実現できそうです どのようにリクエストを投げれば良いかがわかったところで Postmanを使って試してみたいと思います      リクエストを作成Postmanを起動して MethodをPOST URLのところには以下の値を設定します   bash urlbodyに以下のJSONを貼り付けます AuthenticationのところにAPI Keyを設定するのも忘れないようにしてください              text    この画像に含まれている値をJSONで返してください                url    ここにBaseデータを貼り付ける      画像をBaseに変換するまず GPT Vに渡したい画像をBase形式に変換します Baseとは バイナリデータをテキスト形式で表現するエンコーディング方式です まず 以下のサイトにアクセスします そしてDRAG   DROP IMAGES     のところにGPT Vに渡したい画像ファイルをドラッグします 今回は以下のレシートを読み込んでみたいと思います    note warnアップロード可能なファイルサイズはMBに制限されていますので ファイルサイズが大きい場合は画像ツール等を使ってサイズを小さくする必要があります   image png  すると 以下のような形でBASEの文字列が出力されますので 右上のcopyボタンをクリックします   image png  次に Postmanからのリクエストの渡し方は以下の様な形で urlのところに貼り付けてください   image png      結果確認Sendをクリックすすると 以下のようなレスポンスが返ってきました単価やSubTotalなど きちんと読めてそうですね さすがGPT   ただ ボルチーニがボルテージになってしまっているのは気になります    まとめPostmanを使って画像 バイナリ ファイルを渡すやり方をみてきました プログラムを書けば同じことはできますが プログラム上で試す前に こうやってAPIを呼んで動きを確認することができるのはとても便利ですよね Postmanを使えば 一度投げたリクエストは保存をしておけば必要な時に何度も呼び出して利用することもできますのでとても便利です 無料で利用できるので有効に使って 開発効率を上げていきましょう ,16,2023-11-20
65,65,【画像認識】efficientNETに、grad-CAMを適応し、予測の根拠を視覚化する。,画像処理,https://qiita.com/senbe/items/e127cf721c0750e046a4,  更新履歴     文章を補足 タイトルを変更   初めに 前回   過学習できちんと可視化出来なかったのでリベンジです 今回は 昨今注目の efficientNET  に医用画像を学習させ gradCAMで予測根拠の可視化の実験をしていきます 以下のコードで colaboratory に ドライブをマウント  できます 全体のコードを参照しながら 一部抜粋し解説していきます 全体のコードを参照しながら 読んで頂くと 分かりやすいかもしれません 全体のコード：  使用するデータセット：参考にするネットワークアーキテクチャ：最終的に以下のように CAMのヒートマップと入力画像を合成したような画像を出力します   image png    学習の背景を整理する今回のデータ数は約枚で これをさらに学習データ検証データ テストデータに分割するので 比較的学習に使えるデータ数は少ないと言える よって モデルは imagenet  で学習済みのモデルをベースとし 新たに出力層を追加し   転移学習    させる   転移学習に使用する efficientNetの imagenetで 学習済みモデルをインストールする   データ拡張 データ全処理先程も言ったとおり 学習に使用するデータセットが少ない このような場合には    データ拡張 Data Augumation     をし過学習を防ぐ 以下一部コード抜粋   TensorFlowのKerasライブラリからImageDataGeneratorをインポートする   ImageDataGeneratorは画像データの前処理やデータ拡張のために使用される   トレーニングデータ用のImageDataGeneratorインスタンスを生成する   このインスタンスは トレーニングデータに対して特定のデータ拡張や前処理を行う     rescale        画像のピクセル値をからの範囲に正規化する     zoom range         画像をランダムに拡大縮小する ここでは の範囲で変更      width shift range         画像を水平方向にランダムにシフトする  の範囲で      height shift range        画像を垂直方向にランダムにシフトする  の範囲で    検証データ用のImageDataGeneratorインスタンスを生成する   通常 検証データにはデータ拡張を適用しないため ここでは正規化のみを行う     rescale       画像のピクセル値をからの範囲に正規化する   トレーニングデータセットを生成する   ImageDataGeneratorを使用して データフレームから画像データを読み込み 前処理を適用する     train df     使用するトレーニングデータフレーム       directory train path     画像のパスがデータフレームに含まれているため コメントアウトされている     x col    image      画像ファイルのパスが格納されている列の名前     y col    class      ターゲット ラベル が格納されている列の名前     class mode    binary      クラスモードをバイナリ クラス分類 に設定     batch size   BATCH     回のバッチで使用するサンプル数を指定     seed   SEED    データ拡張をランダムに適用する際のシード値   転移学習のモデルを準備するコンピュータでどの様に微分をするのかは 一つの研究分野として成立する程奥が深い 今回 tensorflowの   tf GradientTape    機能を使用すし微分するが この機能はモデルのレイヤーがネストされると 計算グラフが繋がらなくなる問題がある よって tensorflowで転移学習を行う際には grad camを適応するレイヤーがネストされないように気をつける必要がある 以下のように   pretrained model B input と pretrained model B output でメタ情報を取り出し それを繋げることによりefficientNetモデルがネストされないようになる   インプットとアウトプットのレイヤーのメタ情報を抜き取る   efficientNetのアウトプットとカスタマイズで追加した層とを繋げる    一部省略 後に解説する 難しく感じると思うが model summary  を利用してモデルがネストされていないか確認できる model summary  した際に 数行だけしか表示されない場合 モデルはネストされてしまっている efficientnet bが展開されて表示されている場合は成功で  添付した全体コード  の出力のように冗長に表示される   学習済みのモデルの特定のレイヤーだけを 再学習させる予め学習されたパラメータは 新たに学習させるデータの特徴量を抽出するとは限らない  imagenet  は約万枚の画像を数千種以上のカテゴリに分類する為に最適化されているが 通常専門的な医用画像の様な画像は含まない そこでcamを適応するレイヤーに対して新たなデータを再学習し 新たなデータ 今回の場合は医用画像 の特徴を抽出する様にする せっかくimagenetで学習させた既存のパラメータを再学習させる事は 転移学習の利点を破壊する事であるから 再学習させるレイヤーはCAMを適応するレイヤーのみに限りたい 今回の様に 新たなデータセットが少ないと imagenetで学習されたパラメータを再学習させる程 基本的には精度が低下する現象が確認される  通常こうしたファインチューニングは 新たなデータセットに対する認識精度を向上させる目的で行われる 今回の様に データセットが少ない場合には認識精度がむしろ低下する為 通常行われない 認識精度の低下と引き換えに CAMの可視化を改善させる目的で行う事は特異なケースで注目に値する  以下はimagenetで学習済みのefficientNetの層のみを再学習させた場合の認識精度である   image png  以下はimagenetで学習済みのefficientNetの層を再学習させた場合の認識精度である 上記の層の場合と比較して 数 ほど認識精度が低下している なお 層以上のレイヤーを再学習した場合には 精度は割を切った 再学習させるレイヤーは最小限に抑えたい   image png  レイヤーをforで回して 再学習したいレイヤーが見つかったら trainableをTrueにする 他のレイヤーはFalseにし 再学習しないように設定する   先程省略されたコード gradCAMなので 勾配計算をしないといけない ここで先ほども言った通り    tf GradientTape    の仕組みを使用する これは自動微分の仕組みで 以下の様にwith内で行った計算を 計算グラフとして記録する この計算グラフを使用して 誤差を伝播し勾配を計算する 誤差逆伝播という 詳しくは ゼロから作るDeep Learning  で学べる  入力画像はNumpyではなく テンソルに変換する   入力する画像のインデックス指定  NumPy配列をテンソルに変換以下で grad CAMを適応するレイヤーを指定している 指定したレイヤーの出力 特徴マップ を出力する中間モデルを作成している 先程tapeに記録した情報を使用して tape gradientで勾配を計算することが出来る 最後にmatplotlibで表示するために正規化 最大値で割る を行っている   グローバル平均プーリング  ここでのweights 重み は 通常の機械学習の文脈における重みではなく   Grad CAMの文脈で特定の特徴マップがどれだけ重要であるかを示す量 weights   np mean grads  axis        Grad CAMの計算  conv output  はバッチのindex番目に対応している 今回は枚のみなので  のみ存在している   CAM ヒートマップ を出力先程 計算したcamをmatplotlibで可視化し 結果を確認する   CAMと入力画像を合成生成されたCAMは 入力画像よりも画素数が落ちる そこで scipyライブラリのzoomモジュール  を使用して 簡易的なアップサンプリングを行う   xの特徴マップをxにアップサンプリングzoom factor            ターゲットのサイズ    元のサイズ  プロット  カスタマイズしたカラーマップの使用CAMが反応しなかった箇所を無色透明にし 見やすくする そのためにカラーマップをカスタマイズする   範囲を変更ある一定の値以下を 無色透明にするカラーマップが出来た 作成したカラーマップを適応する   元の画像  CAMを透過してオーバーレイ表示  まとめ最終的に出力された画像は 横隔膜あたりに反応しているのでしょうか 僕は放射線科医じゃないのでよく分かりませんが 肺炎の診断にCPangle 肋横隔膜角 が使われることがあるので そのへんに反応しているのかと思われます   こういう医用画像データセットってオープン化が進んでいるとはいえ まだまだ少ないような気がします   医用画像診断は中国などが強いですが 個人情報の取り扱いが比較的緩いのが後押ししているのだと思われますね 僕も病気でCT画像を撮ったことがありますが 画像データを蓄積 活用している様子はなかったですね そのへんを日本でももっと整備してくれると 研究しやすいのですが        参考文献等    初学者必読 Google Colaboratory とは 使い方 便利な設定などをわかりやすく解説  AI Academy運営事務局   参照日       Google Colaboratory Google ドライブにマウントし ファイルへアクセスする方法 二ノ宮   参照日      転移学習とは AI実装でよく聞くファインチューニングとの違いも紹介   AIsmiley編集部 参照日      自動微分と勾配テープ TensorFlow   参照日   ,1,2023-11-15
66,66,画像認識モデル(YOLO)を使った物体検出アプリ作成に挑戦！！,画像処理,https://qiita.com/suinikotai_aidemy/items/7848640288091a4a66d9,   まえおき私は自動車業界にて自動運転システム開発に従事しています 緊急ブレーキ AEB や先行車追従 ACC 等のシステム側に携わっており 周囲の状況を把握する物体認識技術はチンプンカンプン 近年 DeepLearningや画像認識技術の発展が著しく 勉強しないと時代の波に乗り遅れる   という状況から Aidemy プログラミングスクール AIアプリ開発講座にて勉強しました 本ブログはAidemy Premiumのカリキュラムの一環で 受講修了条件を満たすために公開しています    本記事の構成①成果物紹介②作成アプリの検討③画像認識モデル実装の前準備④アプリ実装  モデル学習  アプリ公開 ⑤感想    ①成果物紹介私が作成したアプリケーションは下記    ②作成アプリの検討Aidemy講座では python勉強 CNNによる文字認識アプリ実装までを学びました 講座で学んだ内容に近い成果物作成をしようかと思いましたが Aidemy講師陣のサポートも受けれることから せっかくなので今の仕事に近く 興味がある 内容に挑戦することにしました その結果選んだテーマは 最新 有名 の画像認識モデルを用いた車載カメラからの物体検出としました     ③画像認識モデル実装の前準備 画像認識モデル選定  物体検出　手法 と調べると いくつかヒットしました R CNN YOLO SSD    会社で聞いたことがあったYOLOというモデルを直感で選択しました YOLOは従来手法より高速で高精度なアルゴリズムなようです  学習データセット 車載カメラの映像を使って学習をしてみたかったので KITTIのオープンデータセットを使うことにしました    ④アプリ実装     モデル学習      環境googlecolab     モデル学習の為に利用VScode python       アプリ公開用のコード作成      学習データKITTIデータセット      記述コードここからは記述したコードで説明する 　環境依存対応おまじない文字コード起因のエラーのようでググって対策  googleドライブをマウント アクセスできるようにする グーグルドライブをマウント 学習データ等をグーグルドライブに配置するので参照できるように準備  YOLOパッケージのインストール  必要なパッケージをインポート必要パッケージのインポート 軽量化の為にリサイズ サイズは事前学習済みのモデルと同様にpxにする  リサイズ前のデータパスとリサイズ後にデータを配置するパスを指定  リサイズするサイズを指定RESIZE   リサイズ後の画像データフォルダの作成          画像の読み込み          画像の大きさを取得  縦 横 チャンネル         height  width  channels   img shape          pxに対して 縦横比の計算        ratio   RESIZE   max height  width           新しい大きさを計算          リサイズ          保存画像リサイズKITTIの画像サイズ pix が 縦 横     となっており YOLOの事前学習モデルの入力サイズより大きかったので 軽量化も兼ねて 事前学習モデルと同様にpixにリサイズし 別のフォルダに格納  YOLO入力画像サイズがの倍数である必要がある為 の倍数になるようにパディング  代表画像ファイルを読み込む  画像の高さと幅を取得するwidth  height   img size  画像の高さと幅をで割り 余りを求める  余りがでない場合は パディングする    パディングするピクセル数を計算する  パディングした画像保存先を指定  パディングする色 黒 を指定      画像ファイルを読み込む      画像の右端と下端にパディングを追加する      パディングした画像を保存するYOLO入力画像サイズの制約への対応  ラベルファイル情報をリサイズ後の画像に適した情報に変換し YOLO学習ファイル構成に置き換える  画像ファイルを読み込む  画像の高さと幅を取得する  画像ファイルを読み込む  画像の高さと幅を取得するwidth   height   img size  ラベル情報を書き換える比率を算出   YOLO学習ファイル構成に変換前の 学習用のラベル情報と画像の格納先 訓練用と評価用に分解する為のディレクトリ生成 画像用 ラベル用 画像データリスト取得 データを訓練  用 検証  用 評価  用に分割  画像データ分のループを回し 訓練用 検証用 評価用に分けてラベルデータを格納していく      idxにより格納先のフォルダ名を振り分け     ラベルファイル名 パスを取得       ラベルの読み込み       YOLO対応フォーマットに変換 画像をリサイズしたので検出矩形座標も変換        画角外にはみ出すラベルを除去する       classを数値に変換       検出対象ではないclassを削除       YOLO対応フォーマットに変換したラベルを出力学習用と評価用にデータを分けるYOLO学習に対応するフォルダ階層に 画像データとラベルデータを分けた KITTIのラベル情報は検出矩形の左上と右下の座標を示す形式になっているので YOLOフォーマットへ変換した 画像をリサイズしているのでその分の補正も実施した また 今回は 車両  歩行者 に識別対象を絞り 不要な検出種別に関する情報を削除した  YOLO学習用の設定ファイル yaml を作成 車両と歩行者を識別する   学習開始model YOLO  yolovn pt  history model train data   content drive MyDrive myYOLO yolo yaml   project   content drive MyDrive myYOLO save  epochs  patience  batch  lr   imgsz  YOLOパラメータ学習学習用情報を記載したyamlファイルを作成し 事前学習済みのパラメータ yolovm から学習を実行  評価学習したモデルの評価 学習の過程を確認  image png    テストデータで検知結果を確認  推論結果の格納先  推論結果を表示する関数テストデータで実行結果を確認  image png  問題ないレベル感と判断※リサイズに伴うラベル情報の補正間違いにより 全然変なとこに検出し   なんだこれを何度も繰り返した       アプリ作成 学習済みモデルをロード モデル軽量化対策             flash  ファイルがありません              flash  ファイルがありません                受け取った画像をモデルに渡して推論する              レスポンスに画像を添付する              画像をメモリ上に保存する            image   io BytesIO              Image fromarray img  save image   PNG              image seek                レスポンスオブジェクトを作成するYOLOで学習したモデルサイズが大きく アプリ公開サイトのRenderの無料容量オーバー問題が発生学習回数の少ない容量の小さいモデルと学習後のモデルパラメータのみを保存したファイルを用意 モデルパラメータを上書きすることで 容量問題をクリアGit登録→Renderデプロイを経て 画像から車両と歩行者を検出し表示する自作アプリの完成    ⑤感想さらっとやったことを記載しましたが いくつかのポイントで頭が禿げそうになるくらい悩みました  googlecolabで学習していましたが学習時間が長く自動でランタイム接続が切れて学習結果を得られないとか 推論結果の精度が出ない問題 renderのファイル容量オーバー Aidemy講師の方に迅速かつ的確なアドバイスをいただきながら 少しずつ課題要因を特定し問題解決していくことを繰り返すことで 能力進展につながったと思います ここで学んだことを活かして pythonやAIを使い いろんなことに挑戦したいと思いました ,4,2023-11-13
68,68,Python プログラムで PDF から画像を抽出する,画像処理,https://qiita.com/rafluctua702101/items/d11682783c47c8f2ff3b,   はじめに時折 私たちは画像が含まれたPDF文書を受け取ることがあります その中の画像をさらに操作したい場合 どうすればよいでしょうか 画像を抽出してフォルダに保存することは 良い選択肢です 少量の文書を処理する場合は Adobe Acrobatのようなツールを使用して抽出することができます しかし 大量の処理が必要な場合は プログラミングを使用して迅速に抽出することをおすすめします 以下は Pythonコードを例にした方法の紹介です    ライブラリこの PDF ライブラリは Python プラットフォームでの画像抽出をサポートしています  テキストを抽出したい場合は この記事も参照してください  Python  Extract Text from a PDF Document     事前準備  まず  Python   をダウンロードしてインストールします   フォルダーを作成して それに  py  ファイルを追加します    サンプルコードまず 使用する必要があるライブラリをインポートします 新しい PDF ドキュメントを作成します    python PdfDocument クラスのインスタンスを作成するpdf   PdfDocument  画像を抽出する必要がある PDF ドキュメントをロードします    python PDFドキュメントをロードするpdf LoadFromFile  C  Users Administrator Desktop Sample pdf  画像を保存するリストを作成します    python 画像を保存するリストを作成するimages     ドキュメント内の各ページをループし 各ページから画像を抽出してリストに保存します    python ドキュメント内のページをループするリスト内の画像を PNG ファイルとして保存します    python リスト内の画像を PNG として保存する   スクリーンショット,1,2023-11-10
69,69,ArcGIS Proを使ってNDVIの解析をする,画像処理,https://qiita.com/nemiko007/items/969d8a1a88bbd45747ba,  注意初投稿です 文章等拙い点が多々あるかと思いますが 多めに見てください スクリーンショットは年月日撮影 ArcGIS Proのバージョンは  です   目次  はじめに   はじめに   NDVIとは何か ざっくりと   ndviとは何か ざっくりと   ArcGIS Proを使ったNDVIの解析方法   arcgis proを使ったndviの解析方法  　 衛星データの取得   衛星データの取得  　 コンポジット処理   コンポジット処理  　 NDVIの解析   ndviの解析   まとめ   まとめ   余談   余談   はじめに本記事ではNDVIの解析 植生活性度の分布を見る方法 の原理をざっくりとお伝えしたあと  ArcGIS Pro というGISソフトを使ってNDVIを解析する方法を説明します ArcGIS Proはすごく高価なため 使っている人がほぼいなさそうですが その分 マニュアル的なものがなかったため書くことにします 一応 GISとかリモートセンシングわからないよ  って方向けに NDVIの原理ややり方の流れは易しめに書いているつもりなので このつのことだけでも理解していただけたら幸いです なお  QGIS という無料ソフトを使ったNDVIの解析については 以下の記事が参考になると思います 何が言いたいかというと  NDVIをやりたいけど 金かけたくないよっ  って方は本記事を参考にせず 以下の記事を参考にした方がよいということです  まぁ NDVIの解析の流れを把握したいということでしたら 本記事は参考になるかと思います    NDVIとは何か ざっくりと上で少し触れた通り  NDVI の解析は植生活性度の分布を見る際に活用されます NDVIとは何かというと植生の活性度の指標で 衛星データの可視域や近赤外域の反射率を使って算出されます なお 以下の式で計算されます  国土地理院HP  を参照     mathNDVI \frac IR R  IR R このとき Rは衛星データの赤色の波の反射率を表し IRは衛星データの近赤外域の反射率を表します また NDVIは から の値をとります なぜNDVIに近赤外域や赤色の反射率が必要なのかというと 植物が持つクロロフィルの 赤色の波を吸収し 近赤外域の波を強く反射する性質を利用するためです 難しいことをつらつらと書きましたが 要するに NDVIを求めるときは 赤色と近赤外の波の反射率を使うんでっせ ってことが理解出来れば十分です 計算はボタンをポチッとすれば 機械が勝手にしてくれるので   ArcGIS Proを使ったNDVIの解析方法   衛星データの取得＊　GeoTIFFの衛星データがある方はこの項は飛ばして結構です ＊　PNGやJPEGでもできなくはないですが 地図上に表示する際 幾何補正 写真の座標を地図と合わせる補正 の手間がかかると思うので おすすめしません NDVIを解析するためには衛星データが必要です なので データをダウンロードしましょう まず 以下サイト LandBrowser を開きます そうすると以下のような画面が出てくるはずです そしてお好みの地域をドラッグで映します 今回は北海道にしました  ほっかいどうはでっかいどうなので… ちなみに写真が撮られた日は Display Menu の Date から確認できます 例えば 以下の画像の場合は年月日に撮影されたと見ることができます 上にある Save をクリックすると以下のような画面が表示されるはずです  Get Color Image からEPSGをに PNGからTIFFへとクリックして変えます そして  Band  Band  Band  Band をクリックしデータをダウンロードします なぜ Band    のデータをダウンロードするのかというと これらはそれぞれ青色の波長 緑色の波長 赤色の波長 近赤外域の波長のデータであるためです サインインしろとかいう案内が出たら 指示の通りに登録してください    コンポジット処理 ArcGIS Pro を起動すると以下のような画面が表示されます 左上の 新しいマップ をクリックし 新しくマップを作ります そして  データの追加 をクリックし 先ほどダウンロードしたつのデータを追加します そうすると以下のような衛星画像が表示されると思います そしたら 上の 解析   ツール を順にクリックし 右に表示されたウィンドウから コンポジット と検索します  そして  コンポジット バンド をクリックします そうすると以下のような画面が表示されると思います そして 入力ラスター を先ほど追加した衛星写真をファイル名の  TIF の前が B   B   B   B の順となるように選択し 出力ラスターは各々の好きな名前にしてください お好みで右のフォルダのマークからファイルを保管する場所を決めてください それができたら 下の 実行 をクリックします そうすると以下のようにRGBに指定した画像を割り当てられ コンポジット処理が施された画像が生成されます そしたら左の RGB の欄の赤色 緑色 青色のついているところをクリックし 赤色に Band  緑色に Band  青色に Band を指定します そうすると植生が赤く表示され見やすくなります ここまでできたらあと少しです 上の 画像 をクリックし 指数をクリックします スクリーンショットでは見えていないのですが この後に NDVI をクリックします そしたら以下のような画面が表示されるので 以下の図のように設定してください そして OK をクリックします  余談：頭の良い方は 上の画像を見て 短波赤外と赤を対応させるためには それぞれBandとBandを使わなくてはいけないのでは  と思うでしょう ですが 今回の場合はコンポジット画像では青色の波長 緑色の波長 赤色の波長 近赤外域の波長はそれぞれ Band Band Band Bandに対応している つまり バンドがずれているため 上の画像のように指定してあげる必要があります  すると NDVIが示された白黒の画像が生成されます このままだと見づらいので配色を調節します 今回はNDVIが高いほど緑色に 低いほど赤色になるよう設定します 左の NDVI    というファイルを右クリックし シンボル をクリックすると以下のような画面が表示されるため  配色 の右の ▽ ををクリックし 下の画像のように設定します おそらくテンプレートでは赤と緑色が逆になっているので  反転 にチェックします すると以下の図のようになり どの地域が植生活性度が高いかを見ることができます   まとめ本記事では植生活性度の分布を見る方法 NDVIの解析 の原理をざっくりとお伝えしたあと  ArcGIS Pro というGISソフトを使ってNDVIを解析する方法を説明しました リモートセンシングをやった事がない人からしたら終始 なんのこっちゃっ   って感じでしょうが NDVIは赤色の波と近赤外域の波の反射率で求まることと NDVIの解析は大まかに データの取得  コンポジット処理  NDVIの解析 の順で進んでいくことを理解していただければ十分だと思います 以上 ここまで見てくださりありがとうございました   余談本記事は  Nihon University Advent Calendar    に寄せた記事となります 皆さんがプログラミングを使ってる中 プログラミングを使わない かつ どこのボタンをポチポチするかだけしか書かれてない本記事が 参加してよいのかという懸念はありますが 本記事をきっかけにリモートセンシングに興味を持っていただけたら嬉しいです ,0,2023-11-09
71,71,Roblox にて、アバター画像をURLで取得する,画像処理,https://qiita.com/kaedeee/items/42225adfb97d02c82517,Robloxでアバター画像をURLで取得する方法を説明します ゲーム内での使用とAPIを介した外部の使用のつの方法があります   ゲーム内での使用   このURLを使用すると 指定したユーザーのアバターのヘッドショット画像を取得できます USER IDの部分に実際のユーザーのIDを入れて使用します 画像の幅 w と高さ h も指定できます   APIなど外部での使用      luaこのURLを使用すると 指定したユーザーのアバターのヘッドショット画像を取得できます USER IDの部分に実際のユーザーのIDを入れて使用します また 画像のサイズ フォーマット および形状 isCircular も指定できます 以上の方法を使用して Robloxのアバター画像をURLで取得できます どちらの方法も アバター画像をゲーム内で使用するか 外部のアプリケーションやウェブサイトで使用するかに応じて選択できます おしまい ,2,2023-11-08
72,72,【実装】streamlitで衣服の画像認識アプリを作ってみた,画像処理,https://qiita.com/Yuhei0531/items/a67e980c0eddb4441fd8,  はじめに  ezgif com video to gif gif  この記事では streamlitで作成した衣服の画像認識アプリの開発について説明します streamlitはデータ分析や学習モデルの動作確認などに使用され 学習コストが比較的低いことから簡易的なアプリ開発にされることが多いです そのため より複雑な機能をもつアプリ開発に興味がある方は他の方の記事を参考にすることをおすすめします     streamlitとは  image png  Streamlitは データサイエンティストやデベロッパーが簡単にウェブアプリケーションを構築するためのPythonライブラリおよびフレームワークです Streamlitを使用することで データ分析や機械学習モデルの結果を簡単にウェブベースのインタラクティブなアプリケーションに変換できます また    Streamlit Cloud     というサービスも提供されています  Streamlit Cloudは Streamlitアプリケーションをデプロイおよびホスティングするためのクラウドプラットフォームでです Githubと連携することで コミットをすぐさまWebアプリに適用することが出来ます     開発環境 モデル学習 評価 Dockernvidia cuda    devel ubuntu torch     cutorchvision     cunumpy    GeForce RTX ti 実行環境 streamlit cloudtorch     cutorchvision     cupillow   streamlit        学習 評価訓練データにはポピュラーなデータセットの一つである   Fashion Mnist    を使用しました また 学習モデルには  ResNet  の学習済みモデルをファインチューニングしました 以下は実行したコードになります 学習結果は以下のようなグラフになりました   image png    image png  評価データによる正解率 val correct は       程度に落ち着きました 初期は   ResNet  を使用していたのですが GithubではMBを超えるファイルは扱えないとのことで パラメータが軽いResNetを使用しました      実装実装には   streamlit cloud   を使用しました Githubと連携することでクラウド上にWebアプリを実装することが出来ます 以下のフォームに実装したアプリのコードが記載されているリポジトリを入力することで クラウド上でアプリを実行することが出来ます   image png    image png  実行に必要なファイルは以下になります  model py  モデルの構成が記載されたコード model pth  モデルのパラメータが記載されたファイル requirements txt  実行に必要なライブラリが記載されたファイルそれぞれのファイルのコードを以下に記載します     st sidebar write  画像認識モデルを使って衣服の種類を判定します       st sidebar write  判別が可能な種類は以下の通りです                                       画像をアップロード    画像を撮影       実行しっかりと分類結果を表示できていますね   ezgif com video to gif gif    まとめここまで 読んでくださいありがとうございます 今回はstreamlitを使用した衣服の画像認識アプリの作成について説明しました 使用データセットはグレースケールであること 一方向のみの画像であることから実際の写真を分類すると判別精度が著しく下がる可能性があります 今回は基本的な前処理しか行いませんでしたが より学習モデルにロバスト性を持たせたいなら左右反転や回転などの前処理を行うことをおすすめします Streamlitは実装が簡単なことから 学生や機械学習の初学者にはおすすめのツールといえます もし理論は理解しているが実装の経験が少ないという方は この機会に実装してはいかがでしょうか 次回は テキストを使用したWebアプリを実装したいと考えています   参考文献,1,2023-11-08
74,74, C#で画像の透明なピクセルを検知してみる,画像処理,https://qiita.com/kamikawa_m/items/51f178f842aa740f1531,   はじめにイラスト作成が趣味の筆者 わたし イラストの塗り残しが多すぎる  わたし 逆に透明にしておきたいところの消し忘れも多すぎる  わたし そういうのって検知できないのかなあ…… わたし ……わたしにはC って武器が あったな     作るもの   画像の透明部分を検知して可視化できるアプリ   MVPとしては 以下を満たせればよいと考えました   画像を読み込める  読み込んだ画像から透明なピクセルを見つけられる  見つけた透明ピクセルに指定した色をつけられる  画面上で透明ピクセルに色を付けた状態の画像を表示できる超原始的ですが やりたいことは満たせそうなので良しとします    参考にした記事とはいえ画像処理なんかやったことない というわけで 先人のお知恵を借りました    書いたコードUI周りは割愛します 引数に対象の画像のパスと 検知したピクセルを表示する色を渡しています画像のピクセルを配列にコピーすると ピクセルにつきバイト B 青  G 緑  R 赤  A 透明度 の順に格納されます  参考にした記事の本目に詳しいです そこで Aの値がのピクセルを 透明なピクセル と判定して ピクセルを選択した色に変更したBitMapを作成し 戻り値で返すようにしています    c     System Drawing Common Nugetパッケージのインストールが別途必要です using System Drawing     画像の透明ピクセルを検知するpublic Bitmap FindTransparentPixel string filename  Color selectedColor        画像のBitMapデータを読み込む       読み込んだ画像のピクセルを配列にコピーする       結果のBitMapデータを作成する                   透明なピクセルを探す               透明ピクセルを指定した色に変える               透明でないピクセルはそのままコピーする       結果のBitMapデータにピクセルをコピーする戻り値のBitMapは以下のようにBitMapImage型にしてからUIに表示しています  ImageプロパティをxamlのImageコントロールとバインディングしています    C    透明なピクセルを指定した色で表示したBitMapを作成する   結果例えばこんな感じの どこが透明かわからない画像を読み込ませると……こんな感じになります 背景と目が塗り忘れられていることが一目でわかりますね  ちょっとこわい    改善点Aの値がでないと検知できないので 半透明は検知できません スライダー等でUIから検知する透明度のしきい値を変えられたら良かったかもと思います    さいごに実は初の個人開発でした 画像処理 勝手に難しそうなイメージを持っていたのですが  題材が難しくなかったとはいえ サクッと実装できて驚きました 今回は超原始的なアプリですし UIも残念極まりない状態なので 今後はもっとみんなが使いたくなるようなアプリを作りたい と思いますが まずは第一歩ということで ,4,2023-10-31
75,75,Elixir Image で画像の dHash を計算し、類似画像を見つける,画像処理,https://qiita.com/RyoWakabayashi/items/9525d17bd9d7c0b12fc8,   はじめに例えば機械学習用の画像を収集するとき 違うファイル名で同じ画像 というのは邪魔になりますあるいはデータの整理をしているとき 各所にコピーされた同じ画像を集約して 不要なものを削除したい ということもあります完全に同じ画像だけでなく サイズ違いや縦横の移動など 少しだけ加工したものも取り除きたくなりますそんなとき 画像ハッシュを用いることで 類似画像を検出することが可能ですPython の場合は ImageHash モジュールを使うことで画像ハッシュを計算できます本記事では Elixir Image を使って 画像ハッシュの中でも高速な dHash を計算し 類似画像を見つけます実行環境はもちろん Livebook です実装したノートブックはこちら   セットアップ必要なモジュールをインストールします   elixirMix install      image              req              kino            画像の取得元画像として  ElixirConf EU のロゴ画像を取得します  スクリーンショット       png     類似画像生成比較対象としての類似画像を生成します    グレースケール白黒画像にします  スクリーンショット       png      リサイズ半分のサイズに縮小します  スクリーンショット       png      回転度回転させます  スクリーンショット       png      切り取り少しだけ外側を切り取ります  スクリーンショット       png      文字追加画像内に文字を追加します  スクリーンショット       png      別画像Phoenix のロゴを別の画像として取得します  スクリーンショット       png     画像の一覧表示元画像と他の画像を並べて表示します  スクリーンショット       png     ハッシュの計算以下のようにして dHash を計算することができます結果は以下のようになります   elixir  ok   サイズを指定しない場合  dHash は  桁のビット配列    桁のバイト配列になります値を整数にしてみます結果は    になります進数の文字列に変換してみます結果は   ECFDFDC   になります進数の文字列に変換してみます結果は      になります別画像の dHash も取得し 進数の文字列にします結果は      です元画像と別画像の dHash を並べてみますこの dHash の値同士がどれくらい違っているか は   ハミング距離   で計算しますハミング距離は各ビットの排他的論理和 XOR の合計 つまり各桁の   を比較したとき 値が違う桁の数です上の例だと  XOR は以下のようになります      dHash                                                              元画像     別画像     XOR      XOR の  の個数は  なので 元画像 dHash と別画像 dHash のハミング距離は  になります   ハミング距離の計算上記のような計算をするのは大変なので  Elixir Image では  Image hamming distance  につの画像を渡すだけで dHash のハミング距離を計算できるようにしてくれています各画像についてハミング距離を計算し 画像とともに表示しましょう結果は以下のようになります  スクリーンショット       png  おおよその基準として   以下ならほぼ同じ画像   以下なら類似画像と判断できます結果を確認してみましょう 比較対象     ハミング距離 判定結果 補足                         元画像                同一画像 本当に全く同一の画像なので想定通り  グレースケール          同一画像 白黒にしても同じ dHash になりました  リサイズ               同一画像 リサイズしても同じ dHash になりました  回転                別画像   回転すると距離が大きくなりました      切り抜き             類似画像 切り抜き具合によりますが 近くなっています  文字追加             ほぼ同一画像 小さい文字を加えたくらいだと同じ画像と判定できます  別画像              別画像    別画像なので想定通りです       dHash の場合 実は計算の過程でグレースケール化 リサイズを実行していますなので色の違いやサイズの違いは dHash では無いものと見做されます逆に回転や切り抜きのような 概形が変わる場合は距離が大きくなりますaHash や pHash など 別の種類の画像ハッシュもあり どの画像ハッシュを使うかによって どういう画像を類似画像と見做すか が違ってくるので注意しましょうElixir Image では現状 dHash のみサポートしています   まとめElixir Image を使って  dHash による画像の類似度を計算できました 類似 の基準によっては別の画像ハッシュを使わないといけないので その点は注意しましょう,20,2023-10-31
76,76,【実装】Vision Transformerをスクラッチ開発してみた,画像処理,https://qiita.com/Yuhei0531/items/4fdb4b6c086d8858e9be,  はじめに今回はVision Transformerという画像認識のモデルをゼロから実装してみました 概要について理解している方は  コード部分   コード からお読みください   Vision TransformerとはVision Transformer 以下ViT は 画像認識タスクのための深層学習モデルアーキテクチャであり 従来の畳み込みニューラルネットワーク CNN に代わる方法として提案されました ViTは自然言語処理タスクで成功を収めたTransformerモデルの考え方を 画像データに適用するものです   image png  具体的には以下のような手順で画像の多クラス分類を行っています    画像を  N×N  のパッチに分割する  原論文では×のパッチとしている    各パッチを  全結合層  に入力しパッチ単位のベクトルを作成する これを仮想的なトークンと見なす    トークン化したベクトルに Bertと同じようにclsトークンの埋め込み行列を結合し   Position Embedding  を行う    この過程でバッチサイズ×トークン数    × チャンネル数×パッチサイズ×パッチサイズ に変形することができる このベクトルをエンコーダに入力する    エンコーダの出力から  clsトークン  のベクトルのみを抽出し 再度全結合層に入力 分類ヘッドから確率分布を計算する ViTは大規模なデータセットで事前学習され 特定のタスクに合わせてファインチューニングされます そのため比較的少ないラベル付きデータで高性能な画像認識モデルを構築できるという利点があります 加えて 画像認識タスクで従来のCNNベースのモデルに対抗する性能を示し 特に大規模なデータセットや高解像度の画像認識においてSoTAを達成し優れた成果を示しました   image png    コード以下が実装したコードになります    note warn  警告  以下は個人で開発したコードです 誤った部分や 冗長な部分がある可能性があります    データセット今回は ViTが動作するかどうかの実験的な実装なので データセットにはクラスと比較的少ないCIFARを使用しました    エンコーダ   学習 評価学習に時間がかかるため ここでエポック数をとしています またローカル環境のGPUのスペック上バッチサイズはが限界でした   まとめViTの実装を通して 一番ためになったのはMulti HeadAttentionを含めたTransformerエンコーダをゼロから作ることで 大規模言語モデルの理解がさらに深まりました 次回からはTなどのエンコーダ デコーダモデルの実装も視野に入れたいと思います     参考文献,1,2023-10-31
77,77,日本三大提灯祭り 360年続く伝統の二本松提灯祭りの提灯を最新のAIで画像認識してみた,画像処理,https://qiita.com/DekaGodzilla/items/191b933cbd53415f98bf,  目的二本松提灯祭りでは 台の太鼓台に提灯をつけて町内を曳き回す その提灯は 台それぞれで異なり 字名 町名 の頭文字 もしくは文字 を表している 字毎に異なる提灯を物体検知できないか検証してみた 二本松提灯祭り とは  台の太鼓台にそれぞれ個余りの提灯をつけて町内を曳き回す   Wikipedia  より引用物体検知 とは  デジタル画像処理やコンピュータビジョンに関連する技術の一つで デジタル画像 動画内に映っている特定のクラスの物体を検出するものである   Wikipedia  より引用  YOLO物体検知のライブラリとしては YOLOを使用する   YOLO V AIでじゃんけん検出  を参考としました ラベリングについても詳しく解説してあり とても参考になりました   提灯の字名 町名 提灯は紅提灯 赤 で 白抜き文字で字名 町名 を表している  本町 の場合は  本   image png  また 表面と裏面にそれぞれ 本 があり 見る角度によって 文字の一部がかけたり 表面 裏面の文字が半分ずつ見えたり さまざまな見え方となる   image png   亀谷 かめがい  のみ 表面と裏面で文字が異なり  龜 と 谷 となる   image png  他の字は  竹田  松岡  根崎  若宮  郭内 の文字目で以下の通り   image png  なお  松岡 の 枩 は  松 の偏 へん  木 と旁 つくり  公 を縦に並べた異体字である 個人的には 赤みが強いのが好みなので 字画の少ない 本  竹 が好きである 画像認識する際も 字画が少ないほうが有利ではないかと思う    亀谷 は 表裏で違う字なので 画像認識では不利になるのではないか  松岡 は 異字体ではあるが これを教師データとして学習すれば 問題なく画像認識できるはずである   教師データ通常 画像認識の教師データ 画像 を取得するのは大変なことだが 台の太鼓台に個もの提灯がついているので 枚の写真で  個の教師データが取れる  太鼓台の周囲面で個なので 枚の写真 ≒面 では  個となる     IMG  jpg  この画像の場合 個の教師データを抜き出した 提灯の文字が正面を向いているものだけでなく 斜めを向いているものや 表裏文字が半分ずつ見えているものなど 人間が認識できるものは全部抜き出した   image png  太鼓台の屋根の上に提灯を取り付けるので 見上げるアングルの画像が多い   image png  この教師データを抜き出す作業 ラベリング バウンディングボックス作成 が 個個必要なので  labelImg  というツールは使えたものの この作業は大変だった 字毎に用意した画像数 抜き出した教師データの数は下表の通りである できるだけ字毎でばらつきがないようにしたつもりではある   字   画像数   教師データ数    本町          亀谷          竹田          松岡          根崎          若宮          郭内          学習この教師データを使って学習する まず 設定ファイル chochin yaml を作成する 参考とさせていただいた じゃんけん検出 と同様 学習データと検証データは同一のものとしている 認識結果が字名そのまま表示されるように 漢字でクラス名を記述した    yaml  Chochin dataset  train and val data as   directory  path images     file  path images txt  or   list   path images   path images  train    trainval    train  number of classesnc    class namesnames    本町   亀谷   竹田   松岡   根崎   若宮   郭内  字名 日本語 が正しく表示されるように オリジナルのtrain pyに 以下の行を追加した これを train JP pyとし 以下のコマンドで学習実行した   学習結果学習が完了すると yolov runs train chochinフォルダ以下に 学習結果が保存される Fカーブと PRカーブを見ると うまく学習できているようである   評価 物体検知 学習に使った画像とは別に 評価用の画像 各字枚ずつ を用意し testフォルダに格納する これを以下のコマンドで評価する    shellpython yolov detect py   weights   yolov runs train chochin weights best pt   source   test    data train chochin yaml   save csv   hide conf   imgsz  これで認識した提灯にラベル付けした画像を  yolov runs detect exp 以下に保存してくれる 各字の認識結果 各枚 は以下の通り かなりうまく認識できている   字   評価用の元画像   認識した画像     認識した提灯前述の通り 丸い提灯の文字は 見る角度によって さまざまな見え方をするが 以下の通りうまく認識できている   image png  さらに 提灯の一部しか見えてない場合や傾いている場合でも うまく認識できている   image png   本 の上が一部だけしか映ってない場合も認識できているので 人間を超えた神レベルと言ってもよいかもしれないが 逆に 他の字 若 と区別がつかず 誤認識もしている  正解は 本町 なのに 若宮 と予測 これは 若 の草冠にも見える    image png  表裏で異なる字となる 龜  谷 は どちらの字も認識できている   image png     認識結果一覧  yolov runs detect exp predictions csv に 画像毎の認識結果 ラベル の一覧が出力されるので 正解数などの評価指標でまとめる まず predictions csv を以下のコードで一覧化する なお 集計しやすいように ファイル名の文字目は ラベル        としてある    python  結果まとめ   本町      亀谷      竹田      松岡      根崎      若宮      郭内     縦 予測 横：正解縦軸が認識結果 横軸が正解ラベルとなる 例えば  本町 と認識したうち 正しく 本町 と認識したのが個あり 誤って 亀谷 と認識したのが個  若宮 と認識したのが個 というように見る よって 対角成分が正解 対角成分以外が間違いなので になるのが望ましい      本町   亀谷   竹田   松岡   根崎   若宮   郭内    本町                         亀谷                         竹田                         松岡                         根崎                         若宮                         郭内                          正解数 正しく予測できた数 この一覧表から 正解数を可視化する 正解数は この行列の対角成分 np diag である   正解数 正しく予測できた数   数値を表示する関数正解数にはばらつきがある 第一位は 本町 で最下位が 竹田  ともに字画が少なく認識しやすそうと予想していたが 異なる結果となったのは以外である   output png     適合率 予測した中で 正ししく予測できた割合    python  適合率 予測した中で 正ししく予測できた割合 これも 本町 が第一位  本町 と予測したものは全て正解   亀谷 が最下位であり 表裏で違う文字のため 他の字の文字を  龜 もしくは 谷 と認識しやすかったと考えられる 特に 郭内 を間違うケースが多く 画像を見ると  郭 の旁 つくり も偏 へん も どちらも 亀谷 と認識していることがある どちらも字画が多く認識が難しそうだが 逆に 郭内 を 亀谷 と認識したのは件と少ない   image png     再現率 実際の提灯の中で 正しく予測できた割合 再現率については 写真を見て提灯を数えるのが面倒なので 画像で提灯と検知できた数 他字含む を分母として計算する    python  再現率 実際の提灯の中で 正しく予測できた割合      写真を見て 提灯数えるのが面倒なので 画像で提灯と検知できた数 他字含む を分母として計算する再現率は 亀谷 が第一位 適合率と再現率は互いにトレードオフの関係にある ということが出ている つまり  亀谷 は 取りこぼしは少ない 再現率は高い が 逆に 誤検知が多い 適合率は低い ということになる 最下位は  郭内  字画が多いので 認識が難しい  亀谷 と間違えるのは前述の通り個と最多だが  竹田 と間違えるのが個と 次に多い    F値適合率 もしくは 再現率だけで評価すると誤解しやすいので 一般に F値で評価するのがよいとされる F値は 適合率と再現率の調和平均で求まる F値でも 第一位は  本町 である 下位は  亀谷 と 郭内    考察  丸い提灯に書かれた文字でもうまく検知 認識できた   特に 字画の少ない 本町 の認識率がよい  F値で     同じく 字画の少ない 竹田 は認識が難しかったようだ  竹田 を 亀谷 と誤認識するケース  郭内 を 竹田 と誤認識するケースが多く 認識の難しい 亀谷  郭内 に足を引っ張られたようである   表裏のある 亀谷 や 字画の多い 郭内 は 認識が難しい  F値は         まとめ余年の伝統ある二本松提灯祭りは 昔ながらのやり方を守り続けているので 太鼓台が進む 止まるはもちろん 坂を上る 下るや角を曲がるのもすべて人力である 提灯の明かりも ろうそくの火を灯している 電気はもちろん ガスや灯油も使っていない この昔ながらのやり方で灯っている提灯を 現代のテクノロジー AI で画像認識できた ということは対照的であり 面白い結果が得られたと思う 皆さんのお住まいの地域でも 伝統的なお祭りや行事の特徴的なシーンを画像認識してみてはいかがでしょうか 面白い結果が得られると思います   おまけ認識した結果をYouTube動画にしましたので よかったら ご覧ください ,3,2023-10-29
78,78,[Unity]Unityを使って画像認識してみた,画像処理,https://qiita.com/Kuni29292/items/57748a395223f2c54660,   初めに今回 株式会社GENEROSITY様でインターンシップをさせていただき その中で行った開発について記事を書いていきます 他にもいくつか記事を書いたので 他の記事も見ていただけると嬉しいです    本記事を読む前にこの記事では Unityのインストールなどについては記述しません インストールが終わっている前提で進めていきます インストールについては下記のリンクから行ってください    本記事で出来ることUnityでの画像認識を行うことができる  barracuda png    目次  事前準備   事前準備   Barracudaとは   barracudaとは   開発   開発 　  Barracudaのインポート   barracudaのインポート 　  Modelのインポート   modelのインポート 　  ラベルのインポート   ラベルのインポート 　  Sceneの準備   sceneの準備 　  プログラムの記述   プログラムの記述   結果   結果   まとめ   まとめ   参考資料   参考資料    事前準備Unityのインストールを行い プロジェクトを作成しておいてください 今回は下記のような環境で実行しました Unity    f URP    Windows  他のUnityバージョン レンダリングパイプラインは未確認です     BarracudaとはUnityの公式パッケージとして提供されているニューラルネットワーク推論ライブラリであり 顔認識や画像認識を行えるライブラリです    開発    Barracudaのインポートプロジェクトを開くことができたら Window → Package Managerを開きます 左上のプラスボタンを押し Add by package name   を押します   barracuda png  すると このような画面になります   barracuda png  Nameの部分に下記の文字列を入力してください もしバージョンを指定したい場合は Versionの部分に該当バージョンを入力してください 特にない場合は空欄で大丈夫です com unity barracudaAddを押すと インポートされます Package Manager内にBarracudaがあればインポートができています     Modelのインポート次に推論用のモデルのインポートを行います 今回はこちらのGithubからMobileNet v  をお借りします  barracuda png  モデルをダウンロードします   barracuda png  ダウンロードができたらUnityにインポートします Assetsフォルダ内に配置してください   barracuda png      ラベルのインポート画像認識をした際に 何を認識したのかのラベルが必要なため Label txtを作成し Unityにインポートしておいてください もしくは 下記のGithubよりダウンロードしてください こちらはUnity公式から出ているものです     Sceneの準備プログラムを記述する前に Sceneの準備をします まず Cameraをもう一つ作成します この作成したCameraの画像を画像認識のプログラムに渡します   barracuda png  この作成したCameraのTransformを下記の画像のようにします このときMain CameraのTransformも同じ値にしておいてください    note infoMain Cameraも同じTransformにしなくても動作に影響はありません 実行したときに見やすくするために変更しているだけです   barracuda png  続いて Assetsフォルダ内にRender Textureを作成します   barracuda png  作成したRender Textureを先ほど作成したCameraにアタッチします   barracuda png  最後にScene上にQuadオブジェクトを配置します もし 配置した時点でTransformが原点ではなかった場合には 原点にしておいてください   barracuda png  このQuadオブジェクトにLabelとしてある物体の画像ファイルをアタッチします 今回 私は障子の画像を付けました   barracuda png      プログラムの記述ライブラリとモデルのインポート Sceneの準備ができたので 画像認識するためのプログラムを記述していきます           モデルのロード          ラベルを単語ごとに配列に代入        labels   labelsAsset text Split  \n       private void Update            カメラからの画像をモデルに渡している          処理が終わった瞬間に破棄          TensorおよびWorkerは終了した時点で破棄する必要がある        input Dispose        private void Inference Tensor tensorInput           画像認識が行われる          検出したものをログに出す           終了時に破棄プログラムが記述できたら 適当なオブジェクトにAdd Componentします Emptyで作成したオブジェクトにAdd Componentしましたが どのオブジェクトにAdd Componentしても大丈夫です   barracuda png  Noneの部分にそれぞれ インポートしたモデル Render Texture ラベルのテキストファイルをアタッチしてください   barracuda png  この時点で実行してみると以下のようなエラーが発生すると思います   barracuda png  このエラーは以下のような理由で発生します  The assertion failure points that the script was expecting to receive three components  but it receives four  AssertionException  Assertion failure  Values are not equal Expected         アサーションの失敗は スクリプトが  つのコンポーネントを受け取ることを予期していたのに 次の  つのコンポーネントを受け取ったことを示しています  AssertionException  アサーション失敗 値が等しくない 期待値       先ほど記述したプログラムを下記のように修正してください    結果これで実行をするとログに認識されたLabelが出力され画像認識ができると思います   barracuda png  想定したLabelと違うものが出る場合もあるかもしれませんが モデルの精度だったり 画像によっても認識精度が変わるので 変えたりして試してみてください    まとめ今回 画像認識を行おうとしてUnityで行うためのライブラリを探していました その際に Barracudaを見つけて 簡単な実装で画像認識ができました 他にもいろいろなモデルがあるのでぜひ試してみてください    参考資料,5,2023-10-27
79,79,ノイズ除去にはガウシアンフィルタを使おう！　ほんと？,画像処理,https://qiita.com/nextvision-sugakir/items/5960eade2b2df1a3f48a,  はじめにOpenCVをはじめ 様々な画像処理の中で 思うように画像が処理されない場合 ノイズ除去やフィルタをかけることがあると思います フィルタには様々な種類がありますが ノイズを除去したい場合 とりあえずガウシアンフィルタをかけましょう は間違いです もちろんガウシアンフィルタは ノイズに対して有効な手段のつではありますし 様々な処理で利用されている有名なノイズ除去法です しかし 一言でノイズ除去といっても ノイズ以外の要素がどのように変化するかによって その後の処理の精度も変わってきます 各フィルタの特徴を押さえておくことで より高精度な画像処理が出来ます   ガウシアンフィルタって何者 ガウシアンフィルタは 画像処理において用いられるフィルタの一つで 画像を滑らかにするために使用されます このフィルタは 画像内の細かいノイズやテクスチャを平滑化し 全体的になめらかな画像を生成します 細かい仕組みは割愛しますが 一言で言うと  ある画素に着目し その画素からの距離に応じて重みの異なる平滑化処理を行う というものです 着目した画素と距離が近ければ強いフィルタがかかり 遠ざかるにつれて緩いフィルタがかかるということです これを画像内のすべての画素に対して行うため ノイズの部分に着目した場合 周囲と画素値が大きく異なるため 周りの正常な画素の画素値に近づくように補正 平滑化 されます ノイズの周辺すべてのフィルタリングが完了したとき ノイズが低減されている という仕組みです このフィルタは ガウス分布関数 正規分布関数 を基にしているため ガウシアンフィルタと呼ばれています   なぜ ガウシアンフィルタだけではダメなのか 結論から言うと エッジがボケてしまうことです エッジは 隣合う画素との輝度値の差が大きい領域のことを指します ガウス分布関数で考えると 中心からの距離しか考慮されてません 輝度値の差はまったく考慮されていないので 輝度値の差が大きいエッジの部分も同じように平滑化されてしまい ボケてしまいます 白い紙に左右を両断する黒い線があったとき 線の部分と線でない部分は画素値が大きく異なり これにガウシアンフィルタをかけると 黒い線は背景の白に 背景の白は黒い線に 緩やかに寄ってくため 線がぼやけてしまいます   ほかのノイズ除去のフィルタについて   メディアンフィルタこのフィルタは平均化ではなく中央値を取るというものです 平均化せずに 着目した画素の周囲の画素の中央値を取るので エッジがぼやけることがありません エッジを残したい場合はこちらのフィルタを使うとより鮮明になります メディアンフィルタを使うデメリットとしては エッジ以外の部分 特に細かい柄や薄い模様など が全体的にぼやーっとしてしまうことで 元画像の情報が失われる可能性があります    バイラテラルフィルタバイラテラルフィルタは ガウシアンフィルタに輝度値の差を考慮に入れたフィルタです 先程のガウシアンフィルタの説明で 輝度値の差を考慮していないため エッジがぼやける という話がありましたが そのデメリットを解消するとても優秀なフィルタです と 言いたいところですが このフィルタはとても処理に時間がかかります 仕組みとしてはガウシアンフィルタのガウス分布関数の次元をつ増やして 元軸にカーネル中心からの距離を横軸にしたときの正規分布 ガウシアンフィルタ を 新規の軸に輝度値の差の正規分布を配置します 新規の軸は元軸とは対称的に 輝度値の差が大きいほど重みを小さくします これにより エッジを残したままノイズを除去することが出来ます 単純計算で乗分増えるわけなので 処理時間がかなり遅いです 高精度ですが リアルタイム処理や 速度重視のシステムには使えない といったところです   ノイズ除去手法のメリットデメリットを一覧化してみる  フィルタ   メリット   デメリット    ガウシアンフィルタ   処理速度が若干速い   エッジがぼやける    メディアンフィルタ   エッジをくっきり残せる   エッジ以外がぼやける    バイラテラルフィルタ   エッジも残せて全体的にも綺麗   処理時間が長すぎる  以上のことから ガウシアンフィルタは 処理が早い分 リアルタイム処理に使えますが フィルタリングした画像を出力するのであれば ぼやぼやの画像になってしまいます なので使いどころとしては 比較処理の前処理として用いるのは有効だと思います とにかくエッジをくっきり残しながら手早くノイズを除去したい場合は メディアンフィルタを 時間がかかってもよいのでとにかく元画像の情報を損なわないようにするのであればバイラテラルフィルタが有効です   おわりノイズ除去をする際は どのような処理に取り入れるのかをよく考えてフィルタを選択することで より高精度なフィルタリングが出来ます ,3,2023-10-25
81,81,画像処理の実践,画像処理,https://qiita.com/fsd-shuhashi/items/64580442f5b9b1f53777,前回 画像処理の入門ということで OpenCVを使って画像の読み書き グレースケール化や値化 簡単な図形の描画を紹介しました 前回の記事はこちらになります 今回は もう少し実践的な内容を紹介したいと思います 前回同様 今回もレナの画像を使用します  レナの画像    画像の切り出し画像を読み込んで その中の一部だけを抽出する処理になります 処理は非常にシンプルです 入力画像 top bottom  left right 外の四角が画像全体となり top bottom left rightで指定した値が以下のように囲まれ 切り出しされます   スクリーンショット     png  ソースコードはこちら   画像読み込み  切り出しoutput   img        画像書き込みcv imwrite  output bmp   output 実行結果はこちらになります   image png    画像の平滑化平滑化とはぼかす処理のことです OpenCVにはいくつかの平滑化手法がありますが 今回は一般的な平滑化処理 cv blur で記述します   画像読み込み  平滑化k   output   cv blur src img  ksize  k  k    画像書き込みcv imwrite filename  output bmp   img output ksizeは カーネルサイズを意味します 詳細は 下記のサイトを参考にしてください 簡単に言えば 数値を大きくすればするほど画像がぼけます 実際の出力例をいくつか見てみます   image png  その他の平滑化の手法は以下のサイトを参考にしてください   エッジ検出エッジ検出は 画像の色の境目が急激に変わる場所のことをエッジと言い それを検出することです エッジ検出にもいくつかの手法がありますが 今回はCanny法という手法を使用します   画像読み込み  エッジ検出output   cv Canny image img  threshold   threshold    画像書き込みcv imwrite filename  output bmp   img output つの閾値をもとにエッジ検出を行います 値が小さいと 色の変化が小さい箇所でも検出され 値が大きいと色の変化が大きな箇所でも検出されないことがあります 実際に いくつかパラメータを調整して試してみます   image png  このように 値を調整してエッジ検出を行っていきます その他のエッジ検出は以下のサイトを参考にしてください   顔検出顔検出とは名前の通り 画像にある人の顔を検出することです   画像読み込み  顔検出  検出箇所を四角で描画  画像書き込みcv imwrite filename  output bmp   img img Haar Cascadesを使った顔検出を使用します 詳細は下記のサイトを参考にしてください OpenCVにはHaar Cascadesを用いた顔検出がデフォルトで使用できます それがhaarcascade frontalface default xmlというもので 私の場合は下記のパスにこのファイルがありました 実行結果は以下の通りになります   image png  プログラムの流れは以下の通りになります   画像読み込み  顔検出を適用し 検出結果を取得する   つでも検出できた場合は 赤い四角で検出箇所を描画する 今回はlistに検出結果が入っており listは検出した座標と幅と高さが返ってきます   画像書き込み  目の検出目の検出は 顔検出ほぼ同じプログラムになります 実行結果は以下の通りになります   image png    顔にモザイクをかけてみようここまでの内容を組み合わせて 実際に簡単なソフトを作ってみます 今回は 顔にモザイクをかけるものを作ってみます コードは以下の通りになります   画像読み込み  顔検出          顔検出した箇所を切り抜く        cutout   img y y h  x x w           平滑化でぼかす        filter   cv blur cutout                 ぼかした箇所を上書きする        img y y h  x x w    filter  画像書き込みcv imwrite filename  output bmp   img img このコードを実行結果が下の画像になります   image png  処理の流れは以下の通りになります   画像読み込み  顔検出をかけて 顔の座標を取得する  取得できた数だけ を繰り返す  顔検出した箇所を切り抜く  切り抜いた画像を平滑化でぼかす  ぼかした箇所だけ入力画像を上書きする   画像を書き込む図にすると こんな感じになります   image png  折角なので 目だけぼかしてみた結果も出してみましょう   image png  綺麗にぼかすことが出来ました 最後に 別の画像で顔のぼかしと目のぼかしをやってみたいと思います 使用する画像は下の画像になります   person jpg  まずは 顔をぼかします   image png  カ所 誤検出されていますが 人ともぼかすことが出来ました 目だけぼかしてみましょう   image png  こちらは 残念ながら右の人がぼかすことが出来ませんでした レナの画像に比べると 顔や目の大きさが小さいので 精度はその分落ちてしまいました   まとめ今回は 画像処理の実践的なものとして以下のものを紹介しました   切り抜き  平滑化  エッジ検出  顔検出  目の検出紹介したものは一部の画像処理技術で 実際にはまだまだありますので 興味がある方は調べてみてください それでは 今回はここまでにしたいと思います ,5,2023-10-21
83,83,なぜ「画像アップロード機能」はしんどいのか,画像処理,https://qiita.com/fooramu/items/5bcf5ee405e2a7acd886,  概要画像アップロード機能ってそれだけ聞くとつの機能なのですが 実は終わりが見えづらい機能ですよね どこまでもリッチに機能追加できるし セキュリティ対策も山ほどありますし 見積もりが難しい  もちろん仕様がカッチリきまっていればそれでOKです   なぜなのか     画像をどこに置くんだ問題ローカル環境なら全然問題ないですよ 本番環境だと複数台構成前提でしょうからSやGCSなどに置く前提になりますよね そうなると    APIキー発行して    あっ権限なかった  ○○さんにSlackで連絡っと       次に本番用とステージング用とフォルダ構成を考えて       あ〜〜  むしゃくしゃ      Yahooニュースみよう        良さげなライブラリを探すの面倒問題   このライブラリいいね評価も高い あ でもメンテナンスもうされてないか       おっと こっちのはドキュメントがすごくわかりやすい    やっぱり前使ってたあれにするか〜    あ〜〜  むしゃくしゃ      ホッテントリみよう   すみません 先人の方たちのライブラリには大変お世話になっております 皆様のおかげで日々飯が食べれております 本当に感謝です      UI UXをどうするんだ問題  ドラッグ ドロップあり    複数枚同時   キャンセル処理は   プレビュー機能は   アップロード中の見せ方どうする 無限にあります おそらくここが一番大変 イケてるサイトを参考にするしかない      エラーハンドリングはどこまで頑張るんだ問題  JPEG PNG HEIC HEIF WebP     ファイルサイズ  ファイル選択直後のJS側によるバリデーションどうする無限にあります バリデーションはある程度こっちで実装しちゃいましょう      画像のリサイズ等を対応するのか問題  アップロード時に自動 バックエンド側 で自動圧縮する   リサイズする   アップロード前にユーザ側のクロッピング 切り取り 機能搭載する う〜んヒットしたら搭載しましょう        NGな画像をアップロードされたらどうする問題  運営側の承認後に掲載されるルールにする いや運用でカバーしよう        Chrome以外のブラウザでちゃんと動くのか問題これは特に　 の UI UX の部分の事   総括リストアップしてみると まだまだありそうですが とりあえずつ  当たり前にやりなさい という内容なのだが やっぱり 仕様が決まっていないと 大変 ,0,2023-10-19
84,84,DeepStream について調査中,画像処理,https://qiita.com/nonbiri15/items/987f9b98df5162a16416,DeepStream SDK はNVIDIAのGPU環境で  は AI ベースのマルチセンサー処理やビデオ 音声 画像の理解のためのストリーミング分析ツールキットです 以下の内容は DeepStream の実例を調査することで DeepStream についての理解を得ようとする 初心者のメモです    NVIDIAの公式のgithub にあるdeepstream のサンプル   qiita 度カメラでDeepStreamSDKのnvdewarperの最新版を試してみる  データの表示python スクリプトでの実行例,0,2023-10-16
85,85,1枚の写真から三次元ボクセル表現が行えるMonoSceneを構築してみる［Ubuntu22.04］,画像処理,https://qiita.com/itisyourcall/items/9a7da80679b0294afd3f,  画像 png  枚の画像から次元に復元できるD Semantic Scene Completion  SSC  技術に触れてみました TeslaAI day のプレゼン曰く    くらい  Tesla車には似たような推論モデルが使われてるっぽいので 多分ホットな手法  デスTrainingを行うので 低スぺのグラボだと厳しそうです CUDAは構築してないと動かないと思います ※この記事は 年月現在のものです Githubはこちら↓  MonoScene論文を読む限り 深度データは必要なくRGB画像があればボクセル表現できてしまうらしい   つよつよPCスペックやはり 要求されているPCスペックはかなり高そうです 以下に 今回構築したPCのスペックを記載します   OS：Ubuntu  LTS  CPU：th Gen IntelⓇ core i K×  ←強い   RAM：GB ←強い   GPU：NVIDIA GeForce RTX  ←強い   必要ストレージ：最低GBほど データセットを一通り試すならGBは必要  前提構築済CUDA   Anacondaのインストール一応condaベースの構築なのでanacondaのインストールから 入ってる人は飛ばしてください ダウンロードしたshファイルを実行し 全部yesで進みます  enter連打だとnoが選択され インストールされなくなってしまうので注意 名前の前に base がつくようになっていれば成功 前準備おわり   MonoSceneの構築すべてHome直下に置いています  パス内の user はユーザーネームのことです    データセットの設定 NYUv 学習に必要なデータセットを準備します Semantic KITTI NYUv KITTI と種類用意されていますが 今回はNYUvを使用します  他のデータセットはとんでもなく容量を食います Semantic KITTIは後述   NYUvのデータセットは下記リンクからダウンロードできます  GB ↓解凍した後 Home直下に置きました めちゃくちゃ時間がかかります GPUを複数搭載している方が良いもかもしれません 現在最強のグラボRTXだと デフォルト設定で時間弱で終わりました 最後のGPU数などはPCに合わせてください epoch数はデフォルトでですが 変更することもできます 推論した形状を可視化します 今回はmayaviを使用しましたが OpenDを使用する方法もgithubでは紹介されています pip install tqdmpip install omegaconfpip install hydra coreいざVisualizationを実行しようとすると以下のエラーが出ました モジュールがインストールされていないようでした 下記を実行しました conda install cythonpip install vtk mayaviそれでもまたエラーです 要約するとまだモジュールが足りないみたいですね 下記を実行しました pip install pyqtやっとこさ です 下記を実行します   pkl の可視化したい画像は任意です 動きましたー   追記：データセットの設定 Semantic KITTI 容量が多そうで避けていましたが Semantic KITTIも一応試してみました 基本的にNYUv一緒ですが データセットの準備に一癖ありました 下記から データセットをダウンロードします このダウンロードには サイトへのログインが必要です 所属やら何で使うのかを明記する必要があったと思うので お気をつけて 次に下記からvoxel dataをダウンロードします   SemanticKITTI voxel data   MB   が今回使用するものです このつを組み合わせることでデータセットが完成するハズでした 後述しますが これだけでは動きませんでした   データセットの作成先ほどダウンロードした二つを組み合わせます   odometry data set  のファイルの中に の番号があります 同様に  voxel data set  に対応する番号のフォルダがあると思いますので 全部コピペしましょう 対応関係は以下の通りです           │       └──  　 ここまで全部やるここから先は基本的にNYUvと一緒ですが 一応記載してい置きます 以下を実行すると エラーが出てしまいました   Tr  ってなんぞや と言われてしまいました おかしいな と思い  calib txt  を確認してみると なんと  Tr  の行がありませんでした アップデートで消されてしまったんですかね 色々調べると   Tr  が含まれている  calib txt  データを発見しました 下記からダウンロードできます 二度手間ですがこの  calib txt  を  のフォルダ毎に全部コピペしましょう ※ここから先は基本的にNYUvと一緒です   Training kitti epoch数は RTXで実行した結果 時間強かかりました epoch数を変えるか Pretrained modelsの使用をお勧めします Semantic KITTIのPretrained modelsは以下のURLからダウンロードできます 最後のGPU数などはPCに合わせてください   generate output py  を実行した日時でファイル名が変わるのでコピペの際は注意実行すると 画像のようなボクセル描画を確認できました   snapshot png    おわりにひとまず構築して 動作確認程度はできました もうちょっと触ってみて 進展があれば追記していこうと思います ,0,2023-10-16
86,86,画像処理の入門,画像処理,https://qiita.com/fsd-shuhashi/items/0bd0efa3411eee0682f7,最近 弊社で画像処理の案件が増えているらしく 学生時代に研究でかじっていた私にドンピシャな案件がやってきています そこで 画像処理を少しでも知ってもらうように 今回は画像処理の基本であるOpenCVのライブラリを使って簡単な画像処理を実施していきたいと思います   準備  今回はPythonを使用して 画像処理を行います Pythonが使用できる環境を用意してください   ライブラリはOpenCVを使用します Pythonがインストール出来たら下記のコマンドでライブラリをインストールしてください     pip install opencv python    pip install numpy  ライブラリが正しくインストール出来ているか 確認します 以下のサイトから今回使用する画像をダウンロードします      レナの画像      ダウンロードしたらプロジェクト直下にファイルを置いて 下記のコードをコピペして実行してください     コードを実行したら先ほど ダウンロードした画像がウィンドウに表示されます     これで 準備は完了です       画像を読み込む      読み込んだ画像を表示する      キーボードが押されるまで 処理を止める    cv waitKey        ウィンドウを閉じる    cv destroyAllWindows    画像処理の基本   画像の読み込みと書き込み画像処理を行うということはまず 以下のつが出来ないと始まりません   インプットとなる画像を読み込む   読み込んだ画像を加工して ファイルとして吐き出す 画像を読み込む方は先ほどのサンプルコードに書いた通り以下のコードで実行できます     pythonimg   cv imread  ファイル名  これで 指定したファイルをimgという変数に格納します 次に書き込みは以下のコードで実行できます     pythoncv imwrite  ファイル名   img 画像データ ファイル名には書き込みするファイルの名前 画像データには読み込んだ画像変数を入れます imgという変数にcv imreadで読み込んだ場合は imgと記載します 試しに以下のコードを実行してみてください このコードが実行されると lenna std bmpと同じ場所にoutput bmpとファイルが出力されます 中を見るとlenna std bmpと全く同じものになっているはずです    画像処理を行う上の基本知識    ピクセルとは 実際に 画像処理を行う前に基本知識だけを抑えておきます 画像というのは ピクセルの集合体で構成されます ピクセルとは 画像を構成する色付きの小さな点のことで画像データの最小単位となります 例えば フルHDという言葉を聞いたことがあると思いますが これは × 個のピクセルの集合で構成されています イメージ的にはこんな感じでしょうか   image png  この四角がピクセルになります 書くのが大変なので 簡略化していますが 実際は縦に個 横に個四角があるイメージになります そして このピクセルには色情報を持っています 色情報の持ち方はカラー画像とグレースケール画像で異なります グレースケール画像の場合は ピクセルごとに の値で持ちます カラー画像の場合は ピクセルごとにRGB値でそれぞれ で値を持ちます ゲームをされている方はキャラメイクの髪色を決めるときにRGB値を調整することがあると思いますが それと同じイメージです 図にするとこんな感じでしょうか   image png  数値はそれぞれのピクセル値を表しています グレースケール画像の場合はピクセルにつの値を持ち に近づくほど黒く に近づくほど白くなります   image png  カラー画像の場合は ピクセルごとに赤 緑 青のつの値を持ち それらを組み合わせて表示します RGB値を全て同じにすると 白黒画像と同様の表現をすることも出来ます     画像の座標系画像の座標系は 少し異なります 一般的な座標系は下の画像の通りとなります   image png  数学でよく見るやつですね 左下の座標が   となり 右に行くほどxの値が大きくなり 上に行くほどyの値が大きくなります 一方で 画像の座標系は下の画像のようになります   image png  x軸は変わりませんが y軸が反転します 画像の左上が   で下に行くほどyの値が大きくなります ここを勘違いすると苦戦しますので 注意してください    画像をグレースケールにしてみよう前置きも終わったので 実際にOpenCVを使ってみます まずは 画像をグレースケールにしてみます コードは以下の通りになります   画像を読み込む  画像の高さ 幅を取得するheight   img shape  width   img shape    出力画像を初期化output   np zeros  height  width     np uint   画像の高さ分ループ      画像の幅分ループ          グレースケール値              ピクセルごとに入力画像のRGB値を加算            val    img y  x  col           RGB値の平均値を出力画像に設定        output y  x    val     画像を保存するcv imwrite  output bmp   img output コードの解説をすると以下の通りになります   入力となる画像を読み込む   入力画像の高さと幅を取得する   出力画像用の変数を初期化する  画像の幅と高さの分だけ  を実行する   ピクセルごとに入力画像のRGB値の合計値を求める   RGB値の合計から平均を求め 出力画像に値を保存する   結果を画像として 保存する このコードを実行すると以下のような画像が出力されます   image png  これで カラー画像からグレースケール画像に変換することが出来ました    画像を値化してみようグレースケール画像に変換することが出来たので 今度は画像を値化してみます 値化とは 名前の通りつの値にすることです つの値とはか つまり白or黒だけで画像を表現します コードは以下の通りになります   画像を読み込む  画像の高さ 幅を取得するheight   img shape  width   img shape    出力画像を初期化output   np zeros  height  width     np uint   画像の高さ分ループ      画像の幅分ループ          グレースケール値              ピクセルごとに入力画像のRGB値を加算            sum    img y  x  col               グレースケール値を求める        val   sum             ピクセル値が未満の場合はにする          ピクセル値が以上の場合はにする  画像を保存するcv imwrite  output bmp   img output コードの解説をすると以下の通りになります   入力となる画像を読み込む   入力画像の高さと幅を取得する   出力画像用の変数を初期化する  画像の幅と高さの分だけ  を実行する   ピクセルごとに入力画像のRGB値の合計値を求める   RGB値の合計から平均を求める   RGB値が平均が未満の場合は ピクセル値を そうでない場合はにする   結果を画像として 保存する グレースケール化の処理とベースは同じです グレースケールの場合は RGBの平均値をそのまま設定しましたが 値化はRGBの平均値が未満なら 以上ならにピクセル値を振分けます コードを実行すると以下の画像が表示されます   image png    ライブラリでグレースケール 値化してみようここまで 画像をピクセルごとに計算して グレースケール及び値化しましたが 既存のライブラリで実行することが出来ます   画像を読み込む  画像をグレースケールにする  画像を値化する上のコードを実行すると グレースケールと値化をした画像が出力されます グレースケールや値化は実は 様々な手法があり 先ほどの出力とは少し異なります グレースケールの場合は 以下のサイトを参考を見ると計算式が異なることが分かります   image png  こちらが出力結果になります 左がピクセルごとに計算した結果 右がライブラリを使った計算結果になります 見比べてみると左の方が全体的に明るくなっていると思います 次に値化は大津の値化というものを今回は使用しました 簡単に説明すると大津の値化は入力画像ごとに閾値を自動で設定してくれるものになります 詳細は上のサイトを参考にしてください ピクセルごとに計算した場合は 閾値をと手動で設定しましたが 大津の値化ではになるとは言い切れません 実際の出力結果を比較してみましょう   image png  左が閾値を手動で設定したもの 右が大津の値化を実行したものになります 大津の値化の方は鼻や口がより白くなっていることが分かります 実際に閾値を確認するには 以下の箇所のretに値が入っています retの値を出力するとでした よって 閾値はであることが分かります    一から画像を作ってみようこれまでは 入力画像を加工して新たに画像を作りましたが 最後に一から画像を作ってみます   出力画像を初期化output   np zeros         np uint   線を描画するcv line output                      thickness    塗りつぶしなし円を描画する  塗りつぶしあり円を描画する  長方形を描画するcv rectangle output                      thickness    画像を保存するcv imwrite  output bmp   img output それぞれの関数の意味は次の通りになります   線の描画        python    cv line 出力画像   始点のx座標  始点のy座標    終点のx座標  終点のy座標    B  G  R   線の太さ     点注意することはピクセル値の指定です 見ての通り     で青色で直線が表示されているので 色の指定は R G B ではなく  B G R になっていることに注意してください 直線に限らず その他も同様になります   円        python    cv circle 出力画像   中心のx座標  中心のy座標   半径   B  G  R   線の太さ  線の種類     塗りつぶしなしの円を描画する場合は線の太さを以上にします     塗りつぶしありの円を描画する場合は線の太さを にします   長方形        python    cv rectangle output   左上x座標  左上y座標    右下x座標  右下y座標    B G R   線の太さ     長方形は図形の左上と右下の座標を指定することで 表現します 実際の出力結果は以下の通りになります   image png  画像では後から描画したものが手前に来ます 今回は 青の直線→赤の円→黄色の円→ピンクの長方形の順番で描画したので 青の線より赤の円が手前に 黄色の円よりピンクの長方形が手前に来るようになっています   まとめ今回は 画像処理の第一歩ということでOpenCVを使って色々試してみました 画像処理を始めたい方のお役に立てれば幸いです ,19,2023-10-15
87,87,OpenCV + YOLOで車をぼかす,画像処理,https://qiita.com/yanagi-m/items/608b90d48f58a7be4547,   はじめに人物の顔をぼかす記事の続きです 今回は人ではなく車をぼかします 今回もChatGPTの力を借りながら初めて画像処理に挑戦してみたので その結果を備忘録として残しておきます    やりたいこと画像に車が映っていた場合 車全体をぼかす   テストで使用する画像今回は  写真AC  のフリー画像を使用します    環境Google Colaboratoryを使用します    YOLOのセットアップYOLO You Only Look Once とは 年に発表された物体検出手法です 画像全体をグリッド分割し 各セルが物体を持つか及びその物体のカテゴリと位置を一度の処理で判定します   Wikipediaより まず YOLOを動かすための環境をセットアップします 以下のコードを実行することで 必要なライブラリと重みファイルを取得できます    python git clone  cd darknet sed  i  s OPENCV  OPENCV    Makefile sed  i  s GPU  GPU    Makefile sed  i  s CUDNN  CUDNN    Makefile make wget    画像をアップロード次に 検出対象の写真をアップロードします    ぼかし処理    非最大抑制  NMS  を適用して 重複するバウンディングボックスを統合または除去する     param boxes  list   バウンディングボックスのリスト     return list  重複を除去した後のバウンディングボックスのインデックス    YOLOを使用して画像のナンバープレートを検出し ぼかし処理を行うこんな感じにぼかすことができました ちなみに ぼかした位置が分からない時があったので 分かりやすいようにぼかした範囲を赤枠で囲むようにしました   ダウンロード  png     ぼかしを強くするぼかしの強さは  cv GaussianBlur    関数の第引数であるカーネルサイズに応じて制御していて カーネルサイズが大きいほど ぼかしは強くなります ただし カーネルサイズは奇数である必要があります 試しに カーネルサイズをにしてみるとこんな感じです    pythoncv GaussianBlur roi           ダウンロード  png  若干 ぼかしが強くなりました   カーネルサイズ    カーネルサイズ       ダウンロード  png       ダウンロード  png     ちなみに cv GaussianBlur    関数の第引数は横方向の標準偏差を指しています 値が小さいほど よりカーネルの中心 ≠画像の中心 に強くぼかしがかかり 逆に 値が大きいほど より均一にカーネル全体にぼかしがかかるようになります    おわりに今回はこちらの企画の一環で 写真をアップロードする際にプライバシーな情報が映っていたらぼかしたい と思い 挑戦してみました 当初 ナンバープレートのみぼかそうと考えていましたががうまくできず  車を検出し 車自体をぼかすようにしてみたところ 思うようにぼかすことができました 余談ですが 企画自体は一旦クローズとなりました  経緯などはこちらのブログに載せています 気になったかたは是非ご覧ください ,5,2023-10-10
88,88,OpenCV + RetinaFaceで顔をぼかす,画像処理,https://qiita.com/yanagi-m/items/7650345068fef8f33113,   はじめに画像に映りこんだ人物の顔をぼかしたい というタイミングがあり ChatGPTにサポートしてもらいつつ初めて画像処理に挑戦してみたので その結果を備忘録として残しておきます 今回は 画像の読み込みや表示 ぼかし処理に OpenCV を 顔の検出に RetinaFace を使用してみました    やりたいこと画像に人物が映っていた場合 顔をぼかす   テストに使用した画像こちらの 写真AC  のフリー画像を使用します    環境Google Colaboratoryを使用します    準備まずは pipでライブラリをインストールします    python pip install tensorflow opencv python retina face次に ライブラリをインポートします    顔検出とぼかし処理準備が終わったら Colaboratoryに画像をアップロードします  ファイル選択 ボタンを押して 画像をアップロードします 画像がアップロードできたら OpenCVで読み込み → ぼかし処理を行います  ぼかし処理では 検出された各顔に対してぼかしを適用し ぼかされた部分を元の画像に反映するということをやっているようです  結果はこんな感じになりました 斜めの顔や横顔も認識してぼかすことができました  tada   blur png     補足顔の認識とぼかしをコードのどの部分で行っているのかについて補足します     顔の認識   detect faces 関数は 画像内の顔の位置や大きさなどの情報を返してくれます     顔をぼかすここで 検出した顔の情報を元に 画像の中のその顔の部分だけをぼかしています       顔が画像のどの部分にあるのかを切り取っています       切り取った顔の部分をぼかす処理をしています             はぼかしの大きさ   はぼかしの強さを表しています       ここで 元の画像の中の顔の部分を ぼかしたものに置き換えています    おわりに今回はこちらの企画の一環で 写真をアップロードする際にプライバシーな情報が映っていたらぼかしたい と思い 挑戦してみました ちなみに 企画自体は一旦クローズとなりました  経緯などはこちらのブログに載せています 気になったかたは是非ご覧ください ,8,2023-10-10
89,89,yolo v8でもすみっコぐらしたちをリアルタイムに物体検出してみる,画像処理,https://qiita.com/enya314/items/941d4c8bb18d2673fa25,  はじめにいつの間にやらYolo vまで出てしまっているので Yolo vで使ったすみっコぐらし学習データを使ってYolo vのオリジナルデータ検出に挑戦したいと思います vのときと基本やることは変わらないですね むしろインストールがさらに楽になってます   開発環境windows  ノートPC GPUなし   画像整理編集 物体検知実施Google Colabo   モデル学習  学習データ前回作成したyolo vの学習に用いたすみっコぐらし学習データを利用します 幸いにもデータ形式は依然と変わっていないようです 今は学習データ作成にはRoboflowを使うのがよい   GoogleColabでの学習実施   学習前準備以前同様GoogleのMyドライブ直下に sumikko フォルダを作成して学習データを丸ごと転送しておきます この時点ではこんなフォルダ構成sumikko     dataset yaml     train 学習用画像 枚ほど             IMG     txt　ラベリング情報     val 検証用画像 枚ほど   学習するのでGPUをONにしておく      MyDriveのマウントデータセットを読み込むためにMydriveをマウントします  pipからyolo vをインストール   wandbのインストール学習結果をリアルタイムで確認するためのツールとなります yolo vでも現在対応しているようです 使用するには以下のサイトへの登録が必要です 個人で使う分には無料のようです      pip install wandbYoloモデルのトレーニング時に 登録したアカウントに学習データをリアルタイムで連携しているようです 上記サイトにログインすることで学習状況が見られます    学習の実施※wandbを利用する場合は実行時にアカウント情報を入力する必要があります すべてのセル実行等では途中で止まりますのでご注意ください    wandbによる学習推移確認以下は最終結果ですが このようなグラフがwandbのダッシュボードでリアルタイムに更新されていきます metrics  image png  train val  image png     結果をドライブに移動結果をドライブに移動しておく   ローカルPC側の準備前回同様にpipで環境構築していく    Python環境の準備bit版のPythonをインストール  先に仮想環境を作成 任意のフォルダに移動しておく    yolovのインストールPytorch等必要資材もまとめてインストールされるみたいでｓ    モデルの移植学習時に以下のパスに学習結果を保存していたので content drive MyDrive sumikko resultsモデルの場所は以下となります  name は学習時に指定したnameとなります  content drive MyDrive sumikko results  name  weight best ptモデルをダウンロードしてyolovフォルダの直下に配置します 今回は以下の場所  すみっコ検出確認   物体検出実行source でカメラ番号を指定 show Trueにすると画像も併せて表示されます show TrueにしないとDetect結果だけプロンプトに流れます confで表示の閾値を指定できます   検出結果yolo vと同様にvでも作成したデータで検知できました   image png  以上です お疲れさまでした ,11,2023-10-10
90,90,PyOCR+Tesseract+画像処理でノベルゲームのテキストを抽出する,画像処理,https://qiita.com/MasKoaTS/items/b82d758b158bb7b50d5e,  はじめに私が趣味でノベルゲームをプレイするとき たまに意味を知らない単語や難読単語 実物を画像で調べたくなる単語 作中で話題に上がった料理など が登場することがあります このようなとき 通常は手動でブラウザを開いて検索ワードを入力するのですが 本記事では ゆずソフト作のノベルゲーム 千恋＊万花   Steam全年齢版リンク   を例に この作業をある程度自動化してみます        以下 本記事で当該ゲームにおける特定の場面を切り取った画像をいくつか使用しますが これらはすべてゆずソフト様による 著作権に関するガイドライン に従います footer ゆずソフト作 千恋＊万花 より  求肥 の意味を調べたい 具体的には Pythonで以下を順に試してみます   OCR 光学文字認識 の導入  メッセージウィンドウの切り抜き OCRによるテキスト抽出  画像処理によるOCRの認識性能向上  簡単なGUIアプリの作成  OCRの環境構築OCR Optical Character Recognition   光学文字認識 とは 活字や手書きの文章を画像として読み込み これを文字コードの列に変換する技術を指します 様々なOS上でOCRを実施するためのエンジンの一つとして  Tesseract   が挙げられ これをPython用のOCRツール  PyOCR   と組み合わせることにより PythonのプログラムでOCRの処理を実現できます    Tesseractの導入Tesseractの導入手順 Windows を簡単に示すと 次の通りです    Tesseract公式ページ   より Tesseractのインストーラをダウンロードして実行する   インストール先を選択してインストールする 詳しい導入手順を知りたい場合は 下記のリンク等を参照してください    PyOCRの導入PyOCRについては NumPy等の一般的なPython用ライブラリと同様の方法で導入できます 例えば    powershellpip install pyocrで導入できます    OCRの動作確認Tesseract PyOCRの導入が完了したら 早速動作確認を行ってみましょう 事前に画像処理ライブラリ  Pillow  のインストールが必要です    powershellpip install Pillowサンプルとして次の画像 出典：太宰治 走れメロス  を使用します   OCRエンジンのリストを取得してTesseractを指定  対応言語のリストを取得  画像の文章を読み込む続いて  tesseract get available languages    により 導入したTesseractに対応している言語のリスト  langs  を取得できます 今回は日本語文章に対するOCRを行いたいので  jpn  が含まれていればOKです 環境によって細部は変化しますが 次のような出力が得られます メロスは激怒した 必ず かの那智暴虐の王を除かなければならぬと決意した メロスには政治がわからぬ メロスは 村の牧人である 笛を吹き 羊と遊んで暮して来た けれども邪悪に対しては 人一倍に敏感であった  邪知暴虐 の 邪 が 那 と誤認識されていますが これ以外の部分は正しく変換されていることが分かります   OCRによるノベルゲームのテキスト抽出では 実際にノベルゲームでテキストが表示されるメッセージウィンドウに対してOCRを適用し テキストを抽出してみます    メッセージウィンドウの切り抜きまず ノベルゲームにおいて文章が出力されるメッセージウィンドウの矩形領域を画像として切り抜きます 事前にPythonのGUI自動化ライブラリ  pyautogui  のインストールが必要です    powershellpip install pyautoguiまた 後ほど簡単なGUIアプリを作成するための準備として PythonでGUIを組むためのツールキット  Tkinter  を使用します   矩形領域の設定  GUIウィンドウの雛形を準備      指定した矩形領域のスクリーンショットを取得 保存  msごとに再実行 矩形領域の定数は環境に応じて適宜変更してください 上記を実行すると   \ m ms   ごとに指定した矩形領域のスクリーンショットが  txt image png  として保存されます    メッセージウィンドウに対するOCRの適用先で得られた矩形領域の画像に対して OCRを適用します   矩形領域の設定  OCRエンジンを取得  GUIウィンドウの雛形を準備      指定した矩形領域のスクリーンショットを取得 保存  msごとに再実行       OCRで画像の文章を読み込む  矩形領域の画像 プレーン ：  出力文字列 プレーン ：     関東だと求肥を使うるが くWe  関西だと小却粉や山合ど    かなんだうででさ出力された文字列のうち半分ほどが正しく認識されていないことが見て取れます 一般に 文字列の背景にカラフルな模様などが描かれている場合 正確な変換が行われないことが多いです よって OCRを行う前に 文字列の背景に存在する余計な模様などを取り除く必要があります    画像処理によるOCRの認識性能向上まず 矩形領域の画像をグレースケール画像に変換します   矩形領域の画像 グレースケール ：  出力文字列 グレースケール ：     関東だと求肥を使う志 が少くWぐ  関西だ厨小麦粉や山合    がかなんだうGきI次に グレースケール変換された矩形領域の画像における各ピクセルについて 画像における最大輝度からの差を一定の倍率に拡大します 変換後の輝度が    未満になる場合 変換後の輝度は    とします             このような処理は グレースケール画像における文字列の輝度と背景の輝度の差が十分大きく 文字列の輝度が    白色 に近い場合にのみ有効です 文字列の輝度が    黒色 に近い場合は 画像における最小輝度を参照すると良いです     このような処理とは別に 閾値としてある整数値  k     \leq k \leq    を定め 輝度が  k  以上のピクセルの輝度を    そうでないピクセルの輝度を    として二値化する方法も考えられます 筆者の環境では輝度の差を拡大する処理の方がOCRの認識性能が上だったため 二値化する方法は採用していません 例えば 画像における最大輝度が    であり 拡大倍率を    とするとき 輝度    のピクセルの輝度を    \max             imes          に変換し 輝度    のピクセルの輝度を    \max             imes          に変換します   矩形領域の画像 輝度の差を拡大 ：  出力文字列 輝度の差を拡大 ：     関東だと求肥を使うことが多くて 関西だと小麦粉や山芋と    かなんだってさ この時点で認識性能はかなり改善されていますが ゲーム側の設定を変更することにより さらに認識性能を高められる可能性があります 例えば 本記事で取り上げているノベルゲーム 千恋＊万花 の場合    システム設定  →  テキスト  →  ウィンドウ透明度  の順に選択し ウィンドウ透明度の数値を    に変更する   システム設定  →  テキスト  →  フォント選択  の順に選択し メッセージウィンドウにおけるテキストのフォントを 源ノ角ゴシックB 等に変更するという操作を行うことで 認識性能がさらに向上します   矩形領域の画像 ゲーム側の設定を変更 ：  出力文字列 ゲーム側の設定を変更 ：     関東だと求肥を使うことが多くて 関西だと小麦粉や山芋と    かなんだってさ ここまでの処理をPythonのソースコードに記述します 事前に  numpy  のインストールが必要です    OCRエンジンのリストを取得してTesseractを指定  矩形領域の設定  GUIウィンドウの雛形を準備      画像をグレースケール変換して最も輝度の高い部分を強調      指定した矩形領域のスクリーンショットを取得し OCRを適用  msごとに再実行       OCRで画像の文章を読み込む  \ m ms   ごとに指定した矩形領域の画像を取得し そこからOCRでテキストを抽出する流れは先程までと同様ですが 上記のプログラムでは 矩形領域の画像にOCRの認識性能を向上させるための処理を行う関数  highlight white    を追加しています この関数は グレースケール変換を行った矩形領域の画像における各ピクセルの輝度をNumPy配列に格納し 各要素について画像における最大輝度からの差を一定の倍率に拡大する処理を行った後 配列をグレースケール画像に変換したものを返します   単語検索を効率化するGUIアプリの作成ここまでの説明を踏まえ 冒頭で述べた目的 ノベルゲームのテキスト内に登場した単語の検索をある程度自動化すること を達成するための簡単なGUIアプリを作成します 先にアプリのソースコード全体を示します   矩形領域の設定  OCRエンジンを取得  GUIウィンドウを作成  テキストエリアの作成  検索ボックスの作成  検索ボタンの作成      画像をグレースケール変換して最も輝度の高い部分を強調表示  マウスカーソルがGUIウィンドウ内に存在するか判定      指定した矩形領域のスクリーンショットを取得し OCRを適用  msごとに再実行       ウィンドウ内にマウスカーソルがないときのみ有効          OCRで画像の文章を読み込む      選択範囲のテキストを検索ボックスに貼り付け  msごとに再実行    矩形領域の設定とGUIウィンドウの作成まず 矩形領域の定数を設定してOCRエンジンを取得する過程は 先程までと同様です 次に 先程はGUIウィンドウを最低限の部分のみ作成しましたが 今回は必要な分のウィンドウサイズ及びウィンドウのタイトルも設定します また GUIウィンドウ内にマウスカーソルが存在するかどうかを判定したいので この状態を格納する変数  enter flag  の初期化を行います    python   GUIウィンドウを作成   テキストエリアと検索ボックスの作成続いて TkinterのWidgetの一種である  ScrolledText  を用いてテキストエリアを作成します OCRによって抽出された文字列がこのエリア内に格納されます また 検索する単語を格納するための入力ボックスも作成します こちらは  Entry  Widgetを用います テキストエリア内の文字列をドラッグしたとき 選択範囲の文字列がそのまま格納されます    python   テキストエリアの作成  検索ボックスの作成   検索ボタンの作成次に 単語検索を実施するボタンを作成します これらのボタンをクリックすると自動でWebブラウザが開かれ 入力ボックスに格納された単語の検索が行われます    今回は    種類のボタンを作成しており それぞれ  Google検索 すべて     Weblio検索    Wikipedia検索    Google検索 画像    に対応しています    python   検索ボタンの作成   マウスカーソルの存在判定前述のマウスカーソルがGUIウィンドウ内に存在するかどうかの判定は GUIウィンドウの参照  root  に対して  bind    を設定することで実現します    python   マウスカーソルがGUIウィンドウ内に存在するか判定   矩形領域の画像取得とOCRによるテキスト抽出矩形領域の画像取得と画像処理及びOCRは 基本的に先程までと同様ですが マウスカーソルがGUIウィンドウ内に存在しない場合にのみ有効とし 抽出されたテキストを先のテキストエリア内に格納する処理を追加しています       指定した矩形領域のスクリーンショットを取得し OCRを適用  msごとに再実行       ウィンドウ内にマウスカーソルがないときのみ有効          OCRで画像の文章を読み込む   選択範囲の貼り付け前述のテキストエリア内における選択範囲の文字列を入力ボックスに格納する処理は 関数  paste txtbox    によって実現されます 関数  get txt image    と並行して   \ m ms   ごとに繰り返し実行されます       選択範囲のテキストを検索ボックスに貼り付け  msごとに再実行    GUIアプリの実行以上の実装を踏まえ このアプリを実行したときの画面は 次のようになります メッセージウィンドウに表示されたテキストがOCRによって抽出された後 GUIウィンドウのテキストエリア内に格納されます この中の検索したい単語をドラッグで選択して    種類のボタンのうちいずれかをクリックすることにより 当該単語の検索が行われます 画像検索を行う場合も同様に 検索した単語を範囲選択し 画像検索に対応するボタン ここでは Google検索 画像   をクリックすれば良いです   まとめ以下 本記事のまとめです   OCR Optical Character Recognition   光学文字認識 とは 活字や手書きの文章を画像として読み込み これを文字コードの列に変換する技術を指す   OCRエンジンの一種であるTesseractと PythonでOCRを行うためのツールPyOCRを導入することで PythonでOCRを実施するプログラムを作成できる   ノベルゲームでテキストが表示されるメッセージウィンドウのスクリーンショットを取得し これに対してOCRを適用することにより テキストの抽出を行った   先のスクリーンショットに対して OCRをそのまま適用するだけでは正しく認識されない文字が多い グレースケール画像に変換して各ピクセルについて最大輝度からの差を拡大する等 適切な画像処理を施すことによって認識性能を高められることを確認した   OCRによるテキスト抽出と GUIツールキットTkinterを組み合わせることで ノベルゲーム中で気になった単語の検索を効率化できるGUIアプリを作成した   おまけ：GUIアプリのデモ動画,5,2023-10-03
91,91,画像分析モデルを作成するツールの比較: Custom vision と AI Builder と Lobe,画像処理,https://qiita.com/ItachInotanukI/items/2b63ba6d044f2895424e,   はじめに　画像分析とは 画像から情報を抽出する技術のことです 例えば 画像内の物体の種類や位置を特定したり 画像に含まれるテキストや顔を認識したり 画像の品質や感情を評価したりすることができます 画像分析は さまざまな分野で応用されており ビジネスや教育 医療などに役立っています 　画像分析を行うには 人工知能  AI  の一種である機械学習  ML  を利用します 機械学習とは データからパターンやルールを学習し 未知のデータに対して予測や判断を行う技術のことです 機械学習では 学習したパターンやルールを表現するものをモデルと呼びます 画像分析では 画像から情報を抽出するためのモデルを作成します    今回のテーマ：Custom vision と AI Builder と Lobe の比較それぞれの説明は後述するとして まずは一覧表で確認します Custom vision と AI Builder と Lobe の両方とも画像分析モデルを作成できるツールですが それぞれにメリットとデメリットがあります 以下に比較表を示します   項目   Custom vision   AI Builder   Lobe    ライセンス費用   Azure のサブスクリプションと Custom vision のリソースが必要   Power Platform のライセンスと AI Builder のライセンスが必要   Lobe のアカウントが必要    データ保護   クラウド上で処理される   クラウド上で処理される   ローカル上で処理される    データ量   大量の画像をアップロードできる   大量の画像をアップロードできる   ローカルストレージに依存する    学習時間   クラウド上で高速に学習できる   クラウド上で高速に学習できる   ローカル上で学習するため時間がかかる    精度   高精度なモデルが作成できる   高精度なモデルが作成できる   データやパラメーターに依存する    統合性   Azure サービスや他のアプリと統合できる   Power Platform サービスとシームレスに統合できる   Power Platform 以外のサービスやアプリと統合する際には手間がかかる  この表からわかるように Custom vision と AI Builder と Lobe は 画像分析モデルを作成するためのツールとして それぞれに特徴があります Custom vision と AI Builder はクラウドサービスであり Lobe はローカルアプリです また 統合性の面では それぞれに適したサービスやアプリが異なります Custom vision は Azure サービスや他のアプリと統合することができます AI Builder は Power Platform サービスとシームレスに統合することができます Lobe は Power Platform 以外のサービスやアプリと統合する際にはエクスポートやテンプレートの利用などの手間がかかることがあります したがって 画像分析モデルを作成するためのツールを選択する際には 自分のビジネスシナリオや目的に応じて 以下のような観点から判断することが重要です     コスト    画像分析モデルの開発や運用にどれくらいの予算があるか     データ保護    画像分析モデルを作成する際に利用する画像はどこから取得するか 画像の保護が必要か     データ量    画像分析モデルを作成する際に利用する画像はどれくらいあるか     学習時間    画像分析モデルを作成する際にどれくらいの時間がかかっても良いか     精度    画像分析モデルの精度はどれくらい高くなくても良いか     統合性    画像分析モデルを利用するアプリやサービスは何か 例えば 以下のような場合には それぞれに適したツールを選択することができます   コストやデータ保護が気にならず 大量の画像を高速に高精度で分析したい場合は Custom vision や AI Builder を選択することができます   コストやデータ保護が気になり 少量の画像をローカルで分析したい場合は Lobe を選択することができます   Azure サービスや他のアプリと連携する必要がある場合は Custom vision を選択することができます   Power Platform サービスと連携する必要がある場合は AI Builder や Lobe を選択することができます    ここから各サービスの説明   Custom vision とはCustom vision は Azure の Cognitive Services の一部で 独自の画像識別子モデルを構築 デプロイ 改良できるサービスです Custom vision を利用するには Azure のサブスクリプションと Custom vision のリソースが必要です¹ Custom vision では 以下のような機能が提供されています     画像分類    画像全体に  つ以上のラベルを適用します     物体検出    画像内の物体の位置と種類を特定します Custom vision の特長は以下のとおりです     カスタマイズ性    画像分類や物体検出などの事前定義されたモデルをカスタマイズしたり 独自のモデルを作成したりできます     高速性    クラウド上で高速に学習やデプロイができます     精度    高精度なモデルが作成できます     統合性    Azure サービスや他のアプリと統合できます Custom vision のデメリットは以下のとおりです     コスト    Azure のサブスクリプションと Custom vision のリソースが必要であり コストがかかります     データ保護    クラウド上で処理されるため データの保護が気になる場合があります    AI Builder とはAI Builder は Power Platform の一部として提供される AI モデル作成サービスです Power Platform とは Microsoft が提供するビジネスアプリケーションプラットフォームで Power Apps や Power Automate などのサービスが含まれています AI Builder を利用するには Power Platform のライセンスと AI Builder のライセンスが必要です² AI Builder では 以下のような機能が提供されています     画像分類    画像全体に  つ以上のラベルを適用します     物体検出    画像内の物体の位置と種類を特定します AI Builder の特長は以下のとおりです     カスタマイズ性    画像分類や物体検出などの事前定義されたモデルをカスタマイズしたり 独自のモデルを作成したりできます     高速性    クラウド上で高速に学習やデプロイができます     精度    高精度なモデルが作成できます     統合性    Power Platform サービスとシームレスに統合できます AI Builder のデメリットは以下のとおりです     コスト    Power Platform のライセンスと AI Builder のライセンスが必要であり コストがかかります     データ保護    クラウド上で処理されるため データの保護が気になる場合があります    Lobe とはLobe は Microsoft が提供する画像分析モデル作成アプリです Lobe を利用するには Lobe のアカウントが必要です³ Lobe では 以下のような機能が提供されています     画像分類    画像全体に  つ以上のラベルを適用します     物体検出    画像内の物体の位置と種類を特定します Lobe の特長は以下のとおりです     簡単さ    ローカルで動作するアプリなので インターネット接続やクラウドサービスの設定などは不要です 最低  枚の画像でモデルを作成できます 動画からも学習データを取得できます     柔軟さ    TensorFlow や ONNX などのさまざまなフォーマットにモデルをエクスポートしたり Web やモバイルアプリのテンプレートを利用したりできます また 最近では Power Platform とも連携できるようになりました     精度    データやパラメーターに依存しますが 高精度なモデルが作成できます Lobe のデメリットは以下のとおりです     データ量    ローカルストレージに依存するため 大量の画像をアップロードできない場合があります     学習時間    ローカル上で学習するため 時間がかかる場合があります     統合性    Power Platform 以外のサービスやアプリと統合する際には エクスポートやテンプレートの利用などの手間がかかる場合があります    まとめこの記事では 画像分析モデルを作成するためのツールとして Custom vision と AI Builder と Lobe の違いと特長について比較しました Custom vision と AI Builder は Microsoft が提供するクラウドサービスですが Lobe は Microsoft が提供するローカルアプリです それぞれにメリットとデメリットがあります 自分のニーズに合わせて最適なものを選択することができます 画像分析は さまざまな分野で応用されており ビジネスや教育 医療などに役立っています 画像分析モデルを作成するためのツールを利用すれば コーディングや数学などの専門知識がなくても 簡単に画像分析モデルを作成できます ぜひ Custom vision と AI Builder と Lobe の両方を試してみてください ,0,2023-09-30
92,92,【ChatGPTアプリ有料版】音声会話を聞いてみた。,画像処理,https://qiita.com/kabumira/items/0b3749bbe68d73b6cec7,　どうも カーブミラーです 　今回は ChatGPTアプリ 有料版 の音声会話について YouTuberかわごんさんが動画にしましたので それに乗っかりますw   note info本記事は ChatGPT 無料版 では行なっておりません 行なえませんwあしからずw　ChatGPTファンの中には 　　ご存知の方も多いかと思います 　有料版　 Plus   Enterprise User の　　みなさんが 得られる特典機能 　画像認識と音声会話という　　マルチモーダル機能について　　　語ります 　　　　おいら 無料版なのに 泣 　この機能は 　　発表から二週間で 　　　順次 開放されていきます 　現在は 　　まだ一部の人のみが　　　開放されている状況です 　ランダム ということは　　ないとは思いますが 　　　その順番については 　　　　みんな頭をひねっています 　まぁ それはともかく 　　はじめましょう 　まずは 　　画像認識のことは 　　　棚上げしますw　　　　わからんしw　なお 　　開放されているかは 　　　Settingsに Speech　　　　という項目ができていれば　　　　　使えます 　　または 　　　画面上部に 　　　　マークがあれば　　　　　使えます 　なかったら 　　開放をお待ちください 　Settingsで　　New Futuresを選択 　　　Voice Conversionsを　　　　オンにします 　チャットモードになると　　音声選択モードになります 　　　●●●●と　　　　名前が個あるので　　　　　いずれかの名前を選択 　名前 声優さんだそうです 　　Sky　　Breeze　　Juniper　　Ember　　Cove　選んだら Confirmボタンを押す 　それで 　　会話モードに　　　なります 　さて 　　あとは 　　　若いおふたりにお任せして……　としたいが 　　誰も若いとは言っていないし 　　　お見合いじゃないしw　まぁ 　　やり取りは 　　　YouTuberかわごんさんの　　　　動画を確認すべきかも   ChatGPT  音声マルチモーダルAI ス マホアプリの新機能登場   　この動画の最後に　　特典ならぬ 　　　英会話練習Promptが　　　　ありますよ 　最後に 　　X Twitterにて 　　　ツイートされた　　　　画像認識の例を　　　　　紹介します 　箇条書きで　 落書きの感想を求めたり　 ×のクロスワードクイズを　　　解かせたり ✘ 　 数独を解かせたり ✘ 　 電卓を作ったり　 図表の読み取り　 写真撮影や絵画描写のアドバイス　 栄養管理　 状況説明や画像の場所推定　 読みにくい手書きメモを読む　 電子部品の判別　あ あ 　　無料版なのに 　　　なんで こんな記事を　　　　書いているのやら 泣 　でも 　　いろんな可能性を　　　期待しているので 　　　　OpenAIだけではなく 　　　　　AIベンダーには 　　　　　　頑張って欲しい　　　　　　　ものですね 　今回は ここまで ,0,2023-09-30
94,94,画像に文字を入れる,画像処理,https://qiita.com/ekzemplaro/items/fd5e38751d15ee10a087,   入力画像   出力画像    変換スクリプト   確認したバージョン,0,2023-09-27
95,95,画像形状を 16:9 に変換,画像処理,https://qiita.com/ekzemplaro/items/07144f8408ce22e46bd0,フォルダー内にある   jpg を一括で   に変換する方法です 変換前の画像    jpg  変換後の画像,0,2023-09-26
96,96,画像わからないデータエンジニアが画像系コンペに参加してソロ銀メダル取った話,画像処理,https://qiita.com/Lana2548_t/items/81a3536e12a53ef1c7d6,   はじめに若干釣りっぽいタイトルで恐縮です画像についてあまり詳しくない現職データエンジニアが 単騎で画像コンペに突っ込んでいき どのようなプロセスでメダルを獲得するに至ったの を共有する記事となっております 画像コンペの進め方がわからない人間がどのようにして進めていったのかを共有することで これからコンペに参加してみたいな〜と思っている方へのモチベーション向上 後押しになれば良いなと考えております 細かいアーキテクチャや使った手法 ツールについてはあまり言及できていないと思います ライトに読んでいただくことを目的としたいので予めご了承くださいまし    書いている人間のステータスについて  現職データエンジニア BigQueryが友達   統計検定級を取っているため 数式を見てもアレルギーは出ない  仕事で画像をはじめとした非構造データを扱うことは少ない      最近文章系はちょっと増えた  AIについては若干の知識があり 自然言語処理系はなんとなくわかっている つもり       　Embeddingの理解はとっかかりなくできたレベル  画像処理については チュートリアルでよくある手書き数字の分類をしたことがあります 程度      登竜門に到達した してない くらいのレベル  ちゃんとコンペ参加したことがなかった      とりあえず他人のコードを真似して回だけ投稿した実績は多数あり ドヤ   英語はちょっとだけ読める  読んだことがあった画像系AIの本      ゼロから始めるディープラーニング と       PythonとKerasによるディープラーニング          サーーっと読んだだけで何も覚えてない  肝臓が弱すぎて日本酒は飲めません      どんなコンペに参加したの 結果は Nishikaで年月〜年月日まで開催していた  日本酒銘柄画像検索   というコンペティションに参加しました 内容としては 日本酒に絞った画像検索です 例えば 下記の図のように 獺祭 の写真を検索クエリとしての画像としたときに 日本酒のデータベースから 獺祭 の写真を引っ張ってくる みたいなタスクを行えるようなモデルの開発をするのが目的で 引っ張ってきた画像がちゃんと獺祭の写真撮ってこれてるの や 獺祭 以外の画像はちゃんと弾いてる という観点から評価が行われ モデルの優劣をつけるコンペティションになっています   スクリーンショット       png  評価指標にはMean Reciprocal Rank というもので評価されています なんぞ って感じですが  引っ張ってきた画像がちゃんと獺祭の写真撮ってこれてるの  獺祭 以外の画像はちゃんと弾いてる  の観点から評価が行える指標で 検索をする上で自分が目的としている対象が上位に来ることはどの検索エンジンでも望まれることで それを反映でき 定量的に評価できるようなものになっています  実は評価指標について途中で運営側のミスが発覚し修正されていますが 後述します  結果としてはタイトルの通り 銀メダルを取得でき 順位としては位 人中で上位 には入れているという結果でした 後述しますが 個人的には もーちょっと頑張れたのでは  と思っている結果でもあります   スクリーンショット       png     取り組んだ流れ    コンペ開始直後 月 このタイミングでは また面白そうだけどむずそうなコンペ始まったなぁ   とりあえず参加だけしとくか と考え 参加ボタンを押しました 運営から提供されているサンプルコードをGoogle Colabに丸ごとコピペし 実行して投稿  何もわからん の感想だけが残り Switchを手に取りゼルダをプレイし始めた覚えがあります       月上旬参加者の方が有志でベースラインを投稿してくださったのをみて また丸コピして実行しました 画像系のコードの書き方に慣れていない部分が大半で  DataLoaderって結局なんぞ   albumentationsって初めて聞いたぞおい     学習率にスケジュールかけるなんてあるんか など新しい発見と自分の未熟さを痛感しながら一行づつわからないなりに丁寧に読み進めました この時からChatGPTなどを使ってコードの解説を行わせながら知識を深めていきました 読み終わった際に 何だかいけそうな気がする というどこから来たかもわからない自信を元手にやる気を出してここから進めていくことと相成りました     月中旬月初旬に参考にした他の方のベースラインを改修していく形で 自分自身のベースラインを作成する形にしました いつ 後で読む フォルダに突っ込んだか覚えてもいない Pytorchでpretrainモデルを用いた画像向けSiameseNetworkのメモ  という記事がどうやら使えそうということがわかり 早速組んでみてワクワクしていたところ惨敗 後から分かったことで問題設定とあってなかったというわけですが その当時はそれすらわからず  できんから不採用 という形で 他の角度系アルゴリズムを用いて進めました  よくわかる角度系深層距離学習 前編  がとてもわかりやすくここから急速に理解が進み始めました またこの時期は同時並行して過去の似たようなコンペで上位解法を物色し始めていました 主に参考になったのはNishikaの過去コンペの AI×商標：イメージサーチコンペティション 類似商標画像の検出 の位解法  です PCAについてはほぼ丸パクリしてるくらいです     月上旬QiitaやZennを読み漁りながら何とか自分自身のベースラインを提出 ロスの下がり具合がかなり良かったので期待しながら提出するとスコアは手元のものと比べ 約分の程度でした 普通手元のスコアと投稿後のスコア LB がここまで乖離することはほとんどないイメージだったため 原因も追及せずに おお   センスないな俺    とかなりショックを受けてここで一度やめてしまいました ゲームと同じで 自分が考えた攻略方法がまるで太刀打ちできなくクソゲー認定してしまった時と同じような感情をここで抱きました   スクリーンショット       png      月中旬この時期は順位を上げるというより 単純な知的好奇心が芽生え  角度系のアーキテクチャってもっと具体的にどういう仕組みで動いてるんやろか  を明瞭にしたくお勉強の期間として頑張っていました 実際に学んだ内容が日本語で解説されている記事がほぼないことに気づいて 備忘録として CosFace ArcFaceとElasticFaceへの拡張  という形で記事に残したりしていました 以前からChatGPTを活用しながらコード生成させたり エラーを解析させたりしていましたが 論文を要約させたり 中身について質問するという使い方をするようになり始めました これがかなり有効でわからないところは 猿でもわかるように説明しろ  と命令して 中学生が理解できるレベルまで噛み砕いて説明させていました また自分の理解が正しいかどうかも聞けるのが すげぇなぁOpenAI と感心しながら利用していました   スクリーンショット       png      月下旬この時期に事件が起きました 運営側で投稿時に使っていた評価指標が実は間違えていたということが参加者の提言から発覚し 評価指標の見直し 正しい評価指標を使ったランキングのし直しが実施され まさかの上位に躍り出てしまいます   スクリーンショット       png  実は自分がやっていたことが間違いではなく 地味にスコアを上げる行動だった ということがこの時にわかり 再度やる気が出てきて色々やり始めます またこの時期にいろんなアーキテクチャの実験があらかた完了したフェーズだったので さらに大きなモデルの構築に進むためにGoogle ColabをPro にして気合いを入れながら学習を進めていました 外にいる間も寝る直前もwandbという学習経過を見れるサービスを使って 今どんな感じかなぁ と観察しながら楽しんでいました ↓なんかそれっぽいダッシュボード  スクリーンショット       png      月この時期は学習などはあらかた終了し  後処理 と呼ばれる検索の精度を既存のモデルを使って細かく調整していくフェーズに入りました  Google Landmark Retrieval Kaggleの画像検索系の大きなコンペ 解法のお手本になることが多いです  をはじめとする過去の画像コンペの上位解法を真似したり 改変しながら効きそうな手法を色々試していました が かなり頭打ちの状態だったので何しても精度が良くならずに悶々としながら作業を進めていました 最終的には諦めてしまい ここがダメ  お祈りで ShakeUpしないかなぁ     と祈りながら最終週を過ごしていました   スクリーンショット       png      コンペ終了 月日 細かい解法についてはコンペのトピックで共有しているため割愛しますが 最終的にモデルのアンサンブルで位でした お祈りShakeUpは叶わず むしろShakeDownしてしまう始末に    位解法     反省点色々ありすぎて全部は上げられません が いくつかピックアップしてみます     データを見なさすぎたAIコンペを進める上で壊滅的なことですが 検索対象 検索用画像 トレーニング画像を 程度しか見ずにコンペを進めていました 他の上位者の解法を見ると  ラベルづけの間違いがあった  複数の銘柄が写っている写真があった など 精度にクリティカルに影響しそうな内容に言及されている方がいらっしゃって  やっちまった とその場でもう猛省しました AIコンペ全体に言えることだと思いますが  人間が単純に理解しやすいことはAIにも理解しやすい と考えております というのも 人間にとって判断が難しいことってAIにとっても難しいことが多いです 例えばラベル付が間違えている場合  え この写真って〇〇って名前の銘柄じゃないん     となるようにAIも同じです なのでデータをしっかり見た上で データのクレンジングやモデル作成の方針決めを行うのは よく言われていることだけどなぜよく言われているのかが今回で身をもって経験できたと思います なので   データは絶対にみましょう       検証の多様化がなかった最初に他の方のベースラインを見て進めていった と前述しましたが 他の方の解法を見ると物体検出系のモデルを使って 瓶を大きくするように加工していたり 複数の瓶がある場合はそもそもトレーニング画像からは削除する のような工夫をされている方がいらっしゃいました これも データを見ろ  に通ずるところですが 考えてみればノイズになりそうだなぁというのは直感的にわかりますよね 私も 抜本的に効果的な前処理とか必要だよなぁ〜 と思いつつも目の前の学習の精度を上げることにしか興味がなくなっていた時期がありました なので   いろんな観点から検証をしてみましょう 失敗は成功の元です       検証のサイクルが遅すぎた特に使用するモデルのbackbone EfficientNetやSwin Transformer  ResNetなど の検証でより良いものを選んだ方がいいよなと考えていましたが それを検証するのに全てFoldに分けてループ実行する のような無駄時間を使って全て検証をしていました よくよく考えれば最初は適当にtrain  validに分けて数epochやればいいものの Foldepochを全部やって あ ダメやん という判断を繰り返すことを回程度やっていました 時間の無駄でしたし Google Colabのクレジットを大量に消費するだけの行為でしかなかったです なので   検証はステップに分けて細かくやろう 見切りをつけるのは早めに      やって良かったこと    ChatGPTの活用もはやこれの恩恵でしかないと思っていますが 今回のコンペではフル活用しました 前述した通り コードの生成 エラーの解析 論文の要約など 専門的なことでわからないことはとりあえずChatGPTさんに聞いてました     記事の公開つしか公開していないですが 細かいアーキテクチャの内容を理解するのに記事を公開することを目標にするのはいいことだなと痛感しました やはりアウトプット最強説は間違いないと思います     コンペに参加しきったことなんだかんだこれが大きいかなと思います 私自身も参加ボタンだけ押してsubするかしないかみたいなコンペはじゃきかないです ですが 参加し続けてみていろんな知見を得て最終的にメダルを取れたことは自分にとっての自信にもなりますし 今後のコンペ参加へのハードルを大きく下げてくれる要因にもなるなと思いました 論文読み込むなんて大学以来することないと思っていたことも 楽しく感じられるようになるんだ    と成長にもつながったと思います    データエンジニアのキャリアとしての影響そんなにないかな とも思いますが 昨今のLLMの台頭によって とりあえずAI  みたいな風潮は少なからず増えてきた中で 実際にAI開発するならどんなプロセスを踏むの という部分で具体的な内容を経験として得られたことは 顧客と話す中で なんとな〜くぼやっと話す からある程度説得力を持たせるための材料になったと思います なので  自分はAIエンジニアじゃないから     AIなんか難しそうだし    とならずに どんな職種の方でもとりあえず参加してみるのはとてもいいことじゃないかなと思っています もしかしたら共感してくださる方がいるかもですが 特にAIコンペは作業興奮めっちゃあるように感じます笑   おわりに画像コンペにソロで初参加したデータエンジニアがもがいていた体験談をつらつらを共有させていただきました 正直しんどいなぁ〜〜と思う時期もありましたが 結果的には成績も残せて良い経験に繋げられたので やり続けることが結局大事なんだなと感じました Nishikaはトピックでの意見交換がそこまで盛んではないため あまり初心者には優しくない環境かもしれませんでしたが 逆に自分で考えることが増やせるな とプラスに捉えていたので あまり問題ではなかったように思います 今後は他の方の解法の実施や Kaggleへの参加 チームでの参加もしてみたいなと考えています ,14,2023-09-25
97,97,Pythonを使用してPDFドキュメントからテキストと画像を抽出する方法,画像処理,https://qiita.com/iceblue/items/48c8d3ab0310fc77a431,PDF文書のさまざまな特殊なスタイルを保持しながら 編集 コピー 検索などが難しいものです PDF文書のコンテンツを処理する必要がある場合 テキストと画像をPDF文書から抽出して これらのコンテンツをさらに操作したり 他の目的に使用したりするために ツールを使用することが選択肢となります この記事では Pythonを使用して  PDFファイルからテキストと画像を抽出して保存する方法  について説明します この方法には  Spire PDF for Python  が必要です 公式ウェブサイトからダウンロードするか pipを使用して直接インストールすることができます pip install Spire PDF   PDFファイルからテキストを抽出する方法  ExtractText   メソッドは 個々のページのすべてのテキスト 空白を含む を抽出し 文字列として返します したがって ドキュメントの各ページを反復処理し このメソッドを使用して各ページのテキストを抽出し テキストファイルに書き込むことで PDFファイルからテキストを抽出することができます 詳細な手順は以下の通りです：    PdfDocument   クラスのオブジェクトを作成します   抽出されたテキストを保存するためのテキストファイルを作成します   ドキュメントのページを反復処理し   PdfPageBase ExtractText     メソッドを使用してページのテキストを取得し テキストファイルに書き込みます   PdfDocumentクラスのオブジェクトを作成するpdf   PdfDocument    PDFドキュメントをロードするpdf LoadFromFile  サンプル pdf    抽出されたテキストを保存するためのTXTファイルを作成するextractedText   open  output 抽出したテキスト txt    w   encoding  utf     ドキュメントのページを反復処理する      ページを取得する      ページからテキストを抽出する    text   page ExtractText        テキストをTXTファイルに書き込む    extractedText write text    \n  extractedText close  pdf Close    サンプル    抽出したテキスト     PDFファイルから画像を抽出する方法画像を抽出する方法は テキストを抽出する方法と似ています 各ページを反復処理し   ExtractImages   メソッドを使用して個々のページからすべての画像を抽出し リストとして返します その後 リスト内のすべての画像をファイルに保存します 詳細な手順は以下の通りです：    PdfDocument   クラスのオブジェクトを作成します リストを作成します   ドキュメントのページを反復処理し   PdfPageBase ExtractImages     メソッドを使用してページ内の画像を取得し 作成したリストに追加します   リスト内の画像を保存します   PdfDocumentクラスのオブジェクトを作成しますpdf   PdfDocument    PDFドキュメントをロードしますpdf LoadFromFile  サンプル pdf    画像を保存するためのリストを作成しますimages       ドキュメントのページを反復処理します  抽出した画像を保存します  抽出した画像      上記は PDFファイルからテキストや画像を抽出するためのSpire PDF for Pythonの使用方法についてです PDFファイルの操作について詳しく知りたい場合は  Spire PDF for Pythonのチュートリアル  をご覧ください  また  Spire PDFフォーラム  で議論に参加することもできます   ,3,2023-09-22
98,98,カウント画像のランダムサンプリングによる超解像度化の実装方法,画像処理,https://qiita.com/yusuke_s_yusuke/items/2bec75bbcbd78e4d2478,  はじめにカウント画像は 天文学や光学顕微鏡の画像など 様々な分野で幅広く使用されています カウント画像の空間スケールを詳しく知るために 画像の実ピクセル以下の空間情報が必要となる場面があります その一つの手段として超解像があり 古典的な手法であるニアレストネイバー法やバイリニア補間から 最新の機械学習手法まで さまざまな手法が研究されています 本記事では その中でカウント画像の超解像化に広く使われる 非常にオーソドックスなランダム化に基づいた手法について実装方法等を紹介します   ランダム化による高解像度化ランダム化による超解像化は以下のようなイメージで実装されます   image png    観測されたイベントの空間情報が与えられます   本来は 手順のように無限に細かい情報がありますが 観測時に実ピクセル単位でサンプリングされます   ランダム化による超解像化では ピクセル毎に任意の画素に分割し ランダムにカウント値を振り分けます  カウント値のため 振り分けは整数単位になります    ランダム化による補完は意味のある確率分布を仮定していないため 実ピクセル程度のガウシアンフィルタで平滑化します   方法ここでは つのガウス分布で得られる簡単なシミュレーション散布図を用いて 実装について紹介します 本記事で使用したコードは こちら  にあります      各種インポート     観測用の散布図の用意観測用の散布図を用意します この散布図の各プロットを検出する際にグリッド毎に集計されて観測画像として得られます   image png     python 散布図を生成するためのコード  次元ガウシアン分布から乱数の散布図  単位目盛りあたりのグリッドを作成コードの説明    np random seed    で再現性のため乱数の種を指定します     x grid      y grid  の箇所の     は 浮動小数点の影響で  np arange    の配列の選び方が変わってしまうことを避けるために      という小さな値を加えています     mean       mean       cov  で     と   を中心とする分散の次元ガウス分布に従う乱数を生成します   この後 ヒートマップを作るにあたり見やすくするために  x grid      y grid  で目盛りずつグリッドを引きます このグリッドとピクセルが対応します ※ガウス分布を見やすくするために 各分布の中心にx印をつけていますが シミュレーション上で特に意味はないです      観測画像化散布図からグリッド毎に集計して観測画像を作ります   image png     python 観測画像の作成コード  グリッド上のデータ点のカウントを計算コードの説明    image   hist T  は   hist  に配列が x y の順で入っているので   ax imshow    で読み込めるように y x に変換します ※もちろん numpyではなくmaptplotlibのモジュールで次元ヒストグラムを表示することもできます 本記事では この先の超解像化で画像として扱うためnumpyで計算しますが 下記に散布図の座標を保持したヒストグラムの参考コードを載せておきます  x y が散布図の座標とリンクしているのがわかります このコードは ただ散布図をヒストグラム化しただけですが 超解像化した場合の座標の保持を行う場合は結構な工夫が必要となります      ランダム化による超解像化観測画像をピクセル毎にピクセル値に従いカウントずつランダムにつの領域に振り分けて超解像化します 得られた画像のサイズが×倍増えていることが確認できます   image png     python ランダム化による超解像化コード       ランダム化による超解像化    Args         image  ndarray   超解像度化を行う入力画像 整数値のNumPy配列を想定        subpx n  int   各ピクセルをsubpx n×subpx n個のサブピクセルに分割するかを指定する整数    Returns         ndarray  入力画像を超解像画像   ランダム化によるサブピクセル化した画像を作成 ピクセル毎にsubpx n×subpx nに分割 コードの説明    ax imshow    の  vmax image max    subpx n     は 超解像前の解像度におけるカラー範囲に規格化するために  subpx n    で割っています      ガウシアンフィルタによる平滑化ランダム化は意味のある確率分布を前提とした処理ではないため 超解像化された画像自体は誤差の影響を受けやすく したがって 元の観測画像のスケールに近い平滑化を行う必要があります ここでは 元の観測画像のスケール程度としてガウシアンσをσ  としています このσのパラメータ値は元の観測画像のピクセルにσの情報が含まれることを示しています   image png     python ガウシアン平滑化のコード  ガウシアン平滑化コードの説明    gaussian sigma   subpx n     は グリッドが×のときの観測画像と同程度の空間スケールにするための平滑化のため使っています このパラメータにより ×のピクセルサイズにσ入る計算となります   SciPyの  gaussian filter    関数内での  output np float  は結構重要な部分です    subpx image  は整数型のデータのため   output  パラメータを指定しないと 出力も整数型になり 統計的な情報が少ない画像に対しては丸め込まれてしまい平滑化が正しく行えないためです   精度の検証ここでは 超解像画像の精度検証のため 散布図からグリッドサイズを調整して直接作成した本物の観測画像と比較します 下図の説明です   左図：散布図のグリッドの座標を × でサンプリングした本物の観測画像   中央図：散布図を×のグリッドでサンプリングした画像をランダム化によって超解像化した画像   右図： 左の画像 中央の画像   左の画像を取ることで 左図を基準としたときの変動率を表したもの   上段：平滑化する前の画像   下段：観測画像の×のスケール σ   でガウシアン平滑化後の画像   image png  上図上段より 平滑化前ではカウントが付近の明るい領域で ± 程度の変動率が見られ統計誤差の範囲内   \sqrt  \sim     \\   で ある程度説明できることがわかります また 下段の×のグリッドのスケールに平滑化後の結果では ×におけるカウントレートがであることから 統計誤差   \sqrt  \sim     \\   は 程度であり その範囲内で一致していることがわかります    python 精度検証用のコードfig  axs   plt subplots     figsize        リファレンス画像 散布図からスケールに合わせたグリッドで作成した元画像   ランダム化によるサブピクセル化した画像を作成  リファレンス画像との差分  ガウシアン平滑化パラメータ  リファレンス画像のガウシアン平滑化  ランダム超解像化した画像のガウシアン平滑化  ガウシアン平滑化後のリファレンス画像との差分   その他の超解像サイズのパラメータによる結果ここでは 超解像化のコードの  sub px  パラメータを変えた際の結果について見ていきます それぞれの下段では ガウシアンフィルタによる平滑化のσを  gaussian sigma   sub px     としていて   sub px   の空間スケールに平滑化しています 下図の全ての結果において 平滑化後の下段の結果は統計誤差以内の変化率であることがわかります     sub px   の結果 もちろん ランダム化した場合も変わらない 下段は  gaussian sigma   sub px     より σ  のガウシアンフィルタで平滑化しているため ピクセルにσ入る計算であり若干平滑化されますが 上段の構造はほとんど保持されます 他の  sub px  での平滑化の結果は この下段の結果に近くなるはずです   補足 ランダム化が問題となるケース 統計が多い画像において ランダム化を用いた場合 ピクセルの境界が残ってしまうことがあります 以下は 本記事のシミュレーションで使用された画像の倍統計量が多い例で   sub px   と設定した結果です   image png  上段左の本物の観測画像と比較して 上段中央に表示されている×ピクセルの観測画像を×にランダム化した超解像画像では ×ピクセルのグリッドの線が残ってしまうことが確認できます この現象は ランダム化に基づく一様な分布と 実際のデータの統計的な特性の違いから生じるものです 統計量が少ない場合 この境界部は統計誤差が主要因となり それほど目立ちませんが 本ケースでは×ピクセル程度のスケールでのガウシアン平滑化の重要性がより強く実感できると思います    python ランダム化の問題となるケースの実装コード  補足 ランダム化が問題となるケース   次元ガウシアン分布から乱数の散布図  本編のコードから倍統計の良い画像  単位目盛りあたりのグリッドを作成  グリッド上のデータ点のカウントを計算  リファレンス画像 散布図からスケールに合わせたグリッドで作成した元画像   ランダム化によるサブピクセル化した画像を作成  リファレンス画像との差分  ガウシアン平滑化パラメータ  リファレンス画像のガウシアン平滑化  ランダム超解像化した画像のガウシアン平滑化  ガウシアン平滑化後のリファレンス画像との差分  まとめ本記事では 超解像化の古典的手法として比較的単純ではありますが 広く使われるランダム化について実装しました 私はつのガウス分布から生成される画像を用いてシミュレーションを行い その結果 統計誤差の範囲内で超解像化ができていることを確認しました ただし ランダム化はピクセル毎に一様な確率分布に仮定しているため その仮定から乖離が起こる場合に注意が必要です その例として 空間スケール内でピクセル以下の細かな構造がある場合や 補足で紹介した統計が非常によく統計誤差に比べて一様分布の仮定における乖離が顕著に出る場合が挙げられます 超解像化の方法は特定の状況に依存しますが ランダム化はその手法の一つとして画像処理や超解像化に興味を持つ読者の皆様にとって 有益な情報となりましたら幸いです   参考文献,1,2023-09-20
99,99,Python初心者がCNN画像認識で洗濯タグ識別をやってみる,画像処理,https://qiita.com/TO-FU/items/1f4b9634f80ab264ec3c,  目次 はじめに 実行環境 ソースコードの解説　  画像収集と処理　  学習用データと検証用データの作成　  モデルの定義と学習　  モデルのトレーニングと結果の可視化 及びモデルの保存 製作したアプリ 苦労した点 おわりに   はじめにはじめまして IT未経験で転職を目指す代半ばの男性です 今回 教育訓練給付制度を使用してAidemyさんのAIアプリ開発講座 ヶ月 を受講しました このブログは その最終成果物としての課題です     アプリ開発の経緯日常生活でおしゃれな服を洗濯する際に洗濯タグに書かれた記号や指示が理解しづらいことが多くどの洗濯方法が最適なのかが分からない瞬間が何度もあり 画像から洗濯タグの内容を判別できたら便利だな と思ったのが経緯となります しかし 洗濯タグの種類は現在で種類にもなります   column  kv x jpg  今の私の技術では全てのタグを種類を識別させることは難しいのでまずは  洗濯不可 と 液温度を限度として洗濯可能 の種類に絞ってアプリ開発をしました  　　　　　　　　　　　　  wash    jpg  　　　　　　　　　　　  colum jpg  ※液温度を限度として洗濯可能を以下 洗濯可能と省略します    実行環境 Visual Studio Code Google Colaboratory Python      ソースコードの解説     画像収集と処理まずは 洗濯可能の画像と洗濯不可の画像をWEBサイト上から手動で拾ってきます スクレイピングを使用して集めようかとも思いましたが 関係ない画像が多かったので手動で各枚ずつ集めトリミングを行い googleドライブに保存します  トリミング前    jpg   トリミング後 この時実際の使用を想定して周りにあえて余計な線などを残します    jpg      画像の水増しOpenCVを用いて画像を度と 度のものを加え 各画像を約枚まで水増しを行う   入力フォルダと出力フォルダのパスを指定  回転角度 度 を指定angles             度と度の回転  出力フォルダを作成  入力フォルダ内の画像ファイルを処理          画像読み込み          画像の中心座標を計算        height  width   img shape           center    width      height               回転行列を作成          画像を指定した角度で回転           新しいファイル名を生成          回転後の画像を保存print  画像の回転が完了しました   OpenCVライブラリを使って画像をリサイズし googleドライブフォルダに保存します   wash画像を処理  保存先フォルダを作成      保存先のファイルパスを生成      リサイズされた画像を保存  保存先フォルダを作成      保存先のファイルパスを生成      リサイズされた画像を保存     学習用データと検証用データの作成必要なライブラリをインポートしたら Googleドライブから洗濯可能と洗濯不可の画像を読み込み画像サイズを×に変換しリストに格納します この時 水増し前のデータも別でリストに格納します import cv 画像や動画を処理するオープンライブラリ 洗濯タグ画像の格納 os listdir   で指定したファイルを取得  洗濯タグ画像を格納するリスト作成種類の画像を集約してXに学習画像 yに正解ラベルを代入します 種類の時は   種類の時は  とラベルを増やしていきます    python qiita rb np arrayでXに学習画像 ｙに正解ラベルを代入 正解ラベルの作成 水増し前のデータの代入 正解ラベル 水増し前 集約した学習用データをランダムに並び替えます  データセットの画像データは学習用に全ての画像を 検証用として水増し前の画像を使ってデータリストを作成しました 初め実行した際に学習用   python qiita rb 配列のラベルをシャッフルする 学習データと検証データを用意正解ラベルをone hotベクトルに変換する one hotエンコーディングについてわかりやすく説明しているサイト  がありました エンジニア初心者の方がいれば参考にしてみて下さい    python qiita rb 正解ラベルをone hotベクトルで求める     モデルの定義と学習Kerasモジュールを使用して学習済みモデルVGGを読み込んで転移学習を行います VGGモデルから出力を受け取り Sequentialモデルを用いて追加の層を定義します    python qiita rb モデルの入力画像として用いるためのテンソールのオプション 転移学習のモデルとしてVGGを使用 モデルの定義 活性化関数シグモイド 転移学習の自作モデルとして下記のコードを作成 vggによる特徴抽出部分の重みを層までに固定 以降に新しい層 top model が追加 自作モデルとvggモデルを連結させ新たなモデルに代入します モデルの学習プロセスを設定し 指定された損失関数を最小化するように最適化アルゴリズムを設定します   損失関数と最適化関数の設定     モデルのトレーニングと結果の可視化及びモデルの保存evaluateメソッドで 損失と精度のスコアを算出しその後コンパイルを行います    python qiita rb グラフ 可視化 用コード モデルを保存以下が出力された検証データの損失と精度のグラフです  水増し後 png  epoch数はで訓練し 評価ははじめは 検証用データに水増しした画像含む全データを入れていて その所為で精度が異常に高くなっていると思い 検証用データに水増し前の画像のみを入れてみたものの精度はあまり変わりませんでした 実際にアプリ上で試してみた結果 トリミングしてある画像に対しては高い正解率が出ましたがやはりタグ全体の写った画像の正解率は低かったです 通常 val accuuracyが になることは無いので 画像データが少なすぎる事で過学習をおこしているものと思います もう少し画像データを集める事ができれば 過学習を起こさずに正しい精度に落ち着くと推測できます    製作したアプリ完成したアプリ   おわりに成果物作成にかけれる時間が少なく当初予定していた機能よりも簡易なものになってしまいましたが受講終了後には 物体検出を使ってより精度の高い複数の認識できるアプリの制作を行いたいと思います これから色々実装したい機能などもあるので これから実用的になるところまでやってみようと思います 最後に つまずく点は多々ありましたがそれを解決できる力も付いたと思います この経験を生かして次の就職先を見つけれるよう頑張っていきます ,1,2023-09-19
100,100,医用CVの前処理「データ拡張」に新しい手法をおすすめします:MedAugment,画像処理,https://qiita.com/xxyc/items/e8084618454376026e66,画像系でよくある医用画像に対して 非常に良いデータ拡張手法MedAugmentを共有したいと思います 通常のデータ拡張手法 例えばAugmixやAutoAugmentは 一般的な画像処理には適していますが 医用画像の分類やセグメンテーションにおいては性能が高くないことが多く 単純な回転や拡大の方が効果的な場合もよくあります MedAugmentは 医用画像に特化した手法です 中身は少し複雑ですが 簡単に説明します 開発者は APとASのつのデータ拡張操作グループを定義し つ合わせて個の拡張手法を含む形で設計しました    PNG  これに基づき 特定の組み合わせを持つ サンプリング戦略 が導入されています さらに 独自の マッピング戦略 もあります  APはピクセル PIXEL に関連する手法で 例えば 輝度 コントラスト ポスタライズなどです  ASは空間 Spatial に関連する手法で 例えば 回転 拡大 平行移動などです ①サンプリング戦略 医用データは輝度に非常に敏感で 連続的にAP操作を行うと 出力画像が実物とかけ離れ モデルの学習に悪影響が生じます そのために 開発者は モデルをまずM回のAS操作を行い その中にランダムなAP操作を回含むようにする AP操作は回以下で Mは回 回 を条件として設定しました    PNG  ②マッピング戦略 MedAugmentに単一のハイパーパラメータを使用するため 開発者はiという単位を追加しました このiは 各操作の強さをコントロールするために使用されます 例えば ポスタライズは学習の精度に大きな影響を与えるため iはポスタライズのコントロールをより厳しくにし 結果としては学習がより良くなるといいます  こちらについての具体的な実現方法は私もよく理解していません      PNG  MedAugmentはマッピング用のハイパーパラメータを一つだけにまとめて それを i として表現しています この i は操作の強さを調整するためのものです 例として ポスタライズは学習の精度に影響を及ぼすため  i はポスタライズのコントロールをより厳しくにし 結果としては学習がより良くなるといいます  こちらについての具体的な実現方法は私もよく理解していません   数年前Kaggleで行われた人間のタンパク質を分類するコンペのデータを使って MedAugmentの効果をテストしました 既存のコードはそのままで データ拡張部分だけをMedAugmentにして どれだけ精度が上がったのかを確認しました 結果としては 精度自体 fscore は約  弱上がりました    から   全体的な学習過程もより安定にしています ただ Valid lossを見ると 最後の振動が若干大きくなって そちらをもっと収束すれば さらに高い精度が得られるではないかと思います また 元のデータは約 件でなので このデータ量でこのくらい性能出すのは結構良い手法だなと思いました データ件数が少ない場合なら 更なる性能向上が期待できると考えています    PNG     PNG  コードと論文は以下となります 興味あるは試してみてください コード：論文：,4,2023-09-16
101,101,手軽な映像ベースの手、上半身、全身のトラッカーにおすすめの「グローバルシャッター」方式のWebcamについて,画像処理,https://qiita.com/usagi/items/0ec4276ead3b7b62fa7c,     この記事の対象読者  上記のようなアプリを試してみたが手を動かすと認識しなくなったりおかしくなったりして実用の難しさを感じている方   画像認識用のカメラとして比較的安価ないわゆるWebcamを使いたい方   一般的なカメラに広く普及したCMOSイメージセンサーとシャッター方式のローリングシャッターとグローバルシャッターについて画像認識技術への実用の観点から知りたい方     はじめに 結論としてはどのようなカメラを買うのがよいか   重要度     GOOD   NG       シャッター方式   グローバルシャッター方式   ローリングシャッター方式       FPS   高       低          画角   設置場所と撮影対象の距離に合わせる   考えなしに魚眼 超広角 広角 望遠などを決める       歪み   小   大       解像度   大   x    小   x        IR撮影   有   無    画像認識の用途では対象が完全に静止した状態をゆっくり撮影できる特殊な状況を除き グローバルシャッター のカメラを調達するとよい   FPSは高いほどよいが FPSの高さと解像度がトレードオフとなる製品もあるのでFPS以上に対応したものと考えておけばよい   画角は可能な限り事前に設置場場所 撮影対象との距離 撮像面の長さ 撮影対象の大きさ から最適な角度を計算し 近い画角の製品を選ぶ     例  画角  °  のカメラで   m  先の撮影対象を撮影する場合 撮影対象は  tan °    m      m  です     例  撮影対象の大きさが高々半径   m  に収まりカメラの設置場所が撮影対象から   m  の距離の場合 カメラに必要な画角は   atan  m  m       °  です 具体的な比較的安価でGOODな製品の例    ELP USBGSP H     円 など※執筆時点のAmazonで購入可能なものより適当に選択 ※原則的には グローバルシャッター と明記された製品から高FPSかつ撮影予定の環境に適当な画角のレンズの製品を選ぶとよい      画像認識によるトラッキングとWebcamのシャッター方式  CMOSイメージセンサーを搭載したカメラのシャッター方式には グローバルシャッター と ローリングシャッター がある   一般に広く安価に普及しているほとんどのいわゆる Webcam や USB カメラと呼ばれるカメラ製品のシャッター方式は ローリングシャッター である    ローリングシャッター の製品は グローバルシャッター の製品に比べ一般的に圧倒的に低価格かつ非情に多くの製品の選択肢を持つ    グローバルシャッター の製品は高速に移動中の撮影対象でも原理的には歪まない    ローリングシャッター の製品は高速に移動中の撮影対象は撮影結果が変形したり モーションブラー効果を施したようなボケ ブレが生じる 画像認識への応用では致命的なエラーや精度低下の原因となる 参考   グローバルシャッターとローリングシャッターどれだけ違うのか検証してみた― cocopa  ※この記事に添付された画像はローリングシャッターによる歪みがわかりやすい      FPS 解像度 画角  最終的にFPSでモーションデータを生成したい用途であれば 少なくともFPS以上で撮影できる必要があるが 配信負荷などの都合から最終的にFPSのライブを行えればよい場合には少なくともFPS以上で撮影できれば十分となる 最終的にどの程度のFPSのモーションデータが必要かに応じて選択する   解像度は特に大きさに拘る重要性は薄いが もし魚眼や超広角といった広角よりのレンズを撮影に用いる必要がある場合 つまり狭い部屋での撮影に最適化したい場合などはHDないしFHDあるいはそれ以上の解像度の要求について検討する必要が生じる 広角となればなるほど画像は歪みやすく ピクセルあたりに記録される撮影対象の大きさもより大きくなり 相対的に画像認識に必要十分の解像度を維持できなくなる可能性がある 逆に 十分に長い撮影距離を確保できる場合は相対的により低い解像度 例えばxなどでも十二分の画像認識を行いやすくなる 高い解像度と高いFPSの両立 広い画角と歪みの小ささの両立は非情に困難のため 場合によっては撮影条件 主に撮影距離 とレンズの画角を再検討した方がよい場合もある   画角は三角関数により比較的簡単に計算できる もし計算に自信が無い場合は  カメラの画角の計算―Casio   などに頼るとよい カメラの 画角 は厳密には水平画角 垂直画角 対角画角の種類が混在しており 例えば横長画面を基準としたゲームでは垂直画角 視野角 のことを単に画角またはFOVY Field of View Y axis と言う場合が多いが いわゆる一眼レフカメラなどの撮影ガチ勢的な方向でのカメラとレンズの世界では対角画角を単に画角と言う場合が多いなど 状況によっては計算に誤りを生じる原因となり得る 自分が計算したい画角が何れの画角であり 参照するカメラの製品情報や撮影予定の環境情報の画角が何れの画角であるか十分慎重に読み解く必要がある場合もあるため注意する      その他の性能諸元  カメラによっては暗所での撮影がどの程度まで可能かをLUX値で表示している場合もある 暗所や夜間の屋外での撮影に応用する予定があればLUX値についても月明かりがどの程度の値かなど概要を把握して 購入検討の採用とするとよい     参考   最低被写体照度とは 防犯カメラを夜間に使用するには―Canon    ほとんど完全な暗がりでの撮影ではIR 赤外線 機能についても検討する価値が高い 無理に低LUXへの撮影感度を要求するよりも アクティブにIR照射を行う機能のついた製品や カメラ本体とは別にIR照射器の併用を検討するとよい 一般的にCMOSイメージセンサーの特性上 たいていのWebcamは可視光に厳密に限定はせず 特に可視光に近いNIR領域の光にも感度を示す場合が多い また IR専用にチューニングされ 可視光はフィルターする製品もある 逆に可視光にチューニングするためIR領域はたとえイメージセンサーとしては感度があっても何らかのフィルターを装着する事で遮断する製品もある また IR照射が強すぎて被写体が白飛びしたり レンズフードが必要となる場合もある IR撮影を検討する場合はイメージセンサー IR照射器 可視光とIRのフィルターについて注意深く検討する必要がある だいじなこと     Webcamでモーションキャプチャーしたいときはグローバルシャッターのカメラを調達しましょう    ※この記事で いわゆるWebcam として表現した製品はより正確には UVC USB Video Class に準拠したUSBでPCなどに挿すだけで自動的にドライバーも認識されて動作してくれるお手軽で比較的小型のカメラ製品 の意図です ,0,2023-09-14
102,102,[timm] MNISTデータセットを多ラベル問題として解く,画像処理,https://qiita.com/Isaka-code/items/8582c901aaaa4970f520,  TL DRMNISTデータセットの一部を用いて多ラベル分類モデルの訓練を行います モデルの構築と訓練にはPyTorchを使用しています MNISTデータセットは元々は ラベル多クラス分類問題ですが 多ラベルクラス分類問題に変換しているところがポイントです 主なステップは以下の通りです：    データの読み込み    MNISTデータセットの一部をロード    多ラベルの生成    素数     で割り切れるかどうかに基づいて複数のラベルを生成    データの可視化    サンプル画像とそれに関連する多ラベルを可視化    データの前処理    画像とラベルをPyTorchのテンソルに変換し DataLoaderを作成    モデルの構築    timmライブラリを使用して ResNetに基づくモデルを構築    評価    テストセットでモデルの性能を評価この記事執筆のモチベーションは下記です  PyTorchを使用した多ラベル画像分類の基礎的な理解を手助けしたい  timmのベーシックな実装例を紹介したいこの記事とソースコードは KaggleのCodeコミュニティを通して 下記のリンクで共有しています  ソースコードのざっくり解説   データの読み込みこの関数は TensorFlowのKeras APIからMNISTデータセットを読み込み 訓練データとテストデータの一部を返します  using ratio パラメータで 使用するデータの割合を指定できます    多ラベルの生成この関数は 各画像 数字 が      で割り切れるかどうかに基づいて多ラベルを生成します 例えば ラベルがの場合 とで割り切れるので その多ラベルは        になります    データの可視化この関数は 訓練データの一部とそれに対応する多ラベルを下記のように可視化するためのものです   image png     データの前処理この関数は 画像データと多ラベルデータをPyTorchのTensorに変換し DataLoaderを生成します    モデルの構築   pythonclass MyImageModel nn Module   timm ライブラリを使用して カスタムの画像分類モデルを定義しています ここでは 事前訓練されていないResNetをベースとしています    訓練この関数は Adamオプティマイザと BCEWithLogitsLoss 損失関数を用いて モデルを訓練します    評価この関数は 訓練されたモデルの性能をテストデータセットで評価します   参考文献   PyTorch公式ドキュメント     timmライブラリ  ,2,2023-09-12
104,104,ステレオカメラ調査(2023年版）,画像処理,https://qiita.com/nonbiri15/items/a8590a99941f8de4c8b8,なるべく自前のカメラを作らないこと今後も供給されるであろうステレオビジョン ToFカメラなどを用いること 自前のステレオライブラリを必要としないこと 自前のIMUやタイムスタンプを必要としないこと      トンネルの出口での白飛びによって トンネルの外の車両などを見失わないこと       夜間の暗い環境で 黒い衣類を着た人物を見失わないこと       LEDの信号の表示が サンプリング周期の影響で 点灯していないように見えないこと   複数のカメラで      複数のカメラで同期がとれた撮影ができること  Image センサ      グローバルシャッタ方式であること  システムでのステレオ画像を取得できるまでの遅延は何 msec になるのか   そのモジュールの外部に計算資源を必要とするのかどうか   必要な計測範囲の奥行きは何m   何 m なのか   必要な視野角は そういうことに留意して あなたの用途のステレオカメラを選んでほしい XVISIO　の利用kinectは消えてしまった realsense機種によっては 消えてしまった 利用例 CRANE X 画像データを取得できる部分 ステレオカメラって何 原理や用途 車載用の最新動向もチェック   車載カメラの視覚  リコー      リコー 車載ステレオカメラ        リコー 車載カメラ     日立オートモティブシステムズ 現　日立Astemo株式会社      ZMP ADAS 自動運転用ステレオカメラRoboVision    ZF ドイツ   ボッシュ ドイツ    ライブラリが供給されているステレオカメラを利用するのがよいと考える 理由：  自分たちが勝負するところは ステレオ計測ではないはずだ   タイムスタンプ問題 IMUを持っているか  USB  カメラ Tara USBステレオビジョンカメラ  OpenCV Layerを持つ   image png  ブロック図の中にIMUが含まれている     ITD Lab Intelligent Stereo Camera   基線長 baseline lenghth  Resolution FOV での距離精度    image png   引用元  pdf  製品カタログ　Intelligent Stereo Camera 評価ユニット ISC VM  ISC XC  残念なことに タイムスタンプの問題と IMUについての情報が不足している 近距離側が  mからと遠方側に性能がでることを優先する設計になっている このITD Labの代表取締役会長　は実吉敬二さんで  株 SUBARU でステレオカメラの開発を行った方である そのため 車載カメラで距離計測をする分野での距離範囲を意図したものになっている FPGAでの使用を前提としており いわばステレオカメラ開発キットのような状況であり USBカメラをつないですぐ使えるタイプのカメラとは違っている  SeerSense DS Module       stereolab zed x  グローバルシャッターで p fpsIMUがちゃんとある Neural Depth Engine stereolabで開発したDepth計算のライブラリを使っている OpenCVには深層学習以前のステレオ計算のライブラリが含まれている OpenCVに含まれる従来の対応点の視差を元にdepthを算出しているものでは 欠損点ができて その点に対してdepthの推定ができないものになっている 深層学習によって 単眼カメラでも視差を推定できる学習が存在する もちろん ステレオ画像を用いたものの方が 十分に根拠のある奥行きを算出できる  Neural Depth Engine   とはきっとそのようなライブラリなのではないかと推測する  マルチカメラ同期接続されている複数のデバイスのハードウェア同期をフレームレベルで  マイクロ秒以内に実行します  同じシーンの複数の RGB イメージと深度イメージをすべて位置を合わせてキャプチャします この会社のステレオカメラのインタフェースとしては USBのものの他にGMSLのインタフェースによりものとがある 各人の用途によって使い分けること      Stereolab ZED シリーズに関連したソフトウェア 引用元   StereoLabs は深層学習を用いたdepth計算をしている そのため 従来の対応点マッチングで視差を算出できた点についてだけdepthが求まるライブラリと結果が違ってくる 対応点がとれない画素についても depthが返ってくる    Teledyne FLIR ステレオビジョン   深度認識のためのカスタム埋め込みステレオシステムの構築方法     XVISIOToF カメラであると同時にFisheye Stereo カメラ grayscale を持ち stereo depthを計算するengineを持つ 更にRGBカメラ  RGBカメラがToFの視野角に近いので ToFの画像とRGB画像を対応付けやすい      推測：  通常の視差計算ライブラリはgrayscale で行う   だからRGB画像をグレースケールに変換する処理がないほうが遅延が減る   魚眼カメラになると周辺部ほど変化が大きくなる   そのため beyer配列での各画素でのRGBの推定では 影響を受けやすい   そのような周辺部のRGB画素値をグレースケール化するよりは 最初からグレースケールで撮影したほうがのぞましい 同社では 図に示したタイプのステレオカメラを提供している カメラの取り付け位置を自由に変更できるものが多い   Screenshot from       png   引用元   ロボット開発に最適なAI機能搭載ステレオカメラ M S  M WN とSoC NU 取扱い開始のお知らせ    image png   引用元     比較表に含めるべき項目FoV 水平 	FoV 垂直 	FoV 対角 	画素数 水平 	画素数 垂直 	サンプリングレート	グローバルシャッター	IMUの有無	基線長 baseline 	深度範囲	インタフェース	計算リソースの有無    基線長 baseline  を選べるステレオカメラがほしいときには  カメラの位置を固定する前のステレオカメラモジュールを販売している業者があります   それを適切に機械的な要因や温度変化でたわんだりしないようなベースに固定してします   それをキャリブレーションを実施して利用する      そのようなステレオカメラの部品の例以下のページでは奥行きの推定のためにどういう手法があるのか それぞれの手法の特徴を書いている    ロボットの視覚  image png  アシモの場合 頭部の視覚センサの他に腹部に床面センサを持っている   Screenshot from       png   SDKがオープンソース化したステレオカメラが市販されている   ZEDはステレオカメラ以上のものになろうとしている   市販 Stereo Cameraについて調査中   全方位ステレオカメラを調査中   カメラ関連の自前記事の整理   ステレオカメラの領域にも深層学習が及んできている   市販ＴoＦカメラについて調査中   人共存型双腕スカラロボット duAro     協働ロボットデモ 双腕ロボットによる組み立て   入れ損ねても仲間がカバー 台の人型協働ロボットが食材盛り付け連携     ROSで動作が確認できているステレオカメラ       英語のWiki  でも状況の説明は書かれている それによると RealSense部門を縮小している いくつかの製品のEOLが発布されているが ステレオ製品ラインの大部分はまだ入手可能であり その間に新製品がリリースされましたとのことである    ティアフォー社製カメラ  ティアフォー社のカメラは 車載向けに特化したカメラになっている       ハイダイナミックレンジ HDR       高感度 低ノイズ      LEDフリッカー抑制   Orbbec  のステレオカメラもROS対応している   OAK D もROSに対応している  Deploy with Raspberry Pi   Luxonis OAK D LITE Depth AIカメラ 固定焦点版    ROSでOAK D lite depth AI Cameraを動作させる     マシンビジョンの場合のアプローチ  パターン光をプロジェクタで照射して カメラで撮影する AI＋産業用ロボットの領域において 産業用Dカメラ＋画像処理ソフトウェア＋ロボット経路計画ソフトウェアを独自開発 製造 販売するDカメラメーカー ことで D計測する手法がある   構造光法 構造化照明型Dカメラとか 構造光型Dスキャンとか 光切断法とか いろんな呼び方がある   台のカメラだけでは 輪郭近くで対応点がとれない部分が生じる   台のカメラを正方形状に配置して それらからD計測を行っている例もある 　この場合 輪郭近くでの対応点の欠損が減る     マシンビジョンのソリューションを提供している会社の例 AI＋産業用ロボットの領域において 産業用Dカメラ＋画像処理ソフトウェア＋ロボット経路計画ソフトウェアを独自開発 製造 販売するDカメラメーカー 商用ライブラリでステレオ計測について提供しているソフトウェア会社   例：　MATLAB ステレオ ビデオからの深度推定   例題の内容：ステレオ平行化のための算出済みのパラメータを読む 左カメラの動画 右カメラの動画を読む 左カメラのフレーム 右カメラのフレームをステレオ平行化する disparityMap   disparitySGM frameLeftGray  frameRightGray  を計算する 次元の点群に換算する 人を画像を検出して 人までの次元的な距離を求める  Lidar Toolbox 入門  Velodyne だけサポートしている こういったライブラリを使うことで 最初のとっかかりをつかむことができます    LINX 社 HALCON ライブラリマルチステレオビジョン 光切断によるD計測ライブラリを含む ただし 光切断による計測には 光源 カメラなどがいるので 単にライブラリを買ってくるだけではいけない 光源 カメラの選択 データ取得の問題が別にある    ライブラリ  そのステレオカメラは どれだけライブラリをサポートしていますか   また そのカメラでの利用実績はどれくらいありますか    そのステレオカメラはどれくらい継続して供給されてきましたか  継続的に生産が続いてくれそうですか    組み込み屋が ステレオカメラ ToFカメラを検討するときに考慮すること  利用する対象のCPU GPUボードで その機種が対応されているかどうか  CPUの種類  OSの種類とバージョン  GPU利用の有無  カメラとのボード間のインタフェース 例：USB  type C   デバイスドライバの状況  SDKの状況  CPUボード側に必要なライブラリ	例：CUDAのライブラリ	例：OpenCVのライブラリ	例：OpenVinoのライブラリこういったものを考えること    Microsoft がKinectの開発を停止しましたが 継承先があります  マイクロソフトが Azure Kinect DK 生産終了 中国メーカーが技術継承へ  英語での役立ちそうな情報   そのStereoCameraはどの程度サポートされているか 使われているか  対象のボードの種類 対象のインタフェース 対象OS  物理的に接続できていることと デバイスドライバーが提供されていることとは別物である   ROS  ROSを使う場合 利用例の情報が入手できるか   利用できる言語とそのバージョンはどうか    シミュレーション環境でもそのカメラを使いたいかどうか   シミュレーション環境でもそのデバイスが使いたい場合には そのシミュレーション環境でそのカメラのシミュレータが用意されているかを調査する   用意されている場合には 環境の点群データから カメラの入力画像を計算したり そのカメラが算出するはずのdepthを取得できるらしい 以上のように利用する目的によっては 利用できるステレオカメラが絞られる ,2,2023-09-03
105,105,[備忘録] Azure Video IndexerのインデックスをPythonで取得,画像処理,https://qiita.com/yusuke-1105/items/147d4d40b87f13f801ae,  概要Pythonで Azure Video Indexer  に  動画ファイルをアップロードする方法  と   動画の インデックス を取得する方法  を紹介しています 私の備忘録用なため 少し読みづらいかと思いますが 最後までご覧いただけますと嬉しいです   GitHubソースコードを公開しています このレポジトリの内容に沿って解説しています   解説   note Azure Video Indexer 公式ドキュメント     note warnAzure Video Indexerは従量課金制です 詳しくは こちら  \  Azure PortalでAzure Video Indexerをセットアップします \  以下の画像の赤枠部分に表示されているAccount IDとAccess Tokenを取得します \  この リンク  にアクセスし サインイン後  Try it ︎ ボタンをクリックします クリックにより出現したパネル下部の Ocp Apim Subscription Key を取得します \  動画ファイルを onedrive にアップロードし ウェブブラウザで onedrive にアクセスします 画面上部の     をクリックし  埋め込み を選択します   embedding png     noteAzure Video Indexerはmp  mov  …の動画形式に対応してます 詳しくは こちら  \  画面右に埋め込み用コードが出現するので 赤枠で囲まれている部分の埋め込み用リンクを取得します このリンクの embed aspx という文字列を download aspx に置き換えます \   upload video py を実行します Azure への動画アップロードとインデックス作成完了後  get index py を実行して動画のインデックスを取得します 動画のインデックス完了の有無は  Azure AI Video Indexer Portal  で確認できます ,1,2023-09-03
106,106,画像やPDFにある英語のテキストを翻訳するアプリを作ってみた,画像処理,https://qiita.com/wooooo/items/47a8b3dd1501c0adf704,  はじめにエンジニアは分からないことがあればググることが一般的ですが海外のサイトを読む際 テキスト翻訳ツールは便利ですが 画像やPDFに埋め込まれたテキストをどうしますか この記事では 画像やPDF内の英語テキストを簡単に翻訳するアプリを作成する方法を紹介します 今回作成したコードです  環境情報Python   virtualenv   PyPDF   pytesseract   Pillow   mtranslate  画像認識OCRはOpenCVではなくPillowというライブラリを使用しています シンプルにコードを書けます   ディレクトリ構成root 　├ image 　│  └  対象の画像 PDFファイル  　└ app py  コードPILを選んだ理由は シンプルで直感的なAPIを提供し 前処理が簡単な点です   画像からテキストを抽出          画像からテキストを抽出        print f 画像からテキストを抽出できませんでした   e     PDFファイルからテキストを抽出        print f PDFファイルからテキストを抽出できませんでした   e     テキストを翻訳        print f テキストの翻訳に失敗しました   e     ファイルを取得           jpegや jpgや pngファイルの場合の処理                print  抽出されたテキスト                   print  \n翻訳されたテキスト              pdfファイルの場合の処理                print  抽出されたテキスト                   print  \n翻訳されたテキスト             上記以外のファイルの場合        print f ファイルの形式が間違っています   file     実行結果お寿司の画像を読み込んでみます   より引用 抽出されたテキスト QA QSSalmon Shrimp Tuna Squid翻訳されたテキスト QA QSサーモン エビ マグロ イカ画像の中のテキストの Salmon Shrimp Tuna Squid が翻訳されています翻訳結果は問題ないです  おわりに画像認識するアプリは実務で作成したことないので玄人の方から何を言われるか不明ですが 初心者なりに作成してみました 個人開発で画像認識にはOpenCVをしようしていましたが 今回は気分転換でPillowというライブラリを使用しました 画像認識にはOpenCVが強力ですが PillowはOpenCVよりコードをシンプルに書くことができ 前処理が簡単に作成できたりと良い点がありました 皆さんも遊びでコードを書くときは普段使用しているライブラリを使用せずに他のライブラリを使用してみると何か発見があるかと思います この記事が役に立ったと思ったら いいねやストックをお願いします ,2,2023-09-01
107,107,多視点画像で三次元形状を認識する,画像処理,https://qiita.com/mhrt-tech-biz-blog/items/871ddd9ea27c1c35e226,本記事では三次元形状認識を題材とした 外観検査アルゴリズムコンテスト X線CTによる工業製品の内部検査 ボクセルデータからの形状と材質の分類     への提出作品   の解説を行います  注  PythonでのVTK Visualization Toolkit による簡単なボリュームレンダリングの実装例も紹介します    本記事の要約  三次元形状の認識を高速に行う手法としてMVCNN Multi view CNN    を試しました 形状が既知の三次元形状の識別は 多視点画像を用いた手法で比較的高速に精度よく処理可能なことが分かりました   未知形状への対応のため クラス識別タスクではなく 距離学習による照合タスクとしました   画像処理による事前の部品分離精度 および学習データの品質が正解率に寄与していると考えられました   データセットから個程度の部品を抽出し 種類の部品個数の計数にかかった時間はCPU実行で分半ほどでした   VTKを利用して ボリュームレンダリングによる可視化結果から多視点画像を作成しました ボリュームレンダリングの方法もご紹介します    外観検査アルゴリズムコンテスト課題  課題    外観検査アルゴリズムコンテスト課題  年の課題は X線CTによる工業製品の内部検査 ボクセルデータからの形状と材質の分類  でした  ビーカーのような容器の中に 図の左上のような部品が多数入っており それをX線CT撮影した断層画像群がセット 提供されました さらに 容器の中に入っている部品のうち 種類については 部品単体でX線CT撮影した断層画像群も提供されました  課題は 種類の部品が 容器の中にそれぞれ幾つ入っているかを計量するものでした 容器の中には 種類の部品以外の部品も入っており 種類の中には剛体ではない 形状が変わる ものも含まれていました    事前検討    データの観察まず 事前検討として 提供されたデータを眺めて 以下の考察を得ました   部品入り容器をX線CT撮影した各提供データセットにおいて 画像の輝度値は各々で正規化されていて 同じ素材や空気でも 輝度値が異なっていました すなわち 輝度値から材質を推定するのは困難と考えられました   輝度値の違い    データセットごとの輝度値の違い    ひときわ明るく映っている金属部品があり その周囲に金属由来のアーチファクトが発生している影響で周囲の部品の境界が分かりづらくなっていました  下図中白丸内   アーチファクト    アーチファクト例    つの閾値で輝度値の低い部品と背景を分けるのは難しそうでした 以下ヒストグラムを見ると 一番大きいピークの部分は背景 空気部分 だと考えられますが 低輝度値の前景 部品部分 のピークのつが隣接しています   ヒストグラム    輝度値ヒストグラム    つのデータセットあたり 概ねタテpx ヨコpx程度の画像が枚程であり 全量をメモリ上に読み込むとメモリが貧弱なPCでは処理が難しそうでした 評価用PCスペックとして公開されていたものがメモリGBだったのですが 浮動小数点型を扱う三次元ガウシアンフィルタのような処理も適用したため メモリGBのPCではプログラム製作途中にbad alloc例外で頻繁に落ちてしまいました 余談ではありますが 省メモリな処理とするために単精度浮動小数点型を使用 配列を使いまわす等実装上の苦労もありました     三次元形状マッチングの手法本課題における三次元形状マッチングの手法の候補として 以下のようなものを事前に検討しました 前述のとおり データセットごとに異なる輝度値の正規化が行われていたため 本作品では材質を元にした部品の識別は考慮せず 形状のみを考慮した識別を行うことを考えました   断層画像のマッチング  ボリュームデータのマッチング  深度画像 Depth map でのマッチング  多視点ボリュームレンダリング画像でのマッチング  多視点深度画像 Depth map でのマッチング  ボリュームデータ表面の点群同士のマッチング人間が容器の中身を取り出して部品の個数を数える速さを 勝手に 目標にしていましたので 出来るだけ高速な手法が望ましいです 人間が部品の種類別に個数を数える場合 外側の形状を複数の方向から見て部品の種類を判断します 同様の発想で形状のマッチングを行うとすれば 複数視点からの深度情報付の画像を利用することが良さそうであると考えました 加えて 画像であればボクセルデータほどの大きな情報量を取り扱わなくとも マッチングが可能であり 計算機リソース面 計算時間面でも利点があります   部品の識別    部品の識別  当初 またはとの併用として検討していましたが 手元の開発環境の中でもVTKで深度画像が作れる環境と作れない環境があり 評価環境での動作に不安があったため最終的にはの  多視点ボリュームレンダリング画像でのマッチング  としました     多視点画像本作品ではボクセルデータをボリュームレンダリングして作成した多視点画像を利用しました VTKを利用した簡単な多視点画像の作り方について本記事の後ろの方でご紹介 記事内リンク： ボリュームレンダリングと多視点画像作成   ボリュームレンダリングと多視点画像作成  しています    手法概要下図に 本作品の処理の流れを示しています まず X線CTによるボクセルデータを部品ごとに切り離したボクセルデータにした後 部品単位で形状特徴量を計算し 特徴量を比較することで参照部品と同じ部品かどうかを判断します   手法概要    手法概要        部品分離二値化画像処理と三次元ラベリング処理で部品ごとの分離を行いました  通常の二値化処理を行っただけでは多くの部品がくっついた状態になってしまいましたので できるだけ部品同士を切り離すために三次元ガウシアンフィルタでぼかして閾値処理を行い 部品間の接合部分が小さいものは切り離す処理を行いました この処理で切り離すことが難しかったもの あるいは金属由来のアーチファクトの影響で部品形状が一部欠けてしまったものが 最終的に誤認識の原因のつになってしまいました       三次元形状特徴量計算三次元特徴量として 多視点画像を利用した三次元形状認識手法である MVCNNを利用して形状特徴量の計算を行いました 本課題の提供データ中の部品の中には大小さまざまなものが含まれていましたが 多視点画像はすべて同じサイズで作成しています このため 本処理で抽出する特徴量は 大きさの情報が失われ 形状のみに依存するものになっています      MVCNNMVCNNのモデル構造を下図に示します 部品ごとのボクセルデータから作成した 視点の多視点画像を入力として 最終層の出力を次元の特徴量としました MVCNNは元々 多視点画像による三次元形状認識 クラス識別 手法です 本課題において提供された個の参照部品に関して試しに学習したところ 部品分離処理が成功したものについてのクラス識別ではかなり高精度での識別が可能でした  ただし 本課題で提供されたデータの中に含まれていない未知の部品があった場合には 既知のどれかのクラスとして識別するような誤識別が発生しうるため 本作品では距離学習を行い 形状特徴量の比較によって部品種別を判定することにしました 出力層を次元とし 損失関数にTripletLoss   を採用しました  提供データから切り出した部品は全部で種類強ありましたが 筆者の目で見て形状の近いものをまとめてクラスに分類しました 学習は次元の特徴量空間上で 同じクラス同士のユークリッド距離は小さく 異なるクラス同士のユークリッド距離は大きくなるよう学習を行いました  データ拡張として 多視点画像の視点違いと部品の色味 濃淡 違いを採用しました       特徴量比較ある部品について 参照部品と形状が似通っているものほど二つの形状特徴量のユークリッド距離が小さくなるため 形状特徴量間のユークリッド距離に 経験的な 閾値を設けて形状を判断しました  加えて 形状特徴量は大きさの情報を失っていますので 似通った形状で大きさが違うものは判別がつかないことがあるために 部品ごとに切り出した時点での体積についても加味して部品の判定を行いました    結果と考察 個程度の部品を含む 提供されたつのデータセットについて CPU Corei K で分 分半程度の時間で処理できました  判定を誤ったものとしては 部品の分離に失敗しているもの 学習データ作成時に筆者に見分けがつかなかったもの等がありました 画像処理による部品の分離にはまだ改善の余地があったと思います 精度向上のためには深層学習手法の適用も視野に入れた方が良いと思いました また 学習データの品質は大事ですので 適切に分類しましょう ということではありますが 機械学習モデルの出力をそのまま利用するのではなく 後処理で体積を加味した判定を行っていることで アノテーション誤り等によるモデル 学習データ の不出来をカバーできていた面もありますので 前処理 後処理はとても重要です   誤判定の例    誤判定の例     ボリュームレンダリングと多視点画像作成ここまでにご説明した作品では ボクセルデータから多視点画像を生成しています  これはVTKを用いて実現しています PythonやC   最近ではJavaScriptなどからVTKを使って ボリュームレンダリングによる可視化や多視点画像作成が手軽に出来ます 本記事では Pythonでの実装例をご紹介します   注 手元ではPython    vtk    Windowsで動作していました  年当時のバージョンです 参考までに インストールコマンドを下記に示します サンプルスクリプトで使用しているので OpenCVとNumpyも一緒にインストールするものです     console  pip install opencv python numpy vtk    ボリュームレンダリングによる可視化VTKWithNumpyのサンプル   を元にしています px pxの画像を枚読み込んで VTKでボリュームレンダリングする例です render関数はちょっと長いですが スクリプト中のコメントをご参照ください       Numpyの配列をVTKの画像形式へ変換      各次元のサイズを設定      可視化時の透け具合を設定      輝度値以下は透明    alphaChannelFunc   vtk vtkPiecewiseFunction      alphaChannelFunc AddPoint         alphaChannelFunc AddPoint         alphaChannelFunc AddPoint           可視化の色を設定      輝度値以下が赤 が緑 以上は青    colorFunc   vtk vtkColorTransferFunction      colorFunc AddRGBPoint               colorFunc AddRGBPoint               colorFunc AddRGBPoint               volumeProperty   vtk vtkVolumeProperty      volumeProperty SetColor colorFunc     volumeProperty SetScalarOpacity alphaChannelFunc     volumeMapper   vtk vtkFixedPointVolumeRayCastMapper        多視点画像生成で深度画像を生成したい場合は以下      volumeMapper   vtk vtkGPUVolumeRayCastMapper      volumeMapper SetInputConnection dataImporter GetOutputPort         ボリュームデータにMapperとPropertyを反映    volume   vtk vtkVolume      volume SetMapper volumeMapper     volume SetProperty volumeProperty       可視化のウィンドウを設定    renderer   vtk vtkRenderer      renderWin   vtk vtkRenderWindow      renderWin AddRenderer renderer     renderInteractor   vtk vtkRenderWindowInteractor      renderInteractor SetRenderWindow renderWin     renderer AddVolume volume     renderer SetBackground        renderWin SetSize          表示開始      画像のサイズ 枚数 ファイル名を設定      OpenCV等で画像を読み込み      可視化 これだけです 下図のようなウィンドウが現れて ぐるぐる回して見られます   VTK可視化ウィンドウ    VTK可視化ウィンドウ     VTK可視化ウィンドウ    VTK可視化ウィンドウ      多視点画像として保存多視点画像は 上記の可視化ウィンドウの中に見えている可視化結果を カメラの視点を変えて保存するだけで作成可能です  下のスクリプトは視点分の画像保存の例です 筆者が本作品で採用をあきらめた 深度画像生成についてもコメントアウト部分を活かせば動きます     python  上のスクリプトのrender関数内の  　表示開始 コメント以降の行を以下で置き換える    volumeMapper RenderToImageOn        カメラの角度を設定      深度画像も生成できる はず 上記スクリプト中の カメラの角度を設定 のコメント部分の角度の値を変更することで 別視点の画像となります 下の図はぐるっと視点分の画像で作成しました 今回はボクセルデータなのでVTKを利用した多視点画像作成ですが 点群データやメッシュデータならPCL   かOpenD   を利用するほうが便利かもしれません   視点アニメ    視点アニメ     おわりにお読みいただきありがとうございました 何かのお役に立てていただければ幸いです 以上 情報通信研究部　水谷麻紀子でした     MV CNNを用いた三次元類似形状検索 ボクセルデータを多視点画像により形状比較することで高速 省メモリに形状比較   水谷麻紀子  外観検査アルゴリズムコンテスト  注  本作品は 年の外観検査アルゴリズムコンテストで最優秀賞をいただきました   注  なおコンテスト作品では MVCNNの学習プログラムはPythonで実装したものの 提出作品は部品分離 多視点画像作成から推論処理含めてすべてC  で実装しました C  でもほぼ同じような実装で多視点画像が作成できます ,5,2023-08-31
108,108,DBNetの理論備忘録,画像処理,https://qiita.com/G_G_G/items/589470626889791490a3,  目的DBNetの理論を学習した際の備忘録 OCRを行おうと思い まず一般的なモデルについて確認したかったが あまり説明記事が無かったのでまとめてみました   参照原論Liao  M   Wan  Z   Yao  C   Chen  K     Bai  X      Real time Scene Text Detection with Differentiable Binarization  ArXiv   abs    DBNet Differentiable Binarization とは年に発表されたOCRのためのモデル 番重要なのは   モデル概要　下図が一般的なOCRモデル 青線 とDBNet 赤線 のinputからoutputへの流れになっています 　一般的な文字検出は青線のようにsegmentation Unet等 により文字のある部分を検出します そして検出されたエリアの中で文字認識を行います 　DBNetは 図の赤線のプロセスを辿っています segmentationの際に並列して segmentationのアウトプットを二値化  文字エリア  それ以外 する際の閾値も特徴量マップで出力しています こうすることで 画像の場所毎に閾値を最適化することを目指しています   image png  　原論ではこの方法について利点をつ挙げています   精度向上  推論時間削減  後処理を簡略化したため  パラメータ数が少ないモデル 例  ResNet  を組み込んだ際に良いパフォーマンス 　 サイズが大きい方が精度は良いが 小さいモデルでもあまり精度が落ちなかった   threshold mapは推論時には取り除けるため メモリと時間を消費しない　次にDBNetのより詳細なモデル図を示します 前半部分は一般的な物体検出モデルで用いられる特徴量ピラミッドが使われています 原論ではバックボーンにResNetが使用されてますが  これは文章のような縦横の比率が大きくことなる物体検出においては deformableの方が通常よりも特徴量抽出に優れているためです バックボーンの層目と upsamplingされたそれぞれの層における出力を結合 concat して 予測値の特徴量マップ probability map と閾値の特徴量マップ threshold map を出力します この二つのマップからapproximate binary mapを出力します   image png  　approximate binary mapですが  通常は 特徴量マップをsigmoidで に変換して それを閾値 例えば  と比較して二値化します DBNetでは以下の式が適用されます k  ハイパラ 原論ではに設定 \\　このようにすることで  で二値化するよりも 関連するテキストを分割するのに役立ちます また この形による出力は効率的な学習を促進できる点も挙げられ これを含めて損失関数について説明します   損失関数損失関数は以下の式で表せます \alpha  \beta  ハイパラ それぞれ とで設定 \\　  L  s   L  b   はBCEを適用しています ただし 検出対象 positive と背景 negative のバランスを調整するため 誤差を学習するnegativeなセルの数はpositiveなセルの数に対し最大でも になるように調整されます hard negative mining  S  l   最大 になるように調整された中で選ばれたサンプルセット \\　ここで 先ほどの数式で二値化した出力にBCEを適用した際の損失と勾配について説明します ピクセル毎の損失とそれを微分した値は以下のようになります l     l      ポジティブ及びネガティブラベルに対する損失 \\微分すると　損失に対する勾配がk倍されており kで勾配の大きさがコントロールできるため学習効率をよくすることが出来ます 　  L  t   について まず使用するラベルデータは他つとは違います ラベル作成用のアルゴリズム説明は割愛しますが 文字部分の周りを囲むようなラベルデータを作成します この時囲まれた部分 境界線を含む で予測値とラベルデータのL距離を損失にしています R  d   検出エリア内部のラベルの集合 \\y  i    閾値マップ用のラベル  推論　推論時にはprobability mapと approximate binary mapのどちらでも使用できます 原論ではどちらもほとんど同じ結果になったので よりシンプルなprobability mapのみを使用した方法で推論しています probability mapに対して閾値は で二値化して 検出部分をVatti clipping algorithm 割愛したけどラベル作成にも使用されているアルゴリズム に従って拡張します ,1,2023-08-30
109,109,Teachable Machineで調教した柴犬がLINEBotとして姿勢判断をしてくれる,画像処理,https://qiita.com/wamae/items/7e1a605c7996b65c0703,仕事でもプライベートでもスマホやPCなど画面を見る時間が増えました それに伴い 長時間同じ姿勢や座りっぱなしなどが原因でストレートネック 眼精疲労など健康面も危惧されるようになっています せっかくだから仕事でもプライベートでも役に立ちそうなツール作りたいと考えました   ■使用したツールTeachable MachineLINE DeveloperNode RED  ■Teachable Machine の設定まずは学習させるためのサンプルを撮影 良い姿勢だと good崩れた姿勢だとbadと表示されるようにします   image png  ん  画像登録しましたが good bad両方ともNoN と表示されてしまう    note warnおそらく サンプルのように全身移っていないとうまく反応しない様子   image png  ということで 改めて画像プロジェクトから入りなおして作り直すことに   image png  先程同様に 画像を撮影しトレーニングさせます good badだけだとおもしろくないので 他にも別の判定を追加したい 何を追加しようか考えている際に お昼ご飯のあとや徹夜明けのPC作業は眠たくなることと 拡大しても見ずらい文字はついつい画面に近づいてしまうことを想起したのでこの二つを判定で出てくるようにしたいと思います というわけで 追加でウトウト 眠っている状態は　   sleep   　と画面に近づきすぎた場合の　   too close   　を追加   image png  良い姿勢でもbadの判断になるため goodと被っている画像を削除し 他の明るさでも対応するように照明も少し修正    note warnダブっている画像があると   複数の判定に反応  してしまうようです その為 撮影した画像サンプルで似たような画像や ダブっている画像を改めて精査 ボケていた画像やダブりの画像を選んで削除 修正した内容で改めてモデルをトレーニング削除すれば判定もクリアになりました   image png    ■Node RED続いて出力の為のNode REDの設定 ☟ノードを繋げた状態がこのような形になります   image png  ☟まずはWebhookの設定から   image png  Teacheable　Machine のノードでは トレーニング後のURLを設置   image png  続いて テンプレート内ではBotからの出力内容を記述します   image png  Reply Message　LINE　BotのSecretとaccess Tokenを出力したいアカウントからコピペしてきます   image png    image png    image png  出力する言葉が機械的過ぎたので柴犬っぽくしてます   image png  無事に出力できました いい姿勢のつもりでしたが画面に近いとのこと   ■反省点画像投稿だとその都度LINEで画像を投稿する必要があるので少々面倒 リアルタイムで姿勢を判断して画面に表示してくれるようにすればさらに実用的になると感じました アップデート案として定点カメラの設置とCodepenを使用したツールでも実装可能かやってみたいです   image png  柴犬とお友達になりたい方はこちらからどうぞ ,2,2023-08-29
110,110,\間違えた！/を防ぐロゴ判別AIを作ったらとても便利だった(TeachableMachine),画像処理,https://qiita.com/kensan7/items/823ec6b9dae5dc524c19,  Q AIでロゴやアイコンが現在使用してよいものかを判別できるって本当   A 本当です   今回 が 誤って販促物に掲載されていないかを手持ちのスマートフォンを用いて 簡単に確認ができるAIチェッカーを作りました   今回作成したもの  作成の背景私は とある小売業の本社で販促やプロモーションを担当しています そこでほぼ毎日実施している業務のつが   販促物のチェック  です 本社で作成するチラシやバナーに加え 支社で作成した特定のエリア向けの各種販促物についても 随時確認をしています この確認作業において 現在最大の課題となっているのがというものです 具体的に言えば サービスの改廃やアイコンの刷新 使用契約等で  少し前までは使われていたが 現在使ってはダメ というものが販促物に載っていれば 削除または差し替えをするという 万一見逃してしまったら クリティカルな状態が発生する重要な業務なのです このチェックは全ての販促物制作で実施しており ほぼ毎日行っているのですが 判別すべきロゴやアイコンの数が多く 全員が完璧な判別ができていない現状があります そのため 判別がほぼ完璧できるベテランメンバーが全て最終チェックを行うことになるなど 一部のメンバーへの負荷が大きくなっていることが 今回AIによるロゴ判別ツールを作るきっかけとなりました    アイコンの使用判別で抱える本社と支社の現状と共通課題これから 現状の本社と支社のそれぞれの現状と共通課題を洗い出していきます     本社現状 販促担当者全員で回覧をするが 主に担当業務のものを中心にチェックをしている  販促担当者の全員が 全てのアイコンの使用可否判断をできるわけではない     支社現状 都度アイコンの使用について 支社の販促担当者へ情報連携をしているが 五月雨式の連絡のため 使用基準の全てを確認できるツールが無い  支社の販促担当者の全員が 全てのアイコンの使用可否判断をできるわけではない ここから導き出せる共通課題は     確認作業が属人的になっており 人事異動が発生すると判断基準を失う可能性がある  使用基準を確認できるようなツールが現状ない   という点で さらに業務では紙とWEBの双方に携わるため どちらの媒体でも判別ができるツールである必要があります このような本社 支社の共通課題を解決するものとして  Teachable Machine という自身で機械学習モデルを作成できるツールを用いて 解決に向けて取り組みました   使用したツール独自の機械学習のモデルが簡単に作成できてしまうGoogle社が提供している Teachable Machine を用いました  判別の基準となる写真を様々な角度から画像サンプルを撮影 トレーニングを実施 プレビューで確認このたった工程で 機械学習のモデルが自由に作れてしまいます   image png  私は現在使用してよいアイコンと 使用してはいけないアイコンを区別するために 使用してよいもの と 使用不可のもの とつの名称に分けて それぞれサンプル画像を登録していきました ☆各画像撮影枚数が増えれば増えるほど 判別の精度が高まります 目安として画像につき枚程度撮影すると 精度が安定します 最後にモデルをエクスポートした際に発行されるURLは後ほど使用しますので 必ずコピーして保存をしておくことが重要です    ②Node Redプログラミングスキルが無くても 手軽に始められるブラウザで動くプログラミング環境がこの Node Red となります 今回はFlowForgeを利用し Node REDのフローを作成していきました     Node REDフロー図  image png  ここで示されているフロー図を分解し 設定の方法を説明していきます ネットワークのカテゴリから http in をドラッグ ドロップし ノードをダブルクリック 右クリック後に表示されるメソッドには POST を URLには  linebot を入力してください   image png       LINEメッセージ function ノード 機能のカテゴリーから function をドラッグ ドロップし ノードをダブルクリック コード欄に赤枠内のコードをそのまま入力してください   image png       http request ノードネットワークカテゴリーにある http request をドラッグ ドロップし ノードをダブルクリック URLに  種別は Bearer認証  トークンにはLINE Developersで設定した チャネルアクセストークン を入力してください 最後に出力形式は バイナリバッファ を忘れずに選択することを忘れないでください   image png       Teachable Machine ノード分析カテゴリにある Teachable Machine をドラッグ ドロップし ノードをダブルクリック URLには Teachable Machine で作成した独自モデルのURLを貼り付けて 最後に忘れずにimageに必ずチェックをいれてください   image png       template ノード機能カテゴリにある template をドラッグ ドロップし ノードをダブルクリック プロパティには msg payload  そしてテンプレートには 結果は   payload  class   でした  という文言を入力をしました このテンプレートの文言が LINEのトークに戻ってくるものになります   image png       Reply Message ノードLINEカテゴリにある Reply Message をドラッグ ドロップし ノードをダブルクリック SecretとAccess TokenにはLINE Developersで設定したものを入力 またReply Messageの枠に記入した文言は リプライとして利用者へ返信されます 今回は ロゴやアイコンを確認いただいたお礼を返しています   image png    完成上記の手順をおこなった後 Node REDのフローをスタートさせると起動するはずでした Teachable Machineまではデバッグをつないでも大きな問題は発生していなように見え 実際にノードの下に判定された画像と結果が返ってきたのは確認ができました TemplateとReply Messageの設定の再確認や 自宅の通信環境があまり良くないことも原因かと考え 安定した通信環境で再度作成の取り組んだものの 途中で必ず接続が切れてしまい動作しませんでした また新しいシートでのNode REDのフロー作成でも変化が無かったため Makeを使用した方法で動作するか トライをしています   最後に現在はかなり前から所属している従業員でないとわからないような ロゴやアイコンの見分けについても Teachable Machineに機械学習をさせることで 容易に判別できることがわかりました 私は現在小売業に従事をしていますが LINE Botを用いれば店頭の販促物でもロゴやアイコンのチェックをして 旧バージョンの販促物がついたままになっていないかの確認ができたりして 非常に拡がりのあるツールであると感じています 絶対にこのLINE Botを完成させます ,0,2023-08-29
111,111,TeachableMachineを使い、「一般物品」「消耗品」を判別できるようにしてみた,画像処理,https://qiita.com/okamoto0016/items/1c38cc6378d3827e69e5,TeachableMachineを使い  一般物品  消耗品 に判別できる TeachableMachine codepenを使用して免税販売時に 一般物品  消耗品 をスマホカメラで写して判別できるようにしてみました 免税販売時の留意点   image png  免税対応している店舗へ販売応援に行くと  一般物品 と 消耗品 の区別  ができず 都度 スタッフに確認しなければいけなく手間がかかっていました   また 店舗スタッフでも社歴の浅い方は 私と同じく 一般物品 と 消耗品 の区別ができず 私と一緒に右往左往の状態でした 今回 TeachableMachinを学び スマホカメラで写すだけで区別できるのではと思ったのが開発のきっかけです   免税対象商品には  一般物品 と 消耗品 に分けられます   image png    image png   消耗品 を販売する場合は  消耗品 と 一般物品 を開封できない袋に梱包する事が義務づけられています   image png  売場に 一般物品 と 消耗品 が混在しておりますので 目視にて区別 対応する必要があり  消耗品 であるにもかかわらず間違えて 一般物品 にて対応する事案が発生しがちでした スマホカメラで写すだけで 一般物品 と 消耗品 が区別できれば このようなミスを撲滅できると考えました   使用したツールシンプルにつのツールです  Teachable Machine CodePen全体の流れ  ① Teachable Machineで商品の画像と商品名と区分 一般物品 or 消耗品 を　登録します 画像はつの商品に対して短時間に枚以上記録してみました ② Teachable Machineで作成したURLをスマートフォンで開くとCodePen上 　でスマートフォンのカメラで商品カテゴリー 一般物品 or 消耗品 が　表示できるようになります Teachable Machineの設定  Teachable MachineはGoogleが提供している機械学習のモデルが作成できるサービスで 画像  音声  ポーズ の種類のモデルでAI認識が可能です 今回のツールでは商品の 画像 を学習させます 実際に売場をまわってサンプル商品を撮影しました 　商品　　　　区別　キャップ　　一般物品　カサ　　　　一般物品　クリーム　　消耗品パターンとして 商品カテゴリー  一般物品 or 消耗品 を登録しました サンプル画像の取込み サンプル商品の画像を撮影していきます 　　商品の写真は 表面 裏面 右側面 左側面等最低枚ずつ登録します 　　① 商品の表面　　② 商品の裏面　　③商品の右側面　　④商品の左側面  image png   サンプルの登録が終わったらこれらのサンプル写真をTeachable Machine AIに　　学習させます 　　画面の真ん中にモデルをトレーニングするというボタンがあるのでここを　　押します 　　ボタンを押してトレーニングが終わると右側にカメラの画像とその下にAIが　　画像から判断した結果がパーセンテージが動きながら表示されます 　  image png  　　サンプルの商品どれかをカメラに映して精度高く判断できているか　　チェックしました 　　写真では右下の部分  商品カテゴリー クリーム  区別 消耗品が　　判定されています 表面や裏面 で試してみましたが 高い精度で　　判定しました 　　ここまでではパソコンのカメラの使用が前提ですので 商品をパソコン　　まで持ってこないと活用できず現実的ではありません    スマートフォンカメラで検索できるようにCodePenへ渡すためのTeachable　　Machine設定の続きです 　　正しく動いているチェックが終わったら モデルをエクスポートするを　　押します 　　次のこの画面でモデルをアップロードするを押します 　　真ん中くらいに共有可能なリンクがあるのでコードのp jsを選択します 　　このp jsタブにある長いコードをこの後 CodePen設定で使用しますので　　コピーしておきます   image png  CodePenの設定CodePenはオンライン上でHTML CSS Javascriptを書くことができるサービスです 今回はTeachable Machineとつなげ スマートフォンカメラで商品を認識できるようにしてみました  CodePenを開くと箱がつ並んでいて左から HTML  CSS  JS と　　並んでいます 使用するのは一番左の箱 HTML 部分です 　　ここに先にコピーしておいたTeachable Machineの長いコードを　　貼り付けます   image png   CodePenの右上にある四角いボタンを押します 　　メニューの中から上からつめ FullPage Viewを押すとコード入力の画面が　　非表示になります 　　スマホにURLを送った後 スマホの画面でFullPage Viewを選択するとコード　　が表示されている画面が非表示になります 　無事 スマホカメラで消耗品の判別ができました    店舗スタッフにみてもらったら これ便利  と褒めて頂きましたが  全部の商品を撮影するのは難しいわね との課題も    今後の課題  今回はシステムの動きを勉強するため すぐ役に立ちそうな免税販売時における　 一般物品 or 消耗品 　判別ツールを開発してみました Teachable Machineに画像を取り込むとその精度は高く充分実用に耐えると感じました 店舗スタッフの 全部の商品ｗｐ撮影すｒのは難しいわね との課題を頂きましたので相対的に数の少ない 消耗品 のみを判別し それ以外の商品の場合を 一般物品 として判別できるようにすれば より実用に近く活用できるのではと思いました また スマホカメラで写すときに できればスマホ背面のカメラで写せればと感じましたが やり方がわからなかった点は 次回への課題と考えております 以上 Teachable Machine CodePenツールによるスマートフォンカメラで免税販売時における 一般物品 or 消耗品 を自動で区別できるツール開発の流れでした ありがとうございました ,1,2023-08-28
114,114,スポーツの動画解析_part1_テニスボールの検出,画像処理,https://qiita.com/G_G_G/items/f536c2cac250640ee2b9,  目的動画分析 opencv したことの備忘録 スポーツの動画解析 具体的に何をどう進めるかは検討中 をしたいと思い 一旦手元にあるデータでテニスボールのトラッキングを試みた 一旦半日かけてやってみて何が難しそうか見てみようスタンス   参考qiitaでopencvを使用したボールの検出例の内件を参考にさせていただきました    OpenCV Python版 でテニスのボール軌道を検出する        Python   OpenCVで野球ボールをトラッキング     使用した動画　昔観客席からスマホで撮影した indian wells ATP の映像 分析データは違う試合ですが 決勝はフェデラー対ワウリンカの試合をかなりいい席で見れて幸せでした   の記事の動画と違う点は  斜めから撮影  固定されていないため手振れしている  サーブだけでなくラリーが入っている   ラリーの途中でボールが画面の外に出てしまっている もっと検出に適したデータを用意したいが 一旦手元にあるデータでトライ フレーム切り抜き   image png    実施内容最初は基本的に参考記事に沿うような形でまずやってみました  方針：テンプレート画像 テニスボール を用意して opencvのmatchTemplate機能でボールを検出する  参考の二つの記事の内容を基に フレーム間の差分を特徴量 以下 差分画像 とすることで 動いている物以外の特徴量を削減した 差分画像例 分析自体は白黒にして実施しています 下に白黒verもあり   image png  検出画像赤い四角が検出結果 四角の左上にはmatchTemplateの結果が緑色で載っている   出てきた課題点と対策  ボール以外の検出が多い  単純な丸なので 形の似ているかつ良く動いているものがよく誤検出されました 　特に選手の足と腰の誤検出が多かったです 対策  元のボールの画像ではなく 差分画像中に検出されたテニスボールをテンプレート画像にする ただ 動いている箇所 手振れ 選手 観客 が多いかつテニスボールのただの小さい丸なので 根本的に消すのはこの動画で差分画像を用いる検出方法では難しい印象でした   ボールが画像中に見えない or ボールの動きが無い 画像で違う場所を検知してしまう 対策  検出の閾値を設定    閾値以下の場合は検出結果を表示しない 閾値設定しない場合のmatchTemplateの値は下図のようになる template valueが大きい時間帯と小さい時間帯が交互に来て波打っている ボールの有無や画像全体で検出された動きの量の違いでこのようになっていると思われる   image png    ラリー中のボールが速いと テンプレートと形状が違うため検知できない 例 ボールが線になっていて 最初の方法ではラリー中の素早いボールは全く検知できませんでした 対策  早いボールを検知するためのテンプレートを作成 affine行列変換で°毎に回転させて 色々な方向に動くボールを検知する   結果良かった点：最初は検出できていなかった ラリー中のボールも検知できるようになりました 良くなかった点：線形な形状の特徴量に対する誤検知が増えてしまいました     考えられる改善方法  前のフレームの検出位置を次のフレームの検出位置に紐づける    特定の箇所へのmatchTemplateの値をいじれば 前の検出に近いところで検出しやすいように出来そう   撮影方法をどうにかする    手振れとか色々あったので    テンプレートを工夫する   色々なボールの動き 形状に対するテンプレートを作れば精度は伸びしろあり   差分画像に頼らない検出方法   YOLO等 opencvの方法とYOLOでの違いを比較してみたい   今回学んだこと ボール検出系だと 検出物の形状変化に対する汎化性能を持たせることが必要  時系列関係を捉えた検出の方がロバストな結果になりそう 既存の方法を調べてみる  ニューラルネットワークの物体検出系モデルを使わなくても データがきれい 撮影点が固定 撮影角度が良い ならば工夫次第である程度精度は出せそう   ソースコードgithubにもあげています 検出動画を表示する部分を下記に共有 ,7,2023-08-10
115,115,【OpenCV】4.6.0から4.7.0でのArUcoモジュールの更新箇所抜粋,画像処理,https://qiita.com/koichi_baseball/items/d51373e7fd6dddb57d1f,  はじめにOpenCVを使用してArUcoマーカの認識をしようとしたらバージョンの更新によって関数名の更新があったのでまとめます Web上に公開されているサンプルプログラムでも同じバージョンでないと基本的に動かないので辞書的な使い方で参考になれば幸いです    noteここでは ArUcoマーカの生成 認識 描画等  最低限  の関数のみ記載することとします より詳細な情報は公式ページを参照してください   バージョンサンプルプログラムは         で動作確認したものです   サンプルプログラム   マーカ画像の作成マーカ画像の周りに余白を設けるために少し工夫しています  白い画像のimgを用意してそれに重ね合わせている  参考     マーカの読み込み 描画  更新箇所   辞書の読み込み辞書を取得する関数のうちの一つがなくなりました  getPredefinedDictionary   は  と  で変わっていないのでそれを使うのがベターだと思います ver  で Dictionary get   を使用すると下記のエラーが発生しました    マーカの生成    drawMarker    generateImageMarkerマーカを生成する関数名が変わっていました    Python  Ver   cv aruco drawMarker dictionary  id  sidePixels   Ver   cv aruco generateImageMarker dictionary  id  sidePixels ver  で drawMarker   を使用すると下記のエラーが発生しました C  の場合は include対象も変更されているので注意してください   image png     マーカの認識    パラメータの取得マーカを認識する際に使用するパラメータ変数を取得する関数の関数名が変更になっていました コンストラクタだけでよくなったようです  少なくともメンバ変数から create は消えました   image png      マーカの認識 描画C  の場合は include対象も変更されているので注意してください   image png    おわりにバージョン情報の記載は大事ということに気付きました  必要があればほかの関数についても追記します ,9,2023-08-01
116,116,Python初心者が犬・猫の画像認識をやってみる,画像処理,https://qiita.com/AnnoJou/items/86ba0f91b8c2cc747573,  目次  はじめに  実行環境  ソースコードの解説  Google Colaboratoryとの連携  画像データの入手  学習用データと検証用データの作成  モデルの定義と学習  画像分類の結果と出力  実行結果と考察  製作したアプリ  おわりに    はじめにこちらはAidemyさんのAIアプリ開発講座 ヵ月 を受講した成果報告となります 製作したアプリのソースコードと解説をブログという形で投稿させていただきます  テーマ 犬と猫の画像分類  受講中に学んだ男女識別のアプリ作成を基にして 身近な動物である犬と猫の画像分類アプリを製作しました   catdog jpg      実行環境OS：Windows Google Colaboratory Visual Studio Code Flask利用のため     ソースコードの解説     Google Colaboratoryとの連携ソースコードを実行した際にGoogleドライブの画像データをGoogle Colaboratoryから参照できるように 下記コードを実行します 詳細なやり方は下記リンクを参照      画像データの入手学習及び検証に必要な画像は kaggle comにてデータセットを検索して入手しました  ◆Cat and Dog kaggle com   犬 猫ともに学習データ枚ずつ 検証データ枚ずつのデータセットです 今回はデータセットの中から枚ずつに絞り Googleドライブに格納しました ※今回はデータセットから画像を入手しましたが Webページから必要な情報を自動で抜き出すスクレイピング icrawlerやBeautifulSoupなど でも多くの画像データを収集可能です      学習用データと検証用データの作成必要なライブラリをインポートしたら Googleドライブから犬と猫の画像を読み込み 画像サイズを変換してリストに格納します    python   ライブラリのインポート  Googleドライブのファイルパスを指定する   画像を格納するリストの作成  各カテゴリの画像サイズを変換してリストに保存犬と猫の学習用データをつに集約します 正解ラベルの付け方は コンピュータが理解出来るように 犬   猫 ではなく      という値を与えます ※種類以上の分類の場合は         と正解ラベルを増やしていきます    python   犬と猫の画像を一つのリストに集約 及び正解ラベルを設定する 集約した学習用データをランダムに並び替えることで 学習の偏りを防止します データセットの画像データは学習用として  検証用として に分割して データリストを作成しました    python  ラベルをランダムに並び変える   学習データを  検証データを に分割する one hot表現は クラス数 今回は犬 猫のつ と同じ長さの配列を用意し 正解のクラスに対応するインデックスのみを それ以外をにすることで 全てのクラスの値を平等に扱うことができるエンコーディング手法です    python   正解ラベルをone hotの形にする       モデルの定義と学習VGGを読み込んで転移学習を行います VGGモデルから出力を受け取り Sequentialモデルを用いて追加の層を定義します 　→VGGから出力を次元のベクトルに変換します 　→ノードの全結合層を追加 活性化関数はReLUを使用 　　sigmoid関数を試したが ReLU関数の方が正解率が高かったためこちらを採用 　→過学習を防止するためのドロップアウト層を追加 　　 の学習データを訓練時に無効とすることで モデルの汎用性を高める    中略   　→ノードの全結合層を追加 ここのノード数は分類するクラス数と同じ数とする   モデルの定義VGGに含まれる全ての層を学習に取り込むと データ量が多いうえ学習時間がかかるため 層までを使用する  VGGの特徴抽出を層までに固定 　→損失関数lossと最適化アルゴリズムoptimizerの設定 　→compileした内容で訓練を実施 　→一度の学習に使用するデータ数 　→訓練用データを繰り返し学習するサイクル数 　→学習中の表示方法 　　    ：プログレスバーを表示しない     ：表示する     ：結果のみ表示する    python   損失関数と最適化関数の設定  訓練を実行する      画像分類の結果と出力あらかじめ正解ラベルとして 猫      犬    を割り振っていたので 入力画像の分類が  となれば 猫    となれば 犬 と出力する関数を定義する    python   画像を受け取り 名称を判別する関数evaluateメソッドにより汎化精度 新規データに対する精度 を算出する    python   精度の評価学習で使用していない新しい画像を読み込ませて 先ほど定義したpred関数で入力画像の分類予測を行う    python   pred関数に写真を渡して分類を予測    実行結果と考察犬と猫それぞれ枚の学習データから訓練したモデルに対して  猫 の画像を入力して正しく識別するか確認します   testcat png  画像サイズ：×バッチサイズ：エポック数：正解率：    画像エポック png  mnistの時よりも大きめの×の画像サイズで学習を行なったところ 出力結果は  と低く 判定も間違っていたためハイパーパラメータの見直しが必要となりました 学習に使用した元画像を見ると 対象が映っている画像の面積が小さいものもあったため 画像サイズを大きくすることでより特徴の抽出がされやすくなるのではないかと考えました 尚 アイデミーの先輩方の記事を拝見したところ Gitの容量制限があるとのことなので サイズが大きすぎない×の画像で学習を実行してみます 画像サイズ：×バッチサイズ：エポック数：正解率：    画像エポック  png  学習させる画像サイズを×に変更したところ 正解率が  に上がって判定も正しく行われていたため 学習画像のサイズを大きくしたことが効果的だったことが分かりました 出力結果のVal accについて エポック数がを超えた辺りから正解率が横ばいになり始めたため 設定するエポック数は程度でよいのではないかと思いました 上記以外に正解率を向上させる要因として ①学習画像の枚数を増減させる ②学習モデルの層の数を追加 削減する ③学習モデルの最適化アルゴリズムを変更する ④バッチサイズを変更する といったものが考えられるため 今後の課題として取り組みたいと思います     製作したアプリ完成したアプリがこちらになります 画像を送信すると 犬か猫を判定してくれます  Cats and Dogs Classifier    Cats and Dogs Classifier png  以下はデプロイの際に手間取った所です 次のようなエラー文が表示されました どうやら最適化アルゴリズムのSGDが認識されていないようでした  kerasのリファレンス  を確認したところ となっており  optimizers SGD lr   のところは合っていることを確認しました SGDが認識されていないエラーの話だったので 試しにライブラリのインポート部分 from keras import optimizers をプログラムに追加したところ 無事にデプロイすることができました 最後に ファイル構成と最終版のソースコードを載せておきます   構成 png     python   ライブラリのインポート  Googleドライブのファイルパスを指定する   画像を格納するリストの作成  各カテゴリの画像サイズを変換してリストに保存  犬と猫の画像を一つのリストに集約 及び正解ラベルを設定する  ラベルをランダムに並び変える   学習データを  検証データを に分割する   正解ラベルをone hotの形にする   モデルの定義  画像を受け取り 名称を判別する関数  精度の評価  pred関数に写真を渡して分類を予測  resultsディレクトリを作成  重みを保存            flash  ファイルがありません              flash  ファイルがありません               受け取った画像を読み込み np形式に変換             変換したデータをモデルに渡して予測する  自PCでのチェック用    おわりに成果物の作成にかけられる時間があまり残っていなかったため 全ての要因は検証できませんでしたが パラメータの調整によって正解率を  まで上げることが出来ました Python初心者であった私がCNNを使ってプログラムを作成できるようになったのも Aidemyさんの教材やチューターさんのご支援あってのことです 本当にありがとうございました これまで学んできたことを活かして 色んな課題に取り組んでいきたいと思います ここまで御覧いただきありがとうございました ,6,2023-07-29
117,117,宝塚男役トップスターの舞台化粧顔と素化粧顔は、どれぐらい違う!?,画像処理,https://qiita.com/buyokana/items/4045879326bea0439e9e,  目次  はじめに  宝塚歌劇団について  テーマ  実行環境  学習の流れ  実践       スクレイピング      学習 テスト用データセットの作成      モデル学習      精度の検証  まとめと考察  アプリはこちら  はじめに記事を目に留めて下さり有難うございます 人生で初めてブログなるものを書きました    自己紹介  高校生の時に好きになった初めての贔屓が退団して抜け殻となり 昨年の月組公演 グレート ギャツビー で月城沼に足を取られてヅカオタに返り咲いた宝塚ファンです 以前から機械学習に興味があり AidemyさんのAIアプリ講座でpythonの基本から勉強した成果をまとめてみました プログラミング自体が未経験で 最初は簡単なif文すら怪しい状態でしたが か月間でここまでできるようになった という軌跡を見て頂けたら嬉しいです   宝塚歌劇団 来年は創立周年   について宝塚歌劇団は年に結成された 未婚の女性のみで構成される歌劇団花 月 雪 星 宙の組と スペシャリストで構成される専科に分かれている兵庫県宝塚市にある宝塚大劇場と 東京都の日比谷にある東京宝塚劇場の専用劇場を持つ宝塚歌劇団公式サイト↓  テーマどんなテーマでアプリを作るのか正直悩みました 仕事絡みで  AlphaFold という蛋白質の立体構造を計算してくれるツールに興味があって いつか使いこなせたらな というのも勉強を始めた動機のつだったので 仕事に少し寄せた内容にしようかなとも思いました …が アルゴリズムを見て眩暈がしたので 思いっきり趣味に走りました やったこととしては   花 月 雪 星 宙の組に所属する男役トップスター名の舞台化粧顔を 機械学習モデルに学習させる⇒素化粧顔を正しく判別できるか    というものです 舞台化粧は 同一人物でも洋物や日本物で目の描き方なども全然違うし 日本物だと見慣れているはずの顔も あれ誰     単純に私の顔判別能力が低いだけかもしれない   と混乱することがあるので 和洋混合の舞台化粧顔を使って学習したモデルで素化粧顔を判別させるとなると 一体どうなることやらと先行きが既に不安ですが 初めて作った機械学習モデルでどれぐらいの精度を出せるのか楽しみです   学習の流れ  ．スクレイピング  web自動化ツールの  selenium  を使って googleから組の男役トップスター花組：柚香光月組：月城かなと雪組：彩風咲奈星組：礼真琴宙組：芹香斗亜の写真を集めてきました 分で理解するSelenium柚香光  image png  月城かなと  image png  彩風咲奈  image png  礼真琴  image png  芹香斗亜  image png  皆さん素敵ですね   ´艸｀ 宝塚歌劇団公式HP スタープロフィール  ．学習 テスト用データセットの作成  顔の部分だけをトリミングした加工写真を作り トップスター名の舞台化粧顔と素化粧顔に分けてローカル環境に保存しました   ．モデル学習   学習用データ数が少なく   VGG   を使って転移学習を行いモデルを構築しました CNN Convolutional Neural Network とは 人間の脳の視覚野と似た構造を持つ  畳み込み層  という層を使って特徴抽出を行うニューラルネットワークです 以下引用 VGGは オックスフォード大学の研究グループが年に発表したCNNモデルです シンプルな構造ながらImagenetの画像認識コンペで位を取った高精度なモデルで 今もKerasやPytorchに学習済みモデルが用意されています   ．精度の検証  構築したモデルの完成度はいかに では 実際に書いてみたコードを順に載せていきます もしここはこう書くといいよ みたいなコメントを頂けたらとても喜びます…   実践    スクレイピング    前準備  初めに環境構築を行います    python  shellsudo apt  y update  ダウンロードのために必要なパッケージをインストールsudo apt install  y wget curl unzip  以下はChromeの依存パッケージ  Chromeのインストールでした ChromeDriverのバージョンは Google Chromeよりも数値が小さくなっているので問題なさそうです 最後にseleniumをインストール    python pip install selenium環境構築ができましたので スクレイピング開始です     Googleから画像を取ってくる  まずは必要なモジュールをインポート GoogleのURLを取得し ページの最下部までスクロールして要素を全て取得 要素のURLを保存 MAX      取れてくる写真の枚数が枚を超えることはなかったので固定これで Googleの画像検索で出てきた画像のURLを取ってくることができました 続いてディレクトリを作って URLから画像を取得し保存していきます    保存先  ディレクトリを作成写真は 舞台化粧顔と素化粧顔の写真 ＋その他雑多なもの が各トップスターのディレクトリに大体 枚ぐらい取得できました      学習 テスト用データセットの作成続いて モデルに学習させるデータセットとテスト用データセットの作成です     顔検出器によるトリミング  今回は顔の判別をしたいのですが 取得した写真には複数人が写っていたり 背景の割合が大きかったりノイズが大きい写真が殆どだったので openCVライブラリにパッケージングされているカスケード分類器 CascadeClassifierを使って 顔の部分だけをトリミング加工しました   顔検出器の初期化  ディレクトリを作成  フォルダ内の画像ファイルを取得  顔の部分を切り出し 保存      画像のパス      画像の読み込み      グレースケールに変換      顔検出      顔の部分を切り出し 保存ここで顔だけトリミングされた 月組トップスターの月城かなとさんを見てみましょう  トリミング前   image png   トリミング後   image png  …顔が良い 尊  トリミング処理をすることで顔ではない部分が切り出されることもありますが そういった写真は手動で省いていきます 今回のテーマは    舞台化粧顔で学習したモデルは 素化粧顔を判別できるか     という内容なので トップスター名ずつ 舞台化粧顔と素化粧顔に分けて ローカル環境に枚ずつ保存していきました  ここが一番地道な作業でした  学習用データとしてはかなり枚数が少ないのは重々承知ですが 柚香：枚月城：枚彩風：枚礼：枚芹香：枚のトリミングされた舞台化粧顔の写真を取得することができました 柚香さんと芹香さんは 顔検出器を追加でやってみたのですが うまく写真が集まりませんでした       データセットの作成  続いて 上で集めてきた写真を使ってモデル学習に使用するデータセットを作成していきます 学習用もテスト用も作り方は同じなので ここでは学習用データセットの作り方を下記に記します Google Driveにローカル環境に保存していたフォルダをアップロードし Google colabolatoryとGoogle Driveを接続しておきます   image png  セルの横に出てくるアイコンの 右から番目を押すとマウントされます 必要なモジュールをインポート  osとcvは上でインポート済み 各makeupディレクトリに保存した組トップスターの写真を 各人の名前の変数に格納 写真を×ピクセルにリサイズし 色調を調整してリストに格納    python  trainデータを作成するための空リストを変数にセット  柚香光  月城かなと  彩風咲奈  礼真琴  芹香斗亜 柚香光  月城かなと  彩風咲奈  礼真琴  芹香斗亜 と教師ラベルを付けた学習用データセットを作り データをシャッフル    python データセットの作成 シャッフルこれで 学習用データセットが完成しました 同じようにテスト用データセットも作成しておきます 完成したデータセットのディレクトリ構造はこちら 学習用データセットのディレクトリ構造テスト用データセットは 同じ構造でフォルダ名をnatural スター名としました 最後に 教師ラベルのone hotエンコーディング処理 男役トップスターは名いるので クラス分けはつです 後ほど作成するモデルを使って素化粧顔を判定させるときに           なら 柚香光          なら 月城かなと          なら 彩風咲奈          なら 礼真琴          なら 芹香斗亜という風に判定できるようにしておきます      モデル学習データセットが作成できたところで モデル学習に移っていきます 参考文献 最初にこちらで型を作りました 必要なモジュールをインポート モデルオブジェクトを定義し  add  メソッドでレイヤーを追加 VGGの重みが更新されないようにしておきます 学習のためのモデルを設定    python  最適化手法はSGDを採用     精度の検証 で作ったモデルを使って いよいよ顔の学習を実行していきます さて結果は…どーん    image png  accuracyは学習データに対する精度 val accuracyはテスト用データに対する精度を表しています Epochが進むとaccuracyはいったん上がり あとはずっと横ばいです val accuracyに至っては学習前が一番精度が高く出ています 結果として素化粧の顔に対する正解率は割弱という お察しクオリティのものが出来上がってしまいました これでは贔屓の顔もろくに判別できない状態です れいこちゃん 月城さん  ごめん…グラフとしてプロットするとこんな感じです   image png  ちなみに 転移学習をせずに普通のCNNで学習をしたときはこのようになりました   image png  Epochが進むに従ってaccuracyは上がっていますが val accuracyはずっと横ばいです つまり 舞台化粧顔に特化したモデルになってしまい 明らかに過学習状態であることが見て取れます この後精度を改善すべくmobilenetを使った転移学習も試みましたが 精度は特に変わらず   機械学習は難しいです mobilenetを使った転移学習の結果  image png  最後に 顔を判定する関数    pythonlabels dict      柚香光     月城かなと     彩風咲奈     礼真琴     芹香斗亜    まとめと考察初めて作ったモデルの精度はイマイチだったものの ゴールデンウィークの頃はpythonの基本構文もまともに書けなかった状態から か月で何とか機械学習をするところまで進むことができました モデルを作るにあたって色々調べたところ 機械学習界隈は進化が早く 次々と新しいモデルが出てくることを知りました 勉強のし甲斐がありそうです 今回の学習用のデータとテスト用のデータは 同一人物だけどちょっと毛色の異なるデータを使用していたので 距離学習なんかも面白そうだなと思いました 距離学習つの物が同じなのか 異なるものなのかを判定できるみたいです 最後まで読んで頂き有難うございました   アプリはこちら  トップスターの素化粧の顔を入れてみてください↓  ちなみにこの写真を入れてみたら   image png    image png  一応正解しました ただ 正解率は を切っているのでまぐれですね これからも勉強を続けていきます ,1,2023-07-27
118,118,世界最先端の画像処理モデルCoAtNETを自作してcifar10を解いてみた,画像処理,https://qiita.com/Mizuiro__sakura/items/b93ded51cf7224493f6d,こんにちにゃんです 水色桜 みずいろさくら です 今回は現在 年月日  世界最先端の画像処理モデルであるCoAtNETを自作してみようと思います 記事中で何か不明点 間違いなどありましたら コメントまたはTwitterまでお寄せいただけると嬉しいです ≧▽≦   はじめにまず今回作成したモデルの精度を示します 回の学習で の正解率となっています Vision Transformer ViT と同様に本来の能力を発揮させるためにはImagenetなどで事前学習を行う必要があるため 事前学習なしではこのくらいの精度しか出ません 今回作成したモデルのサイズは以下のような感じです Model   model Total params      MB Trainable params      MB Non trainable params      KB では早速CoAtNETについて解説していきます CoAtNETの主な特徴は以下のつです   前段に畳み込み層 後段にattention層を配置することで 局所的な特徴と 大域的な特徴を把握できるようにした点   DepthWiseConvolution フィルターが枚の畳み込み演算 を用いたMBConv Blockを用いることで計算量を削減している点   AttentionとはAttentionは 深層学習モデルに入力されたデータのどの部分に注目するのかを学習し 利用する仕組み のことです SeqSeqなどの従来手法では入力全体を最終的につの固定長ベクトルに詰め込んで表現するため 入力が長くなると内容を伝えるのが難しくなるという問題がありました それに対して Attentionはデコーダーにおいて 入力系列の情報を直接参照できるようにすることで 入力が長くなっても適切に内容を伝えることができるようにしました 入力全体の内容に加えて パッチをつつ出力する際に毎回 対応する入力系列のパッチを逐次的に考慮しながら変換します また 注目度も含めて深層学習の誤差逆伝播によって学習できます   Attentionの利点  こちら  を参考にしました   高い性能が期待できる 現在の世界最高精度クラスのモデルの多くはAttentionを用いている   高速に学習できる RNN は時刻tの計算が終わるまで時刻t の計算をできず GPU をフルに使えません Transformer は推論時の Decoder を除いて すべての時刻の計算を同時に行えるため GPU をフルに使いやすいです    構造が単純  Attentionの構造  こちら  を参考にしました   image png  Attentionの基本はqueryとmemory key  value  です Attentionとはqueryを用いてmemoryから必要な情報を選択的に抽出する仕組みです memoryから情報を抽出する際 queryはkeyによって取得するmemoryを決定し 対応するvalueを取得します   image png  例えば 食べ物というQueryに対して   いちご が   が が   好き が くらい という風に Keyはどこにどれくらい注目するのかを決定します 具体的な計算はQueryとKeyの行列積をとります 行列積をとった後 softmax関数にかけることでAttention weightが得られます Attention weightはvalueから情報を取得する際に どこにどれだけ注目するのかを示しています Attention weightとvalueの行列積を計算することで Inputのどこにどれだけ注目すればいいかという情報を持ったoutputを得ることができます   CoAtNETの実装ではさっそくCoAtNETを実装していきたいと思います まず今回作成したコード全体を示します   cifarをロードし ラベルをone hotベクトル化する  各種変数を定義するdim       相対位置エンコーディング後のサイズconv layer       畳み込み層のレイヤー数ここからはコードの解説をしていきます まずcifarのロードと 各種変数の定義を行います   cifarをロードし ラベルをone hotベクトル化する  各種変数を定義するdim       相対位置エンコーディング後のサイズconv layer       畳み込み層のレイヤー数つぎにモデルを作成していきます 最後にFC層を実装します   終わりに今回はCoAtNETを自作して cifarを解いてみました ぜひ参考にしてもらえると嬉しいです では ばいにゃん    参考,3,2023-07-27
120,120,UnityWebGLでjslibへ画像を受け渡すときの設定値,画像処理,https://qiita.com/kazuki_kuriyama/items/8efb7565175746665567,  はじめにUnityWebGLでWebフロント側 jslib へ画像を受け渡す際に 利用環境によっては他プラットフォームとは異なるTextureImportSettingsの設定を行う必要があるため記録を残します   ブラウザで発生するエラー要約すると以下の内容になっています   読み込み可能になってない  圧縮テクスチャが使えない  設定例TextureImportSettings　で対象のファイルを開き 以下の設定を行うとエラーが発生しなくなります   image png  以上,0,2023-07-24
121,121,Vision Transformer(ViT)を自作してcifar10を解いてみた,画像処理,https://qiita.com/Mizuiro__sakura/items/d5d872fa43633930638b,  はじめにこんにちにゃんです 水色桜 みずいろさくら です 今回はVision Transformerを自作してcifarを解いてみようと思います 解説にあたっては極力数式を用いずに解説するつもりです もし記事中で間違い 不明点などあればコメントまたはTwitterまでお寄せいただけると嬉しいです まず今回作成したモデルの精度を見てみましょう 回の学習で正解率 となっています 通常Vision transformer ViT はImagenetなどで事前学習を行うため 事前学習なしではこのくらいの精度しか出ません 現在 年月日  SoTAを達成しているモデルはTransformerとCNN Convoluional Neural Network の組み合わせで出来ています Vision Transformer ViT を理解することはこれらのモデルを理解する下地になってくれると考えます では早速Vision Transformer ViT について解説していきます   Vision Transformer ViT とは年にGoogleから発表されたモデル Vision Transformersの特徴は以下のつです   SoTA State of The Art を上回る精度を従来の約 の計算量で達成したこと   畳み込みを用いずにTransformerのみを利用していること   画像パッチ 画像を分割したピースのようなもの を単語のように扱うこと   アーキテクチャ モデルの構造 はTransformerのエンコーダ部分であること Vision Transformer ViT では入力画像をパッチに分割し Flatten 複数次元を持つ要素を一次元の要素に変換する処理 することで 一つ一つのパッチを単語のように扱います このパッチに位置エンコーディング パッチの位置情報 を付加したものが入力になります   Vision Transformer ViT のアーキテクチャ  image png   Vision Transformer ViT の論文  より引用Vision Transformer ViT のアーキテクチャは上図のようになっています まずパッチをFlattenし 線型射影 Dense：通常のニューラルネットワークのように全結合層をかませる します これをTransformerのエンコーダ部分に入力し 最後にMLP Headと呼ばれるモデルに入力します なおここでMLPとMLP Headが登場しますが 二つは似て非なるものなので注意して下さい   Transformer EncoderTransformerのエンコーダ部分はLayerNormalization 上図のNormに当たります  MultiHeadAttention MLPというつから構成されます LayerNormalizationはつのサンプルにおける各レイヤーの隠れ層の値の平均 分散で正規化します 詳しい解説は こちらの記事  をご覧ください MLPは簡単に言えば全結合層をつ繋げたものです 特に難しい点はないので割愛します   AttentionとはAttentionは 深層学習モデルに入力されたデータのどの部分に注目するのかを学習し 利用する仕組み のことです SeqSeqなどの従来手法では入力全体を最終的につの固定長ベクトルに詰め込んで表現するため 入力が長くなると内容を伝えるのが難しくなるという問題がありました それに対して Attentionはデコーダーにおいて 入力系列の情報を直接参照できるようにすることで 入力が長くなっても適切に内容を伝えることができるようにしました 入力全体の内容に加えて パッチをつつ出力する際に毎回 対応する入力系列のパッチを逐次的に考慮しながら変換します また 注目度も含めて深層学習の誤差逆伝播によって学習できます   Attentionの利点  こちら  を参考にしました   高い性能が期待できる 現在の世界最高精度クラスのモデルの多くはAttentionを用いている   高速に学習できる RNN は時刻tの計算が終わるまで時刻t の計算をできず GPU をフルに使えません Transformer は推論時の Decoder を除いて すべての時刻の計算を同時に行えるため GPU をフルに使いやすいです    構造が単純  Attentionの構造  こちら  を参考にしました   image png  Attentionの基本はqueryとmemory key  value  です Attentionとはqueryを用いてmemoryから必要な情報を選択的に抽出する仕組みです memoryから情報を抽出する際 queryはkeyによって取得するmemoryを決定し 対応するvalueを取得します   image png  例えば 食べ物というQueryに対して   いちご が   が が   好き が くらい という風に Keyはどこにどれくらい注目するのかを決定します 具体的な計算はQueryとKeyの行列積をとります 行列積をとった後 softmax関数にかけることでAttention weightが得られます Attention weightはvalueから情報を取得する際に どこにどれだけ注目するのかを示しています Attention weightとvalueの行列積を計算することで Inputのどこにどれだけ注目すればいいかという情報を持ったoutputを得ることができます   MLP HeadとはLayerNormalizationと全結合層 Dense を繋いだものです こちらも難しい点はないので割愛します   Vision Transformer ViT の実装ここからは実際に実装を行なっていきます まず今回実装したコード全体を示します お忙しい方はコピペして使ってみて下さい   学習率のスケジュールを定義する  cifarをロードし ラベルをone hotベクトル化する  各種変数を定義するdim       線型射影後のサイズまず準備として学習率のスケジュールを設定します   学習率のスケジュールを定義する次にcifarをロードし ラベルをone hotベクトル化 一つの要素がでそれ以外の要素がであるようなベクトル します   cifarをロードし ラベルをone hotベクトル化する次に各種変数を定義します   各種変数を定義するdim       線型射影後のサイズそしてモデルを定義していきます まず画像をパッチに分解し 線型射影します 次に位置エンコーディングを行います 次にMaltiHeadAttentionに入力します ここで次元の問題で次元拡張を行っています また残差接続も行っています次にMLPに入力します 元の論文とは異なりBatchNormalizationrとDropoutを挿入しています これによりより高い精度で回答することが可能になります 以上の操作をレイヤー数繰り返します 最後にMLP Headを実装します モデルの学習を行います   終わりにここまでVision Transformerを実装する方法について書いてきました この記事が皆さんのお力になれば幸いです では ばいにゃん〜   参考,7,2023-07-21
122,122,「このクーポンは使える？」AIが即座に答えるクーポン判別アプリを作ってみた！,画像処理,https://qiita.com/Rs_n_ishii/items/fe12c3ad424a27d53c22,  週間チャレンジ第弾   前回は週間でLINE Botを作成してみました 今回は   AIを利用した業務改善アプリ  を週間で作成してみました    週間でできたAIアプリがこちら   なぜ クーポンの判別 アプリを作成しようと思ったのか私が働く会社は数百店舗をフランチャイズ運営 管理する会社です 毎月多数のクーポンがいろいろな媒体から発信され 店舗からは このクーポンは使えるの   このクーポンは何  等の問い合わせが月に数十件寄せられます 特に管理している店舗数が多い担当者にはより多くの連絡が寄せられます… 現在発行されているクーポンは  種類…  電話で聞かれてもすぐに答えられません   そこでAIの力を借りようと思い立ちました    作成手順それでは作り方を説明します 今回は実際のクーポンではなく サンプルのクーポン種類を用意し 判別できるように学習させました     用意したクーポンと条件  AIクーポン画像 png  ①②は 使用可能　利用期限年    紙のタイプのクーポン③は 使用不可    紙のタイプ ②と色味が似ているものを用意④⑤は 使用可能　利用期限なし    スマホの画面提示    使ったもの⇒独自の画像 音声 ポーズを認識するようコンピュータをトレーニングします ⇒ブラウザ上でHTML CSS JavaScriptのコードを記述し リアルタイムで表示を確認しながら開発をすることができるサービスです たったこれだけです 想像以上に簡単でした     Teachable Macineの学習  AiTeachableMacine png  まずは各クーポンの写真を撮影し 学習データを作成します 項目を設定し 各クーポン枚ずつ程度撮影します 枚も  と思いますが パソコンのカメラで連写ができるのであっという間です    note info似ている色のクーポンはたくさんいろんな角度で撮影したほうがより精度が高まりました   AiTeachableMacine png  作成したモデルをエクスポートをして   p js  の方のタブを選択 コードをコピーします     CodePenの設定  codepen png  アカウントを作成し HTMLの部分に先ほどコピーしたコードを貼り付けます そして少し待つと…下の画面にカメラと判定結果が出てきます ほぼこれで完成です リンクをコピーすればスマホでもPCでも使用できました 実際のコードはこちらです   クーポン判別   on こちらの記事を参考に作成しました fumiyaさんの記事 CodePenの使い方 知らない人向け      作成してみて正直びっくりするくらい簡単で驚きました AIって何かわからないけどすごいもの くらいに思っていたので今回の挑戦でとても身近に感じました また 実際に職場の人に見てもらったところ  高齢の店舗スタッフでも簡単に使えるのがとてもいい   と言ってもらえました 今回はCodePenへの連携で終わりましたが LINEへの連携なども可能らしいので もっと実用的な挑戦をしてみたいと思います ,0,2023-07-19
123,123,1週間で請求書を判別できるAI作りに挑戦してみた,画像処理,https://qiita.com/yanxiuyonga/items/1cfaaa66539df5dbaf3f,   月末の請求書支払いが大変 仕事をしていると必ず関わることになるのが  請求書  ではないでしょうか 発行をしたり支払いの処理をしたりと 業務全体の工程数も多いです 私の働く部署も例外ではなく 月末が近づくと対応しなくてはいけない請求書が束で届きます   image png  請求書の量が多いと仕分けをするのも一苦労ですが 多忙な担当者だと請求書が届いたことすら把握できていないという人もいました そこで 簡単に請求書の種類を判別し 支払期日や担当者をリマインドをしてくれるシステムがあれば 情報の見落としや連携ミスがなくなるかもしれないと思ったのが作成のきっかけです    作ってみた請求書に見立てた付箋を画像で判別し 判別結果がLINEBotへ届きます LINEBotに転送される文字は Googleスプレッドシートから抽出されています  追記 Twitterに投稿した動画の縮尺が小さかったため修正しました    使用ツール   Googleスプレッドシート  各ツールの説明はリンク先の参考資料をご覧ください ＊MakeとLINE Developersについては私の 初投稿記事  もぜひご覧下さい   実際の請求書が手元になかったため わかりやすく項目分けをするために種類の付箋で代用しました 各付箋の写真は枚 枚ほど撮影し 正面だけでなく 引きの画像 や少しカメラからはみ出すくらいの大きさの写真も撮影しました   image png  種類の判別モデルがありますが プレビュー画面では採用費請求書の判定スコアが になっています  作成過程で変更した箇所 試作品を作成する過程で 請求書の項目名を英語に変更しました →日本語だと表記ゆれが起こる可能性があり 英語が推奨されるため     Node RED    ノードの全体像    image png  PCのカメラによる撮影を起点に Teachable Machineによる画像判定 機械学習 を行います   各ノードの設定を表示    プロパティの後ろの Paylord    部分には Makeと連携させた変数名を入力します   テンプレートは  payload  class  と入力します     この時入力文字の前後に不要な空白や  がないかチェックします    作成の過程で変更した箇所のメモ   シナリオの全体像    image png  Node REDでTeachable Machineの判定結果を受信して スプレッドシートの文面をLINEへ送信させます   WebhookのURLを新規のWebブラウザに貼り付け 変数名を登録します   今回の変数名は  shiharai  で登録しました    Googleスプレッドシート    Googleスプレッドシートの中身を照合します   通知用のLINEBotを作成し LINEBotに通知されるように変数を登録します    本来作りたかったモデル今回は勉強不足で実施できなかったのですが 今後は下記の流れに変更しレベルアップできたらなと思っています   Webカメラで請求書の画像を撮影  Googleスプレッドシートに写真の撮影日と資料名が記録され LINE経由でリマインドされる  LINEへ 対応済み と返答すると労りのメッセージが返ってくる　 毎月のルーティン業務も労りあって作業したいですね      まとめ作りたいと思ったものを自分で作成することができると 業務内容に変更があっても修正することができます 作って終了 満足ではなく 実際に役立つツールにするためにこれからも足りない知識を身に着けたいと思います  参考記事  ,3,2023-07-19
124,124,Teachable Machineという画像識別するシステムを使って、こんなのあったらいいなを作ってみたけど難しかった！,画像処理,https://qiita.com/kumiko_kimura/items/4238f4f9fb3cd265446b,      画像識別するAIを使ってみたので何かに活かしたいな〜画像識別AIというものを知る機会があったので それで仕事の役に立つ事は何かないかな と考えていて職場の人にも話をしてみたところ 販促用のロゴを識別できるものがあったらいいかも  という意見をもらいました 私の所属している部署では自社の販売促進用の資材に使用されているロゴや自社キャラクター画像などの資材を管理しているチームがあります パンフレットなどの資材は数百点あり作り直したりする時にはこの画像がどの資材に使われているか対象を絞り込むことがとても大変です これまでロゴなど探したい画像が入っているものを探すときにはキーワード検索しかできないので ※キーワードの打ち方で結果にバラつきがあるというから困るわけで 画像で対象の資材を探せるものを作ってみることにしました     やりたいこと  ：LINE Botから写真画像を送信    ：画像判定した結果によって対象の資材名を返したい      使用ツール  Google スプレッドシート         画像を学習させる画像プロジェクトを選択　→　新しいイメージプロジェクト　→　標準の画像モデルを選択  スクリーンショット       png    スクリーンショット       png    スクリーンショット       png  タイトル名を入力　→ ウェブカメラ　→　長押しして録画 PCのカメラが起動  →　画像を連写何パターンか撮ったら モデルをトレーニングする を押して待ちます※ロゴの代わりに私の下手な絵でやっていきます       スクリーンショット       png    スクリーンショット       png  トレーニングが終わると右側にプレビューが出てPCのカメラに先ほど学習させたものを写すと識別します  スクリーンショット       png  正しく識別できていることが確認できたのでこのイメージモデルを他のシステムで使えるようにします上の画像の右上青枠の モデルをエクスポートする を選択して　→　モデルをアップロード→　共有可能なリンクが生成されます  スクリーンショット       png    スクリーンショット       png  ※この共有可能なリンクは ：Node REDのTeachable Machine ノードのところで使用しますここで作成したモデルは Google ドライブに保存すると後で編集したりモデルを追加したりすることができるので便利です  スクリーンショット       png           Node REDの設定      ＜結果＞ やりたかったことができなかった  スクリーンショット       png    スクリーンショット       png  カメラで撮ったものは画像を識別してくれるがLINEBotから送った画像は受信すらしていない様子     スクリーンショット       png  色々調べてline imageというのを挟んでみたり   Node REDにこだわらずMakeやCodePenのやり方などを記事で知って試すもわからなかった   LINEBotから画像を受信してTeachable Machineで画像識別をした結果をもとに 対象の資材の名前を返すためにGoogleスプレッドシートで資材一覧を作成し API連携できるようにする ：Googleスプレッドシートを作成  スクリーンショット       png  ：GoogleスプレッドシートのURLをコピーしておく  スクリーンショット       png    スクリーンショット       png  ：API名を入力し GoogleスプレッドシートのURLを貼る  スクリーンショット       png  ：APIが作成出来たら API URLをNode REDで使用します  スクリーンショット       png  このあとNode REDに設定していくんですが やり方がまだできていないので今回の記事はここまでです Teachable MachineでモデルのエクスポートしたときのURLをNode REDのTeachable MachineノードのURL部分に貼ります  スクリーンショット       png        感想   実装までは至れませんでしたが 引き続き完成させるまでトライしたいと思います これが自社内で使えるようになったら対象を絞る 検索するといった作業が楽しくなるな〜と思いました 画像の読み込ませ方は 他にも検討の余地もあるかなと思うのでLINE Botのように投げかけたら返すという仕組み以外でも画像を識別してくれる機能自体はとても有効だと思いました この機能を後は何とどう連携させていくかということを職場の方達と理想やイメージを膨らませたいです ちょっとしたことの工夫が業務を行う上でのモチベーションUPにつながるかもしれない と うまく言い表せないけど何かの手応えを感じています 今の自分にはハードルがとても高くて現時点ではまだできていないけど 作ってみよう と挑戦したおかげで得られたものがあることを感じた ,3,2023-07-19
125,125,回答スピードアップ！写真を撮ったらその商品券が使用できるかAIでチェックできちゃう！,画像処理,https://qiita.com/izumiya_t/items/073c01d79366d05d1882,  AIってなんじゃ こんにちは AIという言葉をよく聞くけどいまいち理解していないまま今日に至ったOLです 調べてみると 人間の知能をコンピュータによって再現する技術のこと なんだそうで…ふ ん   仕事で活用出来たら便利 私は商業施設の運営管理という仕事をしてるため よくお客さまやショップスタッフさまから 〇〇色の〇〇って書いてある商品券使えますか  等と問い合わせがあるのですが 回答までお時間を頂いてしまうこともしばしば…ショップスタッフさまから 早く教えて  と言われると申し訳ない気持ち 大焦り しかも自分の作業が止まってしまう   質問してこないで  心の叫び   じゃあショップスタッフさまから問い合わせが来ないようにする 事務所スタッフがすぐ確認できる状態にしちゃえ ということで 最近教わった Teachable Machine を使用して商品券を写真で撮ったら使用できるか否かをその場で確認できるツールを作りたいと思います   使用ツール  作成方法について商品券の写真を送ったら その商品券が使用できるものなのか判断できるように商品券のデザインを学習させます   teach png  それぞれタイトルの欄に記憶させるものの名称 今回は商品券の名称 を入力し ウェブカメラから画像サンプルを学習させます    note warn色々な角度で撮影し学習させること   teach png  今回は種類の商品券を学習させました それぞれ学習させたら  トレーニング  を選択し そのままの画面で処理が終わるまで待ち 完了したら モデルをエクスポートする    モデルをアップロード  を選択 アップロードが完了したら  共有可能なリンク  にURLが表示されるのでメモしておく   teach png     note warnURLは後ほど使うのでメモを忘れないこと Node REDはプログラミングが分からなくてもプログラミングを作成できるもので それぞれの機能がアイコンで表示されるため分かりやすくなっています   全体図    kouseizu png  Node REDに下記つのノード ＝機能 を追加します teachable machineで学習させた内容を使用するための機能PCで撮った写真を読み込んでくれる機能読み込んだ写真をNode RED上に表示してくれる機能画面右上の本線からパレットの管理を選択   パレット管理 png   ノードを追加 を選択し 追加したいノードを検索して追加する   パレット管理 png   Teachable Machine をダブルクリックして URL欄にTeachable Machineを作成した際にメモしたURLを貼り付け   NodeRed png    NodeRed png   image preview のボタンを押すと写真が撮れて使用できる商品券か判断してくれるようになります ※カメラが正常に作動せず動作確認ができませんでした…涙  全体図    コメント     png  青字のURL部をコピーし chromeの新規ページへ貼り付け URLの最後に  syouhinnken JCB使用可能  と入力してエンターキーを押す   グーグル png  上記画像のように表示されたことを確認 そろそろ頭がパンクしそうです…私がコンピューターだったら頭から煙が出ていることでしょう…LINE  無題 png  Toの欄にLINE Developersから確認した ユーザーID を入力 Messageの欄には  この商品は．商品券  を入力   LINE png  頭がパンクしました   できた…のか できてませんできた と言いたいところですが Node REDでの作業の段階でカメラが作動していないため動作確認まで至りませんでした 成功していれば ↓ のような通知が来ていたはず   image jpeg  心半ば…悔しい…同じようなトラブルが起きた方 解決方法がわかるよ という方がおられましたらぜひコメントで教えてください   反省と今後Node REDでカメラが作動しない理由が分からず その後の作業に支障が出てしまった…仮にカメラが作動していたとしてもスマホだけで完結していないため 手軽さに欠けている…ショップさまで使用されているタブレットで商品券の写真を撮る→使用可否がその場ですぐ分かる という流れが理想なのでまだまだ修正が必要です 今回 作成する中で躓いてしまう部分も多くありましたが Teachable Machineを活用すれば画像認識や音声認識のシステムを自分でも扱うことができるのか と驚きが強かったです 頭から煙が出そうだけどまた新しいデジタルツールを 少しだけ 使えるようになった… わくわく,4,2023-07-19
126,126,Qiita に画像を貼る方法,画像処理,https://qiita.com/nanbuwks/items/e0c9f02c1b556dde140a,Python で ファイルを OSファイルマネージャで開く PNG に編集ソースファイルを埋め込んで再編集可能にする そのでは  Qiita に貼る PNG 画像ファイルに作成ソースファイルを埋め込んで再編集可能にしました Qiita に画像を貼る方法として   ファイルドラッグ ドロップ  ファイルコピー ペースト  画像コピー ペーストがあります 再編集可能な PNG 画像ファイルを扱うにあたり Qiita への画像を貼る方法を調べてみました   環境Ubuntu   LTS    ファイルドラッグ ドロップファイルマネージャなどでファイルを表示して   image png  ドラッグドロップするやりかた    ファイルコピー ペースト同様にファイルマネージャなどでファイルを表示してコピー ペーストするやりかたです    画像コピー ペースト例えば Twitter などから画像をコピー ペーストします   image png  上のようにしてコピーしたものをQiita に貼ったものが以下になります   image png  LibreOffice などでもこのようにしてコピーして  image png  Qiita に貼り付けすると以下のようになります     image png    段階	形式  LibreOffice	表  クリップボード	ビットマップとテキストの複合  画像としての貼り付け先	ビットマップとテキスト  現在は ヘルパーを実行する必要がありますが ヘルパーを実行するとファイルが表示されるので     image png    これを Qiita に貼ります   pngtest png  これは 先のファイルのコピー ペーストとは異なり ディスプレイに表示された画像をコピー ペーストするものです ローカルPC でなくとも Web サービス上でのコピー ペーストがファイルレスで行うことができます ディスプレイ画面キャプチャー 貼り付けも これと同じとみなすことができます 画像コピー ペーストでは 元画像が jpg だったとしても Qiita に貼ると PNG 形式になるようです   画像ファイルの属性情報を保持しながら Qiita に画像を貼るjpg ファイルには EXIF 情報などが含まれます PNG にもチャンクとして独自情報を含むことができ 再編集可能な PNG ファイルはそのようにして作成しています 以下は編集ソースが含まれている PNG ファイルですが 上記の コピー ペースト で情報は保持されるでしょうか   pngtest png  結論から言うと 上記    のやりかたでは保持されて  のやりかたでは情報は失われてしまいました   クリップボードを使って画像を格納 Qiita に貼る仕組み    のやりかたは以下の資料から推察するに ウィンドウマネージャの機能としてクリップボードにファイルパスとファイル属性が格納されるようです 一方   のやりかたではクリップボードにビットマップデータが格納されるようです Webブラウザはクリップボードから画像を貼るのは 以下のメソッドを使ってデータを取得していると推測します   画像ドラッグ ドロップのように見えてファイルドラッグ ドロップUbuntu の標準画像ビューア eog にて画像を表示させている状態から 画像を右クリックでコピー ペーストしたり マウスでドラッグ ドロップでも画像ファイル属性情報は保持されました ここでは eog からウィンドウマネージャの機能を使って クリップボードにファイル情報を格納している すなわち   および   と同じやり方なのだろうと思います   Qiita に再編集可能な PNG を貼る方法を考える再編集可能な PNG を属性を失わずに貼るには 再編集可能な PNG を OS 上のファイルとして作成し     または   のやりかたでコピー元をクリップボードに登録し  Webブラウザで Qiita に貼る必要があります このためには 以下の手順を行います   ツールで 再編集可能な PNG ファイルを OS のファイルシステム上に作成する  それをクリップボードにファイルとして格納する  クリップボードのファイル属性データを Webブラウザ上の Qiita に貼るいちいち手でそれをするのは利便性が悪いので これを自動化してみます    ツールで 再編集可能な PNG ファイルを OS のファイルシステム上に作成する汎用ツールを作成してしまえば すべての png ファイルに どのようなデータを格納もできますが 利便性を高めるためには画像描画ツールに 再編集可能PNG を作る機能を含めてしまうべきでしょう LibreOffice については以下のように 現在開発中です  PNG に編集ソースファイルを埋め込んで再編集可能にする その    クリップボードにファイルとして格納する上の画像描画ツール上の機能で作成したファイルをクリップボードに格納するのを自動化しようと考えましたが改めました これは Linux 上だと環境の違いにより動かなくなるリスクが高いからです 次善の策として OS 上のファイルマネージャなどで以下のように作成したファイルを表示し それをコピペやドラッグドロップを手動でする方法にしました   image png     コマンドで画像ファイルを Qiita に貼る こちらも先のコピペやドラッグドロップを手動でするという前提で ここは自動化しないことにしました    ファイルを OS のファイルマネージャで開くファイル  tmp embeddedpng 中に embeddedpng png ファイルだけを作成しそれをファイルマネージャで表示します またはまたはまた 他の OS だと以下のようになるそうです 未検証  またはまたは   ファイルを OS 上で関連付けられている画像ビューアで開くOS 標準の画像ビューア上だと 画面表示のイメージをコピペないしドラッグドロップでファイルとしてクリップボード操作が行われるようです なおユーザによってはクリップボード操作が期待するものでない設定になっていることもあるでしょう 上記のファイルマネージャで開くやりかたと併用が望ましいでしょう Python を使った例です     Ubuntu Linux の場合Python を使った例です またはまた 他の OS だと以下のようになるそうです 未検証  ,3,2023-07-19
127,127,地味に面倒くさいアレルゲンチェック【Teachable Machineを使ってみた。】,画像処理,https://qiita.com/dx_kumagai/items/7fa7ca14c6af6c6a9d15,   Teachable Machineで簡単アレルゲンチェッカーおはこんばんにちは 回目の投稿になります 猛暑だったり 大雨が降ったり今日も地球は忙しそうですが 皆様いかがお過ごしでしょうか 私は出張先でこの記事を書いております 帰りたい 挨拶はこのぐらいにして 今回は    商品を手に取らずにアレルゲンチェックができるアプリを作ってみたい     ということでチャレンジしてみました    なぜアレルゲンチェッカー 皆様はお買い物をするときに   アレルゲンチェック   をされていますでしょうか 私は生まれつきアレルギーがあるので あやしいと思うものは原材料の表示を必ず確認します 特に大変なのは野菜ジュースやスムージーなどで 混ざっている種類が多ければ多いほど 自分のダメなアレルゲンが入っているかどうかを確認するのが面倒くさいです このアレルゲンの表示や原材料の表示は 基本的に   商品パッケージの後ろ側   に書かれていることがほとんどですよね だから 商品を手に取ってみないとわからないんです しかし コロナが流行りだしてからというもの     一度手に取った商品を棚に戻す    という行為に抵抗を感じるようになりました コロナの流行はだいぶ落ち着いてきていますが 予防が必要なことは今後も変わらないと言えます     商品を手に取らずともパッケージやバーコードからアレルゲンチェックができればもっと買い物しやすくなるかも    そんな思いで簡単なアレルゲンチェッカーを作成してみました    使用したツール   Teachable Machine   では画像や音声をAIに認識させ 学習させたサンプルを基に判断させることができます 今回の場合は アレルゲンチェッカーですので商品の画像を学習させます 代表的なアレルゲン    卵 乳 小麦を含むか 含まないか    を判断させるパターンと 商品が映っていない場合の    該当商品なし    を含む全パターンで作成しました        サンプル画像の取り込み   サンプルとなる商品の画像を撮影していきます この時 商品のパッケージだけでなく 商品のバーコードもサンプルに混ぜてみました 最後に 該当商品なし として背景のみを撮影しています これがないと 何も映っていない時でも一生懸命アレルゲンを含むか含まないか判断しようとします AIちゃんは頑張り屋さんですね  何も映ってないよ ときちんと教えてあげましょう ＊背景と言いつつがっつりぬいぐるみが映ってますが今回は   該当商品   が映ってなければいいので無視します        AIに学習させる   サンプルの撮影が終わったら このサンプルたちをAIに学習させます 画面真ん中に    モデルをトレーニングする    というボタンがあるのでここを押します        AIが学習できたかチェックをしてアップロード  トレーニングが終わるとカメラの映像と その下にAIが判断した結果がパーセンテージで表示されます 試しに商品をカメラに映して正しく判断できているかをチェックします 正しくない場合はサンプル数を足してあげるといいかもしれません チェックが終わったら     モデルをエクスポートする    を押します するとこのような画面が出てきますので     モデルをアップロード    を押します 共有可能なリンクが表示されたら  モデルを使用するコードスニペット の    p js    を選択してください こちらの下に書かれているコードを後で使用しますので コピーしてどこかにメモしておくか 画面を閉じないようにしておいてください Teachable Machineの作成は以上です    CodePenの設定   CodePen   とはブラウザ上でコードを記述し 自分で開発ができるWebサービスになります 今回はTeachable Machineをスマートフォンでも使用できるようにするため CodePenを使用しました CodePenで作成画面を開くとつの箱のような表示があります 使用するのは番右にある    HTML    というところです ここに先ほどTeachable Machineの作成でコピーしたコードを貼り付けます コードを貼り付けると 画面下にTeachable Machineと同じようなカメラの映像と結果が表示されます        画面の表示を切り替え   Teachable Machineの動作を確認後 画面右上にある四角いボタンを押します するとメニューが表示されますので     Full Page View    を押します Full Page Viewにするとコード入力の画面が非表示になり カメラの映像と結果の表示だけになります このページのURLをスマートフォンで検索すると スマートフォンのカメラを使用してアレルゲンチェッカーを使用することができます CodePenの作成は以上です    反省 まとめとりあえずは簡単アレルゲンチェッカー完成です サンプル数をどんどん追加していけばもっと活用できそうです ただ     該当商品なし    に関しては背景だけを映すと    その景色＝該当商品なし    になってしまうため 商品が    ない    ことを覚えさせるというのは難しいのかもと感じました 本当のことを言うと  前回のLINEBot作成  を活かして     LINEから画像を送って認識させ 結果を返信する    ということをやりたかったのですが 今の私ではまだまだ力不足でした… 上の半透明になっている部分は まずNode REDでLINEと連携し メッセージのやり取りができるかを確認する為に オウム返し を作成しました メッセージのやり取りが確認できたため 無効化しています 問題になっているのがおそらく以下の部分です   Node RED png  LINEで画像を送った時点で止まってしまうため Teachable Machineが画像として認識出来ていないのだと思います cameraと接続したときにはきちんと動いていたため LINEとTeachable Machineの間に何かしら必要なのだと考えているのですがそれが何なのか分かりませんでした… 難しい… LINEBot作成 Node RED については以上です また 画像認識で解決できそうな課題で他に思い付いたのが 食料品の限度日検討日チェックです お店に並んでいる食料品には 限度日 検討日というものが設定されており 定期的にチェックを行います が これが本当に大変な作業なのです 人員不足時の応援で何度か経験しましたが 商品によって期限が違うし 頭で日付を計算するのも面倒くさいし 商品を一つずつ確認しなくちゃいけないし… 実際日付の認識がどこまで学習させられるのかはわかりませんが これがカメラでパパッと判断出来たら楽だろうなぁと感じました 今回は以上になります 最後までお付き合いいただきありがとうございました ,7,2023-07-19
128,128,"Unet, VAE+Unet, Dncnnを用いて、ガウスノイズ画像を復元してみた",画像処理,https://qiita.com/megane-9mm/items/bb3b2896c450a4abb3cd,  はじめにピンボケ画像の復元をしたいと思い いくつか検証を行ったので記事にしました ピンボケ画像は 一般的にはガウシアンフィルタ ぼかしフィルタ に近似できるとのことで 当初はフーリエ変換を用いた方法 ウィーナフィルタ を検討していました しかし 撮影環境が変わったりピンボケの拡がり方が多様な場合は 汎化性能的に深層学習の方が優位性があるかなと思い 深層学習のモデルを用いた検証を行いました 調べてみると Dncnnなどノイズ除去目的のモデルがあったため dncnn含め以前作成したUnetとVAE Unetを用いました 加えて モデルは復元させる綺麗な画像自体を学習するよりもノイズを学習しやすい傾向があるとのことで  stable diffusionも考え方は似ていますよね こちらもノイズを正規分布として仮定しているため 考え方はほぼ同じな気がします  Unetの出力をノイズを学習させるよう  出力   入力   デコーダーの出力 としました また 精度検証にPSNRを用いて比較を行いました   環境pytorch     cugpu NVIDIA GeForce GTX anaconda※パッケージは多々用いていますが ここでの記載は省いています 詳細はソースコード内を見て頂けると幸いです   データセットについて前回同様 学習データは下記を使用しました   学習時の設定画像サイズ       その他 各パターンのコードを載せるので そちらを確認頂きたいです   Dncnnについて下記参考にさせて頂きました 構造を見ると畳み込みとバッチ正規化を繰り返す単純なモデルではありますが 出力の形状が一番のポイントだと思います こちら  にも書かれていますが 綺麗な画像を復元するように学習するよりもノイズを学習させることで精度を向上させているようです 確かに 直感的には画像自体よりもノイズを学習する方が複雑度が低い気はしますね  今回は このノイズを学習させるという考え方をUnetのモデルにも適応させてみました  単純に ノイズ画像を入力として最後の層の出力をノイズと仮定し 入力とノイズの差分を最終的なモデルの出力とします この出力と綺麗な画像を損失関数に与えることで 出力を綺麗な画像に寄せるように学習する つまり最終層の出力はノイズを学習するようになります   PSNRについて下記参考にさせて頂きました 類似度の指標であり MSE以外は定数になります そのため MSEが小さい つまりつの画像の差が小さい場合はPSNRが大きくなります 逆に MSEが大きい つまりつの画像の差が大きい場合はPSNRが小さくなります よって 今回はPSNRが最も大きくなるモデルを確認すればいいことになります  勿論 オリジナルと復元画像も確認します    ソースコード 共通部分下記 パッケージです 下記 データローダーです オリジナル画像と ガウシアンフィルタを掛けた画像を出力するようにしています            を  に変換          ぼかした画像の作成また psnrの計算部です 前回と同じものを用いています Unetのモデルです 出力変更版はコメントにあるように forward部のみ変えています       Unetの出力変更版こちらも 前回使用したものと同じです           モデル定義時にgpuに渡しているが 何故かここでエラーが生じるのでepsをgpuに渡しているこちらを使用させて頂きました ※出力箇所を入力とデコーダー出力との差分にすべきでしたが忘れていました     結果テスト画像のオリジナルとガウスフィルタを掛けた画像です オリジナルガウスフィルタUnetの復元画像Unet input   デコーダ出力 の復元画像一番いい結果だったのですが 推移のデータを上書きしてしまいました   PSNRは近くまで上がりました   追記今度は ガウスノイズをランダムに生成させてみました ガウスノイズをピンボケと仮定する場合 ノイズが一定になる場合はほぼないと考えられます そこで ガウスノイズをランダムで生成しても上手く復元できるかを確認しました モデルは unet vqvaeを使用しました 出力は input   デコーダ出力でノイズを学習させる方法を取りました  vqvaeについては どこかで書ければと思います 下記 参考にさせて頂いたサイトです   vqvaeはvaeと構造はほぼ同じで エンコーダーで抽出した特徴を異なる空間に変換し デコーダーでその変換後の特徴を復元する流れになっています vaeとの違いは vaeは正規分布を潜在変数としていましたが vqvaeは一様分布のようになっています エンコーダーで抽出した特徴と近い ユークリッド距離の近さ 潜在変数を選択します ※ユークリッド距離とコサイン類似度についてトランスフォーマーのQKVと考え方は近いと思います では 結果を記載していきます オリジナル画像ランダムにガウスフィルタを掛けた画像復元画像  まとめ結果的には VQVAE   Unet input   デコーダ出力 形式が最もオリジナル画像に近くなりました PSNRも近くと 他の結果と比べてもいい結果になりました input   デコーダ出力形式の方が復元結果が良かったことから 冒頭でも述べたようにモデルはノイズを学習するように設計した方が ノイズ除去に関しては良い精度になる可能性が高そうですね 加えて VQVAE形式の方が通常のUnetよりもいい結果になりました QKV形式を用いるトランスフォーマーが良いといわれる一因なのでしょうか Dncnnもinput   デコーダ出力形式を試すべき というより 正しい形式がこちらですかね  ですが Unetに比べて学習に時間がかかることから 今回は考察までに留めておきます   終わりにピンボケ画像の復元を目的に本記事のような検証を行いました ノイズ除去モデルをエンコーダーデコーダー形式で行う場合は 出力形式を input   デコーダ出力 にした方が良い結果でそうだと分かりました また PSNRも画像復元の指標としては中々使えるなと感じました VQVAEの記事を書いてからVQVAE   Unetの結果を載せるべきですが 先に載せてしまいました transformer関連も色々試しているので 順番は適当で書いていくと思います では また次の機会に ,0,2023-07-15
130,130,Elixir Image でノイズ除去,画像処理,https://qiita.com/RyoWakabayashi/items/52e2e22ae01fea313a47,   はじめにElixir Image に    で追加されたノイズ除去を Livebook で実行 視覚化してみます実装したノートブックはこちら   セットアップ必要なモジュールをインストールします   画像の準備  home livebook vix noise jpg  のパスに以下の画像が保存されているものとします  noise png  この画像を  Image open  で開きます   ノイズ除去 Image reduce noise  で中央値フィルタによりノイズ除去を実行します第引数  window size  でノイズ除去の強さを制御します デフォルト    スクリーンショット       png  左が元画像 右がノイズ除去後です小さい点々がなくなっています   まとめかなり簡単にノイズ除去が実装できました,12,2023-07-10
131,131,三角測量のPython実装,画像処理,https://qiita.com/Hiroaki-K4/items/3328086967839b71bab4,   About Triangulation同じシーンを台のカメラで撮影する場合 それぞれのカメラの位置 向き 内部パラメータがわかっていれば 対応する点の次元位置を復元することができます これは それぞれのカメラのレンズ中心を始点とする その点の視線を求めることができ 台のカメラのレンズ中心とシーン内のその点を結ぶ三角形を計算することができるからです これは三角測量と呼ばれています    Camera Matrix and Triangulation画像の x y に投影されるシーン内の点 X Y Z は 透視投影のモデルから次のような式で表されます fはスケール定数で Pij i    j       はカメラの内部パラメータと運動パラメータから定まる係数です 式  は次のように書き換えることができます 行列P  Pij はカメラ行列と呼ばれます 三角測量を行うには あらかじめカメラ行列Pを計算しておく必要があります いわゆるカメラのキャリブレーションです つのカメラ行列をそれぞれ P  Pij  と P′  P′ij  とすると シーン中の点 X Y Z を各画像上の点 x y   x  y  で観測したとき 誤差がなければ  x y   x  y  から次のように X Y Z を計算することができます 式  の分母を払って整理し 台目のカメラについても同様にすると X Y Zについて以下の連立一次方程式が得られます これはつの未知数X Y Zに対するつの方程式であり それらは互いに線形従属で 独立なものは個です したがって これらつのうちどれかつを抽出して解を計算すれば 残りの方程式は自動的に満たされます しかし これらを同時に解いても同じ解が得られます 具体的には 式  の行列Tとベクトルpを用いて 式  を次のように書いて 両辺にTの転置を掛けることで 式  を解くことができます これは式  に最小二乗法を適用したことと同義です xで微分すると 以下の式が得られます    mathA \intercal Ax A \intercal b   Triangulation from corresponding points with errors観測点 x y   x  y  に誤差がある場合 台のカメラで定義される視線は必ずしも点で交わるとは限りません この場合 補正する方法があります 合理的な方法としては 観測された対応点 x y    x  y  を視線が交わる位置まで画像上の座標を補正して次元位置 X Y Z を計算することです 視線が交わるための必要十分条件はエピ極線方程式であり 基礎行列Fはカメラ行列P P′から求められます 観測された対応点 x y   x′ y′ から式  を満たす位置 xˉ yˉ​   xˉ′ yˉ​′ への最短の補正を考えます 以下の移動距離の二乗和を最小化します    mathS  x \bar x     y \bar y     x\prime \bar x \prime    y\prime \bar y \prime   ag  このSは再投影誤差と呼ばれます ここまでの処理のコードは こちら  です 以下のコマンドで三角測量が試せます    bashpython triangulation pyテストデータには 以下のつの視点のカメラ画像を使用しました ,0,2023-07-10
132,132,Elixir Image でコントラスト調整、鮮明化,画像処理,https://qiita.com/RyoWakabayashi/items/c44200bcbda0552c91aa,   はじめにElixir Image に    で追加されたコントラスト調整 鮮明化を Livebook で実行 視覚化してみます実装したノートブックはこちら   セットアップ必要なモジュールをインストールします   画像の準備  home livebook vix normal jpg  のパスに以下の画像が保存されているものとします  normal jpg  この画像を  Image open  で開きます   コントラスト調整 Image contrast  でコントラスト調整します第引数  contrast  が　  より小さいとコントラストが弱まり    より大きいとコントラストが強まります  スクリーンショット       png  左が元画像 中央がコントラスト  右がコントラスト ですコントラストを弱めると 全体的に白いもやがかかっていますコントラストを強めると 影が強調されてハードボイルドな雰囲気が漂います他の方法も試してみましょう  スクリーンショット       png   Image apply tone curve  は どの明るさの場所を強調するか を指定できます shadows     で影を弱め   mid points    で普通の明るさを強調し   highlights    で明るい場所を特に強調しています全体としては強い光が当たったような印象になりました Image local contrast  は局所的にコントラストを調整し 明るいものの近くにある影を強調する   鮮明化  スクリーンショット       png  左が元画像 右が鮮明化した画像です全体の色調や明るさは変えずに ボヤッとしていたところがくっきりしています   まとめこれでピンボケ写真をキレイに加工できますね,10,2023-07-10
134,134,Elixir Image でヒストグラム平坦化（明るさ調整）,画像処理,https://qiita.com/RyoWakabayashi/items/06860e5dac1d5513b3d8,   はじめに以前   evision   でヒストグラム平坦化を実行しました今回は Livebook で Image によるヒストグラム平坦化を実行 視覚化します実装したノートブックはこちら   セットアップ必要なモジュールをインストールします   画像の準備  home livebook vix dark jpg  のパスに以下の画像が保存されているものとします  dark jpg  暗くて何が写っているか見えませんねこの画像を  Image open  で開きます   ヒストグラム平坦化ヒストグラム平坦化を実行し 結果を並べて表示します  スクリーンショット       png    左上  元画像  右上  デフォルト 第引数 bands     all   で実行した結果  左下  bands     each  で実行した結果  右下  bands     luminance  で実行した結果右上 右下では明るさが補正され 何を写っているか分かるようになりました第引数 bands の値については以下を参照 all では最も暗いところが真っ黒 最も明るいところが真っ白になるよう 色の値を引き伸ばします each は RGB 毎に値を平坦化します 今回の例ではほとんど効果がありませんでした  luminance は輝度だけを平坦化します   まとめこれで暗い画像や明るすぎる画像も見やすく加工できますね,10,2023-07-09
135,135,Elixir Image で射影変換（台形補正）,画像処理,https://qiita.com/RyoWakabayashi/items/d96a79c08509472eb6b4,   はじめにElixir Image にバージョン    で射影変換 台形補正 の機能が追加されました以前の記事では Evision の形式に変換してから Evision で台形補正し  Image に戻す というようなことをしていましたが その必要はなくなりました実装したノートブックはこちら   セットアップ台形補正を行う場合   Nx   が必要になるため 一緒にインストールしておきます   elixirMix install      image              nx              req              kino            画像の準備適当な画像をダウンロードします  筆者近影  射影変換の効果がわかりやすいようにテキストを用意します  スクリーンショット       png  テキストを画像に重ねます  スクリーンショット       png     実行  第引数  変換対象の画像  第引数  変換前の四角形の各頂点座標  第引数  変換後の長方形の各頂点座標  第引数  オプション注意点としては 第引数は任意の四角形ではなく 長方形でなければいけません本来は台形補正に使う目的なので 変換前の画像で斜めに写っているものを長方形に補正する想定だからです今回はわざと台形に歪ませるので 変換前の四角形を左右にはみ出させています  スクリーンショット       png  確かに台形に変換できましたオプションで  background   blue  を指定したため 背景が青くなっています   まとめElixir の画像処理まわりは今まさに発展途上なので どんどん機能が追加されていきますね,9,2023-07-09
136,136,CNN+ViTモデルの傾向【サーベイ】,画像処理,https://qiita.com/wakayama_90b/items/96bf5d32b09cb0041c39,  画像認識モデルの強　CNN vs ViT画像認識分野において スタンダードとなるつのベースモデルがある．それは CNNとViT．今回は CNNとViTの違いと組み合わせモデルについて紹介する．      カーネル範囲で特徴抽出          局所的な認識に有効→エッジなどの低レベル特徴を認識      ImageNetで高精度化          様々なタスクにおいて高性能      画像をパッチに分割し全体の関係性を捉える          浅い層から受容野が広い→高レベル特徴を認識      超大規模モデルで高精度          Transformerベースの可能性は無限大  形状 CNNとViTはテクスチャ重視 形状重視  CNNとViTのモデル構造は全く異なるため それぞれの持つ認識特性も異なる．異なる認識特性を可視化するために 下図の画像を 猫 か 象 と答えるかで分析する． 象 と答えたモデルは認識対象のテクスチャを捉え  猫 と答えたモデルは認識対象の形状を捉えることになる．分析結果として CNNは 象  ViTは 猫  人は 猫 の結果になった．この結果から 局所的な認識のCNNはテクスチャに依存し 大局的な認識のViTは形状に依存することが分かる．  ノイズ CNNとViTノイズへの頑健性について CNNとViTは認識特性が異なるため 認識する画像によって得意不得意な画像がある．分析の結果 CNNは高周波を認識 ViTは低周波を認識する特性がある．そのため 画像全体に高周波ノイズを付与した場合 高周波認識を行うCNNに影響があり 精度を低下させる．反対に 低周波ノイズを付与した場合は ViTの精度が低下する．下図は縦軸に分類精度 横軸で右にいくほど高周波のノイズを付与していることを示す．CNNとViTは認識特性の違いから ノイズの強さ特性も異なる．また 画像に自然ノイズ 敵対攻撃などを加えノイズへの頑健性を調査した．結果として 有効なモデルがあるわけでななく 引き分けの結果である．  ViTの問題点ViTの大きな問題点として以下の点がある．これらを解決するように派生モデルが誕生する．ネタバレ→  解決策のつとしてViTの構造にCNNアーキテクチャを導入する．    計算量が膨大      入力特徴量を畳み込みによりダウンサンプリング      キー バリューをダウンサンプリング      SAの計算範囲を制限  細かい認識が苦手      CNNの持つ局所的な認識能力を獲得  ViTはImageNetで精度向上しない      CNNの持つ 近い画素は関係が深い バイアスを獲得   ViTはImageNetで精度向上しないViTの問題点のつとして JFT Mで学習した場合に精度向上するが ImageNetで精度向上しない問題点がある．実社会にViTを導入する場合 億枚の画像を保有するJFT Mを用意することは 現実的ではない．データセットが小さくても有効なモデルが理想である．そこで JFT Mで学習したViTと ImageNetで学習したViTの違いを調査した．下図に各ViTのAttention weightと平均距離を示す．FT MのViTは浅い層に局所的な認識 深い層に大局的な認識をする．一方 ImageNetのViTは大局的な認識のみである．ImageNetで精度向上していない原因は浅い層で局所的な認識ができていないことと考える．これを解決するために ViTのモデル構造を改良し 局所的な認識ができるViTを実現しようとする．具体的な方法として CNNを導入するかSAの計算範囲を制限する．また 学習によってロスが最も小さくなるような重みパラメータを探索する 局所的最小値に坂を使ってボールが転がり落ちるような表現 ．下図は そのロス関数をDに可視化したもので ResNetは凸損失 ViTは非凸損失である．非凸損失はデータの頑健性が高いメリットがあり ボールを落とす速度のように損失を小さくするためのスピードが遅く 大規模な学習と平滑化手法で問題を解決する．Loss landscape visualizationsはViTのような非凸損失になることを目指しているが ViTには負のへシアン固有値がある問題点がある．  CNN   ViTモデルCNN   ViTモデルの先駆けとして Hybrid ViTがある．Hybrid ViTはViTの原論文で紹介され ResNetのステージまでの認識を行い．その出力された特徴量をViTの入力とした．その結果 バニラのViTと比べて 少量のデータセットで精度向上した．また 分析の結果から VITの浅い層ではCNNの同じような特徴表現をしていることが分かった．この浅い層の局所的な認識をCNNにやってもらおうとする試みから CNNの出力をViTの入力とする構造を採用するモデルは多い傾向にある．      浅い層で局所的な認識→細かい認識      特徴量のダウンサンプリング→計算量削減    キー バリューをダウンサンプリング        計算量の削減      SAの出力はクエリのサイズに依存      ダウンサンプリングは畳み込み層でやらなくてもいい．poolingでも．なんでも      畳み込みとSAの認識特性を交互に獲得    SAの計算範囲を削減        計算量の削減      局所的な認識をするSA    階層型        ViTは浅い層から深い層までのパッチと受容野の変化がない      階層型モデルはステージが上がるにつれてself attentionの受容野とパッチの受容野が変化 広がる   モデル一覧各モデルの解説リンクあります．  ppaas png    Vision Transformerのしくみ  パッチ特徴はConvで抽出するViT Early Convolutions Help Transformers See Better   ViTから近傍バイアスを追加 ConViT   SA内に畳み込みやノルムや活性化関数を導入 LeViT   ViTのキーとバリューを小さくして計算量削減 PvT   ViTのキーとバリューを小さくするPVTの進化 PVTv   ViTのKとVをPyramid Poolingで計算量削減 PT   ViTに局所的バイアスを持つCNNを追加 CeiT   ViTの線形射影を畳み込みに変更したら苦手克服 　 CvT   ViTにCNNをたくさん組み込んだモデル CMT ,36,2023-07-01
137,137,基礎行列から透視投影行列を求める,画像処理,https://qiita.com/ground0state/items/d5ee2f47d2d54b5724d2,   概要前回の続きです カメラの内部パラメータが未知の場合 F行列の自由度がに対して 内部パラメータの自由度が 回転と並進のパラメータがあるので F行列から全てのパラメータを求めることはできません ただし カメラのカメラ座標をワールド座標に一致させることで ワールド座標からカメラ座標への射影行列を求めることができます    F行列の再導出透視投影行列を次のように書きます  X  が正則であることを仮定しす 射影は次のようになります F行列は次のように表せます これがF行列の別の形式による導出です F行列を定義する際に 定数倍の任意性があります ここで  F は f     になるように定数倍し 次の式で定義します スケールを固定したことの意味を確認します  \boldsymbol k  の定義式に  ilde \boldsymbol e  \   imes を左からかけて 次の式を得ます  ilde \boldsymbol e    は同次座標のため値が定まっています したがって  F のスケールを変更することは カメラ座標の相対的なスケールを決める自由度を固定したことになっています    カメラパラメータの推定さて  F が与えられたとき  P   P  を求めることができるか考えます まず  F を特異値分解します  ilde \boldsymbol e    は最小特異値に対応する左特異ベクトル \boldsymbol u   に比例します  ilde \boldsymbol e    は同次座標なので  \boldsymbol u   をその第成分で割ることで求めることができます この式から X  を次のように書くことにします  \boldsymbol b  は任意の次元ベクトルです 実は 画像枚からの外部パラメータ推定にはambiguityがあり  \boldsymbol b  の不定性があります これは ワールド座標の点を画像座標に射影する際 この射影を不変に保つような座標変換 H が存在するためです  H は imes の任意の正則行列です 同次座標に作用しているため これは射影変換になっており 自由度はスケール分を除いたです 逆に この射影変換を利用して 計算に都合の良い座標を選ぶことにします まず次の変換 H a を考えます これにより 次の変換 H b を考えます これを作用させます さらに次の変換 H c を考えます これを作用させます 以上で 透視投影行列 P   P   を求めることができました あとは前回の三角測量と同じ手順で次元再構成できます    実装        画像座標の 次元成分を正規化する             正規化点アルゴリズムでF行列を推定する           正規化      エピポーラ拘束式      特異値分解で連立方程式を解く      最小特異値を厳密にとし F行列をランクにする      正規化を戻す    F   T T   F   T      f をにする          次元にする          同次座標の場合      画像の対応する点を求める      F行列を推定する中村恭之  小枝正直  上田悦子  OpenCVによるコンピュータビジョン 機械学習入門   講談社サイエンティフィク   徐剛  辻三郎   次元ビジョン   共立出版   ,5,2023-07-01
139,139,エピポーラ幾何とカメラパラメータの推定,画像処理,https://qiita.com/ground0state/items/70ec4a6ffe337b800af3,   概要エピポーラ幾何から導かれる座標間の拘束行列を使用して カメラパラメータを推定する方法を紹介します 記法は前回の記事に基づきます    エピポーラ幾何エピポーラ幾何とは 二つのカメラ画像間での対応点の位置関係を表現する幾何学的な手法です この手法は 二つの画像を用いた次元復元や物体追跡に使用されます エピポールは 一つ目のカメラから見たときの もう一つのカメラの中心の投影点です つまり 一つ目のカメラから見て他のカメラがどこに見えるか その位置がエピポールです 逆も然りです エピポーラ線は 一つ目のカメラ座標上の点に対応する可能性がある 二つ目のカメラ座標上の点を集めた直線です 一つ目のカメラ座標上の点が次元空間のどの座標に対応するかは 深度の情報がないため分かりません したがって 二つ目のカメラ座標上のどの座標に対応するかも分かりませんが エピポーラ線上にあることは分かります エピポーラ線は 立体視の問題を線上の探索問題に簡略化するのに役立ちます エピポーラ平面は 二つのカメラの中心と次元空間中の点を通る平面を指します この平面はエピポーラ線を生成し その交差点はエピポールとなります カメラのレンズ中心 C  を基準にしたカメラ座標のベクトルには添字を カメラのそれには添字を付与することにします したがって  \boldsymbol M \  c   と \boldsymbol M \  c   は空間上の同じ点ですが 座標系が異なるので成分の値が異なることに注意してください カメラ座標 の単位長は同じ大きさであるとします エピポーラ平面に着目し エピポーラ平面内に含まれるベクトルとエピポーラ平面に垂直なベクトルの関係から ある拘束式を導きます ここで外積は歪対称行列 S  \boldsymbol t \  c     imes  で表すことができます カメラのカメラ座標における \boldsymbol M \  c     \boldsymbol t \  c   をカメラのカメラ座標における \boldsymbol M   c   に変換する行列を R とします 以上つの式を最初の式に代入します を用いると 次の式を得ます  E を基本行列 E行列 と呼びます 定義から E行列は外部パラメータのみで求められることがわかります この方程式は 導出過程から分かるように エピポーラ線 \ell  上の任意の点とエピポーラ線 \ell  上の任意の点が満たすべき拘束関係を表しています 次に カメラ座標から画像座標へ変換します  A   A  をそれぞれのカメラの内部パラメータとします カメラ座標の同次座標を ilde \boldsymbol x  \  c    ilde \boldsymbol x  \  c   をとします これを \boldsymbol x \  c    \boldsymbol x \  c   で解いて 画像上の点の拘束関係を表す式に代入します ここで 基礎行列 F行列  を定義しました 定義から F行列には外部パラメータと内部パラメータの情報が含まれていることが分かります E行列同様 F行列は画像座標上のエピポーラ線 \ell  上の任意の点とエピポーラ線 \ell  上の任意の点が満たすべき拘束関係を表しています F行列の使い方ですが 例えば カメラの画像座標上の点を一つ選び  ilde \boldsymbol m    とします すると  \boldsymbol \beta  F ilde \boldsymbol m    として エピポーラ線 \ell  の方程式 ilde \boldsymbol m    op \boldsymbol \beta   を求めることができます F行列は一つ目の画像座標の点をもう一つの画像座標の直線に写す写像とみなせます エピポールは任意のエピポーラ平面に含まれています したがって 次の式が成り立ちます エピポール ilde \boldsymbol e    ilde \boldsymbol e    はそれぞれ F op F と FF op の最小固有値の固有ベクトルとして求めることができます  F 行列から求めるので このベクトルは画像座標上のベクトルです E行列でも同様にエピポールを求めることができ その場合はカメラ座標上のベクトルになります    F行列の推定F行列を推定し カメラパラメータを求めます 画像座標の同次座標を成分で次のように表します 画像と画像の対応点が分かれば 方程式を立てることができます 対応点をスタックした行列を X とすると    mathX\boldsymbol f    と書けます これの解は X op X の最小固有値の固有ベクトルとして求められます ただし F行列のスケールの不定性および 次元歪対称行列の行列式が  になること から F行列の自由度はです 対応点つで解くアルゴリズムを点アルゴリズム 点以上を使ってSVDで解くアルゴリズムを点アルゴリズムと呼びます 点アルゴリズムには問題が知られており 同次座標のスケール次元の大きさがピクセル次元の値と比べて非常に小さいために数値計算がうまくいきません そこで ピクセル次元を正規化し    \sqrt  \sqrt  の範囲に収まるようにして点アルゴリズムを解くアルゴリズムを正規化点アルゴリズムと呼びます    カメラパラメータの推定カメラの内部パラメータが既知の場合 エピポーラ幾何の章で計算したように 求めたF行列をE行列に変換できます E行列の定義は 次の式でした E行列を特異値分解で分解し カメラパラメータを求める事ができます Eを特異値分解します    mathE USV op  imes  の歪対称行列の特異値三つのうち 二つは正で同じ値 一つは  になることが示せます 詳細は下記記事に譲ります したがって  S は次のように書けます  \boldsymbol t \  c   はエピポール \boldsymbol e   と平行なので 定数倍の不定性を除いて EE op の最小固有値の固有ベクトルとして求められます これは特異値  に対応する  U に含まれる特異ベクトル \boldsymbol u   です ここで カメラのカメラ座標のスケールの取り方には任意性があることを利用し 単位長を \boldsymbol t \  c   のノルムにとることにします したがって  \boldsymbol t \  c   \pm\boldsymbol u   とできます このスケールの選択によって 次元再構成する際のワールド座標のスケールは他の方法で決定する必要が生じることに注意してください 次に  R を求めます まず 行列 W を導入します  W は次の性質を持っています E行列を次のように書き直します これを直交行列と歪対称行列に分けます  R を行列式  の連結成分 すなわち右手系にとることにします 以下では  \det UV op   であったとして話を進めます  R  U W V op および S  \sigma V W    \Sigma V op とすると それぞれ直交行列と歪対称行列になっています これで R を求められました ただし  W を W     で入れ替えて計算してもこの結論が導けるので  R UWV op か R U W     V op を選ぶ不定性が残ります 以上から結局  \boldsymbol t   c   \pm\boldsymbol u   と R UW  \pm V op の組み合わせで通りの解の候補が得られました 実は F行列と内部パラメータだけでは これ以上解を絞ることができません    透視投影行列の計算一旦解の絞り込みは置いておくことにして 外部パラメータは求まったことにして話を勧めます 外部パラメータが求まると 透視投影行列を求めることができます ワールド座標をカメラのカメラ座標と一致させることにします ワールド座標とカメラ座標の一般的な関係式は次のように書けます 二つの座標の変換は次のように書けます ワールド座標をカメラのカメラ座標と一致させると次のようになります 透視投影行列は次のようになります    三角測量による次元再構成透視投影行列が求まると 三角測量によって次元再構成することができます ベクトルを成分で表します 射影の式 に代入して 式を整理すると次の連立方程式を得ます 解の自由度がに対して方程式がつあるため 最小二乗法で解くことができます    外部パラメータの絞り込み最後に 通りの外部パラメータ候補の絞り込み方法を説明します 三角測量による次元再構成を \boldsymbol t   c   \pm\boldsymbol u   と R UW  \pm V op の組み合わせ通りで行います 通りのうち 通りにおいて再構成された点は 二つのカメラのうち少なくとも一つのカメラの後ろ側になってしまい カメラに映るということに矛盾します 無矛盾となる通りのみが正しい外部パラメータになります 以上で外部パラメータを求めることができました カメラの内部パラメータが未知の場合は 次回解説します    実装        画像座標の 次元成分を正規化する             正規化点アルゴリズムでF行列を推定する           正規化      エピポーラ拘束式      特異値分解で連立方程式を解く      最小特異値を厳密にとし F行列をランクにする      正規化を戻す    F   T T   F   T      f をにする          次元にする          同次座標の場合      同次座標にする      右手系にする      回転行列はつの候補がある    W   np array                               R   U   W   Vt    R   U   W T   Vt      並進ベクトルはE Tの固有値の固有ベクトルとして求められる up to scale       正負の不定性で二つの候補がある    t   U         t    U           つの姿勢候補がある      正しい姿勢を選択する      各姿勢候補について 両カメラの前に再構成された次元点の数をカウント      一番カウントが多いのが正しい姿勢      自然画像からの次元最高性は誤差も大きいため カウントで判断する      前回求めた内部パラメータ      画像の対応する点を求める      F行列を推定する      E行列への変換中村恭之  小枝正直  上田悦子  OpenCVによるコンピュータビジョン 機械学習入門   講談社サイエンティフィク   ,4,2023-07-01
141,141,Vision Transformerはどのように働くのか？,画像処理,https://qiita.com/sister_DB/items/abf6e55e80ec3dd24750,  原論文  要約  multi head self attentionは小さいデータセットで学習させたときに過学習は起こらない  multi head self attentionは精度を向上させるだけでなく convolution nerual networkよりも損失関数が平らになる  multi head self attentionを使うと損失関数の凹凸が多くなってしまうが 大規模データセットと損失関数の平滑化手法によって凹凸を緩和することができる   X  Q  K  V はそれぞれ入力する特徴 query key valueを表す． d はqueryやkeyのトークンの次元数を表す． i は i 番目のヘッドを表す． j は j 番目のトークンであることを表す．   Softmax \frac QK  \sqrt d    は V に対する大きいカーネルであると考えることができる．これより 遠い距離にある特徴から特徴抽出できるようになった   Softmax \frac QK  \sqrt d    は入力する特徴 X によってカーネルの重みが動的に変わると考えることができる．  入力に近い層ではconvolution neural networkのように近傍の特徴から特徴抽出することが知られている  出力に近い層では遠くの特徴から特徴抽出することが知られている   W  b  X  Y はそれぞれ重み バイアス 入力特徴 出力特徴を表す． i j は特徴の i 行 j 列であることを表す． m n は重みの m 行 n 列であることを表す．  カーネルサイズは imes や imes のことが多く 小さいカーネルである．これより 近傍の特徴からしか特徴抽出できない   W は学習によって更新されるが 入力 X に対して変わることがない．つまり カーネルの重みは入力 X に対して動的に変わらない．  vision transformerは小さいサイズのデータセットを学習させたときに過学習は起こらないこれまでは 小さいサイズのデータセットを使ってvision transformerを学習させると過学習が起こると考えられていた．しかし 実験によってvision transformerは過学習が起きていないことが分かった．図 a を見ると テストデータを誤識別した割合と学習時の損失関数の値は比例関係のようになっていることが分かる．過学習とは 学習時のデータの損失が小さくなるが テストデータの誤識別は大きくなることを指すため 図 a の結果からmulti head self attentionのモデルは過学習を起こしているとは言えない．multi head self attentionは学習時の損失関数の値が大きいため画像の特徴をうまく学習できていないと言える．また 図 b を見ると 学習時のデータの数が増えるほど テストデータを誤識別した割合と学習時の損失関数の値は小さくなっていく．これは学習データの数が増えるほど multi head self attentionは画像の特徴をうまく学習できていることを示している．  vision transformerの損失関数は凹凸が多くなる小さいサイズのデータセットを学習させるときに vision transformerが上手く学習できないのはなぜなのだろうか．その答えとして  convolution neural networkを用いた時と比べて 損失関数が平らになりやすい  convolution neural networkを用いた時と比べて 損失関数の凹凸が多くなるのつが挙げられる．  image png  図 b はヘッシアン最大固有値のスペクトラムを表す．横軸はヘッシアン固有値 縦軸はそのヘッシアン固有値の多さを表す．ここで ヘッセ行列とヘッシアン固有値について簡単に説明する．ヘッセ行列とは ある多変数関数が与えられたときに その多変数関数を階偏微分し その偏微分値を行列にしたものである．深層学習における損失関数は数多くの重みをもっているため 多変数関数と考えることができる．ヘッシアン固有値とは ヘッセ行列の固有値を計算したものである．ヘッシアン固有値がすべてより大きければ 狭義凸関数となっていることが分かる．ヘッシアン固有値がすべて以上であれば 凸関数となっていることがわかる．ヘッシアン固有値に負の値があれば 凹凸のある関数となっていることが分かる．深層学習における損失関数は負に近づいていけば近づいていくほどよいことが多い．そのため 損失関数は下に凸の関数であることが望ましい．また ヘッシアン固有値の絶対が大きいと関数の傾きが大きいことを示し ヘッシアン固有値の絶対が小さいと関数の傾きが小さいことを示す．ここで図 b をもう一度確認する．図 b の左側は学習開始直後のヘッシアン最大固有値の分布を示している． ViT や PiT   Swin といったmulti head self attentionのモデルは損失関数のヘッシアン最大固有値が近くに多く分布していることがわかる．また 負の固有値にも多く分布していることが分かる．また  ResNet といったconvolution neural networkのモデルは損失関数のヘッシアン最大固有値がより大きいところに広く分布しており 負の固有値を持たないことも分かる．これより multi head self attentionのモデルは損失関数が平らになりやすく 損失関数の凹凸が多くなることが分かる．図 b の右側は学習開始して回学習したときのヘッシアン最大固有値の分布を示している．multi head self attentionのモデルは全体的に損失関数のヘッシアン最大固有値がより大きいところに分布するようになっていることが分かる．図 a は損失関数を可視化したものである． ResNet が損失関数の傾きが一番大きく 次に Swin が損失関数の傾きが大きくなっている． ViT や PiT は損失関数の傾きが平らに近くなっている．深層学習は損失関数の微分値を使って損失関数が最小になるように学習する．このため 損失関数の微分値が大きいとその分だけ重みを大きく更新することができるようになる．また 凹凸が多いと損失関数の最小値を探すことが大変になるため 凹凸はなければないほうが良い．  vision transformerの損失関数の凹凸を緩和するには  データセットを大量に用意する   クラス分類タスクにおいて 後段のヘッドではCLS tokenの代わりにglobal average poolingを使う  Swin Transformerのようにmulti head self attentionを行う領域を制限する  image png  上図は ResNet と ViT のヘッシアン最大固有値の分布を示している．また  ViT    は学習データの を使って学習したことを示す． ViT と ViT    を比べて分かるように  ViT    は負の固有値を多くとる．これより 少ないデータセットでは損失関数の凹凸が多くなってしまうことが分かる．これより vision transformerの損失関数の凹凸を緩和するには大量のデータセットを用意すれば良いことが分かる．  image png  上図はクラス分類タスクにおいて CLS tokenを使ってクラス分類したときの損失関数のヘッシアン最大固有値の分布とglobal average poolingを使ってクラス分類したときの損失関数のヘッシアン最大固有値の分布を示している．CLS tokenを使うよりもglobal average poolingを使った方が負の固有値の分布が少ない．これよりglobal average poolingを使った方が損失関数の凹凸が少なくなることが分かる．よって クラス分類タスクにおいて vision transformerの損失関数の凹凸を緩和するにはglobal average poolingを使った方が良いことが分かる．  image png  上図はmulti head self attentionを行う領域の広さを変化させたときの結果である．カーネルサイズとはmulti head self attentionを行う領域の広さのことである．図 a はカーネルサイズの違いによるテストデータを誤識別した割合と学習時の損失関数の値を示している．これより カーネルサイズは大きすぎず 小さすぎないサイズが良いということが分かる．図 b はカーネルサイズの違いによる損失関数のヘッシアン最大固有値の分布を示している．カーネルサイズが大きすぎず 小さすぎない時に負の固有値の分布が少なくなることが分かる．これより 広い範囲や狭い範囲に対してmulti head self attentionを行うよりもその中間の範囲に対してmulti head self attentionを行う方が良いことが分かる．よって Swin Transformerのようにmulti head self attentionを行う領域を制限すると良いことが分かる．  multi head self attentionはローパスフィルタの役割を持っているmulti head self attentionはローパスフィルタの役割を持っており convolution neural networkはハイパスフィルタの役割を持っている．  image png  上図は ViT における層の深さと高周波成分の相対対数振幅である．高周波成分の相対対数振幅とは その特徴マップにどれくらい高周波成分の特徴が含まれているのか示すものである．灰色のところはmulti head self attentionであり 白色のところはMLPである．multi head self attentionの直後は高周波成分の特徴が小さくなり MLPの直後は高周波成分の特徴が大きくなる．これより multi head self attentionは高周波成分の特徴を小さくするため ローパスフィルタの役割を持っていると言える．  image png  上図は ResNet における層の深さと高周波成分の相対対数振幅である．白色のところはconvolution neural networkであり 青色のところはプーリング層である．convolution neural networkの直後は高周波成分の特徴が大きくなっている．これより convolution neural networkはハイパスフィルタの役割を持っていると言える．  おすすめの記事同じ論文を解説している記事である．分かりやすく書いてあるので ぜひ読んでいただきたい．,1,2023-07-01
145,145,Javaで画像上にテキストを描画する方法,画像処理,https://qiita.com/blue_islands/items/3010e34a5e120fdd28b6,こんにちは みなさん 今日はJavaで画像上にテキストを描画する方法をみなさんにご紹介します プログラミング初心者の方でも この記事を読んでいただくことで 具体的なコードを使って 画像上にテキストを挿入することが可能になるでしょう     必要なファイルの読み込みまず フォントと背景画像を読み込むためのコードを書きます 以下のように Font createFont   と ImageIO read   を使用します     画像のリサイズ画像を特定のサイズにリサイズします アスペクト比を保ちつつ 新しいBufferedImageオブジェクトを作成し そこに元の画像を描画します     テキストの準備と配置テキストを作成し それを画像上に描画します ここではテキストを中心に配置する方法を解説します テキストの配置は 全体の高さ 行数 x 行の高さ を元に計算し それを画像の高さの半分から引くことで実現します final var originalText    こんにちは 世界 あいうえおかきくけこさしすせそたちつてと      最終的な画像の生成最後に BufferedImageを ImageIO write   を使用して出力ファイルに書き込みます    javaImageIO write img   jpg   new File  output jpg    以上の手順により Javaで画像上にテキストを描画することができます このコードを活用し 自分だけのオリジナル画像を生成してみてください 新たな発見があるかもしれませんよ     改行文字の挿入テキストを適切にフォーマットするためには 適切な位置で改行を挿入することが必要です このプログラムでは  insertNewlines   メソッドを使用して 文字ごとに改行を挿入しています 以上 Javaを使って画像上にテキストを描画する方法についてご紹介しました この方法を使えば 自分だけの画像にテキストを追加し オリジナルのビジュアルコンテンツを作成することが可能になります こちらで作ったソースコードはこちらにアップしておきます それでは みなさんもぜひ挑戦してみてください 楽しいプログラミングライフを ,1,2023-06-28
146,146,基礎行列計算のPython実装,画像処理,https://qiita.com/Hiroaki-K4/items/3ec34fc680a0e162c1ae,   About fundamental matrix  pair png  台のカメラで同じシーンを撮影し 台目のカメラで撮影された画像では点 x  y に 台目のカメラで撮影された画像では点 x   y  にシーンのある点が映っているとすると 両者の間には次の関係が成り立ちます fは 楕円当てはめ  と同様にスケールを調整する定数です F  Fij は台のカメラの相対位置とパラメータ 焦点距離など から決まるxの行列で 基礎行列と呼ばれます  こちら  の本をもとに 基礎行列の計算を実装してみたので その時のコードや結果を共有します 式  はエピ極線拘束条件またはエピ極線方程式と呼ばれます 式  はFを何倍しても同じ式を表すので これを定数倍して次のように正規化します 次元ベクトルを次のように定義すると 式  の左辺は ξ θ  f²であることがわかります したがって 式  のエピ極線方程式は次のように書くこともできます 式  は 楕円当てはめ  と同じ要領です つの画像の対応点から基礎行列Fを計算することは 様々な画像系のアプリケーションの基礎となっています 誤差のある対応点 xα yα   xα′ yα′   α      N から式  を満たす基礎行列Fを計算することは 次のような単位ベクトルθを計算することになります ξαは式  のx xα y yα x′ xα′ y′ yα′に対するξの値です データxα yα xα′ yα′は その真値xαˉ yαˉ xα′に誤差△xα​ △yα​ △xα​′ △yα​′を加えたものとして 次のように書くことができます これらを式  から得られるξαに代入すると 次のようになります ここでξαˉは真値 △ξαと△ξαはそれぞれ次と次の誤差項です これらは以下の様になります 誤差△xαと△yαを確率変数と考えると ξαの共分散行列は次のように定義されます Eは分布の期待値を表します △xαと△yαが期待値 標準偏差σの正規分布に従うなら 互いに独立で次の式が成り立ちます 式  を用いると 式  は次のように書けます すべての要素にσ²が掛けられるため それが抜き出された行列はV ξα と書かれ 正規化共分散行列と呼ばれます 楕円当てはめと同様に 式  の共分散行列において△ξαを考慮する必要はありません また 式  のxαˉ yαˉ xα′ˉは 実際の計算ではxα yα xα′ yα′に置き換えられます 誤差のある対応点から基礎行列Fを計算するには 誤差の性質を考慮して式  を満たすθを計算することになります これは楕円当てはめと同じです    Solution  Least squares method      次のx行列Mを計算する      固有値問題を解き 最小固有値λに対する単位固有ベクトルθを求める 楕円当てはめの記事  で述べていますが これは乗和を最小化するθを求めています    Solution  Taubin method      次のx行列M Nを計算する      固有値問題を解き 最小固有値λに対する単位固有ベクトルθを求める一連の処理を試せるpythonコードは こちら  です ,0,2023-06-26
148,148,楕円当てはめのPython実装,画像処理,https://qiita.com/Hiroaki-K4/items/e29cc422d7205cf19d05,   Abstract円形物体を撮影すると 画像上では楕円となって その形状から物体の次元位置を解析することができます そのため 画像から抽出した点群に対して楕円を当てはめることは カメラのキャリブレーションやロボットの制御などに使われています  こちらの本  をもとに 楕円当てはめを実装してみたので その時のコードや結果を共有します    Elliptic Formulaまず 楕円の式を共有します 楕円の中心を Xc Yc  X軸の長さをa Y軸の長さをb 楕円の傾きをθとすると以下の様になります 円や楕円に慣れるために楕円の式を描いてみましょう  こちら  にpythonで実装したコードがあります 画像上の楕円は 以下の式のように書くことができます fはスケールを調整する定数です 次の次元ベクトルを定義し ベクトルaとbの内積を a  b とすると 式  は次のように書けます    math \xi heta       楕円を当てはめることは θを解くことと同義なので その方法をまとめていきます    Solution  Least squares method式  の形の楕円を 誤差をもつ点群 x  y        x N y N  x y       xN yN に当てはめるには 次の式のA B C D E Fを計算することになります 式  を用いると 式  は次のように書けます 最小二乗法を用いてθを計算する手順を紹介します       次のx行列Mを計算する      固有値問題を解き 最小固有値λに対する単位固有ベクトルθを求める   mathMheta \lambdaheta     これは条件∣θ∣  のもとで以下の乗和を最小化します これを最小化する単位ベクトルθは 行列Mの最小固有値の単位ベクトルです 一連の処理をPythonで実装したコードは こちら  です 誤差を含んだ青点からθが計算されて 下の画像のように楕円が描かれます 最小二乗法はシンプルですが精度が低く 点群が楕円の一部しかカバーしていない場合 想定の形状より小さくて平坦な楕円になりやすいです 最小二乗法の精度が低い理由は データの誤差について全く考慮していないからです そこで 誤差について書いていきます データxα  yαは その真値xα​ˉ​ yα​ˉに誤差△x  △yを加えたものとして 次のように書くことができます これらをξαに代入すると 次のようになります ここでξαˉは真の値 △ξαと△ξαはそれぞれ次と次の誤差項です これを展開すると次のようになります 誤差△xαと△yαを確率変数と考えると ξαの共分散行列は次のように定義されます Eは分布の期待値を表します もし△xαと△yαが期待値 標準偏差σの正規分布に従うとすれば 互いに独立で次の式が成り立ちます 式  を用いると 式  は以下のように書けます すべての要素にσ² が掛けられているので それを除いてV ξα と書き 正規化共分散行列と呼びます 共分散行列V ξα の対角要素は 式  の各要素がどれだけ誤差を起こしやすいかを表し 非対角要素はつの要素間の相関を表します 式  の共分散行列は△ξαのみを用いていますが △ξαを加えても その後の結果にほとんど影響を与えないことが知られています これは△ξαが△ξαに比べて極めて小さいためです また 式  のV ξα は真の値x yを用いていますが 実際の計算では観測値xα​ˉ​ yα​ˉ​に置き換えられています これも結果にはほとんど影響しません 以下では 誤差の統計的性質を考慮して 最小二乗法の精度を向上させる様々な方法を示します    Solution  Iterative reweight      θ​  Wα​  α  … N と定義する      次のx行列Mを計算する      固有値問題を解き 最小固有値λの単位固有ベクトルθを計算する   mathMheta \lambdaheta           符号を除いてθ≒θならθを返す そうでなければ Wαとθを更新した後 再度stepを行う 統計学によれば 重みWαを各項の分散の逆数に比例させるのが最適であることが知られています 誤差の小さい項ほど大きく 誤差の大きい項ほど小さくなる  一連の処理を試せるpythonコードは こちら  です 実行結果である下画像の各点について説明します   青点は誤差を含んだ点  黒点は真値  赤点は最小二乗法による推定楕円  緑点は重み反復法による推定楕円  weighted png     Solution  Renormalization      θ​  Wα​  α  … N と定義する      次のx行列Mを計算する      固有値問題を解き 最小固有値λの単位固有ベクトルθを計算する   mathMheta \lambda Nheta           符号を除いてθ≒θならθを返す そうでなければ Wαとθを更新した後 再度stepを行う くりこみ法は 当てはめる楕円弧が短い場合の精度の悪さを改善する方法です 式  の形の一般的な固有値問題を解くプログラムツールは通常 Nが正値の対称行列であることを仮定していますが 式  からわかるように V ξα は正値ではないので Nは正値ではありません しかし 式  は次のように書き換えることができます    mathNheta \frac   \lambda Mheta     データに誤差があってもMは正値対称行列なので プログラムツールを適用できます よって 最大の一般固有値 λに対する単位一般固有ベクトルθが計算できます 一連の処理を試せるpythonコードは こちら  です 実行結果である下画像の各点について説明します   青点は誤差を含んだ点 途中で楕円弧が欠けている   黒点は真値  赤点は重み反復法による推定楕円  緑点はくりこみ法による推定楕円  renormalization png     Solution  FNS Fundamental Numerical Scheme  method      θ​  Wα​  α  … N と定義する      次のx行列M Lを計算する      M−Lを行列Xとして定義する   mathX M L           固有値問題を解き 最小固有値λの単位固有ベクトルθを計算する   mathXheta \lambdaheta           符号を除いてθ≒θならθを返す そうでなければ Wαとθを更新した後 再度stepを行う FNS法は 幾何学的距離を表すサンプソン誤差Jに対して勾配がとなるθを計算します 一連の処理を試せるpythonコードは こちら  です 実行結果である下画像の各点について説明します   青点は誤差を含んだ点  黒点は真値  緑点はFNS法による推定楕円  fns png     Solution  Robust fitting by RANSAC実際の画像は 必ずしも楕円上の点だけで構成されているとは限らず 他の物体の境界が含まれている場合もあります このような楕円を形成しない点をアウトライア 外れ値 と呼びます 一方 楕円を形成する点をインライアと呼びます アウトライアの影響を受けにくい当てはめをRobust fittingと呼びます 代表的な手法にRANSAC Random Sample Consensus があります RANSACは楕円当てはめに限らず 様々なタスクでデータの外れ値を除去したい時に使えるので かなり有用です       入力点群からランダムに点を選択し ξ     ξを式  のベクトルとする      固有値問題を解き 最小固有値λの単位固有ベクトルθを計算する      求めた楕円θについて 以下の式を満たす入力点群の数 n を数えるdは閾値として決めておきます       入力点群から別の点をランダムに選択し 同じ操作を行う これを何度も繰り返し 楕円の候補の中からnが最大のものを選ぶ       最終的に選択された楕円について 式  を満たさない点はアウトライアとみなし 除去する一連の処理を試せるpythonコードは こちら  です 実行結果である下画像の各点について説明します   青点はアウトライアとみなされた点  赤点はインライアとみなされた点  黒点は真値  緑点はRANSACでアウトライアが除かれた点群でFNS法を用いた推定楕円,4,2023-06-25
149,149,ピンホールカメラモデルとカメラキャリブレーション,画像処理,https://qiita.com/ground0state/items/2f025bbfb84774faf46b,   概要ピンホールカメラモデルを通して 次元の座標がどのように画像の座標と対応するかを紹介します また 次元の座標と画像の座標を対応させるのに必要なパラメータの求め方をカメラキャリブレーションを通して紹介します    ピンホールカメラモデルピンホールカメラモデルは カメラの単純なモデルであり 物体の位置や向きを推定するために広く使用されています このモデルは D復元 カメラキャリブレーション 画像処理 およびロボットビジョンなどのアプリケーションで使用されます ピンホールカメラモデルでは 座標系がたくさん出てくるので 注意しながら進めます 多くの書籍ではつの座標系を考えますが より厳密に次のつの座標系を考えることにします   ワールド座標系 次元座標   カメラ座標 カメラの中心を原点にとった次元座標   画像平面座標 カメラ座標のZ軸がカメラの焦点距離となるX Y平面の次元座標   画像座標 撮像素子で記録された画像の次元座標 まず 任意に設定した次元の座標をワールド座標系と呼びます 添え字に w をつけます 次に ピンホールカメラモデルでは 物体からの光線がピンホールを通過し 画像平面に投影されます カメラのレンズ中心を原点とし 光軸方向をZ軸にとったカメラ座標系です 添え字に c をつけます カメラ座標のZ軸がカメラの焦点距離となるX Y平面を画像平面座標と呼びます カメラ座標上の点 \boldsymbol M  c とカメラ座標系の原点 O c を結んで 画像平面座標へ写像します この射影は次の式で表せます これは行列の積で表現できます 座標を次元拡張し 値をとしたベクトルを用いることで計算が楽になります これを同次座標と呼びます 同次座標のベクトルにはチルダを付けることにします すると射影の式は次の式になります ここで  Q は次のように定義します ところで 同次座標を使用するとワールド座標とカメラ座標の変換は次の式で表せます  R は imes の回転行列  \boldsymbol t  は次元の平行移動ベクトルです 以上でワールド座標系とカメラ座標系 画像平面座標の対応関係が得られました 次に画像座標系を考えます 画像座標系は実際に撮影された画像の座標系で これは一般に画像平面座標に一致しません 光軸と画像座標の原点が一致するとは限りませんし 撮像素子によってはX軸とY軸のスケールが異なります また 画像座標のX軸とY軸は直行しているとは限りません 画像平面座標と画像座標の対応がどのように表されるか考えます まず カメラ座標の設定を思い出すと Z軸は光軸に合わせしましたが X軸とY軸については制約がなかったので 次元回転の不定性が残っています そこで この不定性を使用して カメラ座標のX軸 すなわち画像平面座標のX軸 と画像座標のX軸が平行になるようにします 画像平面座標のスケールに対して 画像座標のスケールはX軸Y軸それぞれ  k\ x  k\ y  倍とします 図から次の関係が読み取れます 同次座標を用いて次のように表せます さらに画像座標とカメラ座標の関係を求めると次のようになります このつのパラメータ fk u  fk u  heta  u   v   をカメラの内部パラメータと呼びます カメラの内部パラメータには 焦点距離 撮像素子の形状 および画像中心点の座標の情報が含まれます 行列の前行列を取り出した行列を内部行列と呼び  A と表します ワールド座標と画像座標の対応は次のように表せます   R  \boldsymbol t   を外部パラメータと呼びます    カメラキャリブレーションカメラキャリブレーションとは カメラの内部パラメータと外部パラメータを決定するプロセスです 内部パラメータは カメラ自身の光学特性を含みます 外部パラメータは カメラの位置と向きを表し 位置 回転を含みます カメラキャリブレーションにより 画像に対する正確なカメラ位置や向きがわかるため D復元 オブジェクト追跡 および拡張現実などのアプリケーションで使用されます カメラの内部パラメータと外部パラメータを次のように表すことにします  P を透視投影変換行列と呼びます スケール変換の不定性があるため  p     として スケールを固定しました 画像の座標と次元の座標の対応から P の値を求めることができます ワールド座標と画像座標の対応は次のように表せます 解いて s を消すと次元座標から次元座標への写像が得られます この式から独立な本の式を立てられるので 自由度の P を求めるには つ以上の対応点が分かればよいです 画像間の対応する点が N 個ある場合 記号を整理して次のように書きます 以上の記号を用いて透視投影変換は次のようになります  Q を求めたら  A が上三角行列であることを利用して RQ分解  を行うことにより  A と  R \ \boldsymbol t   に分解できます    実装今回は自前で画像を用意しました 画像に丸で示した点の座標を対応させて カメラパラメータを推定します     投影行列を分解し カメラのキャリブレーション行列 回転行列 およびワールド座標におけるカメラ中心を取得します     Parameters    P   ndarray        分解する投影行列 形状は      である必要があります     verbose   int  optional        冗長モード verboseが以上の場合 分解の残差平方和 RMSE が表示されます デフォルトは     Returns    A   ndarray        カメラのキャリブレーション行列 形状は      です     R   ndarray        カメラの回転行列 形状は      です     T   ndarray        ワールド座標におけるカメラ中心 形状は     です     Notes    この関数は 投影行列をキャリブレーション行列と回転行列に分解します さらに キャリブレーション行列を用いて変換行列を分解し     ワールド座標におけるカメラ中心を取得します これはDコンピュータビジョンで頻繁に用いられる処理で カメラの内部パラメータ キャリブレーション行列     と外部パラメータ 回転行列と並進ベクトル を取得することで 次元空間における物体の位置やカメラの位置を理解することができます     Rはカメラ座標の各軸の方向を表すカメラ座標の単位ベクトルをワールド座標で見た時のベクトルを行ベクトルとして並べたものです したがって     Rは直行行列とは限りません なぜならば 一般にワールド座標とカメラ座標の単位長が異なるからです このRは単位長を変換するスケール変換を含んでおり     このスケール倍分だけ直交行列からずれます     Tはワールド座標で見た時の ワールド座標原点からカメラ座標原点へのベクトルです tはカメラ座標で見た時の     カメラ座標原点からワールド座標原点へのベクトルです     Rとtをカメラの外部パラメータと呼びます 奥の角の次元座標を画像に写像したのが下の画像です    Reference中村恭之  小枝正直  上田悦子  OpenCVによるコンピュータビジョン 機械学習入門   講談社サイエンティフィク   ,3,2023-06-24
150,150,ホモグラフィ行列の求め方,画像処理,https://qiita.com/ground0state/items/260b423b6fe6b69770c4,   概要画像レジストレーションを例に ホモグラフィ行列を計算する方法を紹介します    画像レジストレーション画像レジストレーションとは つの画像を位置合わせすることであり コンピュータビジョンや画像処理の分野で広く使用されています ホモグラフィ行列は つの平面の間の射影変換を表す行列であり 画像レジストレーションに使用されます ホモグラフィ行列を使用することにより つの画像の座標系を変換することができ つの画像を正確に重ね合わせることができます ホモグラフィ行列は 最低つの点の対応関係を使用して計算されます つまり つの画像内のつの点ともうつの画像内の対応するつの点を知っている必要があります この情報を使用して ホモグラフィ行列を計算し つの画像を正確に重ね合わせることができます ホモグラフィ行列の計算を解説します ある対象物を台のカメラで撮影した場合を考えます それぞれのカメラで撮影された画像をAおよびBとします 画像間の対応する点の一つを \boldsymbol x \ A  \boldsymbol x \ B とします 射影変換は座標に次元加えることで すっきりとした式で書くことができます 座標値を次元拡張し 値を  とします ホモグラフィ行列を  H \  ij  h\  ij  とすると 射影変換は次のように書けます  s は h\    h\    で表すことができるので ホモグラフィ行列の自由度はです 画像間の対応する点つにつき方程式が本立つので 最低点あればホモグラフィ行列を求めることができます 点より多くある場合は 最小二乗法等でホモグラフィ行列を求めることができます 本解説では 最小二乗法を使用してホモグラフィ行列を求めます 式を展開して整理し直すと 次のように書き換えられます 画像間の対応する点が N 個ある場合 記号を整理して次のように書きます 以上の記号を用いてホモグラフィ変換は次のようになります これは解析的に解くことができて 次のようになります  X op X は imes 行列なので 逆行列も比較的求めやすいです また RANSAC回帰等を用いて 外れ値を除きながら回帰分析をしても良いです 画像レジストレーションの場合は 画像のディスクリプタを用いて座標を対応させます これらの対応点には誤った対応点も含まれるため 外れ値に強い手法を使ったほうが良いです    実装データは OpenCVのリポジトリ  のデータをダウンロードしました       OpenCVを使う場合はこっち中村恭之  小枝正直  上田悦子  OpenCVによるコンピュータビジョン 機械学習入門   講談社サイエンティフィク   ,5,2023-06-24
151,151,【OpenCV】画像の差分を取得するとサンリオ間違い探しが高速クリアできる！,画像処理,https://qiita.com/kagami_t/items/2b4db4e2464439a48fb4,  Introduction画像の類似度を測る imgsim  の記事を投稿したところ 画面の差分を確認できないかとリクエストいただきました  imgsim は特徴ベクトルから画像間の距離を測るもので 差分検出とは異なります 画像の差分は OpenCV による検出が分かりやすいため  imgsim の記事と同様に手早く差分を取得して比較するスクリプトを紹介します         ノイズ等も考慮して正確に取得するのであれば SIFT    AKAZE 等の特徴点抽出や 機械学習を用いる必要があります 本記事では基本編で 簡単な画像差分を検出して比較します 勿論アプリや web 画面にも応用可能です そして応用編で 難しいと話題になっているサンリオの間違い探しを差分検出で高速クリアしてみます  間違い探しクリアって誰得  と思ったのですが 非エンジニアの知人にこの話をしたところ CNN よりずっと食いつきが良かったです     基本編   基本編  応用編   応用編   本記事が少しでも読者様の学びに繋がれば幸いです      いいね をしていただけると今後の励みになるので 是非お願いします      環境Ubuntu Python    実装先に結論として出力に使用したソースコードを紹介します 解説はコメントアウトで詳細に書きました  OpenCV に馴染みのない方は参考にしてください     つの画像を比較し 差分を表示する関数       画像を読み込む      差分画像を計算    diff   cv absdiff image  image       グレースケールに変換      カラーマップを適用するために差分画像を正規化      差分画像に重みをかけて枚目の画像の色に反映      枚目の画像を表示      枚目の画像を表示      差分画像 グレースケール を表示      差分画像 カラー を表示      つの画像を比較して違いを検出実行すると ☓ マスに以下の画像を出力します    行  列目  変数 image path で指定した  枚目の画像を出力    行  列目  変数 image path で指定した  枚目の画像を出力    行  列目   枚目と  枚目の差分を白 それ以外を白黒で出力    行  列目   枚目と  枚目の差分をカラーで出力 それでは出力内容を見ていきましょう    基本編 実装したスクリプト   実装 で私のペンギンアイコンから画像差分を検出します   線の有無を検出    枚目に私のアイコン  枚目に横線を一本引いた画像を用意します    線は OpenCV を使うと簡単に引けます       png     横線が浮かび上がりました    差分が視覚的で良い感じです   色違いを検出    枚目と同じ画像を用いて 片方だけ帽子の色を赤くしました    急にサンタさん感が増します      Screenshot from       png     色の変化した帽子が浮かび上がりました    線だけでなく 色の差分もしっかり検出できています    応用編今回の間違い探しで用いる画像は以下の  枚です 間違いは  ヶ所あります  ヶ所全部見つかりましたか 試しに目視でやりましたが  ヶ所で目が疲れてきました    実装したスクリプト   実装 で実行しちゃいましょう 何とか  ヶ所すべて出力できました 右下の木目が若干見辛いですが これ以上はノイズが乗ってくるので検出できただけ良しとします 簡単な間違い探しや Web 画面の差分であればもっと分かりやすいため十分機能を果たせています    おまけ 実装したスクリプト   実装 に至るまでの過程を記載します     差分検出まずは簡単に差分を検出しました  枚目と  枚目の差分を  枚目で出力します 記事作成前の完成イメージはこれでしたが 目視で追うのが少々面倒です     矩形出力何か思っていたのと違う   小さすぎる差分はまとめるようにします     矩形出力 差分のみ           面積が一定以上の場合にのみ矩形を描画  出力結果簡単な箇所は検出できています 基本編の差分であれば十分な気もしますが 悔しいので続けました とはいえ単純な手法で矩形出力は厳しことが分かり  実装したスクリプト   実装 のように差分を浮かび上がらせて完成させました     最後に OpenCV×Python は良い教材が少ないので 私は Udemy で勉強しました  OpenCV の諸機能を淡々と進められるので  Python を抵抗なく書ける程度の知識がある方におすすめです 最後まで閲覧頂きありがとうございました 本記事がお役に立てば幸いです ,58,2023-06-24
152,152,OpenCV 画像の一部を切り出す:座標指定・スライシング,画像処理,https://qiita.com/T-K20077/items/c2a8aa1d957ac06724be,  概要OpenCVの基礎を固めます．第回目は画像のトリミングに取り組みました．  OpenCVとは Pythonライブラリの種 Intel提供のOSS 画像 動画処理専門ライブラリ Pythonの一般的な前処理ライブラリとして挙げられるメリット DPL用の前処理が簡略化でき容易  多様なライブラリと相互性がある  環境Windows VSCode python     画像の一部を切り出す 座標指定 スライシング画像は自分で撮影したケーキの画像cake jpgを使います．コードの先頭にimport cvでライブラリをインポートし OpenCVを使えるよう準備します．画像のwidth×heightを調べます．print関数にimg shapeと記載し 画像サイズは×と分かりました．スライシングはimg top bottom left right に取り出したい座標を指定します．          縦幅 横幅 色相数 を表しています．timg img      切り抜きたい部分の座標を入力cv imwrite  timg jpg  timg cv waitKey  色相数についてコメントアウトで触れていますが はカラー画像 RGB  は二値画像 白黒 を表現しています．  結果出力画像は以下のtimg jpgの結果になりました．元画像のケーキの飾り部分が抜き出せていますね．  scakeR jpg    スライシングについて今回のスライシングはトリミング方法のつを紹介したものです．この方法は任意の座標で 好きなサイズに画像を抜き出すことができます． もちろん画像の縦横幅以内の範囲でですが スライシングについての図解をしていたサイトがあったので 自分でも理解の為に図解を作ってみました．  srice jpg  スライシングは分割の意味があり これをトリミングに用いている方法です．上の図は処理のイメージです．座標を用いて四角形に切り出して抽出します．,5,2023-06-23
155,155,野鳥の自動撮影→撮影した写真と名前でファインチューニングチャレンジ,画像処理,https://qiita.com/ssdsad/items/d83b39dbd8792315704c,  はじめに最近 野鳥にはまってます  ピーピー  ヒヨヒヨ  ツーツーピー 鳥の鳴き声なんてどれも同じに聞こえそうなものですが 野鳥撮影にはまってからは 不思議と聞き分けられるようになりました 中でも キビタキ という鳥がすごい 声が綺麗なうえに 他の鳥のなき声をマネするなんて特技もある ツクツクボウシをマネするキビタキに出会ったときは感動しました 鳥の鳴き声の世界は 本当に奥が深いんですよね ……と 画像処理を銘打っておきながら鳥の音声の話ばかりをする理由はいたってシンプル  葉っぱ多すぎて小さい鳥なんか見つけられる気がしない  一匹見つけるのも一苦労なので 今回はYolovを使いながら野鳥の自動撮影を目指します 本記事では その失敗と過程を記録用に残します   環境ここでいう環境は開発環境ではなく撮影環境 山奥の家の室内にカメラを設置して 窓に向けて撮影します 位置の関係上 日差しの影響がものすごい  概要と目標何かを学ぶとき 漠然と勉強するよりも何か明確な目的があったほうが モチベーションが続くもの 野鳥自動撮影チャレンジを通じてたくさんの野鳥画像を蓄積して ViTや画像関連の様々なタスクにつなげられたらなぁと期待しています とにもかくにも今回は野鳥の自動撮影 野鳥自動撮影チャレンジと題しまして 以下の段階に分けて細々とやっていこうと思います ●段階：野鳥の自動撮影  image png  　　　　回目：opencvの動体検知編　　　　→失敗　　回目：Yolo v編　　　　→失敗　　回目：プログラムと機材整理編　　　　→どうなる     ●段階：撮影写真のラベリング  image png  　　→まだ到達してない●段階：野鳥の名前を学習  image png  　　→まだ到達してない  回目：プログラムと機材整理編いきなり最後のチャレンジから説明 回目回目の失敗は以降の節に記載しています 教訓を要約すれば  カメラだめすぎ  いままでのソース問題起きた時何も分析できなさすぎ  低スペックPCでリアルタイム処理とか無茶すぎ  でもYoloはさいこう  という結論に達したので Yoloの部分はそのままにログやら画像保存やら整理します    機材カメラ：高いＷＥＢカメラ　円　　　　　焦点距離  mm　倍ズーム　　　　　解像度  xＰＣ　：やすいミニＰＣ　　　円　　　　　メモリ GB　　　　　CPU    J コア スレッド   実装概要としては時間を管理しながら無限ループをぐるぐる回して 下の処理を繰り返すだけ ．朝時 時の間は録画．夜時に録画したデータに対して野鳥検出以下 もしかしたら今後何かに転用できるかもしれなくもないから主要なクラスごとに分けて記載     ロガーロガーです ネットに転がってたものを拝借		  ロガーを生成		  エラーレベルを設定		self logger setLevel level 		  ハンドラを登録		fh setFormatter fmt     ファイルハンドラにフォーマッタを登録		self logger addHandler fh     ロガーにファイルハンドラを登録		  標準出力の出力指示がされていたら 標準出力用のハンドラとフォーマッタを登録   時間の管理pythonのscheduleライブラリを使って 時間帯ごとの処理を分けようと思っていたのだけれど 複数のタスクを実行させたり 二重ループ重ねると動かない現象が起きたので地道にやります cst HMS Sinceは 録画を開始する時間  朝時 cst HMS Untilは 録画を終了する時間  朝時分 cst HMS AfterProcは 録画完了後に後処理を開始する時間cst HMS FinalProcは 次の日の処理のために初期化する時間この設定だと朝時 朝時分の間まで録画をする設定 当プログラムでは時間おきに録画データ出力するので 時間おきにここのパラメータが時間ずつ増えて更新される 		  日中録画		  指定時間に画像処理		  日付またぎの処理   画像処理部分Yolo以外の処理を追加する場合はProcessingの抽象クラスから実装 		  今日のフォルダの画像を取得			  実行結果を格納   全体全体は以下の通り時間ごとに録画処理をわけるという思想で作られてるが もしもこの仕様をなくしたい場合は時間先のスケジュールを設定する Tkeeper update hour を消して 		  ロガーを生成		  エラーレベルを設定		self logger setLevel level 		  ハンドラを登録		fh setFormatter fmt     ファイルハンドラにフォーマッタを登録		self logger addHandler fh     ロガーにファイルハンドラを登録		  標準出力の出力指示がされていたら 標準出力用のハンドラとフォーマッタを登録		  日中録画		  指定時間に画像処理		  日付またぎの処理		  パス関連		  フォルダ作成		  今日のフォルダの画像を取得			  実行結果を格納	  パス関連	LitPath   Fol LitPath	 キャプチャ開始		 保存			 次の時間を録画	  カメラ情報取得	  フォルダパス情報取得	  検知		  時間ごとにファイル作成		 今日録画したデータを解析		  日付またぎの処理			 明日の日付で初期化   結果結果は    まだためしていない 今週末試そうわくわく  回目：opencvの動体検知編先に書きますが この章でやったことは失敗に終わります   機材カメラ：やすいＷＥＢカメラ　円　　　　　画角    度　　　　　解像度  xＰＣ　：やすいミニＰＣ　　　円　　　　　メモリ GB　　　　　CPU    J コア スレッド   実装webカメラの画像を取得して 前回フレームの画像と現在のフレームの差分がどれくらい離れてるかをみて検知を行います opencvのよくある作例のやつです   動体検知  結果結果は前述のとおり失敗です 理由は 誤撮影が多すぎる から 閾値をいじれば少しは収まりますが 風が吹けばパシャリ 日商条件がかわればパシャリとなかなか難しい 上記ソースでは  static back という動体検知の比較対象になる画像を定期的に更新するようにしましたが それでもやはり難しい とはいえ 日商条件がころころ変わらない環境 常に風で揺れる物体がない環境であれば使えるソースだと思うので記録として記載   回目：Yolo vx編先に書きますが この章でやったことは失敗に終わります Yoloがわるいんじゃない わたしがわるい    機材カメラ：やすいＷＥＢカメラ　円　　　　　画角    度　　　　　解像度  xＰＣ　：やすいミニＰＣ　　　円　　　　　メモリ GB　　　　　CPU    J コア スレッド先にこの節の結論を書くと  人用のWEBカメラでメートル先の鳥を撮影したうえに しかも解像度xの画像で検知するとか無理だよね  という結論になります    実装流れとしては単純 Yoloで野鳥検知したら画像保存するだけです   結果Yoloはすごい Yoloは悪くありません ネットで拾った画像を検知させると見事に  bird  と検知してくれました しかし結果は前述のとおり失敗です 理由はつあります ．カメラがだめすぎる xの解像度で 画角は度と標準レンズなので画が遠い 野鳥をとれたとしても ピクセル以下くらいしかない ．ソースがダメ 確実に成功するだろうという謎の自身の元 ドストレートなソースをかきましたが このソースだと 野鳥検知しないと何一つ画像を保存しないので 問題が起きた時になにも改善ができない 致命的  ．万円の低スペックPCでリアルタイム処理とか無理すぎ,0,2023-06-20
156,156,単眼深度推定モデルのMiDaSのonnx形式ファイルのダウンロード方法のメモ,画像処理,https://qiita.com/yukitakaGrid/items/e77403955c7e9ae82530,  MiDaSとは  大規模かつ多様なデータセットで学習しているため より幅広いタスクや環境で効率的に実行することができる  スケール感がないため マッピング プランニング ナビゲーション 物体認識 D再構成 画像編集など メートル単位の深度を必要とする場面での有用性は低下する  Stable Diffusion  以下はMiDaSのモデル一覧です 横軸は秒あたりのフレーム数 縦軸は性能 円の大きさはパラメーター数です   ダウンロード方法   python をセットアップする python 以降はこの後のパッケージのインストールでエラーになる可能性あり   ↓python のインストール方法インストールが完了したら 以下のコマンドで自分のパソコンにちゃんとダウンロードされているか確認します python   versionこのように表示されたらダウンロード成功です  私はver  をダウンロードしました Python      pytorchで読み込めるモデルの重みファイルをここからダウンロードします      OpenCVをインストールします  pip install   upgrade pippip install opencv python   Pytorchをインストールします      onnxパッケージをインストールします  ここでpython 以降だとエラーが出る可能性あり      以下のコマンドを実行する     カレントディレクトリにmodel fb onnxのファイルが出力されたら成功です   ,3,2023-06-20
161,161,Open Imagesから特定のクラスの画像を取得してBBoxを見てみる,画像処理,https://qiita.com/hayato0522/items/815164d300bb9b51dbd5,    はじめに 本 book  の物体検出しようと思った際 データセットを自分で作るのが面倒だと思い 既存のデータセットを使おうと思って調べました 結局自分で作ったほうが綺麗なデータで学習できるかなと思い そうしようと思ってますが 折角調べたのでいつか使うときのために残しておきます 実行はjupyter notebookです 目的：データセット Open Images Dataset から特定のクラスの画像とアノテーションデータを取り出す  本編公式ページにもデータのダウンロードについては説明が書かれており それに沿って行うだけです あと下記記事も参考にしました  FiftyOne を使って Open Images Dataset のデータを取得する    fiftyoneのインストール   python pip install fiftyone  ダウンロード引数として open imagesのバージョンを指定 open images以外のデータセットでも使えるものもあり  max sampleはダウンロードする最大画像枚数上記を実行すると画像とアノテーション情報がまとまったdetection csvなどが取得できます detection csvはこんな感じ 一部表示  ImageIDは各画像の名前と一致しており これをキーとして各BBoxとラベル名が取得できるようになっています  detection csvにBook以外のラベルも存在してることに気づいた 画像とアノテーション情報を表示してみます   アノテーションデータの読み込み  クラスラベルの読み込み  ラベル名表示させたいのでdetectionにclassesを繋げとく やや雑   クラス名を文字型にしとく  画像 efec jpg の読み込みとサイズ取得img   cv imread  C \\\\fiftyone\\open images v\rain\\data\\efec jpg  y  x   img shape     画像 efec jpg のアノテーション情報を取得  BBoxを画像上に表示出力結果 only matching True にしたのですが本以外のアノテーションもついてしまってました この辺は改めて調べようと思いつつ とりあえずデータフレームの段階で条件を絞ってもう一度表示させます detection classesのクラス名Bookだけのデータフレームを作ってannotation再定義して同じように実行します これで本だけになりました 実際に物体検出の学習で使う際にはこの後 アノテーション情報が入ったcsvファイルをSSDならxml形式 YoLoならtxt形式などに変換する必要がありますが 調べたらそれらのコードは転がってそうです 以上で本だけの画像とアノテーションデータを取得することはできましたが open imagesの画像ではあまり本のアップがなく 引きでの写真が多く いくつかの本がまとまってつのBBoxを与えられているものばかりでした 個人的には本を確実に一冊一冊検出させたかったため 自分ででデータセット作ろうかなと思いました もちろんデータセットの中にが一冊一冊にBBoxが付けられているようなデータもあるので これらのデータセットを使っても問題ないのかもしれませんが   本アップでもまとめてBBoxついてたりする いろいろな画像があった   image png      おわりにデータセットを眺めてると日本っぽい画像もあったりしてなんか見てるだけでも面白かったです 漫画一冊でも表紙のキャラクターに対してたくさんのラベルが付いてて驚きました 今回初めてこういった画像データセットの中身見たので 今後使う機会があればと思います ,2,2023-06-15
162,162,ChatGPTを用いた画像解析LINE botを作ってみた。,画像処理,https://qiita.com/katsuki_ono/items/bb2be8823b60fde5a14c,ChatGPTを用いたLINE botに目 画像認識 をつけようと開発していたら 思ったより単体でChatGPTを用いた画像解析 解説 は面白かったので 単体でツールにしてみました 作ったツールは LINEにユーザーが画像を送ると Cloud Vision API  が画像をテキスト化して そのテキストを元にChatGPTが画像を解説して LINEに返すツールです 今回は長期的に実行中にしてもしなくても良いようなツールなので 手軽に使えるGoogle colabを使用します  コード自体は インスタンスでも機能します  手順は以下です   Cloud Vision APIを有効化する  ChatGPTを用いた画像解析LINE botを実行中にする    Cloud Vision APIを有効化するCloud Vision APIを有効化する手順は以下です    Google Cloud Console   にアクセスし プロジェクトを作成または選択します    Cloud Vision APIページ  に移動し  有効にする をクリックして API を有効化します   必要に応じて同意画面を設定します   画面左側のメニューから 認証情報 をクリックし  認証情報を作成 を選択して サービスアカウント をクリックして 必要事項を記入してサービスアカウントを作成します   作成したサービスアカウントの編集ページを開いて キーのタブを開いて JSONのキーを作成してダウンロードします   作成したサービスアカウントのキーをGoogle colabで使用するアカウントのGoogleドライブに入れます     ChatGPTを用いた画像解析LINE botを実行中にするまず Google colabで新規ノートブックを開き Google colabからGoogle ドライブにアクセスして サービスアカウントのキーを読み込めるように以下の操作を行います ファイルを開く写真の赤枠のマークを押す   ECECA   BBB BBDEF jpeg  Googleドライブをマウントする写真の赤枠のマークを押す メニューが開いたら ドライブに接続を押す   EECC  BFC DB EED jpeg  次に 以下をインストールします 最後に 先程Googleドライブに入れたサービスアカウントのキーのパス LINE ngrok openai APIのキーやトークンを以下のコードに入れてから 実行します ※LINEとngrokの設定は こちら  を参考にしてください    python  LINEのチャネルシークレット  LINEのチャネルアクセストークン              role    system    content    芸術評論家として 以下の一つの作品の作品情報からこの作品を日本語で評価してください    実行したら 生成されたURLを取得して LINE Developers のダッシュボードでwebhook URLを設定します  詳しくは  こちら  を参考にしてください  webhook URLを設定後 LINE Developersで検証を押します 成功と表示されたら 上手く接続出来ています  Google colab側では 上手くいっている場合はと返されます の場合 プログラムに問題ありなどなので 普通のエラーと同じです  ここまで出来たら LINE  botを友達登録して画像を送信してみて下さい 以下のように送信した画像へのコメントがLINEに返ってきたら成功です   最後に元々 Stable Diffusionが好きで 枚程お気に入りの絵があるので これを気が向いた時に ChatGPTに解説させて遊んでみようと思います また何か作ってみます ,4,2023-06-09
163,163,PythonとOpenCVで消しゴムマジックやってみた！,画像処理,https://qiita.com/dancingedamame/items/3a3f689af8b94073c2f8,   robot   消しゴムマジックで消してやるのさ 少し前に 消しゴムマジックCMのこのセリフが流行っていました ちょうど私もGoogle Pixelを持っていたので実際に試してみると 本当に選択した範囲が消えるんですね  感動です そんな消しゴムマジックを再現しよう ということで Inpaintingという技術を使用して再現をチャレンジしました  例：空の鳥を消してみました 消しゴムマジック使用   image png    Inpaintingとは Inpaintingとは日本語で修復するという意味で 選択された範囲の画像を修復する技術のことです 今回は OpenCVのINPAINTを使用して再現を行いました この技術にAIなどを絡ませると より精巧な修復が可能になるようです OpenCVにはInpaintingの方法がNSとTELEAの種類あり 流体力学の理論を適用した画像の補完や 加重平均 勾配を使用して修復されるごとに境界を更新する方法などが用いられています   用意するもの用意するものはつです   画像  マスク画像画像は pixabay  というフリー画像サイトからお借りしました マスク画像というのは 消したい部分だけ塗りつぶされた画像のことです こちらは既存の描画ツールを駆使して作成しました 結構大変でした    左が元の画像で 右がマスク画像です  image png    コードcv inpaint 元の画像  マスク画像  画像修復の半径  NSかTELEA選択 修復半径を変えたりNSとTELEAを両方試したりするのもいいですね    結果の比較NSのradiusとの比較  image png  TELEAのradiusとの比較  image png  NSとTELEAの比較 radius   image png  NSよりもTELEAのほうが比較的滑らかな修復ができている印象ですね   感想OpenCVを使うと少ないコードでも簡単に画像処理ができるのでとても便利でした また AIを組み合わせるともっと自然に修復ができるようなのでまだまだ深堀出来そうです 皆さんもぜひInpaintingを試してみてください 　  参考にしたサイト,8,2023-06-05
164,164,Pythonを使用してモザイクアートを作成する方法,画像処理,https://qiita.com/yaanai/items/f487676f6b5f9d78ade3,  Pythonを使用してモザイクアートを作成する方法モザイクアートは 小さな画像を組み合わせて画像を構築する技術です 今回はPythonを使用してモザイクアートを作成する方法について説明します 小さな画像の収集 類似度スコアの計算 最終的なモザイク出力の生成プロセスをカバーします    はじめに以前 ぺろりん先生のブログから画像を抽出する方法を紹介しました そこで 今回はここで集めた画像を使ってモザイクアートを作ります コードとその解説はchatGPTで作成しました オリジナルの画像はこちらです   Output jpg     前提条件始める前に 次のものが必要です：  コンピュータにインストールされたPython  Pythonライブラリ：PIL NumPy OpenCV pipを使用してインストール： pip install pillow numpy opencv python     小さな画像の収集モザイクを作成するためには 建築ブロックとして使用する小さな画像のコレクションが必要です これらの画像をコンピュータ上の特定のフォルダに保存してください 今回はぺろりん先生のブログから収集した枚の画像を使用しています    コードの理解モザイクアートの作成には 次のステップがあります：  メイン画像の読み込み：まず モザイクを作成するためのメイン画像を読み込みます 画像は 写真やアートワークなど どんなものでも構いません   メイン画像のリサイズ：出力サイズに応じて メイン画像のサイズを最終的なモザイクの寸法に合わせてリサイズします   小さな画像の収集：指定したフォルダから小さな画像を収集します これらの画像がモザイクの構築に使用されます   類似度スコアの計算：メイン画像の各ブロックと利用可能な小さな画像の間の類似度スコアを計算します これにより 各ブロックの最適なマッチング画像を見つけることができます   最適なマッチング画像の選択：類似度スコアに基づいて 各ブロックに対して上位N個のマッチング画像を選択します このサブセットから 最適なマッチング画像としてランダムに選択します   モザイクの作成：最適なマッチング画像を対応するブロックに貼り付け 徐々にモザイクを構築します   出力画像の保存：最終的に生成されたモザイク画像を指定した出力パスに保存します    パラメータの調整モザイク生成プロセスをカスタマイズするために 以下のパラメータをコードで調整できます：   block size ：モザイクの各ブロックのサイズを決定します 必要な詳細レベルを達成するために調整してください    num images per block ：各ブロックに考慮する小さな画像の数を制御します この値を増やすと モザイクにバラエティが追加されます    output size ：最終的なモザイクの幅と高さを指定します より大きなまたは小さなモザイクを作成するために調整してください    コードの実行  このチュートリアルで提供されたコードをコピーし Pythonファイルに保存します 例： mosaic py     必要に応じて 入力フォルダ 出力パス パラメータを変更します   コマンドを使用してコードを実行します： python mosaic py    生成されたモザイク画像は 指定した出力パスに保存されます    結果結果が以下の通り   image png  上の数字がブロックサイズ 下の数字が画像を作成するのにかかった時間 よりも下げると計算が終わらないので断念 分割していくほど元の画像に近づいているけど これじゃない感がすごい 特に顔の部分が難しい やはり 推しの顔面は奇跡の産物なのかもしれない 各セルに対して類似度の高い上位num images per blockの中からランダムに選ぶ でもでもそんなに変わらないけど だと結構ランダムになる 収集したのがブログの写真だったので 似た構図で何枚か挙げているケースがあるので  くらいでランダムにしたほうがバリエーションあってよさそう とかにしちゃうと同じ色に同じ写真になって モザイク感がなくなりそう    コード解析   Big O　記法計算量 ≒画像を作るのにどのくらい時間がかかるか 画像が枚 n m あるので block sizeをにすると × の計算量 確かに数時間はかかりそうな感じする   メイン画像の読み込みとリサイズ  O    ブロック数の計算  O    出力画像の作成  O    画像ファイルリストの取得  O n   小さな画像のリサイズ  O m   類似度スコアのソートと上位Nの選択  O m log m   出力画像のリサイズ  O    出力画像の保存  O  全体的に このコードの時間計算量は O n   m   num blocks horizontal   num blocks vertical   m   m log m  と近似されます ここで n は入力フォルダ内のファイル数 m は小さな画像の数です    類似度計算の仕組み  画像は np array を使用してnumpy配列に変換されます    img と img のピクセル値を要素ごとに引いて つの画像の差を求めます   差の配列の各要素を二乗して 二乗差を計算します    np mean を使用して二乗差の平均値を計算し MSEが得られます   MSEの値が画像間の類似性の指標として返されます MSEは つの画像の対応するピクセルの強度の平均二乗差を測る一般的な類似性指標です MSE値が低いほど 画像間の類似性が高くなります つまり ピクセル値がより近いか類似していることを意味します 提供されたコードでは メイン画像のメインブロックとすべての小さな画像の間のMSEを計算することで 各小さな画像のメインブロックへの類似性を評価しています これにより 類似性スコアに基づいて最適な一致する小さな画像を見つけ 最適なマッチング画像を出力画像に貼り付けてモザイクを作成することができます 全体的に MSEはピクセルの強度を比較することで画像の類似性を定量化する指標であり 提供されたコードではメインブロックと小さな画像の類似性を評価するための仕組みとして使用されています    さいごにオリジナルの画像をどんな画像にするかが結構重要な気がして 人間の表情をどう表現するかが腕の問われるところかもしれない 今回は単純にセルごとのRGB値で見ていったけど 次はCNNみたいな機械学習手法でも試してみたい ,3,2023-06-03
166,166,マスク着用をAIで判定するやつを簡易的に作ってみた話,画像処理,https://qiita.com/CinnamonSea2073/items/6d71f060585624048133,こんにちは しなもんです とある研究で 東京のコロナの影響について調査する課題があった際 テレビでみた    AI判定でマスク着用率を調べる    やつを思い出し簡易的に作ってみました こういうやつです   ※今回作るAIは精度が低く マスク着用を判定するだけしかできません しょうがないね      note warn機械学習を始めて使う初心者が 同じような人のために成功例を紹介するだけです 専門的な解説は一切ありません ごめんね   環境   pythonPython     ぱいそんですnumpy     画像処理をする際に座標計算とかに使いますicrawler     スクレイピングに使います opencv python      画像処理に使います keras     AIモデル作成に使います matplotlib     学習の様子を可視化するために使います dlib     実際に顔を検出する際に使います    pip install  パッケージ名  でインストールできます    note warn警告cmakeを導入しないとinstallできないパッケージがあります 事前に導入しておいてください ※筆者のバージョンは cmake    でした   スクレイピング画像分析用に マスクをしている人としていない人の画像を収集します   画像を収集するメソッド  引数は画像を保存するパスpath 検索ワードkeyword 収集する枚数num     検索ワードにkeywordを入れたときに得られる画像をnum枚収集keywords   集合写真    マスク 東京  bingから 検索ワードにかかった画像を片っ端から持ってきます ほぼ こちらのコード  を使わせていただきました なぜ 集合写真 と マスク 東京 なのかもこちらで解説されています 参考にしたサイトでは取得する画像は枚ですが   後ほど使う顔抽出がかなり使えず 結果的に使える画像が少なくなってしまうため多めに枚にしてあります 各自調整してください   image png  実行すると このようにフォルダが作られて画像が収集されていきます   顔のみを抽出するここが一番苦労しました      カスケード型分類器に使用する分類器のデータ xmlファイル を読み込み     画像ファイルの読み込み     グレースケールに変換する     カスケード型分類器を使用して画像ファイルから顔部分を検出する     顔の座標を表示する    print face      顔部分を切り取る     保存する詳しい解説は こちらのサイト  にて詳しく書かれています   xmlファイルについては こちらをダウンロードし そのファイルパスを書いてください   image png  成功すると 顔の部分だけが切り抜かれた画像が連なっているはずですが かなりの確率で顔以外のものが検出されてしまいます   image png  もっといい方法があるとは思うんですが 私は初心者なので余計な画像は手作業で連番を完全に無視して消しました いい方法があれば教えてください     学習させてモデルをつくる 画像形式の指定 CNNモデル構造の定義 入力層：xxch 畳み込み層  xのカーネルを個使う 畳み込み層  xのカーネルを個使う プーリング層  xで区切ってその中の最大値を使う 畳み込み層  xのカーネルを個使う 畳み込み層  xのカーネルを個使う プーリング層  xで区切ってその中の最大値を使う 全結合層   出力層   マスクありorなしの値  モデルのコンパイル 画像データをNumpy形式に変換         画像の読み出し        img cv imread fname          画像サイズを x にリサイズ ディレクトリ内の画像を集める テスト用画像をNumpy形式で得る データの学習 データの評価 モデルの保存 学習の様子を可視化こちらは完全にコピペさせてもらいました   テスト用顔写真を数枚入れてください   トレーニング用の画像フォルダとテスト用の画像フォルダに同じ画像が無いようにしてください   テスト用写真は何枚でも構いません 学習のスコアを見ながらその都度調整してください   image png  実行すると学習が始まります   image png  終わると このようにウィンドウが出現します 機械学習初心者すぎて何がどうなのかわからないですが   グラフの青い線はトレーニング中の評価で オレンジの線が実際に判別できるかテストした時の評価です ターミナルには正解率が  ロスが とあります 一見正解率が高く よさそうですがロスが非常に大きく 学習できていないか過学習を抑えられていないことがわかります   初心者だからしょうがないね   本来であればここに一番時間をかけるべき部分なのですが 今回はあくまで簡易的な実装を目指しているので 以下の数値で妥協します   image png  正解率が大きく下がりましたが ロスも半分になったのでヨシ  分かってない    note info具体的には  マスクを着用している画像と着用していない画像の量を同程度にする  学習データを増やすためにスクレイピング量を増やす  学習に向いていないデータを削除する  テスト用画像の量を調整する でロスを下げることができました     正しい方法なのかは知りません     モデルを使用するそれでは完成したAIモデルを使ってみます Webカメラでリアルタイムで検出することもできますが 今回は事前に用意した動画から検出することにします   結果ラベル  保存した学習モデルを読み込む  動画ファイルのパス  動画ファイルを開く  動画のフレームサイズを取得  動画の保存用のビデオライターを作成      動画からフレームを読み込む      画面を縮小表示する    frame   cv resize frame           try           顔検出              顔部分を切り取る            im   frame y y  x x             im   cv resize im                   im   im reshape                       予測              枠を描画  マスクない時 v  は赤で強調する              テキストを描画     結果を保存    out write frame      ウィンドウに画像を出力 カメラを開放capture release  out release   ウィンドウを破棄cv destroyAllWindows  参考にしたサイトはこちらです ※動画を保存するような処理が入っていますが 動きませんでした   why    video mp をpythonファイルと同じディレクトリに置いて実行すると     image png  肖像権保護のためぼかしていますが 正答率 にしてはかなり判別することができています   まとめ機械学習初心者でも テレビでよく見るアレを作ることができました  業務用じゃないので精度は悪いし着用率を求めることはできないですが データ収集から活用までできるので 機械学習をやってみよう って人におすすめです それでは     参考,2,2023-06-01
168,168,リアルタイムでYOLOv7動かして小銭の合計金額を表示する,画像処理,https://qiita.com/hayato0522/items/82334766d1afd7bf5097,  はじめにYOLO  最初の論文  はYou Only Look Onceの略称で 一度見ればOK みたいな感じのモデルです 物体の検出と識別を同時に行うことで高速で処理を行うことができ 他の物体検出アルゴリズムと比較し高速であることが特徴的です そのためリアルタイムのアプリケーションでの使用が可能であるとされており 自動運転や監視システムを始め様々な分野で研究されています 今回はその比較的新しいモデルYOLOv  YOLOv論文  を使って折角なのでデータセット自作して小銭の検出をしてみます  本記事投稿時の最新はYOLOv YOLOvシリーズの中ではYOLOv eeが一番精度が高いようですが サイズの大きいモデルになりそうなのでベーシックなものを使います 基本的にjupyter notebookで実行してます  コードは長いので記事中では折り畳んどきます  YOLO原理参考記事  物体検出手法の歴史   YOLOの紹介       流れ  スクレイピングで画像収集  画像整理 手作業   画像のラベル作成 labelImg   データの拡張 オーグメンテーション   YoLo学習  テスト 画像読み込み   動画でリアルタイムテスト    画像収集以前も使用したBing用のicrawlerライブラリ使いました coin list   一円玉    五円玉    十円玉    五十円玉    百円玉    五百円玉    小銭　画像   スクレイピングを行う関数  スクレイピング実行また検索エンジン違えば収集結果も違うかなと思い こちら   Python Google画像検索からオリジナル画像を根こそぎダウンロード  の記事のコード使ってGoogle画像検索からのスクレイピングもしてます  find elements by class name が現在は使えなさそうだったので一部変えてますがほぼ同じものを使用させていただきました 実行にはchromedriverをインストールする必要あります  こちら     ドライバーのパスあと自分で小銭並べて写真撮ったものも使用してます     画像整理小銭関係ない画像や 被り ラベル付けがしんどそうな画像は手作業で削除します また下記コード適当に編集して使ってフォルダ移動させたり ファイルの名前変えたりしてます  別フォルダへ移動      新しいファイル名      ファイル名の変更     アノテーション作業各画像に対してアノテーションを行います アノテーションでは画像内の小銭を矩形 バウンディングボックス  以下BBox で囲み ラベル付けを行います この作業にはlabelImg  こちら  を使いました 使い方はリンク先に詳しく書かれていますが 注意点として画像をlabelImgで読み込む際に絶対パスに日本語が含まれているとエラーになった気がします labelImgを使用してラベル付けを行っていきます ひたすら単純作業で時間もかかるのでショートカットキー使いながら黙々と頑張ります アノテーションが完了すると各画像に対しtxtファイルが生成され その中にクラスの種類や矩形の座標が保存されます 作業の様子  pic png  作業が終わったらデータ拡張前に訓練データと 検証データ テストデータに適当に分けときます ちなみに この面倒な作業を楽にしようという記事もありました  機会があればやってみたい 参考： labelImgとChainerCVを使ってアノテーション作業を楽にする      データ拡張 augmentation データの拡張 augmentation はデータを加工してデータを水増しする作業です 今回はalbumentationsを使いました albumentationsは画像だけでなくアノテーション作業で作ったtxtファイルのBBox座標も変換してくれます pipして使います pip install  U albumentations硬貨なので画像反転 Flip は不要かなと思いつつ 拡張もそんなにしなくても今回は難しいクラス分けでもないかなと思いつつで回転とぼかしと明暗調整だけやりました スクレイピングした画像はほとんどが明るい空間でピントも合って 同じ向きから撮影されたものばかりでしたので この辺の水増しはあった方がいいかなと思いました また回転させたそれぞれのデータ対して ぼかしを行い その後すべての画像に明暗調整を行えば画像枚数は一気に増えますが 実行に時間がかかりすぎるのも嫌なので 元画像に各処理をするだけに留めております albumentationsの回転処理でRandomRotateがありますが これはランダムに回以上度回転させるもので引数のpは回転させる確率 最大 です p とすると必ずその処理がされますが 必ず 回以上度回転 処理がされるわけで 結果として回転されない場合もあるっぽいです なので Affine を使って回転させるのがよさそうです  affine変換公式ドキュメント  ぼかしは blur  　暗くする処理は RandomBrightness で明るさ範囲を  に固定で変換してます こちらの記事のコードを拝借しつつ 部分的に編集しております   albumentations データ拡張による精度向上を検証する    jpgとtxtからデータをインポートするメソッド        class labels         クラスラベルを格納するリストを作成  albumentationsで変換したデータをインポートするメソッド  度回転したデータを返す  度回転したデータを返す  度回転したデータを返す  ぼかしたデータを返す  暗くしたデータを返す  指定のパスにjpgとtxtファイルでデータ保存するメソッド  画像とバウンディングボックスを表示するメソッド画像とテキストファイルを読み込んでデータセットを作成し そのデータセットに各処理を施し PCに保存します   データセットに上記処理したデータを追加保存もともと訓練用画像が全部で枚で 上記のまま実行するとデータセット配列の番目は度回転したデータがあるので適当に確認してみます    python python  度回転画像のデータ確認width  height  color  dataset   image shapedataset   visualize height  width 画像が度回転されており BBoxも正しい位置に変換されています  暗さは元々こんな画像です  オリジナル ぼかし 暗    度回転データセットの各ラベルの数を確認しときます   アノテーションデータ取得  クラスラベル数える本当は全クラス同程度が望ましいですが   ラベル 円玉 がやや少なかったです defaultdict                                 YOLOで学習YOLOvをcloneして使います git clone またwandbを登録してインストールしておくと学習結果やPCの状態をリアルタイムで確認できます yamlファイルを編集します yamlファイルには画像へのパスやクラス数 クラス名などを記述します フォルダの階層実行使うモデルに合わせて重みは予めダウンロードしておきます  ベーシックなvはもともと入ってたと思います サイズの大きいモデルを使うときはtrain pyではなくtrain aux pyにして最後の部分もpからpに    hyp   data hyp scratch p yaml  したりする必要があるらしいです   nameは結果の出力先 run train name   もし OMP  Error   のエラーやメモリ不足っぽい場合は以下のシステムのアクセスを許可する設定のコード実行してから 学習させたり実行時のバッチサイズ小さくします runフォルダ内に実行結果や重みデータ 諸々が保存されます 結果  results png  混同行列からもだいたいきれいに判別できてそうです 背景を硬貨と判別してしまってる部分ややありました 今回ほとんど硬貨のアップ画像なので他のものはあまり映っていなかったですが アノテーションがちゃんとできていなかったものがあり そういう結果になってしまったみたいです この円玉が並んだ画像だと見切れている部分も予測してくれてますが ほとんど見えてないし と思ってアノテーション作業はしていませんでした 少しでも映り込んでいればBBoxつけてラベル付けすべきでした 反省     テスト学習させたモデルを使って推論を行います 見切れていたり重なっているとやや信頼度低めの部分もありますが ほぼほぼ判別できていそうです  二枚目の見切れている円が円判定されてます 表示する文字の大きさや色などを変える場合はdetect pyやplots py内の描画する関数あたり編集すれば変更できます    jpg     jpg      リアルタイムで検出 合計金額計算最後にWEBカメラ映像を入力とし リアルタイムで判別できるようにして あと合計金額表示させてみます 以前行った自分でtensorflowでモデル組んで学習させたときと同じ流れでできるかと思ってましたが モデル読み込んで 諸々処理して が大変そうだったのでdetect pyを直接編集して動かしてます detect pyの不要な部分を全部消し 入力はWEBカメラ映像固定にして あと検出結果に応じてフレームごとに合計金額を計算する部分を追記してます また雑ですが c キーでプログラム終了を追加してます 途中で呼び出している check imshow 関数内にあるwaitKeyについても同様の処理追記してて c キーで終了させるようにしてます  ターミナルで実行する場合は普通にctrl cで十分ですが    最後の行のの部分のiou thresでIoU  IoUについて  の閾値 conf thresで信頼度スコアの閾値決めてます 元のdetect py使えなくなっちゃうので 別名つけて作ってます 実行円くらいのwebカメラなので画質悪いですが モデルはちゃんと機能していそうです 途中から真ん中に円が誤検出されてしまっています 誤検出の信頼度のスコアが低い場合は閾値設定で無理やり消したりはできそうですね     おわりに自分で学習モデル作らなくても精度よく検出してくれるモデルがこんな簡単に使えるのはなかなかいいなと思いました 具体的な層の構造とかも従来モデルと比較して理解していきたいと思います 最新のYOLOvのほうはメモリが足らず vで動かしたのですが その後メモリ増設してYOLOvも実行してみました 流れや精度は大きくは変わりませんでしたが 実行時にvではなかった正規化前の混同行列や各クラスのラベル数なんかも追加されててデータや結果が直感的にわかりやすいなと思いました  青のグラフは画像サイズの分布      参考Qiita記事スクレイピングの仕組みについて： Pythonで画像スクレイピングをしよう  YOLOv参考：  物体検出 YOLO最新版のYOLOvを試してみる　〜デモから学習まで〜  YoLov参考： Yolovのつかいかた 推論 学習   自作データセットでの流れ： ふうたろおう 風太郎 に俺はなる  YOLOv   ,15,2023-05-31
179,179,【Swift】iOSアプリで画像認識をする手段（Visionの活用）,画像処理,https://qiita.com/ryuprogrammer/items/ce0f0f80eebe7009275d,  目次 はじめに 執筆した理由 結論が端的に書いてあります   iOSアプリで画像認識をする手段 CreateMLでモデル作ってみた  Visionだけで手を識別してみた  結論    はじめにこの記事は分で読めます 初投稿のため 分かりにくい部分 間違ってる部分等ありましたら教えていただけると幸いです 分かりやすさ重視でざっくりとした内容なので 具体的なやり方は書いてありません あくまで 技術選定の部分にフォーカスしております 個人開発で 画像認識を用いた後出しジャンケンアプリ を開発しました 開発をする上で  画像認識 の技術選定に手間取ったため記事を書きました 結論としては Vision frameworkのみでじゃんけんの手を識別しました     iOSアプリで画像認識をする手段    iOSアプリ開発で画像認識をする手段の一例です ①Appleの公式サンプルモデルを使用して画像認識②CreateMLでモデルを作成してから画像認識③Pythonとかでモデルを作成してから画像認識④Vision frameworkのみで画像認識    それぞれの選び方まず ①Appleの公式サンプルモデルにやってみたいモデルがないか確認しましょう もしある場合は この記事の役目はここまでです Appleの公式サンプルモデルにやってみたいモデルがなかった場合 モデルを自作する必要があります モデルは ②CreateMLとか③Pythonで作れます  他にもあると思いますが   最後に Vision frameworkのみで画像認識をする方法もあります ちなみに筆者は以下のような流れで開発しました ①じゃんけんを識別する公式サンプルモデルがなかった②CreateMLでモデルを作ってみた →うまくいかない③Vision frameworkだけで手を識別してみた →うまくいった     CreateMLでモデルを作ってみた     CreateMLでモデルを作る手順と注意点CreateMLでモデルを作る手順は主につです  識別したい写真をクラス毎にいっぱい撮影する  CreateMLでトレーニングするまず クラス毎に写真を撮影します このとき 量が大切だと思ってとにかく沢山撮るとうまくいきません 質も大切です 質とは 背景の単色化をしたり あえて画質の悪いものを撮影する 識別するものを様々な角度から撮影するなどです 量は識別したいものによって異なります 例えば  机 と 椅子 を識別する場合は 全く見た目が異なるので そこまで量は必要ありません  各枚程度でもうまく識別できるかも しかし  グー と チョキ と パー のように 同じものの異なる状態 の場合にはかなりの量が必要です  各枚でも足りないかも 次にCreateMLでトレーニングをします CreateMLではクラス毎のファイルを突っ込んでトレーニングボタンを押すとモデルを作ってくれます 詳しくは他の記事を参考にしてください     じゃんけんの手のモデルをCreateMLで作るじゃんけんの手は 同じもの 手 の異なる状態 なので グーチョキパーそれぞれ枚以上撮影しました さらに 何も写っていないクラスも別途用意しました これらを用いてモデルを作りましたが識別率は 以下でした       CreateMLが適している場面CreateMLなどの パターン認識では 同じものの異なる状態を識別するのは難しいです 大量に撮影すればいけるけど    なので CreateMLは 見た目が全く異なるもの を識別するのに向いています     Visionだけで手を識別してみた     Vision frameworkで手を識別する方法Vision frameworkでは 顔 体 手の 座標データ が取得できます 他にもいろいろできますが 座標データを使用するのでVision frameworkでできることの詳細は省きます 座標データを取得したら 位置関係 角度 長さ を求めます この位置関係の情報を用いて識別します     じゃんけんの手をVisionで識別① 親指以外の 指先 第二関節 手首 の座標データを取得します    swift   指先let indexTip   points VNHumanHandPoseObservation JointName indexTip rawValue   location     zerolet middleTip   points VNHumanHandPoseObservation JointName middleTip rawValue   location     zerolet ringTip   points VNHumanHandPoseObservation JointName ringTip rawValue   location     zerolet littleTip   points VNHumanHandPoseObservation JointName littleTip rawValue   location     zero   近位指節間 PIP 関節 ＝ 第二関節のことlet indexPIP   points VNHumanHandPoseObservation JointName indexPIP rawValue   location     zerolet middlePIP   points VNHumanHandPoseObservation JointName middlePIP rawValue   location     zerolet ringPIP   points VNHumanHandPoseObservation JointName ringPIP rawValue   location     zerolet littlePIP   points VNHumanHandPoseObservation JointName littlePIP rawValue   location     zero   手首let wrist   points VNHumanHandPoseObservation JointName wrist rawValue   location     zero②  手首から指先までの長さ  手首から第二関節までの長さ を算出   swift   手首から指先の長さlet wristToIndexTip   distance from  wrist  to  indexTip let wristToMiddleTip   distance from  wrist  to  middleTip let wristToRingTip   distance from  wrist  to  ringTip let wristToLittleTip   distance from  wrist  to  littleTip    手首から近位指節間 PIP 関節の長さlet wristToIndexPIP   distance from  wrist  to  indexPIP let wristToMiddlePIP   distance from  wrist  to  middlePIP let wristToRingPIP   distance from  wrist  to  ringPIP let wristToLittlePIP   distance from  wrist  to  littlePIP ちなみにdistanceは以下のように三平方の定理を用いて求めます ↓   swift   画面上の点間の距離を三平方の定理より求める③ それぞれの指で②のつの長さを比較して 指が曲がっているか判定 手首から指先までの長さ より 手首から第二関節までの長さ が長いとその指は曲がっています    swiftwristToIndexTip   wristToIndexPIP④ 曲がっている指によってグーチョキパーの識別   swift   HandPoseの判定 どの指が曲がっているかでグーチョキパーを判定する ifwristToIndexTip   wristToIndexPIP   wristToMiddleTip   wristToMiddlePIP   wristToRingTip   wristToRingPIP   wristToLittleTip   wristToLittlePIP         本の指が曲がっていないのでぱー       本の指が曲がっているのでぐー       IndexとMiddleが曲がっていないのでちょき結果は 識別率 以上という結果になりました     Vision frameworkのみでの識別が向いている場面じゃんけんの手の識別のように 座標データから簡単に識別できる場合や座標の変化量を使用したい場合に適しています 座標の変化量を使用する例としては  姿勢管理アプリで体がどれくらい曲がっているか などです     結論今回 じゃんけんアプリでは④のVision frameworkのみで手を識別しました しかし 他にももっといい方法があるかもしれないので あくまで参考程度にしてください じゃんけんアプリのコードは以下からご覧になれます  画像認識を用いたじゃんけんアプリのコード  最後まで読んでいただきありがとうございました もし間違っている部分等あれば教えていただけると幸いです ,4,2023-05-27
184,184,Unityシーン容量削減勉強会 第2回 ～テクスチャ編～,画像処理,https://qiita.com/segur/items/4559edc2e0d4353204d9,  Unityシーン容量削減勉強会 第回  テクスチャ編 本記事は シーン容量削減勉強会の第回の資料ですが 第回を読んでいなくても問題ありません 興味があれば 第回も読んでいただけると嬉しいです       Unityシーン容量削減勉強会シリーズ   Unityシーン容量削減勉強会 第回  容量順でソートする編     Unityシーン容量削減勉強会 第回  テクスチャ編  ← 本記事   Unityシーン容量削減勉強会 第回  モデル編      Unityシーン容量削減勉強会 第回  アニメーション編     テクスチャの設定を見直そう スペックの高くない視聴端末でも快適に動作するように Unityシーンの容量を削減することは重要です 大抵のUnityプロジェクトでは テクスチャが容量の大部分を占めるので テクスチャは積極的に軽量化しましょう テクスチャファイルを選択すると   Inspector  ウィンドウに以下のような設定項目が表示されます   image png  今回はこれの解説をします これらの設定項目はあくまでビルド時に利用するものであり 変更しても元のテクスチャファイルに影響はありませんので 気軽に変更していろいろ試してみましょう   オススメの値結論を知りたい人向けに オススメの値を掲載しておきます   プロパティ                    オススメな設定         順番に解説します   現状のビルド後容量を確認する Inspector  ウィンドウの下の方にテクスチャ画像がプレビューとして表示されています さらによく見ると 下図の赤字のように容量が表示されています これが現状のビルド後容量です   image png  このプレビュー画像と容量を参考にしながら 調整していきましょう   Alpha Source  image png   Alpha Source  は テクスチャの不透明度をどこから取得するのか指定するオプションです   Alpha Source            機能                     容量    用途例              None                   透過なし                  低     不透明画像       Input Texture Alpha    Aチャンネルから不透明度を取得   高     透過PNG画像     From Gray Scale        RGBの平均値から不透明度を取得   低     グレースケールマスク    透明度を利用しない場合   None  を選択   して容量を削減しましょう 以下のようなグレースケールのマスク画像の場合は   From Gray Scale  を選択しましょう   DissolveTexture jpg    Generate Mip Maps  image png   Generate Mip Maps  は ミップマップを生成するかどうか決定します ミップマップとは 元のテクスチャの の大きさ  の大きさ  の大きさ といった具合に縮小していった一連の画像ファイル群です     image png      図転載   Unity Documentation   ミップマップの基本    高解像度のテクスチャが遠くにあると ジャギーが発生するという問題があります 遠い物ほど小さい解像度の画像を使用することで このジャギーを抑えられます 加えて 描画負荷を軽減する効果もあります     image png      図転載   Wikipedia   Mipmap    ただし ミップマップは元テクスチャとは別に保管しなければならないので シーン容量が増えてしまう問題があります そのため 遠くに描画することがないテクスチャは ミップマップを使わない方が容量を削減できます たとえば    UI 用の画像は    ユーザーから離れた位置に描画することがないため    ミップマップをOFFにしましょう     Max Size  image png   Max Size  は テクスチャの最大解像度です 元のテクスチャの解像度が    Max Size  で指定した値よりも大きい場合    Unityはテクスチャを  Max Size  の値に   縮小   します    mermaidgraph LR    A 元解像度 逆に 元のテクスチャの解像度が    Max Size  で指定した値よりも小さい場合      Max Size  を大きくしてもテクスチャの   品質は向上しない   ので ご注意ください    mermaidgraph LR    A 元解像度  Max Size  が小さいほど容量は減りますが 画質も低下するので    見栄えとパフォーマンスのトレードオフ   になります                    の選択肢があります のべき乗になっているのは GPUが効率的に処理できるからです    Max Sizeをどのくらいにすべきか どのくらいの値にすべきか は テクスチャの用途によるので 一概には言えません ただ あくまで私の感覚的には    以上の場合は 本当にその解像度が必要なのか確認したくなります    低解像度でも問題になりにくい画像人間は明度の変化には敏感ですが 色や透明度の変化には鈍感だと言われているので 以下のような画像は低解像度でも問題になりにくいです     例  グラデーション画像以下のような画像は 明暗の変化が激しくないので 低解像度にしても問題にはなりにくいです     例  グレースケールマスク画像人は透明度の変化には鈍感だと言われているので 以下のようなグレースケールマスク画像は 低解像度にしても問題にはなりにくいです   DissolveTexture jpg    Resize Algorithm  image png   Resize Algorithm  は テクスチャ解像度を縮小する際に使用するアルゴリズムです 以下のつの選択肢があります   双一次補間   双三次補間 Bicubic の一種    周辺xの色を元に色を決定   周辺xの色を元に色を決定    ギザギザになりやすいが シャープさは保持しやすい    なめらかになりやすいが ぼやけやすい     図転載   画素の補間 Nearest neighbor Bilinear Bicubic の計算方法     上記の転載画像は拡大表示時のものであり 違いがわかりやすいのですが テクスチャ縮小時はここまではっきりとした違いは出にくいです どちらの縮小も試してみたのですが 正直私には違いを認識できませんでした そのため 基本的にはデフォルトの  Mitchell  を選択していればよいと思います ただ リンギング 文字等のコントラストの高い部分に不自然な輪郭が発生する現象 がもし発生して気になる場合は   Bilinear  に変えてみると改善するかもしれません   Format  image png   Format  はピクセルの表現方法を決定します 以下の選択肢があります   形式           R     G     B     A     bpp   用途例       Automatic                                    下記の中から最適なものを Unity が自動で設定します      R            bit               bit    グレースケール画像  いろいろ選択肢があって混乱しますが     Automatic  が無難です   意図せず  RGBA bit  になっている場合があるそうですが     RGBA bit  は非常に重たいので    透過画像ではないなら   避けるべき   です グレースケール画像なら   R   を選択するのもアリだと思います   Compression  image png   Compression  は テクスチャの圧縮タイプです 以下の段階あります   圧縮タイプ         テクスチャ品質   容量   High  よりも  Low  の方が 容量は減りますが画質も低下するので    見栄えとパフォーマンスのトレードオフ   になります  None  は圧縮をしないので画質は最高ですが パフォーマンスが大幅に低下するリスクがあります   Use Crunch Compression  image png   Use Crunch Compression  はクランチ圧縮を使用するかどうかの設定です GPUはJPEGやPNG形式のファイルをそのまま読み込めないので テクスチャはDXTCやETCといったGPUに特化した形式に変換されます クランチ圧縮は そのDXTCやETCをさらに圧縮する仕組みです 一部のディティールが失われることがありますが 全体の品質を維持したまま    大幅に容量を削減   できます   Compressor Quality  image png   Compressor Quality  はクランチ圧縮の品質です  Use Crunch Compression  にチェックが入っていると設定できます 数字が大きいほど テクスチャの品質は上がりますが 容量も大きくなるトレードオフがあります 私がいろいろと検索してみたところ     に設定している人が多いようです   さいごに今回はここまでです おつかれさまでした 次回はテクスチャ編です 引き続きこちらも実践していただけると嬉しいです 本記事は 以下のページを参考にさせていただきました ありがとうございました    Unity Documentation   テクスチャのインポート設定     Unity Documentation   推奨 デフォルト およびサポートされているテクスチャ形式  プラットフォーム別      Unity Documentation   テクスチャ圧縮形式     Unity Documentation   ミップマップの基本     UNITY アプリのビルドサイズ容量削減する方法 総まとめ     Unity のテクスチャ圧縮について知ってること     画像を美しく加工するために押さえておきたいピクセルの知識      Unity Crunch 圧縮でロード時間は速くなるのか      Unity で作ったアプリのサイズを減らす  の方法 Unity  容量削減      容量削減ノウハウ     画像リサイズのうんちく  補間フィルタ   ,33,2023-05-25
185,185,SIMD命令によるプログラム高速化,画像処理,https://qiita.com/ob_nullpo/items/e7fa776183dca00fc70f,　こんにちは ディマージシェアの技術担当です  以前の記事  で小数点演算を整数演算で近似してパフォーマンス改善を図る方法を紹介しました 今回は 更にSIMD命令を用いてパフォーマンスを改善してみようと思います   SIMD命令　SIMDはsingle instruction  multiple dataの略で その名の通り つの命令で複数のデータを処理します 最近のIntel系CPUではAVXと呼ばれる命令セットが使用でき 命令で個のbit整数 bitレジスタ を処理できます 詳細はgoogleで調べると資料はたくさんあるので ここでは割愛します 　SIMD命令は回の演算で複数のデータを処理できるので高速化に利用できます 理想はコンパイラが自動で良いコードを出力してくれれば良いのですが 現状は難しいようで ソースコード内にSIMD命令を使用するように明示します C言語では SIMD命令と関数がほぼ 対応の組み込み関数が用意されています  avx intrinsics のようなキーワードで調べると出てきます   SIMDを用いたコーディングこのfor文をSIMD化してみます SIMD化するとこのようになります もともとはシンプルだったfor文ですが ちょっと複雑になります   ベンチマークSIMD化前time     secSIMD化後time     sec 倍の性能になりました   まとめ　プログラムの高速化は 様々な手法があり 今回はSIMD命令を採用してみました SIMD命令は命令でたくさんのデータを処理できますが 計算結果を取り出すのに少し手間がかかります 今回は 乗算命令は加算やビット演算と比べるとコストが高いことに着目しています 乗算がSIMD命令で 回で済む代わりに 結果の取り出しにたくさんのビット演算を行っています それでも結果としてパフォーマンスが改善しています CPUの命令レベルでは同じ命令でも 命令ごとにコストが違います 命令ごとのコスト差を意識することで より高速なコードを書くことができます   付録 ソースコードへのリンク  ,5,2023-05-24
186,186,【プログラム高速化】整数演算による浮動小数点演算の近似,画像処理,https://qiita.com/ob_nullpo/items/f6037f8622c848d8cc3b,　こんにちは ディマージシェアの技術担当です  以前の記事  で計算量削減によるプログラムの高速化を紹介しました 今回は使用する演算命令を変えることで更に高速化してみます   整数演算による浮動小数点演算の近似　計算結果の小数部分を切り捨ててよい計算は 浮動小数点の計算の代わりに 整数の掛け算にしても結果が概ね一致します この部分を と書き換えることで 浮動小数点数の掛け算を整数同士の掛け算に書き換えることができます このソースでパフォーマンスを確認してみます   ベンチマーク浮動小数点の掛け算の場合time     sec整数演算に書き換えた場合time     sec使う演算命令を浮動小数点演算から整数演算に書き換えたことにより  倍のパフォーマンス向上が実現できました 初版のプログラムが秒もの実行時間だったことを考えるとかなりの改善です   まとめ　普段のプログラミングではほとんど意識しない命令レベルのパフォーマンス差ですが  アルゴリズム最適化 メモリアクセス最適化あたりの性能向上手段を試した後だと目に見えるパフォーマンス差として現れます ここから更に インラインアセンブラやSIMD化など より高度な細工をすることで更にパフォーマンスを改善することができます 　愚直に実装すると絶望的なパフォーマンスになるプログラムでも ちょっとした工夫や制約を加えることで劇的にパフォーマンスを改善できることは珍しくありません また 採りうる工夫も多岐に渡ります 試してみましょう ,3,2023-05-24
187,187,計算量削減によるプログラム高速化,画像処理,https://qiita.com/ob_nullpo/items/535a32d35a3ff18feca7,　こんにちは ディマージシェアの技術担当です  以前の記事  でスキャンした書類画像を水平にするプログラムを作りました そして  こちら  の記事で並列処理を用いてプログラムの高速化を行いました 今回は別のアプローチでプログラムの性能を向上させてみようと思います   計算量の削減　処理するのは書類のスキャン画像と仮定します その場合 カラー画像やグレースケールの画像として処理する必要はほぼありません モノクロの値データとして処理しても結果は同じになるはずです また 多くの印刷物は黒で塗りつぶされているわけではありません 紙面のうち印字されている面積は高々 ほどです なので ビットマップとしてデータを保持するのではなく 黒の画素がある座標を配列で保持する方法に書き換えることで性能向上が期待できます       黒画素がある座標を配列にする  メモリのアクセス効率の向上　一般的にメモリは連続アクセスは高速であり 不連続なアクセスは時間がかかります 先述のようにデータの持ち方を変えることにより 不連続なメモリアクセスを回避することができます 次元的なメモリ領域に対して 斜めにアクセスするのは非効率ですが データの持ち方を変えたことにより 画素がある座標を頭から最後まで読むだけになります   ベンチマーク　データの持ち方を変える前のプログラムで  core  threadではtime     secでした データの持ち方を変えたプログラムでは time     secとなりました  倍の性能向上が実現できました 更に OpenMPで cores  threadsで実行したところ time     secとなり 更に 倍の性能になりました 扱う画像が値画像という制約が加わりましたが GPGPUと肩を並べる性能向上が実現しました   まとめ　計算量的に重い処理を実装するとき 正面突破を試みると難しい場合でも 少しの制約を加えたり 計算機のアーキテクチャに寄り添った実装を行うことで性能を十数倍に伸ばすことができます 　マルチスレッドやGPGPUを用いた力業で解決する方法もありますが 処理そのものを軽量化 最適化するというアプローチも可能です 重い処理と向き合わなければならないときの選択肢として覚えておきましょう   付録　 ソースコードへのリンク  ,2,2023-05-24
188,188,並列処理によるプログラム高速化,画像処理,https://qiita.com/ob_nullpo/items/06e78685d59057642317,　こんにちは ディマージシェアの技術担当です 以前の記事   でスキャンした書類画像を水平にするプログラムを作りました 作成したプログラムを用いれば 大抵の印刷物の画像は水平にすることができます しかし 高解像度の画像では処理に時間がかかります 印刷を想定したスキャンではdpi dpiで設定されることが多いです 今回は プログラムの一部を並列化することで処理時間の短縮を図ります   並列処理　現代のCPUはマルチコアで構成されているものがほとんどです 理由は単純で シングルコアで性能を伸ばすよりも コア数を増やすことで性能を伸ばす方が電力効率が良いためです プログラムを作成する際 明示的に複数のコアを使用するように記述することで 処理性能を向上させることができる場合があります このように処理を並列に動かすことを並列処理と呼びます   GPGPU　計算機の多くはGPU Graphics Processing Unit と呼ばれる 画面に映像を出力するハードウェアを搭載しています GPUは画面の全ての画素に表示する色を算出する処理を行います その処理の性質上 GPUは小規模なプロセッサの集合体として実装されます 最新のモデルでは万個以上のプロセッサを持っているものもあります このGPUを画像出力以外に 汎用的な計算に応用する技術がGPGPUと呼ばれています GPUとの親和性の高い処理の場合 CPUのみで処理を行う場合の倍以上の性能が得られることがあります   CPUスレッド並列処理　CPUスレッドを効率よく使うためにはOSの知識が必要不可欠です CPUスレッド並列で実装する際 厳密にパフォーマンスチューニングしたい場合はスレッド処理を手書きします しかし スレッド処理を自前で実装する場合はデバッグの困難性に直面することが多いです また 並列処理にしたからと言って必ずしも処理が速くなるわけではありません そのため 並列化したい処理がどの程度 並列処理と親和性があるかどうかを確かめる必要があります そのようなとき OpenMPのようなAPIを利用します OpenMPでは for文の前に行  このfor文を並列処理にしてください と宣言するだけで並列処理の実装ができます 　以前作成した 画像の傾きを求める関数の一番外側のfor文を並列処理にしてみます 　入力する画像はdpiでスキャンした画像です 並列化前後の実行時間を計測してみます 使用するCPUはAMD Ryzen  Xです  core  threadでの実行時間time     sec cores  threadsでの実行時間time     sec処理を並列化したことで およそ倍の性能になりました   SIMD　CPUでは スレッドでの並列処理以外にも 演算命令レベルでの並列処理が使えることが多いです SIMD命令と呼ばれる処理です 例えばIntel CPUのAVXという命令セットを用いることで 最大であと倍程度の高速化が見込めます しかし SIMD命令を用いたプログラミングは非常に複雑なため 今回は割愛します   GPGPUを試してみる　nVidia製のGPUを搭載している計算機では CUDAと呼ばれる C言語を拡張したコードでGPU用のコードを記述することができます 　nVidia GeForce RTX  SUPERで実行してみます ソースコードは本記事の末尾に付録として載せます time     sec　CPU coreの倍 CPU  threadsの 倍の性能になりました CPUでスレッド並列 SIMDでチューニングしても GPUの方が速そうです   並列処理を試す勘どころ　一言で並列処理と言っても CPUスレッド SIMD GPGPU など様々な手段が存在します また 並列処理によって処理時間が短くなるかどうかはおおよその推測はできますが 実測値を見るまでは油断できません また コーディングが難しくなる世界ほど実効性能の予測が難しい傾向があります 　私の個人的な感覚ですと   OpenMPを試す  GPGPUを試す  SIMDを試すかどうか決める　のような順位が妥当かと思います OpenMPは行書くだけで実測値が得られるのでコストも安く 得られる性能も良いことが多いです 次に OpenMPが良く効くような処理では GPGPUも効く可能性が高いです もちろん処理の内容次第ですが  GPGPUのコーディングは少し難易度が上がりますが SIMDを丁寧に書くほど難しくありません GPGPUを試してみて CPUスレッド並列の倍 SIMDを用いた場合の論理限界 を超える性能が得られれば検証は終わりにします 倍未満の性能だった場合 SIMDを用いることでGPGPUの実効性能を超える可能性があるのでSIMDを試します   まとめ　計算機リソース的に重い処理でも並列処理の技術を応用することで実用レベルの性能を得られることがあります 現代のCPUはメニーコア化の傾向が強く シングルスレッドでの処理性能向上は既に限界を迎えています CPU以外にもGPGPUという技術もあります 並列処理はコーディングが複雑になりますが 現代の計算機を効率よく使うためには必要不可欠な知識です 実践は難しいかもしれませんが 知識だけはアップデートしておきましょう   付録　ソースコードへのリンク   当社に興味を持たれた方は HP  もご覧ください   ,14,2023-05-23
189,189,スキャンした書類画像を水平にする,画像処理,https://qiita.com/ob_nullpo/items/0d6f8f5886fdec331f20,　こんにちは ディマージシェアの技術担当です 皆さん 書類をスキャンする機会は多いと思います そんなとき  なんか傾いてるんだけど となったことは多いのではないでしょうか 今回は スキャンした書類画像を水平にするプログラムを作成しようと思います 言語はC   使用するライブラリはOpenCVです   入力画像の用意  image png  盛大に傾いてます 結果をわかりやすくするために意図的に用意しました これを   image png  こうします 余白が少しおかしいのは今回のテーマから外れるので無視してください   画像の入出力 回転操作の実装　画像を水平にするためには  画像の入出力  回転操作 が必要です OpenCVを用いれば簡単に実装ができます 今回の環境はWindows   Visual Studioです OpenCVのセットアップなどは各々行ってください googleで調べればわかります           画像データが読み込めなかったときは終了する　main関数のなかに  img png をロード rotate imgで画像を ラジアン 回転  out png に書き込み というコードを書きました   image png  　回転処理が実装できたことがわかります ここでつ問題があります 回転前後の画像サイズを考慮しなかったため 回転によって見切れてしまった部分が現れました これを直します 　OpenCVは便利です 回転後の解像度 平行移動の量を計算することにより 回転させた後の画像が見切れることを防ぐことができます 修正後の出力は次のようになります   image png  あとは main関数でおもむろにを入力していたところに  修正したい角度 を入れることができれば今回の目的は達成できます   画像の傾きを検出する　今回のメインテーマです 画像の傾きを検出するということは コンピュータに対して 水平な状態とは何ぞ を教えてあげる必要があります 今回は 印刷物の多くに共通する特徴 を考えてみます まず 水平な印刷物の 水平方向に積みあがっている有効画素数 をグラフにしてみます   image png  　次に 傾いた印刷物で同様にします   image png  　印刷物の多くは文字を水平方向にレイアウトします タイプライターの頃からのお決まりです そして 水平方向にレイアウトされた印刷物の横方向に注目すると なにも印字されていない水平方向 何かしら文字がある水平方向 に分かれます 上のグラフでもその傾向はよくわかります 今回は 画像を横方向にパースして 有効画素を積み上げた結果 つ目のグラフのような傾向がみられる場合を水平と定義することにします 　先の記述だけだと まだ計算機が扱うには不便です もう少し式を具体的にします 上記のように グラフの線が上下に大きくブレるほど大きな値を取る ようになる計算式を考えればよいので  横方向に積みあがった有効画素数の偏差値の合計 としました この計算方法が良い結果を示すかどうか観察してみます 　画像を 度ずつ回転させながら 上記の式の値を計算し グラフにプロットしてみます   image png  　グラフの形を見る限り良い傾向と解釈できます 度回転させると良いようです 式の値が最大となる角度を求めるプログラムを書いてみます       回転後に必要なサイズ計算      グレースケール化      傾き検出　  inclination detectionは角度をt tまでを split刻みで実際に画像を回転させながら 先述の偏差値がmaxになる角度を返します この際 画像がRGBのカラー画像だと有効画素の解釈が難しいので 輝度でグレースケール化してから処理しています まず 度から度をで刻んで 度単位の結果を算出 得られたmax周辺を更に分割してmaxとなる角度を求めています この関数で得られた値で画像を回転させてみましょう           画像データが読み込めなかったときは終了する得られた画像がこちらです   image png  画像が水平になりました   全体のソースコード      回転後に必要なサイズ計算      グレースケール化      傾き検出          画像データが読み込めなかったときは終了する当社に興味を持たれた方は HP  もご覧ください ,7,2023-05-23
190,190,matplotlibで2次元配列の解像度に合わせた画像を保存する方法,画像処理,https://qiita.com/yusuke_s_yusuke/items/144381fd73986afb0207,  はじめにmatplotlibで 次元配列を画像として保存する必要がある場合に 配列のサイズと同じ解像度の画像を保存する方法をいくつか紹介します 本手法により データの解像度を保ったまま画像サイズを調整し matplotlibの補間による劣化を防ぐことができます 特に 生のデータを扱う方にとって つの配列要素に画素が対応する画像はありがたいと思います 本記事の目標  image png    実装Google Colabで作成した本記事のコードは  こちら  にあります    各種インポート   Python 各種インポート   方法matplotlibで生成される図のピクセル数は   figsize  横  縦   とdpiを用いて    横   dpi  ×  縦   dpi   となります このため 配列の縦横の個数と生成される図のピクセル数を一致させることで 配列の各要素をピクセル単位で表現することができます 本記事では  配列と同じサイズのカラーマップを作成する方法  の紹介します ※本記事の実装コードは 配列以外の情報 例えばカラーバーや軸ラベルなど を図に含めると その情報も画像サイズに含めて生成されるため 配列をピクセル単位での表現が損なわれることになります 簡易的なコードで 配列をピクセル単位で正確に表現するためには 図に含める情報を配列のみに制限する必要があります もしそれらも含めて実装しようと思うと 例えば 配列が小さすぎる場合には 文字などが小さくなりすぎる ドット絵のようになる といった問題が発生し データの損失がないようにするには 配列のサイズの整数倍に拡張したりする必要があります しかし すべてのケースで汎用的に実装することは困難なため 本記事ではこの問題については触れず 配列と同じサイズの画像を生成する方法の紹介に留めています これが一番シンプルで美しい方法だと思います コードの説明    dpi   は    横   dpi  ×  縦   dpi   がピクセルサイズとなり   figsize  に配列のサイズを指定しているため   dpi   にします     tight layout True  で 余白をなくします 配列のサイズと完全に同じサイズの画像サイズにするためです     ax axis  off    は 軸情報を消すために使います 軸情報が含まれると 配列をピクセル単位で表示できなくなるためこの部分も必要です   plt savefig  image png   dpi  は   dpi   で保存します    plt savefig  の  dpi  に何も指定しないと   plt subplots    で指定した場合の  dpi  と同じものが使われるかもしれませんが 私自身調べてもよくわからなかったので 念の為ここでも  dpi  を指定しておきます  出力結果縦  × 横  ピクセルで出力されています 方法でいいと思います コードの説明  fig  ax   plt subplots figsize image shape       dpi    だけでは 余白が含まれてしまいます そこで   fig subplots adjust left   bottom   right   top    で 画像の描画エリア全体をとした比率で指定しており この場合 縦横のエリアを余すことなく指定しています そうすることで ピクセル単位で表示できます 方法でいいと思います コードの説明  fig add axes x  y  width  height   は 表示エリアの   x  y   を左下の座標として 幅と高さをそれぞれ全体をとした比率で指定します   ax   fig add axes            とすることで 描画エリアを縦横余すことなく使うことになり ピクセル単位での表示を可能にしています   まとめmatplotlibで 配列を同じサイズの画像で保存する方法を紹介しました matplotlibは配列サイズに比べて画像のサイズが小さい場合に補完を行われ データの表現が変化してしまうため 数ピクセル単位に関心がある際には注意が必要です データの損失なく画像を見たい場合に 本記事を活用していただければ幸いです また matplotlibの補完の振る舞いについて 私の記事で恐縮ですが以前紹介しているので よろしければそちらもご覧ください   参考資料,2,2023-05-22
191,191,equirectangular画像をcubemap画像に変換する方法,画像処理,https://qiita.com/Hiroaki-K4/items/3c9654a9ac0ea3e916c7,   equirectangular画像とcubemap画像equirectangular画像は度カメラで撮影したデータとして よく用いられるフォーマットです 下画像のように度の景色を枚の写真に収めることができます ただ equirectangular画像は真上と真下のピクセルが必要以上に引き伸ばされて  歪みが大きくなる  という問題があります この歪みを解消するには 度画像で撮影した景色を球とすると それを囲むような立方体を考えて その面に投影するという方法があります そのような面画像は  cubemap画像   下画像 と呼ばれています 今回はequirectangular画像からcubemap画像への変換をPythonで実装してみました データセットとしては Youtuberの Young  さんが公開しているデータを使用しています    Algorithm枚の画像を作成し 最後に枚の画像にまとめることでcubemapを作成します 今回は 真下画像の作成を例として説明します 同じ考え方で他の面の画像も作成することができます       撮像面を作成する真下画像の場合は z軸方向のベクトルを固定して球の真下に撮像面を配置します 撮像面のサイズは 出力画像サイズとなります       次元ベクトルの緯度 経度を計算する球の中心からで作成した撮像面のピクセルごとへの次元ベクトルの緯度 経度を計算します 下画像のφが緯度でθが経度です   plane jpg  緯度を計算するには z成分と次元ベクトルのスカラーのarccosを計算します    math\phi \arccos\frac z  \sqrt x  y  z     phi jpg  経度を計算するには x  y成分のarctanを計算します    mathheta \arctan\frac y  x   theta jpg        緯度 経度を用いて 出力画像のピクセルごとに equirectangular画像 元画像 上の位置を算出するで緯度と経度がわかったので その値を用いて 元画像 equirectangular画像 ではどのピクセルになるかを計算します equirectangular画像では横方向には° 縦方向には°の情報があるので 前のステップで得た緯度と経度を 横方向と縦方向の全体の角度 πとπ で正規化した後 方向の全体のピクセルを掛けることで equirectangular画像上でのピクセル値が得られます あとは このピクセル値を使ってopencvのremapなどで変換すれば 真下画像を作成できます 面全てを変換して cubemapを作成したコードは こちら  です ,3,2023-05-21
192,192,[備忘録]圧縮方式の用語まとめ,画像処理,https://qiita.com/yamayaman/items/533c92fd55701f137dae,画像 動画周りの圧縮方式が色々と似ているので 整理のために記載した     そもそも画像 動画のエンコーディング処理 圧縮 とは圧縮 エンコーディングの主な目的は 動画の記録 保存および伝送するのに必要となるデータの量を削減すること 動画をエンコーディングする多くのアルゴリズムがある 例：MPEG  H   VPなど     JPEG  Joint Photographic Experts Group 人の目に見えない程度の小さいデータを切り捨て保存する圧縮方法 単一の画像 フレームで機能する＝静止画像の圧縮方式 圧縮率を指定することで 画像ファイルのサイズを小さくできる     MotionJPEG  画像の圧縮方式であるJPEGを応用して動画データを生成する圧縮 記録方式のこと JPEG画像をパラパラマンガのように次々と表示して動画とする     MPEG   Moving Picture Experts Group codecs 動画 音声の圧縮方式で 情報量が多い動画や高解像度動画を格納することが出来るファイル形式   MPEG     MPEG     MPEG   といった種類がある MPEGは MotionJPEGが次元圧縮であるのみに対して 時間軸すなわちフレーム間を含めた次元の圧縮方式であることに注意 ,0,2023-05-18
193,193,OpenCV 画像の読み込み・表示・保存,画像処理,https://qiita.com/T-K20077/items/4b473f0ee8d46402cf5c,  概要OpenCVの基礎を固めます．  OpneCVとは Pythonライブラリの種 Intel提供のOSS 画像 動画処理専門ライブラリ 一般的なPythonの前処理ライブラリとして挙げられるメリット DPL用の前処理が簡略化でき容易  多様なライブラリと相互性がある  環境Windows VSCode python     画像の読み込み 表示画像は自分で撮影したケーキの画像cake jpgを使います．コードの先頭にimport cvでライブラリをインポートし OpenCVを使えるよう準備します．画像の読み込みはcv imreadで行います．表示はcv imshowで行い 引数に任意の表示名 画像を入れた変数を記述しましょう．コードを実行した際 画像が一瞬しか表示されないのでcv waitKey  を追加します．任意の時間キーが押されるまで結果を表示させることができます． 画像読み込み関数img cv imread  cake jpg   画像表示関数cv imshow  CarrotCake  img cv waitKey    result png    画像の保存画像の保存はcv imwriteを使用します．引数には 保存するときの名前 画像を入れた変数を記述します． 画像読み込み関数img cv imread  cake jpg   画像保存関数cv imwrite  CarrotCake jpg  img   cv waitKeyについて画像の読み込み 表示で触れたcv waitKeyについて補足します．画像を表示する際にcv waitKeyを記述しない場合 プログラムは実行直後に終了してしまいます．この関数の引数に任意の数値を持たせることでその数値時間プログラムを実行することが出来ます．数値はms ミリセカンド で換算されます．先程のコードでは引数を  と記載しましたが 引数  は 任意の時間キーが押されるまで結果を表示 させることができます．,0,2023-05-16
194,194,AWS Rekognition × Laravel で画像認識して人数／人物別に分類してみた,画像処理,https://qiita.com/falya128/items/557a734b1f5f37965c50,  はじめにAmazon Rekognition は機械学習を使用した画像認識を簡単に利用できるAWSのサービスです そのサービスを用いた実装例として 今回は画像を人数／人物別に分類して一覧表示するアプリケーションを作成してみました GitHub からダウンロード可能です Amazon Rekognition を用いたアプリケーション作成の参考にして頂ければ幸いです     機能説明   画面構成    人数別の画像一覧アップロードした画像に写っている人数別に一覧表示できます 人数のボタンを押すことで切り替え可能です     人物別の画像一覧アップロードした画像に写っている人物別に一覧表示できます 顔のボタンを押すことで切り替え可能です     画像アップロードおよび削除任意の画像を選択してアップロード可能です 複数枚のアップロードにも対応しております   また アップロードした画像を一括で全て削除するボタンも設置しております    技術構成PHPのフレームワークである   Laravel   をベースとして構築しました バージョンは  です また バックエンドからフロントエンドへの橋渡しとして   Inertia   を使用しました フロントエンドは   Vue   で実装しております デザイン面ではUIフレームワークとして   Vuetify   を使用しました    処理概要    画像に写っている人数の検出本アプリケーションでは ファイルアップロード時に顔データを登録する Rekognition API   IndexFaces   を呼び出しており そのレスポンスには画像に写っている人数分の顔データが含まれております そこで レスポンスに含まれる顔データの数から人数を検出し データベースに各画像に写っている人数を登録するように実装しました     人数別の表示データベースに登録された情報を基に分類して表示するだけで シンプルな処理となっております     画像に写っている人物の検出Amazon Rekognition では管理したい単位でコレクションを作成して顔データを登録できます 本アプリケーションでは  非効率ですが 初期表示のタイミングで Rekognition API    CreateCollection    を呼び出してコレクションを作成しております そして ファイルアップロード時にRekognition API    IndexFaces   を呼び出して顔データを検出し Amazon Rekognition のコレクションへ登録を行っております     人物別の表示まずは Rekognition API    ListFaces    を呼び出して コレクションに登録されているアップロードした画像の顔データを全て取得します ただし この状態では顔データを取得しただけの状態なので ここから似ている顔ごとに整理する必要があります そこで 取得した顔データをリクエストパラメータとして Rekognition API    SearchFaces    を呼び出して 似ている顔を検索します その際 全ての顔データごとにAPIを呼び出すと大量のリクエストを送ることになってしまうので 検索結果として返却された顔データは除外していき リクエスト回数を減らすように工夫しております 上記の手順を行えば 似ている顔ごとにデータを整理できるので 画像を人物別に分類して表示することが可能となります    おわりにAmazon Rekognition の活用を目的として 画像を人数／人物別に分類して一覧表示するアプリケーションを作成してみました   AWS公式の開発者ガイドの情報を基に探り探りで実装したので もっと良い方法があるのではないかと思いながらも なんとか実現したいことは出来たので良かったです ,4,2023-05-09
195,195,OpenCVの基本操作例まとめ,画像処理,https://qiita.com/hayato0522/items/eed486c6a848907180c7,自分用OpenCV備忘録 気が向けば今後追記していく 本文中の各リンクはOpenCVチュートリアルページに飛ぶ 画像は適当に拾ってきたフリー画像 ここに各コードを記述する  キー押したら描画画面を閉じるcv waitKey  cv destroyAllWindows     描画関係線の太さは特に指定する必要ないが見やすさのため太くしている 他にも線の種類などの設定もある    python  ベースの画像を作る どちらでも  img   np zeros        np uint img   np full           np uint   線を引く img  始点座標  終点座標  色  線の太さ cv line img                      thickness      矩形 img  左上頂点座標  右下頂点座標  色  線の太さ cv rectangle img                         円 img  中心座標  半径  色  線の太さ   塗りつぶし  cv circle img                 矢印 img  始点座標  終点座標  色  線の太さ cv arrowedLine img                         楕円 img  中心座標  各方向の長さ 径   色  線の太さ cv ellipse img                             円弧 img  始点座標  各方向半径  円弧角度  始まり角度  終わり角度  色  線の太さ cv ellipse img                               マーカー img  マーカー中心座標 　色  マーカーのタイプ  マーカーサイズ   折れ線 img  各頂点座標  Trueで始点と終点を繋げる  色  線の太さ   多角形 img  各頂点座標  色   凸な多角形を描画 fillPolyとほぼ同じ だが凸面用 pts   np array                                cv fillConvexPoly img  pts  color            テキストを書く img  テキスト  フォント  サイズ 　色  文字太さ   画像表示cv imshow  img   img 実行結果  pic png  画像を読み込めばその上に描画ができる    python  画像読み込みimg   cv imread    irasutoya png    テキストを書く  頑張ってハートを作るOpenCVでは日本語入力に対応していないため どうしても日本語を入れたい場合はPILライブラリを用いて入力できる  Qiita内にも投稿複数あり    変換関係     フリップ 反転    python  画像読込img   cv imread    irasutoya png     上下反転   左右反転    対角反転  画像をまとめる  画像を保存 表示左から順に 元画像    上下反転    左右反転    対角反転   pic jpg       リサイズcvでshapeで画像のピクセルを取得する際 PIL size の場合と 高さ 幅 の順番が異なるので注意    python  画像読込  画像の縦横サイズ 縦 横  BGR  を取得height   img shape  width   img shape    リサイズ  リサイズカッコ内は横 縦   上記はこのコードでも同じ縦横どちらも 倍したものはアスペクト比は同じだがピクセル数が半分になっている 元画像は 縦px  横px   概ね各サイズ比に合わせて表示している リサイズを使ってモザイク処理もできる 以下のコードではリサイズを用いて画像を縮小した後 元のサイズに戻している 画像サイズを戻す際  interpolation   cv INTER NEAREST とすることできれいに補完されてしまうことを防ぐ    python  画像読込  画像の縦横サイズ 高さ 幅  BGR  を取得height   img shape  width   img shape    画像サイズを縮小した後 元サイズに戻すことでモザイク処理  保存と描画縮小サイズを小さくすればするほど戻した時のモザイク具合は大きくなる 左から順に 元画像    上記コード    上記コードのinterpolation指定無ver       回転   python  画像読み込み  画像の縦横サイズを取得height   img shape  width   img shape   回転の中心定義                        center    int width    int height     回転 中心座標 回転角度 スケール   画像の保存と表示元の画像の中心部を中心に度回転している      射影変換   python  画像読込  画像の縦横サイズを取得height   img shape  width   img shape    変換する前の座標                         変換後の座標  射影変換  保存と表示座標   ピクセルが変換後には height   width  の位置に移動する感じ  サイズの関係で見切れていてわかりにくいが      トリミング   python  画像の読込  画像の縦横サイズを取得height   img shape  width   img shape    中心定義                        center    int width    int height    トリミング 画像中心から上下左右にピクセルの範囲   保存と描画画像中央部が切り取れた    色関係     グレースケール化  画像を読み込むときに白黒指定  カラー画像として読み込んだ後に白黒に変換     閾値処理 閾値処理  閾値処理はある値より大きければ  小さければみたいな処理 大きく分けて大域的二値化処理 threshold と適応的二値化 adaptiveThreshold がある 大域的処理では画像全体に対し同じ閾値を用いて処理を行うが 適応的処理では画像内の小領域ごとに閾値を計算するため 影や照明などによってムラがある画像に対して有効である 第引数の閾値処理種類によって結果の感じが変わる 他にも cv inRange を使えばある値からある値までの範囲を閾値として設定することができる    python  画像読み込み  threshold img  閾値  閾値処理の最大値  閾値種類   出力はつ retは大津の二値化を行う場合に用いる   adaptiveThreshold img  閾値処理の最大値  適応的閾値アルゴリズム 種類ある   閾値種類  x近傍領域  y近傍領域 画像内に明暗がある場合は適応的処理の adaptiveThreshold がよさそう      色反転   python  色反転     輝度平滑化輝度の平滑化はグレースケール画像にのみ適用可    python  輝度平滑化順に 元画像    輝度平滑化     フィルタ処理 平滑化とか 平滑化は特定のピクセルを周囲のピクセルの平均など 輝度 色 で均す処理 カーネルサイズで範囲は指定 カーネルサイズが大きいほどぼやける感じ      ブラー処理指定したカーネルサイズの領域の平均で平滑化  cv boxFilter   でも同様の処理    python  ブラー処理 カーネルサイズを指定 順に 元画像    blur カーネルサイズ×     blur カーネルサイズ×        メディアン処理指定したカーネルサイズの領域の平均で平滑化 中央値で決めるため 第引数は奇数のこと ごま塩のようなノイズに有効とのこと 平滑化リンク先参照     python  メディアンブラー処理 カーネルサイズは奇数      ガウシアン平滑化周囲のピクセルまでの距離に重みを持たせる カーネルサイズは縦横ともに奇数であること    python  ガウシアンブラー処理 img  カーネルサイズ  横方向標準偏差  縦方向標準偏差      画像の膨張と収縮 モルフォロジー処理 opencv  画像の膨張や収縮といったモルフォロジー処理を組み合わせることでノイズ除去ができる 膨張はカーネルサイズ内の一番輝度の高い画素を選び 収縮はカーネルサイズ内の一番低い画素が選ばれる    python  カーネルサイズ左から順に 元画像    収縮    膨張  女性の顔や服を見ると収縮では暗くなっており 膨張では明るくなっていることがわかる 収縮後に膨張を行うことをオープニング また膨張後に収縮を行う処理をクロージングと呼び ノイズの除去に使われる  わかりやすい例はモルフォロジー処理 opencvのリンク先    python  オープニング  クロージング   オブジェクト除去 オブジェクト除去 opencv  先ほどの壁掛け時計画像に対し マスク画像を用いて時計を削除する マスク画像は削除したいオブジェクトの領域を指定するためのもの    python  画像とマスク画像読み込み  inpaint img  マスク画像  修復される点周りの半径  修復手法   保存と表示左から順に 元画像    マスク画像    時計の除去結果 マスク画像は下記コードで作成した    python  マスク画像生成mask   np zeros        np uint cv rectangle mask                        cv imwrite    mask jpg   mask 今回は事前に時計の領域を確認しながらマスク画像を作成したが オブジェクト検出APIなどを使えばより簡単に同様の処理が行えるだろうと思う またAPIを用いなくとも 下記の例では閾値処理を用いて比較的簡単にマスク画像を作ることができる    python  画像読込  画像を汚す  保存  閾値処理でマスクを作る  上手く線が除去できない場合はマスク画像を膨張させ マスク領域を拡大するなどの工夫がある  kernel   np ones       np uint   mask   cv dilate mask  kernel   マスク画像での除去実行  保存と表示左から順に 適当に汚した画像    生成されたマスク画像   除去結果  除去する領域の周りのピクセル情報から除去するため 広い領域の除去の場合違和感が残るが 線については綺麗に除去できている    おわりにOpenCVでは他にもオブジェクト検出や二つの画像を用いての処理 動画処理 多彩なAPIなどまだまだ多くの内容があり 今後学習する機会があれば追記したい ,1,2023-05-08
196,196,HEIC形式のファイルをJPGにまとめて変換するShellScript,画像処理,https://qiita.com/officeAJT/items/c143bf65b90bb42094bb,  はじめに本記事は 職業：旅人のプログラミングの超素人が 備忘のために記載した記事となります 普段 旅をこよなく愛し 日本中 時には海外へと旅行へ出かける日々 カメラも大好きなので カメラをぶら下げ写真を撮り歩く そして blogで記事にしている 少し前に AndroidからiPhoneに機種変更をした iPhoneで撮影した写真も 使おうと思ったら HEICという謎な拡張子に blogはASPを利用しているがHEIC形式だとアップロードができない 手動で変更なんて出来ない ということで ShellScriptでなんとかしてみよう      HEIC形式とはひとまず 今時っぽくChatGPTに聞いてみた 要は jpgより圧縮される画像形式で最近の携帯に使われているよ ということですね  HEIC High Efficiency Image Format とは 静止画像を圧縮するためのファイルフォーマットです HEICは MPEG Moving Picture Experts Group によって開発されたもので 静止画像の圧縮に最適化されています HEICは JPEGに比べて高い圧縮率を実現でき 同じ画質の画像であってもファイルサイズが小さくなるという特徴があります  HEICは iOS 以降のiPhoneやiPadなどのデバイスで標準的に採用されています また Android  Pie以降のデバイスでもサポートされています HEIC形式の画像を閲覧するには HEIC形式に対応したビューアーやアプリが必要です 一方 HEIC形式に非対応の場合は 変換ツールを使ってJPEGなどの形式に変換する必要があります   早速ShellScriptを作ってみる点 前提として ImageMagick が必要となります 画像加工で過去に自分で調べてインストールはしているので 改めて別記事で記載するようにいたします VSCodeを使って書いていきます また PCはMacとなります まずは Bashシェルスクリプトのお作法としての記載 正直 これってなんで書くんだろう と素朴に思いましたので 調べました  助けてChatGPT  これはBashシェルスクリプトを利用しているものなんだよ ということを分かる様にしてるんですね つまり Bashスクリプトを書く時は  絶対必要  ってことですね     bin bashは Bashシェルスクリプトの最初の行に記述される特殊な行で シェルがこのスクリプトを解釈する際に どのプログラムを使用してスクリプトを実行するかを指定するためのもの    はシバン shebang と呼ばれる記号で その後ろに続くパスは実行するプログラムのパスを指定します この例では  bin bashというパスが指定されており Bashシェルがスクリプトを実行するために使用されます  シバンを使用することで シェルスクリプトを実行する際にbash のように明示的にBashシェルを指定する必要がなくなるため スクリプトの実行が簡単になります 次に今回のScriptで行う処理を書いていきます 今回行う処理は下記となります   変換元のHEICファイルがあるフォルダを指定する  変換後のJPEGファイルを保存するフォルダを指定する  変換元のファイルを取得して JPEGに変換するでは まず最初は   変換元のHEICファイルがあるフォルダを指定する   です     変換元のHEICファイルがあるフォルダを指定する今回は 変換元の画像を格納するため work用のディレクトリを準備して指定してみます  input dir という変数で HEICファイルのあるディレクトリを指定します ディレクトリは それぞれ異なると思いますので 自身の指定したい場所となります ちなみにMacでディレクトリが分からない という時は ひとまずターミナルを開いて 対象のフォルダをドラッグすると すぐにパスが表示されます  もっとスマートなやり方があれば…教えてください    変換元のHEICファイルがあるフォルダを指定するこれで   変換元のHEICファイルがあるフォルダを指定する   は完了です 次は    変換後のJPEGファイルを保存するフォルダを指定する   となります     変換後のJPEGファイルを保存するフォルダを指定する変換したファイルをどこにoutputするか ですね 自分がわかりやすい場所に保存されれば良いかと思います 変換前のフォルダの指定と同じようなやり方で  output dir と変数をつけます   変換後のJPEGファイルを保存するフォルダを指定するこれで   変換後のJPEGファイルを保存するフォルダを指定する   は完了です 最後は    変換元のファイルを取得して JPEGに変換する   となります     変換元のファイルを取得して JPEGに変換するさて ここが今回の本題です HEIC形式のファイルをJPEGに変換して出力をします 今回やりたいことは worksディレクトリの中のHEIC形式の画像を全て変換したい となります よって for文を利用して フォルダ内の画像の数だけ処理を行うようにします for文の定義とは…からスタートとなります 解説を読んでいくと 値リスト つまり条件 にマッチするもののつ目の値を変数に設定する doとdoneの間の処理を実行する 次は つ目にマッチしたものを変数に設定して処理を行う これを 全ての値リスト 条件 を完了するまで繰り返す ということになります つまり 今回であれば 変換元のHEICファイルの数だけ処理を繰り返すよ ということになります for 変数 in 値リスト  処理done参考 引用：UNIX LINUXコマンド シェルスクリプト リファレンス様上記を参考に早速 各処理の内容を入れていきます 変数は file と指定してみます 次は値リストの部分です 今回は 変換元フォルダのHEIC形式のファイルです 変換元フォルダは 変数として input dir を定義しています その中の HEICファイルです アスタリスク ＊ を使うことで ファイル名のワイルドカードが使えるようです となると  input dir    HEIC  という形で定義をしてみます →ここで   HEICの後のセミコロン ； をつけていないことでエラーが出ました  おまけに記載しています これで値リストが出来上がりました 続いては処理部分です output用のディレクトリに ファイル名を hogehoge HEIC から hogehoge jpg に変更する そして 画像のフォーマットをHEICからJPGに変換します まず outputのファイルの指定です この後の変換で必要なので  output file という変数の定義を行っておきます そしてファイル名も色々と調べていたところ  basenameコマンド というのを使えば 拡張子など除外したファイル名を取得できるようです HEICを除外して jpgをつけてあげれば どうにかなりそうですね for文の最初に  file と変数を定義しています この変数はHEICのファイルたちです ここから basenameコマンド で HEICを除外してみます 最後にjpgと名前をつけておきます 下記となりました   変換元のファイルを取得して JPEGに変換する最後に ImageMagick を使ったフォーマット変換となります  ImageMagick をインストール済みであれば 下記のコマンドで変換可能です  convert  今回の例 convert hogehoge HEIC hogehoge jpg上記のように convert コマンドを利用していきます HEIC JPG共に変数を定義しておりますので それを記載すると下記です   変換元のファイルを取得して JPEGに変換する参考：imagemagickの使い方日本語マニュアル様これで処理部分も完成です ShellScriptとして保存をして 実際に実行となります   出来上がったShellScriptまず 今回書いたShellScriptは下記です   変換元のHEICファイルがあるフォルダを指定する  変換後のJPEGファイルを保存するフォルダを指定する  変換元のファイルを取得して JPEGに変換するworksフォルダに 変換したい画像を格納します あとは Macでターミナルを起動して いざ実行 completeフォルダに続々と変換されたファイルが出来上がっていきました 成功です これで 今後も楽に変換して アップロード出来ます   おまけ    ハマったこと最初実行すると 下記のエラーが出ました 予期しない文字列がいるよ ということで出るそうな 結論からいくと  HEICとdoの後にセミコロン ； が必要でした doの行を変えていれば無しでも良かったみたいですが ここでセミコロンがあるとないで リストの終わりが分かんねーよということと理解しました せっかくハマったので 今回は あえて行を変えずに行っちゃおうと思います   変換元のHEICファイルがあるフォルダを指定する  変換後のJPEGファイルを保存するフォルダを指定する  変換元のファイルを取得して JPEGに変換する    ハマったこと最初試した時は 画像の拡張子が大文字なことを忘れ 小文字で heicと記載 出力されたファイルが picture heic jpg というカッコ悪いことになって やり直したのは内緒です… 今後 どちらの拡張子でもいけるように調べてみようと思います ,2,2023-05-08
197,197,特徴点の移動量が一定になるように動画を画像に分割する,画像処理,https://qiita.com/Basco/items/0e45441995a41031356c,MarkDownがよくわかっていないので あんまりきれいにはかけないかもです 備忘録的に進めていければと思います ①きっかけ非GPS環境で飛ぶようなドローンを使ってSfM処理をしようとすると 必要なラップ率を満たすように画像を作成するのが難しいから ②解決方法特徴点の移動量を取得したすべてのフレームに対して計算して 各特徴点のマグニチュードを計算し その平均値の変動量から計算すればいいのでは③コーディングカレントディレクトリのInputフォルダ内にmpでデータを入れて 閾値を調整したら実行 ,0,2023-05-05
198,198,AndroidでML Kitのベースモデルを使って画像にラベル付けする,画像処理,https://qiita.com/Yoo6309/items/6a8532acabf18afacb8c, ML Kit  は Googleの機械学習用のSDKだが  ここ  に書いてある通り データをスマホのみで処理するSDKである   ML Kit は Google のオンデバイスの機械学習のエキスパティーズを Android と iOS のアプリに提供するモバイル SDK です 以前は確かクラウド型のものもあったのだがと思ったら  移行ガイド  に以下のような記述があり ML Kitはオンデバイスのみになったと書かれている    年  月  日に デバイス上の API とクラウドベースの API を区別しやすくするために Firebase 向け ML Kit に一部変更を加えました 現在の API セットは 次の  つのプロダクトに分割されました と言うわけで ML Kitを使えば Googleに画像を送信することなく ラベル付けができる ML Kitによる画像のラベル付けは  画像のラベル付け  に従うだけなので ここでまとめるまでも無いのだが 備忘録としてまとめた ML Kitには ベースモデルとTensorFlow Liteによるカスタムモデルがあるのだが ここでは まずお手軽に ベースモデル  でのラベル付けを試した まず ML Kitの依存関係を build gradle に記述する 依存関係は 学習モデルをアプリにバンドルする場合とGoogle Play開発者サービスを使う場合とで異なる 後者の場合は 以下のように記述する APIレベルを以上に設定する 以下の例ではに設定している 後で使うので viewBinding も設定している    app build gradleandroid      defaultConfig          minSdk     buildFeatures          viewBinding trueモデルを自動ダウンロードするために  AndroidManifest xml に以下の記述を追加する もっとも簡単な例として デバイス内の画像にラベル付けするコードを書いてみる ML Kitでは 画像を InputImage オブジェクトにしなければならない 画像ファイルのURIから InputImage を作成するには以下のようにする InputImage image   InputImage fromFilePath this  uri  ラベル付けするためには 画像ラベラーのインスタンスを作成する  onCreate の中で registerForActivityResult を使って画像ファイルをダイアローグで選択してファイルのURIを取得し  process メソッドに投げるコードは以下の通りである 結果は ImageLabel のリストで返される  ImageLabel にはつけられたラベルのテキストとインデックス番号と確信値が格納されている 以下の例では テキストと確信値を TextView に表示している あとは ボタンを押すと このが起動するように設定すれば終了である 以下の例では 画像を選択したいので URIには  image    を指定している 実行例を以下に示す 正しく    猫    と判定している ソース全体を以下に示す ちなみに アプリ情報によるとストレージ使用量は    MB   となった ,1,2023-05-05
200,200,【CloudVisonAPI】シフト表からカレンダーに予定を入れる作業を自動化したい【Python】,画像処理,https://qiita.com/ryosuke-horie/items/d0b9b73b3af5d19bb1c7,   はじめに使い物にはならなかったのですが 学びにはつながったので共有します 主に GoogleCloudVisonAPIに対する知見の共有となります    目次    作ろうとしたアプリケーションについて    実装した機能    問題点 あきらめた理由     開発環境    知見の共有    今後の修正案    最後に     作ろうとしたアプリケーションについてヶ月のシフト表がカレンダー形式のPDFで共有されています このカレンダーは日毎に区切られていて 他の人のシフトも記載されているものです PDFだと自分のカレンダーに取り込むことができないので 画像認識を利用してテキストから自分のシフトを取得して カレンダーへの追加まで自動化できないかと思いつき着手しました      実装した機能以下を実装しました   PDFをJPEGに変換するプログラム      Google Cloud Vision API は 直接ローカルに保存された PDF ファイルからテキストを抽出することはできないため  JPEGをGoogleCloudVisonAPIに送信し カレンダーをテキストに変換するプログラム  上記で取得したテキストから自分のシフトのみを取得し リストにするプログラム     問題点 あきらめた理由 もともとのPDFの画質が荒かった 人の目であれば問題なく読めるが 一部の文字がつぶれていたり 薄くかすれてしまっている そのため GoogleCloudVisonAPIでテキストに直したときに文字がうまく認識できなかったり カレンダー形式の書き方のため 日付のみが並んでしまい 自分のシフトのみ取得することが困難だった      開発環境windows Python   poppler    x  PDFからJPEGへの変換のために利用      知見の共有    PDFから画像への変換pdfimageライブラリを利用する 設定については下記記事を参考にしました  PythonでPDFを画像に変換する Windows   下記コードのように書くとPDFをJPEGに変換可能です       PDFファイルを画像に変換      画像ファイルを保存つ目の引数のdpiで解像度を指定しています デフォルトはで より大きい値を指定すれば実行時間と引き換えに解像度を高くできるようです     GoogleCloudVisonAPIの設定以下の手順で行います ChatGPTに質問すればいい感じに説明してくれるので概要だけ述べておきます  Google Cloud Consoleにアクセス  プロジェクトの作成  Vision APIを有効にする  サービスアカウントの作成  サービスアカウントにロールを付与      サービスアカウントのロールを設定する画面       ロールを選択 をクリック       Cloud Vision APIの管理者 を選択  キーの生成とダウンロード      キーを生成する画面で  キーの生成 ボタンをクリックし キーの種類として JSON を選択  JSONファイルの配置      アプリケーション用のフォルダのルートディレクトリ配置  環境変数の設定      Windows上で使用するので 環境変数の編集を行い 新規追加を選択し  GOOGLE APPLICATION CREDENTIALS という名前でJSONファイルまでのパスを指定します       画像を読み込む      特徴量設定      検出されたテキストを返す上記のようにコードを作成しました  AnnotateImageRequest のオブジェクトを作成することで 特徴量設定や 言語をあらかじめ指定することができるようです  Cloud Vision のドキュメント     Cloud Vision API     Google Cloud       今後の修正案  現在はローカル環境のPDFファイルを利用しているため PDFをJPEGに変換しています       Vision APIでPDFから直接テキストに変換するにはGCS AWSでいうS の利用が不可欠なようなので避けていました   PDFから画像への変換でより良い方法を探す       現状 JPEGへの変換で解像度を高めに設定しても 画質が落ちてしまう       pdfimage以外の方法で画質を保てるものを探す      最後に読んでいただきありがとうございました もし修正案などが思い当たる方がいましたら 何卒ご教授をお願い致します ,2,2023-05-01
201,201,【画像整理】グレースケールのような彩度の低い画像を一括で整理するPythonスクリプト,画像処理,https://qiita.com/average35/items/09f1eb9358c875c19b23,  動機画像生成してたら画像が死ぬほど大量に出来てしまったので せめて真っ黒画像や白黒画像のようなものは一括で削除したいと思い 自動で分別してくれるコードを作りました    使い方以下のコードをコピーして その新規ファイルを move low saturation images py として保存 例えばパスが \○○○○\stable diffusion webui\outputsxtimg images\なんとか\ほにゃらら png の画像ファイルに対してフォルダ分けしたい場合 \○○○○\stable diffusion webui\outputsxtimg images\ の直下に move low saturation images py を配置して ダブルクリックして実行してください ライブラリが無いと言われた場合は 別途   bashpip install opencv python numpy等としてパッケージをインストールしてください WindowsだったらコマンドプロンプトかPowerShellで   batpython  m pip install opencv python numpyと入れればインストールできると思います こうすることで 彩度の低い画像がのフォルダに自動で整理されます   画像の彩度が低いかどうかを判定する関数      画像をBGRからHSVに変換      画像の彩度の平均値を計算      平均彩度が閾値未満の場合 Trueを返す  サブフォルダを作成する関数      サブフォルダのパスを作成      サブフォルダが存在しない場合 作成  カレントディレクトリおよび子ディレクトリ内の彩度が低い画像をサブフォルダに移動する関数      カレントディレクトリとそのすべての子ディレクトリを再帰的に探索                  PNGファイルのみ処理                      画像のファイルパスを取得                      画像を読み込み                      画像の彩度が低いか判定                          サブフォルダを作成し そのパスを取得                          彩度が低い画像をサブフォルダに移動  カレントディレクトリを取得  サブフォルダ名を設定  閾値を設定threshold     彩度が低い画像をサブフォルダに移動する関数を実行   調整方法 注意下から行目の  threshold    の部分の数字を  程度で変動させればフォルダ分け範囲を設定できます ただ 髪の毛 服 背景などが白黒などの単調な色だった場合や 日陰の場面の場合には 彩度低めと判定されてしまうことがあります なので 一つ一つ目視で画像チェックしてから削除するのをオススメします    余談このコードは年月時点のGhatGPTのGPT モデルに書いてもらいました やっぱChatGPTすげえや ,0,2023-05-01
202,202,似ているポケモンをさがせ！(Bag of Visual Words),画像処理,https://qiita.com/yellow_detteiu/items/868984e3f8ac965dc4a5,  はじめに　昔   ポケモンをさがせ    って本ありましたよね 小学生のころ地味に好きでした 　あれを 自動的に見つけ出してくれる方法があれば それ以上のことはないですよね  ウォーリーを探せ もそうですが 　今回はそれを機械学習の技術で実現します といっても 探し出すのは 大量の画像の中から あるポケモンに似ているポケモンです   Bag of Visual Words　皆さんは Bag of Words BoW という技術を御存知でしょうか 自然言語処理をするたびに毎回出てくるアレです 各文章の中で それぞれの単語が何回出てくるかのベクトルを OneHotEncodingを使って作成する技術で 自然言語処理の分析をするのに最も基礎的な手法です 詳しく知りたい方は ネット上にいくらでも転がっているので その辺の記事か  ゼロつくの自然言語処理編  辺りをお読みください これを画像に応用してしまおうというのが このBag Of Visual Wordsです 出力結果だけ見たいという殊勝な方は 出力結果の欄から御覧ください え　画像には単語という概念は存在しないので Bag of Visual Wordsでは 複数の画像からいくつかの代表的な局所特徴を取り上げ それを単語のように扱います この代表的な局所特徴のことを コードワード と呼び コードワードの集合体を コードブック と呼びます 　それぞれの画像の中で 各コードワードがどのくらい現れているかをヒストグラムで表したものが特徴量ということになり これが自然言語処理で言う 各文章のベクトルのようなものですかね 　また  コードワード の選び方としては 各画像から抽出された局所特徴群を k meansなどでクラスタリングすることにより いくつかのグループの中での代表的な局所特徴群を探し出す方法がとられます 　最後にこれらを手順化すると おおむね次のようになります ．各画像から SIFT記述子などの局所特徴を抽出する ．すべての画像から抽出してきた局所特徴群から クラスタリングの手法を使って代表的なもの コードワード を選び出す ．各画像の中で それぞれのコードワードがどの程度出現するか調べる 各画像の特徴量 ．比較したい画像同士の特徴量の距離を調べることで それらの画像がどの程度似ているか調べることができる 　なんか画像の一部を単語のように扱うって Vision Transformer ViT みたいですね 自然言語処理にヒントを得た画像系の手法は 皆このような発想を取るのでしょうか 画像版のBag of Wordsがあるなら 画像版のtf idfとかもあっても良くないですか   実装    実行環境 Windows Python Google Colaboratory    コード　まず いつも通り使うライブラリをインポートします Colaboratoryを使う場合は 画像を読み込むためにドライブにマウントします 適当に画像を読み込みます 今回はポケモンの画像を使いたいので kaggleのdatasetから こちらのデータ  を使いました ポケモンの種類は サンムーンのメルメタルまで 剣盾は入ってないのね SVは分からんからカンベンしてくれー泣  画像自体は確認したところ枚ありました    Python   画像の読み込み   表示用グリッド  最初の数枚を表示　各画像の局所特徴を取得します 今回は こちらの記事  を参考に OpenCVのakazeという局所特徴を抽出しました AKAZEは 画像の拡大 縮小 回転 焦点のぼやけ等に対して SIFTよりも頑強性のあるKAZEをベースに さらに計算量が減るように改善した特徴量のようです また SIFTは特許権が付いており 商用利用にはライセンスが必要ですが AKAZEはそのような必要はないようです OpenCVでは他にもいくつかの特徴量を使えるようなので あとで他のものも試します 記事を書き終わった後に知った記事なのですが こちらの記事とか参考になります    Python   局所特徴の抽出    出力結果 以下省略 　続いてコードワードの作成です 今回はクラスタリングに処理が高速なsklearnのMiniBatchKMeansを使っています  こちらも 先程の記事  を参考    Python   局所特徴をクラスタリングして コードワードの作成    出力結果　続いて 作成したコードワードが 各画像にどの程度表れるかを調べます     出力結果array                                  ヒストグラムはこんな感じです 一連の流れはとりあえずこれで以上です これをクラス化してみます 引数でクラスタの数 特徴量の種類 距離を測る計算式を選べるようにしてみました     局所特徴の取得    コードワードの選定    調べたい画像内で 各コードワードがどのくらい出現しているか    調べたい画像に似ている画像を表示する       表示用グリッド      すべての画像について似ているものリストを作成し 調べたい画像との距離を測る          すべての画像について 比較用の似ているものリストを作成する      似ている画像の表示　あとはこれを実行するだけです   似ている上位n位までの画像を出力  調べたい画像と 上位位までの似ている画像のヒストグラムを描画fig   plt figure           グラフを描画するsubplot領域を作成   出力結果　まずは先程のコード通りに出力したのがこちら 同じ画像を入力しても k meansの結果がその度に変わるので レコメンドされる画像が多少変わります 一番左上の画像が 全く同じため最も似ている画像 要するに調べたいと入力したポケモンです   のオー png    のオー png    のオー png  　やはり微妙に変わりますね ノオーにはゴツイポケモンが似ていると判断されることが多いようです ホエルオーがやけに強いですね ドータクンは確かに形が似てるかもな   chiruta png    chiruta png    chiruta png  　チルタリスにはふわふわとしたポケモンが上位に来ているのがポイント高いです やはりちゃんと特徴を検出できてるんだなーと思います よくできた手法ですね おっさんは確かに 色もフワフワ部分も似てる  お見事   aruse png    aruse png    aruse png  　我らが神 アルセウスのご登場です ミュウツ―やゼクロムなどが降臨するのはさすがっス クロバットやエアームド ラティアスは確かに形が似てるのかもです ニンフィアは   四足歩行だからかな   苦笑   eipam png    eipam png    eipam png  　さて今度はめったに活躍機会のないエイ パムおくんです 小柄なポケモンが出力されていますね  コアルヒーが結構似てるんですね 　さて今度はクラスター数をまで下げてみました 　なんか少し毛色が変わりましたね サイドンとかヤドキングとかバンギラスとか トゲトゲしているポケモンが多く出てきている印象です サマヨールとかも確かに似てるかもな ホウオウとかまで出てきていますね 　エイパムについてもクラスター数でやってみました 　今回も小さめのポケモンが多いですが 進化系もチラホラ出てきています 確かにプラスルとかジュペッタとかカポエラーとか似てるかもな 　続いては クラスター数で feature methodを変えることで 抽出する局所特徴の種類を変えてみました まずはcv BRISKというものです 　相変わらずゴツイゴツイ ローブシンやヨノワールを引っ張り出してきたのはなかなかの功績じゃないですかね  やはり局所特徴を変えると少し見方も変わるんですかね 　続いて有名なSIFT 　おお やや小さいポケモンも出てきましたね 手足の向きを重要視している感じでしょうか ゴツイ系をひたすら出力していたAKAZEくんとはまた違う趣ですね  ニドクインやヤドラン ツンベアーなどはたしかにな という感じがしますが やっぱり発展的な他の局所特徴と比べると見劣りするかな  　今度はクラスター数  feature methodはデフォルトの AKAZE で 類似度を計算する計算式を変えてみます まずは 本来BoVWで使うものとされているカーネル関数 サポートベクターマシン SVM とかで出てくるヤツでしょうか 本当は最後にから引く計算式なのですが 距離が遠いと値が大きくなるよう統一したかったので 実装の中ではからは引いていません 　うーん ゴーリキーやゴローニャなど こちらも手足の向きでしょうか 普通の距離 ユークリッド距離 の方が良い気もするなぁ  　続いてコサイン類似度です こちらも類似度計算では定番ですね 距離が遠いと値が大きくなるようにしたいので 逆数を取りました 　ゴツイポケモンが多いですが 先程とは少しメンツが違いますね  特にブーバーン ハピナス ダストダス ドータクンは似てますね  ブーバーンなんで今まで出てこなかったんや    というかダストダスの名前忘れててググりました 笑 　とこんな感じで クラスター数や局所特徴の種類 距離の計算方法を少し変えるだけでも色々と結果が変わりました 面白いですね  　最後に 調べたい画像と 上位画像のヒストグラムも併せて出してみたのがこんな感じです 本当はヒストグラムもクラスの中で描画するようにしたかったのですが 類似画像と重なったりしてしまって 色々調べたけどダメでした ﾃﾍｯ　クラスター数    局所特徴  AKAZE  距離の計算方法  コサイン類似度確かに類似度の高い画像はヒストグラムも似ているのですね というかガルーラとかランクルスとか似てるなおい   おわりに　いやーBoWの画像版なんて余裕かと思いきや 難しそうな計算式が出てきたり実装手順が多かったり クラスを組むまで整理できずコードがごちゃごちゃしたり 意外に苦戦しました もう内部で計算してくれるライブラリに感謝感謝ですね 深層学習を使わずとも こうしてレコメンドみたいに類似画像をバンバン出せるのは結構感動しました しかもポケモンでやったのが良かったな 自画自賛  これからもこういう深層学習以外の手法もどんどん深入りしていきたいですね  数学とかできるようになれば自己流の手法も開発できるようになるんかな  　それではまた会う日まで   蛍の光   参考 Bag Of Visual Wordsについて 画像認識 　講談社　著：原田達也 複数画像の読み込みと表示 matplotlib 複数のグラフの描画 matplotlib重ねずに別のグラフを表示する方法 Akazeについて 特徴量抽出法一覧,3,2023-04-30
203,203,画像のテクスチャ情報を調べる(局所バイナリパターン),画像処理,https://qiita.com/yellow_detteiu/items/58d4b046071ada77594b,  はじめに　はい またもや  画像認識  講談社　著：原田達也   を読んでいて 実装したくなったものがあるので 同じ日に連続で画像処理関係の記事を投稿します 　今回は 局所バイナリパターン LBP  というものです 名前からしていかにも難しそうな雰囲気が漂っていますが 内容は割と単純で すぐに実装できそうな内容だったため やってみました   局所バイナリパターン LBP とは　局所バイナリパターンと local binary pattern は 画像などのテクスチャ情報を表すための記述子で 記述子とは 局所領域の内容について記述された情報のことです 要するに画像の一部分の特徴量のようなものですね 計算量が少なく高速であること 輝度の変化に頑強 画像の明るさが変わっても 同じ物体として認識できる であることなどがメリットとして挙げられます 　英名から推測できるように 局所的な情報をバイナリ 進数 を使って表します 具体的には ある中心画素の周辺の画素について それぞれの画素と中心画素との差分を取り その差分が正の値 中心画素よりも周辺画素が大きい場合 は そうでない場合はを設定し それらに左上から    i    局所領域がn × n なら i               n n を重みとして足し合わせていきます 例えば×の局所領域で 差分を に変換したベクトルが                  なら 　LBP となります 要するに周辺画素との画素値の大小関係を進数の一つの数値で表している ということですね 考えた人すごい  　さて あとはこれを実装するのみです 簡単そうでしょ    実装    実行環境 Python Google Colaboratory Windows    コード   Python  必要なライブラリのimport　前回同様 使うライブラリをインポートして Googleドライブ内にあるデータを読み込むためにドライブにマウントします 今回は前回も活躍した こちらの画像を使います   piyo jpg  　カラー画像のままだと カラーチャンネルがつあって扱いにくいので グレースケール化します 　続いてLBPを計算するための関数を定義します    Python def LBP image    画像の配列を入力に受け取り そこから各画素におけるLBPを算出して 各画素の値をそのLBPに変換した配列を返す関数  width   image shape    height   image shape    image   np pad image      チェックする半径分 周囲に余白を追加  チェックするときに範囲外に出ないようにするため         中心画素と周辺画素との差分を取る      diff   np zeros        diff     image x   y     image x  y       diff     image x  y       image x  y       diff     image x   y     image x  y       diff     image x   y      image x  y       diff     image x  y        image x  y       diff     image x   y      image x  y       diff     image x   y     image x  y       diff     image x  y       image x  y       diff     image x   y     image x  y         周辺画素の方が高ければ そうでなければにする         それぞれの位置の に重みをかけて足す        元々の中心画素の画素値に LBPを代入する これだけです あとは用意した画像を関数にかけて表示するだけです このような不思議な画像ができました これがテクスチャ情報のようです   ダウンロード    png  続いて LBPは 輝度の変化に強い とのことなので それも実験で確認してみます まず先程の画像の各画素値にを加えて より明るい画像を作成します このような画像ができました   ダウンロード png  　続いて この画像からLBPを算出すると どのような画像が出力されるか確かめます   ダウンロード    png  　同じなんですよね  目視だと本当に同じか確認できないので 数値上も本当に同じか確かめます 　ということで シンプルなのにテクスチャの情報を表現できて輝度の変化にも強い 万能   なLBPの紹介でした   おわりに　画像処理めちゃくちゃ楽しいですね 特に古典手法はロマンがあってワクワクします GANとかも素晴らしいんですけどね 沼にハマってしまいそうです 　それではまた   参考  画像認識 講談社 MLP 機械学習プロフェッショナルシリーズ 　著：原田達也 局所バイナリパターンについて 記述子の定義,1,2023-04-29
204,204,画像の隅っこを検出してみた。(SegmentTest),画像処理,https://qiita.com/yellow_detteiu/items/3c2cc09c8d0b058544a8,  コーナー検出器について　皆さんは 画像を見るとき イラストを描くとき 隅っこを注意してみていますでしょうか 隅っこと言っても 一番端のつの隅のことではないですよ 写っているもの 又は描かれているもののうち 角ばっている部分のことです 　天才的な絵師以外は 普段そのようなこと気にも留めていない人がほとんどであることと存じます しかしながら 世の中には この 隅っこ を探求し続け  隅っこ に情熱を燃やし続けた人たちがいます そのうちの一人こそ かの有名なS T コーナーさんです  うそです  　前置きはともかく いくつかそのような物体の角を見つけ出す コーナー検出器 があることを知りましたので 今回はそのうちの一つ  セグメントテスト を自己流で実装してみました 他にも  コーナー検出器 には ヘシアンコーナー検出器   モラベックコーナー検出器   ハリスコーナー検出器 など 様々な種類があるようなので 気になる方は是非調べてみてください    セグメントテスト　セグメントテストについて簡単に触れると ある画素について 周辺の画素との輝度を比較して 大きく異なる部分とあまり差がない部分に分けます そして 周辺画素がすべてあまり差がない部分なら その部分はほぼ一色に塗りつぶされた場所になりますし 差がある部分とない部分が同じくらいの割合なら その部分はエッジ 辺 になります 大きく異なる部分があまり差がない部分よりも多くなる 又はその逆の場合は その部分はコーナー 角 ということになります   　これを手順化すると 大体以下のようになります 注目画素をp  周辺の画素を調べる円の半径をr  どのくらい異なるかのしきい値をtとする ．pよりr離れていて真上 真下の位置にある画素の画素値がpとt以上差異があるか調べる どちらもt未満であれば pはコーナーではないとして棄却する ．でpが棄却されなかった場合 pよりr離れていて真横の関係にある左右二つの画素についても同様に調べる  これまで調べたつの画素のうち つ以上がpよりt以上明るい 又は暗い という条件を満たさない場合は pはコーナーではないとして棄却する ．でもpが棄却されなかった場合 周辺画素すべてについて画素値の関係を調べる 　今回はこれを 実装の手間や処理時間を考えて  実装できなかっただけじゃないの  って声が聞こえてきた気がしますが 空耳か幻聴でしょう   もっと簡潔にしたやり方でPythonで実装してみました   実行環境 Windows GoogleColaboratory ランタイムタイプ：CPU  Python  実装例   Python  必要なライブラリのimport　まずはいつも通り使うライブラリをインポートします 今回は画像を表示したり編集するくらいしかしないので 実質PIL  numpy  matplotlibくらいしか使いません tqdmは解像度の高い画像を処理するときなど 進捗状況を見たかったので使いました 　まぁこれはGoogleColaboratoryを使いたい人がやってください 蹴　なんか色々読み込んでますが 要するに好きな画像を自由に読み込んでくださいってことです そのあと画素値を編集するためにnumpyのndarray型に変換しています 　カラー画像の場合は モノクロに変換しておいてください 今回はコーナーを検出するだけなので あえてカラーのままやる必要もあまりないかと思われます 　さて ここからが本番です コーナー検出を行うための関数 segment testを作成します  関数名をキャメルケースにするかスネークケースにするか地味に迷いました 今回あまり関係ないですが    モノクロ画像の配列を受け取り コーナーを検出した形の画像を返す関数  image  モノクロ画像の配列  r  探索したい円の半径  threshold  どのくらいの差で画素値が大きく異なるとみなすかのしきい値  width   image shape     画像の幅を取得  height   image shape     画像の高さを取得  image   np pad image  r    チェックする半径分 周囲に余白を追加  チェックするときに範囲外に出ないようにするため       check points results   np array       それぞれの画素がpとt以上異なるかの結果を格納するための配列        else    注目画素値よりしきい値以上明るい場合        image x  y       注目画素値を白くする        image x  y       注目画素値を黒くする　今回は手順の 周辺の全ての画素について調べる という工程はせず 上下左右つの画素についてのみ調べました  というか斜めの画素とかどう調べるねん  　また チェックするときに注目画素pが端の方だと そもそも真上に画素ありませんよ とかなりそうなので numpyのpadという関数を使って余白を追加しています  普通DeepLearningの畳み込みとかで使うもんな気がするけど 　出力する画像は コーナーと判定された画素については白く 逆の場合は黒くなるように画素値を変更しています 普通コーナーを黒くしたいと思うのですが 逆の方が仕上がりがきれいでしたのでこうしてます また 先程の追加した余白は出力するときもそのままにしています これも削除とかしていると 計算時間が増えそうなので 　また 内包表記ではなくfor文ばかり使っていますし もっと処理効率の良い書き方があるかもしれませんが 今回はこれでもそこまで時間かからないのと 分かりやすい方が良いのでこのままとします 　作成した関数を先程読み込んだ画像に適用してみます この場合 半径で周辺画素について調べて 画素値が以上異なるかどうか見ています     出力例．ピッツァ元画像  pizza png  分かりやすい太い輪郭線が付いた画像です 元々の輪郭線がそのまま抽出されたような感じになっています ただ やはり線と線の交点で角になっているようなところは 気持ち強めに強調されていることが分かります 今回は白黒はっきりした元画像だったので しきい値を変えてもほとんど変化がありませんね 解像度の低い小さめの画像だと 半径をあまり大きくしすぎると出力結果が崩れます 試しに半径を小さくして見ると きめ細やかな出力結果になりますが この画像の場合は元画像の輪郭線を抽出しただけのような出力になります ．ゆきダルマン元画像  snowman png  なんとも素朴な手書き感のある ほっとする画像です 元の画像と似たような感じですが なんとなく角っぽい部分や 円形の右下あたりが黒っぽくなっています より角が強調された仕上がりになりました 今回の場合はこのくらいの半径の方が分かりやすいかもしれないですね r     t    ※白黒反転当初やろうとしていた 白黒を反転したものも出力してみました 見にくいですが これはこれで特徴がよく出ているのでアリなのかもしれません ただちょっと 右下あたりに輪郭線が繰り返されてしまっているのがあまりよくないかもしれないです ．ウサギン元画像  usagi    png  どこかの誰かが作ったとされている ウサギのオリジナルキャラクターです グッズ販売中と作者本人から聞いております いや怖い 笑 確かに周囲との差が大きい輪郭線周辺が狙い通り白く出ていますね ちなみに 今回はセグメントテストによりコーナーを検出したいのでこれはただのコーディングミスなのですが の部分のandをorにした方が 割とすっきりとしたきれいな画像が出力されます というかむしろこれで良くないか r    t     usagi png  r     t     usagi r  png  r     t    ※白黒反転  usagi r  png  ．クワガ〇ン元画像  kuwa jpg  さあこっから段々と複雑な画像になっていきますよ  まずはキレイなニジイロクワガタのフリー素材です 私ニジイロクワガタ好きで子供の頃から憧れてたんですよね ムシキ〇グでも強いのよりもこっちが出た時のほうが嬉しかったです うーん 粗い 画像が小さいのに画面が複雑だからですかね    まあでも一応輪郭線とか角っぽいところは検出できているみたいですね 意外にも 半径を大きくすると落ち着きます 木の幹の質感がちゃんと出てるのが面白いですね  しきい値を大きくすると やりすぎた感がハンパなくなります しきい値の上げすぎには気を付けましょう でもこっちの方がむしろ角が分かりやすいか  ちなみに先程のor条件で出力したものはこちらになります いや もうこっち使えよ    kuwa png  ．ニンフィア元画像  アルピノニンフィア png  作者不詳の絵です ｷﾚｲﾃﾞｽﾈ  自画自賛 この世の終わりみたいな形相をしていますが コーナーを検出しようという強い意志がうかがえます 半径を小さくした方がオワッてますね でもはっきりと角が白色で示されています ちなみにこれもor条件のミスコードの方が上手くいってます ．でっていう元画像  yosshi jpg  ヨッシーストーリーと毛糸のカービィの世界観を混ぜたような いや ヨッシーのウールワールドか   なんともほほえましいフリー素材 このクオリティでフリーなのは驚愕 でも これは無理だろー 臭がプンプンします それが上手くいくんですよー むしろ今までで一番きれいじゃね くらいのレベル 毛糸の質感が見事にでているもはや芸術です めちゃくちゃかっこいい  案外細かい質感表現の方が得意なんですかね 左上あたりは元画像では白の中でも比較的あかるい領域だったので グラデーションになっている部分がコーナーとして検出されたのかなー いやはや見事見事 さながらPhotoShopのエンボス効果のよう ヨッシーとポチどちらの質感も平等に表現できています 左上の領域も健在 しきい値を変えると 雰囲気が変わりました ヨッシーの足の付け根やポチの目 口 首輪がまるごとコーナーとして検出されていますね 背景の領域は少し変わりました これも同じように白っぽい部分と茶色っぽくなる部分を分けているのでしょうね それにしても ちゃんとヨッシーの形を認識してる風なのがウレシイ ．piyo元画像  piyo jpg  色んな意味でラスボス感あふれる画像です さすがにこれはキツイか  あーやっぱり厳しいかー でも一応輪郭とか毛の質感ででる   目がトンボみたいになってます くちばしもちゃんと検出できてますね 卵の接地面がちゃんと角として検出されているのがポイント高いですねぇ  調子に乗って半径を小さくしたら 黒い部分ばかりになってしまいました でも本当に重要な 目の周りの細かいボサボサやくちばし タマゴの接地面や足の輪郭が捉えられているので むしろこっちの方が優秀   しきい値をあげると なんだか幽霊みたいになってしまいました 足や タマゴの影 目の中などは色の変化が少ないということなのでしょうね くちばしの影も黒くなっていて 全体的に陰の部分が黒くなっている印象です やりました ついにやりましたよ僕 今回はしきい値が低い方が良かったのですね 毛は色の微妙な変化が多いから   毛の質感 タマゴの輪郭 足の質感などすべてが表現されていてほぼ完ぺきです 半径を小さくすると 今度はデジタルアートのような趣になりました 全体的にスッキリとした印象です 足の質感は一番的確に表していそうです r     t    ※白黒反転白黒反転させてみたものです いかにもやらかした感が漂っていますが コーナーを検出したいという当初の目的は一応達成しているように見えます 例のor条件にしてしまったバージョンです いや貫禄あるやん  魔王降臨してるやん という感じで 皆さんも是非色々な画像で試してみてくださいねー   まとめ 深層学習使え 繊細な質感の表現が意外に得意   画像なので セオリー通りや正解の方が必ずしもよい訳ではないという自由さ 柔軟さがオモシロイ   あれか 角が黒い方で良かったのかも  おわりに　本を読んでいて  案外簡単に実装できるかも   と思い立ちやってみました かなり粗削りな部分が多いと思いますが 深層学習まで使わなくても 結構気軽に検出できるもんなんだなと思いました 　少し前に ライブラリを使わずにnumpyだけで画像を極座標変換するのにもチャレンジしていたのですが カラー画像を変換する場合 数学的な変形が複雑すぎて私には手に負えなくて断念しました いつか再チャレンジして成功させたいです 　あと 最近AIによる画像生成がかなり問題視されていますが これは機械学習でも深層学習でもなく 古典的な画像処理で しかもフリー素材や自作のイラストを使っているのでセーフってことで  　それではごきげんよう  参考  画像認識 著：原田達也 numpy padについて matplotlibのcmap一覧 真面目にやれ    numpy形式の画像の保存,0,2023-04-29
205,205,画像にどんな色が使われているかをクラスタリングで分析する,画像処理,https://qiita.com/hirosuke1610/items/417fbd716a6fec3a521e,  概要画像処理においてもよく使用されるクラスタリングアルゴリズムの一つである  K 平均法  を使って 画像分析する方法をまとめてます K 平均法で分析することで その画像には どんな種類の色が含まれているか 知ることができます   そもそもK 平均法とは   K 平均法 K means clustering    は クラスタリングの手法のつで 与えられたデータセットをK個のクラスタに分割するアルゴリズムです 具体的には まずK個の中心点をランダムに設定します その後 各データポイントがそれぞれ最も近い中心点に属するようにクラスタに割り当てます 次に 各クラスタの中心を再計算し その中心を新しい中心点として使い 再度クラスタリングを行います このプロセスを繰り返し 各中心点が変化しなくなるまで続けます K 平均法はマーケティング調査や自然言語処理にも使われますが画像処理でも使うことができます 画像中のピクセル値をベクトル化し そのベクトルを元に似た特徴を持つ画像をクラスタリングすることができます 知っての通り 画像はRGBデータの集合体です 色自体はRGBの××   種類あります これをK 平均法で大体どれくらいの色が含まれているのかを知ることができます 例えば以下のような画像があります これを個の色で分けたとき 以下のような絵画っぽい画像を作ることができます   ライブラリのインストール必要なライブラリは以下でインストールする     bashpip install  r requirements txt  ソースコード本ソースコードの全体は こちら  においています    画像のパス 名前 切り抜く範囲を指定する設定しやすい用に 先頭に定数 厳密な意味では定数でないが として設定している  IMG PATH で画像ファイルを指定する サンプルで枚画像を用意している   Googleのロゴ  風景の写真  フルーツの写真   IMG NAME で 画像ファイル名を指定する    RANGE X  RANGE Y で どのくらいのピクセル分切り取るか指定する    NUMBER OF CLUSTERS で K 平均のクラスタ数を指定する 例えば と指定すればつの色に分類します 数値が大きくなるほど計算量が多くなるため 時間がかかります    画像を読み込む   Python  読み込む画像を指定する            print  画像読み込みエラーのため 終了します     切り抜き範囲を指定する   K 平均法   Python  K Means法で画像を分析        self img   img    切り抜き範囲の画像を代入      K平均法で計算する             画像で使用されている色一覧  W   H    の numpy 配列           K meansアルゴリズムの収束基準を設定            data colors     クラスタリングするための入力データ            attempts      異なる初期値でアルゴリズムを実行する回数             重複したラベルを抽出し カウント NUMBER OF CLUSTERSの大きさだけラベルタイプが存在する       計算結果をグラフ用にDataFrame化させる          plt用に補正          グラフ描画用文字列          countsの個数順にソートして indexを振り直す    K meansアルゴリズムの収束基準を設定するcv TERM CRITERIA MAX ITER：反復回数が最大値に達した場合に収束判定を行うフラグcv TERM CRITERIA EPS：クラスタ中心が移動する距離がしきい値以下になった場合に収束判定を行うフラグであれば 最大反復回数がで 移動量の閾値が     data colors     クラスタリングするための入力データ    attempts      異なる初期値でアルゴリズムを実行する回数戻り値は以下のつcompactness 各点とその所属するクラスタ中心との距離の総和 labels 各データ点の所属するクラスタのラベル centers クラスタの中心点の座標の配列 要は画像の場合は RGBのリストになっている    図の出力             ヒストグラム用のrgb値カウント数             ヒストグラム用のrgb値カウント数          ヒストグラムを表示する           各画素を k平均法の結果に置き換える このクラスで結果の図の出力を行っています main関数内では以下のように書きます    Python          可視化する           全体画像を表示する          切り抜き画像を表示する           ヒストグラムを表示する          クラスタ数分のRGB値で置き換え画像を生成          各タイトル  ax：全体画像  ax：切り取り指定した後の画像  ax：K平均法の解析結果のヒストグラム  ax：各画素を k平均法の結果に置き換えた画像  サンプル画像でK 平均をやってみる    Google のロゴGoogleのロゴでK平均クラスタリングを行ってみます     元画像  風景    元画像こちらは切り抜いた箇所だけを分析し 結果を出しています クラスタ数を変えてみると かなり違うことがわかるかと思います Kを小さくすると絵画っぽく 大きくするとより写真に近くなります ,3,2023-04-27
206,206,【AWS】S3+Lambda+Rekognitionで画像認識してみた,画像処理,https://qiita.com/falya128/items/8954c4ba524b2bbaf874,  はじめにAWSの Amazon Rekognition を使って画像認識を試してみました 以下の人物画像から顔を検出し 性別と微笑みの有無を判別しています なお 用いた画像は写真のフリー素材サイト photoAC からダウンロードしました   利用するAWSのサービスプログラムを実行するための環境です 今回はNode jsを使用しています 認識させたい画像を保存するためのオンラインストレージです 機械学習の専門知識を持っていなくても簡単に扱うことができる画像／動画分析サービスです LambdaからSとRekognitionにアクセスする権限を付与するために使用します Lambdaで実行した結果が出力されたログを確認するために使用します   処理フロー  Sのコンソールから画像アップロード  画像アップロードをトリガーとしてLambda関数を実行  Sに保存された画像データをRekognitionで画像解析  構築手順※年月末時点のAWSコンソールで作成しています     Lambdaで関数を作成するLambdaで 関数の作成 を開いて 新たな関数を作成します 関数名は任意で構いません ランタイムは Node js を選択します また今回は事前にIAMでロールを作成していないので 新たに実行ロールを作成します      IAMで権限を付与するLambdaで関数の作成が終わったら IAMで権限の付与を行います 先ほど新たに作成したロールがIAMに追加されていますので そちらのロールを変更していきます 許可ポリシーの一覧を確認すると Lambdaの権限が付与されていることが分かります 今回はSとRekognitionの権限が必要なので ポリシーをアタッチ します まずは Sの読み取り権限 AmazonSReadOnlyAccess を選択します 次に Rekognitionの読み取り権限 AmazonRekognitionReadOnlyAccess を選択します どちらも追加できたら 以下のような表示になります      Sにバケットを作成してトリガーを設定するまず 認識させる画像を保存するためのバケットを作成します バケット名は任意で構いません 次にLambdaで関数のトリガーを設定します トリガーの設定を開いて 先ほど作成したバケットを選択します また今回は 画像がアップロードされたタイミングで実行したいので イベントタイプは すべてのオブジェクト作成イベント を選択します      Lambdaでレイヤーを作成する今回はレイヤーを作成して関数に追加します 自PCのターミナルで以下のコマンドを実行して ライブラリがインストールされたフォルダを作成します ちなみにOSはWindowsで実行しています インストールされたフォルダをzip形式に圧縮したら 再びAWSコンソールを開いてレイヤーの作成を行います レイヤーの名前は任意で構いません 作成したzipファイルをアップロードします また 互換性のあるアーキテクチャおよびランタイムは関数を作成する際に選択した値を入力します レイヤーの作成が完了したら関数にレイヤーを追加します 関数の画面からレイヤーの追加を開いて 先ほど作成したレイヤーを選択します      Lambdaでプログラムを記述するLambda関数で実行するプログラムを記述します 実行内容は以下の通りです   アップロードされたSのオブジェクト情報を取得する  RekognitionのDetectFaces関数を用いて画像内の顔を検出する  顔の情報から性別および微笑みの有無を取得してログに出力するプログラムの記述が終わったら Deploy ボタンをクリックします これでプログラムの実行準備が整いました   実行結果それでは実際にSに画像をアップロードして 実行結果を確認してみます まずはこちらの男性が微笑んでいる画像をSのバケットにアップロードします アップロードしてしばらくするとCloudWatchにログが出力されます 作成したLambda関数のロググループに出力されているのでそちらを確認します ログを確認してみると 男性は微笑んでいます と出力されています 次にこちらの男性が微笑んでいない画像を同様にアップロードします ログを確認してみると 男性は微笑んでいません と出力されています   おわりになるべくシンプルな構成でRekognitionの画像認識を試してみました 今回はRekognitionの顔検知機能を使用しましたが まだまだ他の機能があるようです 以下のサイトに呼び出し可能なAPIが記載されています 今後は使用しなかった他の機能を用いて 色々と試していきたいと考えています ,3,2023-04-27
207,207,サンプリングモアレ法で変位を測定する,画像処理,https://qiita.com/jtnutrient/items/0ef0a7592b87e2e2cd53,  はじめに　カメラを用いて 画像の変位分布を測定する手法の一つに サンプリングモアレ法があります 縞模様を張り付けた対象が歪んだ際の画像に対し 元の縞模様を重ねるとモアレ縞が発生します この縞を解析することで 対象のどの部位が大きく歪んだのかを可視化することができます   元の縞画像  歪んだ縞画像  モアレ縞サンプリングモアレ法では元の縞画像の代わりに カメラの画素を使用します すなわち Nつ分の画素で一つの縞になるように設定し 変位した縞画像を間引きます 間引いた間を線形補完することでモアレ縞を生成します 参考文献に洗練されたソースコードがあったため 本記事では実務寄りの可視化方法を試してみました 先人は偉大です   参考文献  一次元に対する変位   画像生成テスト画像を生成します 縦縞画像を生成した後 射影変換により直角台形へ画像サイズを変形させます その後 元の画像サイズで出力させます コードはChatGPT先生に頑張ってもらいました   画像サイズ設定width  height       画像生成  縞の色とサイズを設定  縞を生成  画像を射影変換  変形画像を表示  変形画像を保存   設定した変位の可視化上述のコードで得た射影変換行列を用いて 座標の変位を可視化します 変数 displacement にそれぞれx y 軸方向の変位を格納します 射影変換に関しては nkmkさんの記事  がわかりやすかったです 画像右下において 変位が大きくなっていることがわかります     サンプリングモアレ法先人のコードです     n点に点サンプリングし もとの配列と同じになるように間を線形補間する    モアレ画像の位相マップを作成する     上の関数のFFT使ったバージョン   画像への適応    サンプリングによるモアレ縞の生成  画像読み込み グレースケール化 ヒストグラムをファイル出力 明るさの平均値と標準偏差を画面表示 参考  縞の幅n    測定対象 サンプリングしたモアレ縞を生成    変位の可視化     位相接続した画像を格納するリスト     位相接続しない画像を格納するリスト 縞の幅n    測定対象変位の程度や正確な変位分布にずれはありますが 右下方向への歪みを検出できていそうです  サンプリングモアレ法により検出した変位分布  参考：設定した変位分布   二次元に対する変位二次元に対しては x y 軸方向それぞれの変位分布を取ります 実用的に 縦横に白いラインが入った画像から 画像処理により両方向への縞を生成させます 今回は対象画像を縞模様の半分だけx y 軸方向にスライドし 重ねることで 疑似的に再現しました    画像生成テスト画像を生成します またまたChatGPT先生に頑張ってもらいました   画像設定  画像生成 ドットの描写  画像の保存  マッピングと変位設定  画像の保存  画像読み込み  マッピング 変位設定  メディアンフィルター  画像保存   画像前処理    x軸方向 画像読み込み 反転 ドットのサイズ  画像を垂直方向に波長の半分だけずらす 画像の結合サンプル画像から縦縞を生成しました   vertical stacked image png     変位の可視化中心部位の変位が抜き出せていますが バックグラウンドの修正が必要みたいです     y軸方向 画像読み込み 反転 ドットのサイズ 画像の結合横縞を生成しました   horizonal stacked image png      変位の可視化     画像方向の修正いい感じに変位を抜き出しています   最後にサンプリングモアレ法による変位計測方法を追加しました 先人の偉大なコードのおかけで 本記事は成り立っています 独学マンにとって 技術記事を書いてくださる人には感謝しかありません ,1,2023-04-26
208,208,画像を読み込み、byteArrayで遊び理解を深めるコード,画像処理,https://qiita.com/tammy2/items/09b16ae65335b9e5ed12,   やること画像データをリクエストボディに詰めたりのテストをpythonで実施するための型理解   記事の活かし方下のコードをコピペして 自分の画像 パス で検証を行う そして他に気になる点をprintなどして調査して知識を深める   画像を読み込み byteArrayで遊び理解を深めるコードprint img bin    メモリ格納先が表示される　,0,2023-04-25
210,210,PythonでExcelファイルに画像を挿入する/列の幅を調整する,画像処理,https://qiita.com/kaba_san/items/b231a41891ebc240efc7,  Pythonを使ってExcelファイルに画像を挿入する方法どうもhinaateです この記事では openpyxlとPillowを使用して Excelファイルに画像を挿入する方法を説明します   環境構築まず openpyxlとPillowのインストールが必要です   openpyxlのインストール以下のコマンドをコマンドプロンプトで実行して openpyxlをインストールします pip install openpyxlpip install Pillow  コードの全体像以下は Excelファイルに画像を挿入するためのコードの全体像です このコードを実行することで Excelファイルに画像が挿入されます   画像が保存されているフォルダのパス  Excelファイルを作成workbook   Workbook    アクティブなシートを取得sheet   workbook active  列 A の幅を設定  画像ファイルのリストを取得  画像ファイルをExcelに挿入      画像をPILで開いてサイズを取得      画像をExcelのセルに挿入      セルの幅および高さを設定  Excelファイルを保存  コードの解説詳細にコードを見ていきいます      ライブラリのインポートまた PILライブラリから Imageをインポートしています      Excelファイルをアクティブにする   pythonworkbook   Workbook  sheet   workbook activeWorkbookを使用して新しいExcelファイルを作成し アクティブなシートを取得します      画像ファイルの取得とソート指定したフォルダ内にある 拡張子が jpgまたは pngである画像ファイルを取得します ファイル名を数字でソートするために sort filename関数を定義し sorted関数を使用してファイル名をソートします  この工程は適宜省略可能です       一時的にファイルを保存するフォルダの作成 画像の縮尺を変えるために一時的なフォルダのを作っておきます 一時的に保存する画像ファイルを格納するディレクトリを作成します exist ok Trueは ディレクトリがすでに存在している場合に 何もしないことを示します      画像をExcelのセルに挿入      画像をPILで開いて 倍に縮小      画像をExcelに挿入      セルの幅および高さを設定こちらのコードで画像を開き  倍にして一時的なファルダに格納しておきます そしてその画像のパスを参照し 画像をExcelに挿入していきます 今回はA列の行目から行ごとに画像を挿入していくことをしています このあたりは必要に応じてアレンジしてみてください      列の幅の調整   python  列の幅を自動調整セルの文字数に応じてセルの幅を調整していきます 列の最大の文字数を取得し その文字数を参考にしてセルの幅を決めています セル内で文字が改行されている場合はうまく動かない説がありますが そのあたりはご愛敬ということで        python  列 A の幅を調整画像を挿入したA列は画像の大きさに合わせてセルの幅を大きくしておきます      Excelの体裁を整える   python  一行目の背景色をグレーに設定  一行目のすべてのセルに下線を引く  一行目のテキストを中央揃えにする  一行目のすべてのセルに太字のフォントを設定上記のように書いてあげると 諸々の体裁を調整してあげることができます      Excelファイルの保存  ファイルのフルパスを作成  Excelファイルを保存保存先のパス ファイル名を指定してworkbook saveでエクセルファイルとして書き出すことができます      一時的なファイルの削除   python  一時的なファイルを削除最後に一時的に作成していたファイル ファルダを削除してあげましょう 以上です それではまたQiitaで会いましょう  ←カバ ,3,2023-04-24
211,211,AIによる画像認識でパンの美味しさを推測する,画像処理,https://qiita.com/N_Aki/items/ca64bc77480211692339,  概要     自分焼いたパンが良い出来かどうかわからないという経験 ありますよね 私もそうなんですが 先日自分で作ったフランスパンの断面を見てみると お店で買ったパンと比べて小さい気泡ばかりでした どうやら 発酵具合 こね方 水分の量といったものがイマイチだったみたいです そこで お店のパンの断面と比較して 良いか悪いか判断できるツールを作れたら良いのではと思ったわけです 実際に私の焼いたパンの画像がこちら 全体の画像 上 と 断面の画像 下 です これはお店のパン 大小の気泡がキレイ この見た目の違いを画像認識使って判定させます この記事ではVGGをつかった画像認識で 自作パン おいしくない とお店のパン おいしい を見分けることを目標にしています   目次  準備  モデル作成  学習  モデル評価  準備    必要なものgoogle colaboratoryを使ってモデルを学習させ 学習させたモデルをファイルとして出力します googleアカウントが一つあれば完璧     追記 後半に記載したWEBアプリ化はローカル環境で行っています WEBアプリ化するためには以下も必要です  python  自分は  使いました  flaskなどのモジュール類　アプリ用クラウドにアップロードする際 必要なものをrequirements txtにしてアップロードします 　今回のアプリでは以下を記載しています 　Flask    　numpy    　tensorflow cpu    　Werkzeug    　protobuf    　pillow     Githubのアカウント Renderのアカウント クラウド上でアプリを簡単に公開できるサービスです     画像の準備美味しいフランスパン お店のパン と 自作フランスパン おいしくないパン を準備して撮影します 画像はそれぞれ枚以上は必要 貧乏な私はパンを切り刻んでなるべく個のパンからたくさん写真が撮れるように努力しました 学習では ImageDataGeneratorを使って拡大 縮小 左右反転といった水増し処理も行っています 自作フランスパンは ブリティッシュ ベイクオフ のシーズンでポール ハリウッドさんが解説してくれたレシピで作っています   モデル作成今回利用するのはVGGを利用した転移学習です   VGGとはオックスフォード大学の研究者が作った画像認識用のニューラルネットワーク このモデルは性能が良くて 真似るだけで高い認識精度を得ることができます しかも 万枚を超える画像で事前学習させたモデルを 手軽に利用できてしまうのがよいところ ただし そのまま使うとクラス分類になってしまうので パン画像に使うにはひと工夫必要です     コードと解説モデル作成部分のコードです    python  VGGモデルを使う準備  モデルを定義 weightsはimagenetで学習した重みを流用する  モデルの連結  vggの重みの固定 VGG 関数を使えば VGGモデルをインポートして使用できます  参考URL  Keras Documentation      ざっくり解説引数を詳しくみてみましょう ネットワークの出力層側にあるつの全結合層を含むかどうか Falseにすると含みません 今回は おいしそう   おいしくなさそう のクラス分類にしたいので Falseにします   weights imagenet   ImageNetで学習した重み  を使うかどうかを選択します weights  imagenet にして 事前学習の恩恵を得ましょう 使わずに自分で一から学習するより 学習の収束が早くなって良い結果が得られることが期待できます 入力画像の形状を指定します       は  x  画素でR G Bの色のデータということです もちろん指定した通りに画像変換して入力する必要があります 次に出力層を作っていきます まず Sequentialという種類のモデルを宣言します このあとにどんどん層を追加していきます 最初にFlatten層を追加します これは文字通り入力を フラット にする層で       の次元のデータをフラットにします つまり x  x の大きさ立方体を    x  x     の長さの線に変換 次に Dense層を追加します Dense層は全結合層と呼ばれ 前の層とすべてのニューロンが結合して 画像から特徴的なパターンを抽出してくれます 計算結果は 活性化関数に渡されます 活性化関数 activation はここでは relu を使っています 活性化関数はいろいろあって テストしながら選ぶ必要があります 次の層はDropout層です Dropout層では ランダムに入力ユニットをにします なぜわざわざにして計算結果を捨てるのか どうやら ある入力に依存してしまうと学習した画像だけに強くなって 新しい画像を判別させようとした時にうまくいかなるそうです そこでより汎用的なモデルにするためのテクニックとしてDropoutがあるんですね 最後はDense層です フィルタの数は  になっています これは今回 おいしい  おいしくない の種類の分類する目的のためです 活性化関数で softmax にすると 入力をからの範囲に変換し 合計がになるようにしてくれます 例えば おいしい確率  おいしくない確率 といった結果が得られるというわけですね    学習さて つくったモデルをつかって学習を行います コードを載せていきます     画像の読み込み   python  googleドライブ上に画像を置いているので ドライブをマウントするgoogle colabは長時間操作しないとファイルのアップロードからやり直しになるのが嫌なので googleアカウントのドライブに画像ファイルを保存しておいて そこから画像データを読み込むようにしています  ただし これを行うとcolabからGoogle Driveにアクセスを許可することになります Drive内への不正アクセスのリスクを考慮の上 自己責任で   載せておいてなんですが 他人のコードでは実行しないのが良いです ドライブにマウントしなくても google colabにファイルをドラッグ ドロップすれば同じことができます   ファイル一覧を取得  画像を置くための空リストを作成  リサイズする時のサイズを変数に代入しておく  自作パンの断面画像を取得する  img   cv merge  r g b    RGBの並びになるように修正 OpenCVの関数imread  で画像ファイルを読み込むと色の順番がBGR 青 緑 赤 になる 画像の一覧を os listdir をつかって取得し 画像の枚数分 cv imread で画像を読み込みます cv imread をつかって画像を読み込むと 色の順番がBGR 青 緑 赤 になります しかしこの後の処理ではRGB 赤 緑 青 の順になっている必要があるので cv split をつかって色分解し その後 cv merge で指定の順番に再結合しています 最後にcv resize で画像を指定のサイズにそろえて リストに追加して終わりです    python  同様に おいしいパンの断面画像を取得する同じ処理を美味しいパンの画像に対して行います     画像の水増し ImageDataGenerator そのまま学習に入っても良いのですが 画像が少ない場合は水増し処理をします 水増し処理とは 左右反転した画像や拡大 縮小した画像を自動的に生成して学習に用いることです ImagedDataGeneratorを使うと ランダムに左右反転 画像の拡大 回転を行って学習に追加してくれて便利 パラメータがいくつもあります どんなパラメーターが使用できるかは 参考URLを参照して下さい  参考URL  Keras Documentation     one hotベクトルに変換   python  自作パン画像とおいしいパン画像を統合する  目的変数 y   自作パン     おいしいパン    として y を作成  one hotベクトルに変換学習元とするデータを X  その答えを y として定義します Xは画像 yは自作パン   　 美味しいパン   　としたベクトルです yは今後の処理のためにyはone hotベクトルに変換します     VGGモデルを使う準備    画像サイズは最初に定義した変数を使う    モデルの連結    vggの重みの固定    モデルをコンパイルする   評価を行う    グラフ表示するこれは学習を行い その精度をグラフ表示するコードです このコードを実行すると つのグラフが表示されます なぜつも結果が出てくるのかというと これはクロスバリデーションと呼ばれる手法を使っているためです  参考URL データを学習用と検証用に分けて評価する場合 検証用としたデータは学習には使用できずにもったいないですね そこで クロスバリデーションです クロスバリデーションでは例えばデータをつグループに分けます つ分ので学習用し つのグループを検証用として使います そして検証用にするグループを交代して学習していきます 回の学習が必要になりますが すべてのデータを学習に使うことが出来ています これでデータを有効活用できる というわけですね    モデル評価学習の結果をグラフに表示すると モデルの精度が学習が進むとともに上がっていくことが分かりやすいです 以下は学習時に表示されたグラフの一つです   image png  x軸はエポック数 学習の回数 y軸は精度です  acc は学習用データでの精度  val acc は検証用データでの精度です 学習が進む度に精度は右肩上がりで上がっているので学習が進んでいます しかしエポック数を増やせばまだ精度は上がりそうです     エポック数  →   image png  エポック数をまで増やしてみました くらいまでは精度は上昇を続けて あとは横ばいのようです エポック数はにしておきます     最適化アルゴリズム optimizer  sgd → adam  image png  最適化アルゴリズムを sgd から adam に変えました けっこう違いますね 収束が非常に早くなっています ちなみに sgd  adam の詳細については以下参照ください  参考URL  Keras Documentation あとはDense層のフィルタ数 学習時のバッチ数など変更しましたが 残念ながら改善しませんでした 十分な精度も出ていることですし パラメータはこれで決定です     テストパラメータが決まった後は テストします 先ほどまでは学習用データをつかった学習と検証を行いましたが 最後にモデルが未知のデータに対して精度よく動作するかどうか テストデータを用いて確かめます    python クロスバリデーション無しで 改めてモデルを作成する  モデルの連結    vggの重みの固定    モデルをコンパイルする先ほどクロスバリデーションで個のモデルを作りましたが 今回は最後のテストのために改めてモデルを作ります パラメータは検証で確定させたものを使用します 最終テスト用の画像の準備は 上で記載したテスト用画像の準備と同じなので省略します テストする際は 学習には一切使用していない画像を使用します    python  モデルの評価最後にevaluate関数を用いて 精度を計算します 今回の自分の結果では 以下の通り精度は 以上の良い結果が得られました final test loss   final test accuracy       追記  誤判定する画像について　上の画像は  おいしいパン の画像ですが AI予想は おいしくない でした 　パンの映り自体は悪くなさそうですが 背景に対してパンが小さめだったり 背景に別の皿が混じったりしていることが精度の落ちる一因かもしれません  画像背景についてパン断面の画像を撮影する際 つの背景で撮影しました　．緑の皿　．白い皿　．木のプレートテスト画像も同様です 例えば背景が大きく違うとどうでしょうか 上の写真は手にもって撮影した画像です 同じような画像を枚撮影して試してみましたが枚中枚は誤判定でした 背景による判定結果への影響は大きいと想定されます 背景が固定できない想定なら もっといろいろな背景の写真で学習させるか 輪郭抽出でパン部分を抜き取って学習 判定させるといった工夫が必要でしょう 例えば工場などで良品判定に画像判定を使うなら なるべく背景に余分なものが入らないようにして カメラの位置は固定するような工夫が必要かと思います     モデルを出力する完成したモデルをファイルとして出力します    python  モデルの調整が終わったら モデルデータを出力  resultディレクトリを作成  学習したモデルを保存上記のコードを実行することで google clab内の  content result  　に model h というファイルが作成され PCにダウンロードされます このモデルファイルが動作するか試してみましょう   モデルに画像を適用するテスト  画像データをモデルに渡して予測結果をresultへ代入result   testModel predict data   print  answer       format result argmax    load model関数を使って 引数にモデルファイルのパスを指定します これでモデルが読み込まれました テスト用の画像をNumPy配列に変換し predict関数の引数に指定しましょう 予測結果をresultとして受け取ります  predict関数の出力の個目に結果が格納されていますので resultに代入するのはindex の部分  resultにはおいしくないパンである確率と おいしいパンである確率が入っています 確率が高いほうを予測結果として採用しましょう result argmax  と書くことで大きいほうの値を得られます ちなみに以下のように表示されます 自作パンの場合は answer   　 お店のパンの場合は answer   　となっています    アプリで動作を確認するモデルを組み込んだ実際のアプリはこちら ファイル選択で画像を選んで  画像を判定する  ボタンを押します  すると結果が返ってきます   おわりにAIを使ってパンのおいしさを見分けることができました それも という高精度  おいしさを判断できているわけではないのでは  と言われてしまいそうですが 確かにその通りですね しかし 不良扱いになる欠点が分かっていて データを十分とれるならAIで見分けがつくということがこの結果からわかると思います 今回は 対象を絞ることで高精度を出せたと思います  おいしいパン のサンプルは店のフランスパンのみ  おいしくないパン のサンプルは自作のもので材料や焼き時間は固定して作りました それぞれ 本ずつをカットして断面撮影しています 判定は おいしい  おいしくない のクラス分類にしましたが もっと欠点ごとのパンのサンプルを集めることができるなら   焼き色がたりない  発酵が足りない といった具体的な指摘をもらえるようにする  これはフランスパンじゃない  という判定外分類を追加するといった多分類ができるようになり もっと実用性が増すかもしれません 何度作っても おいしくなさそう と言われ続けたら やる気も失せますしね 最後までお付き合いいただきありがとうございました ,1,2023-04-23
215,215,【色検知】OpenCVで色情報から特定の物体を検出してみる,画像処理,https://qiita.com/Yuzu2yan/items/056778f2707d6931519e,  概要今回はPythonとOpenCVを用いて画像の色情報から特定の物体を検出してみようと思います．この画像処理手法については様々なものがありますが 今回は定番である 元画像をマスクしてラベリングする手法 を用います     実行環境  全体の流れ全体の大まかな流れは次のようになります．これからそれぞれの処理について取り扱っていきますが  はやく結果だけほしい   という方は項の最後まで飛ばしてくださいね ．画像をヒストグラム平坦化する  ．RGB空間からHSV空間に移行させる　　．抽出したい色の閾値でマスクする  ．ラベリングする  ．まとめ    ．画像をヒストグラム平坦化する   なぜヒストグラム平坦化をするの  まずはじめに元画像をヒストグラム平坦化します．これによって画素の輝度のヒストグラムを平らにして画像のコントラストを改善することができます．  上の画像に対して ヒストグラム平坦化を施して輝度を分散させてあげると下のようになります．  輝度が分散されてますね この処理を加えることで逆光等で検知できなかった物体を検知できるようになることがあります．     ヒストグラム平坦化をやってみよう  それでは実際にヒストグラム平坦化をやってみます．これも様々な手法がありますが ここではOpenCVの適用的ヒストグラム平坦化 CLAHE を用います．  これは画像全体ではなく 画像を分割してヒストグラム平坦化することで適度に輝度を分配するものです．    RGB空間だと色成分ごとに補正されて色味が保持されないので輝度と色味を分けるためにRGBで表現されている画像をYCbCr Y：輝度 Cb：青の色差 Cr：赤の色差 に変換  します．  次にclaheオブジェクトを生成します．ここで  clipLimit は区分ごとの平坦化の際ノイズが目立たないようにコントラストを制限するもの  tileGridSize は分割する区分のサイズです． clahe   cv createCLAHE clipLimit    tileGridSize         輝度に対してのみヒストグラム平坦化を適用します．最後に元のRGB空間に戻します．以上をまとめた以下のコードで平坦化後の画像を取得できます．画像によってはパラメータの調整が必要だったり ほかの手法の方が適している場合もあります．    ．RGB空間からHSV空間に移行させる画像は光の原色である赤緑青で表現されていますが このままだと抽出したい閾値を設定するのが困難です．RGB空間はいわば赤緑青それぞれの色を塗り合わせて色を表現しているのでプログラマーが この色が欲しい   と思ってもそれをコンピュータに伝えるのが難しいです     そこで RGB空間からHSV空間に移行させます．HSVとは それぞれ色彩 Hue  彩度 Saturation  明度 Value Brightness のことで画像はHSVでも表現できます．この色彩はいわば色相環のことで  色相環でいう何度から何度までの色が欲しい   といった感じで求める色を指定することができます．  通常 HSVは色彩を ° 彩度と明度を  で表現しますが OpenCVでは色彩を  彩度と明度を で表現します バイトで表現できるデータ量に限りがあるからです   ．この相互変換にはネット上にあるツールを使用してみて下さい．試しに赤色をHSVで表現してみます．   python 赤系のHSV表現  赤色はつの領域にまたがります    np array  色彩  彩度  明度    各値は適宜設定する  このように色によっては閾値がつに分かれる場合があります．RGBからHSVに変換するには hsv   cv cvtColor img  cv COLOR BGRHSV  とするだけです      noteRGB画像とHSV画像は表現方法が違うだけで本質的には同じですが 無理やりHSV画像を表示させようとすると色味が変化するよ   ．抽出したい色の閾値でマスクするここでは ヒストグラム平坦化を施した画像から特定の色を摘出します．閾値は先程のものを使用してOpenCVの inRange 関数を用いてHSVで表された画像を二値化します．  閾値の領域が分かれる場合は足し合わせます．  作成したマスクと元画像の各画素のビット毎の論理積を求めることで任意の色だけを取り出すことができます   これらをまとめて 元画像から特定の色を取り出すプログラムは以下のようになります．  赤色はつの領域にまたがります    np array  色彩  彩度  明度    各値は適宜設定する  先程の画像で試してみると     赤色を検出できましたね 以上の操作が元画像から求める色のみを抽出する方法です．これで色情報による画像処理ができました      ．ラベリングする   連結領域毎にラベリングしていくさて 特定の色を抽出できたので物体を検出できるようにしていきましょう まずは色がつながっている領域をつの物体とみなしてラベリングしていきます．これには連結領域を検出するOpenCVの connectedComponentsWithStats 関数を使います．  必要なのはマスク部分の情報なので先程作成したマスクを渡します．  この関数は ラベルの数とラベリング画像に加え ブロブ 連結している領域  面積 重心の情報を出力してくれます ここで気を付けないといけないのは  画像の背景もつのラベルとして認識してしまう  ことです．そのため このラベルを削除して処理をします．  stats ラベル領域の情報 とcentroids 重心の情報 の中身は以下のようになっています．     python statsとcentroidsの中身stats               x  y  w  h  s      番大きいラベル領域の情報 背景のラベル情報            x  y  w  h  s      番目に大きいラベル領域の情報           x  y  w  h  s      番目に大きいラベル領域の情報centroids               mx  my       番大きいラベルの重心の情報 背景の重心の情報            mx  my       番目に大きいラベルの重心の情報           mx  my       番目に大きいラベルの重心の情報背景のラベルを以下のように削除します．     python 背景のラベルを削除最後に OpenCVの rectangle 関数と putText 関数を用いて各インデックスのラベル情報を画像に追記していきます．  まとめると以下のようになります．    赤色はつの領域にまたがります    np array  色彩  彩度  明度    各値は適宜設定する        背景のラベルを削除      すべてのラベルを表示これも先程の画像に適用すると     それぞれの連結領域にラベリングされてますね  いい感じになってきました      スムージングする  さて 先程の画像ではノイズが多いので画像に平滑化処理を加えます．スムージング処理で画像をぼかすことで特徴量やエッジを調整できます．今回は画像をいくつかの領域に分けて各領域で平滑化する  blurフィルタ  を用います．   blur 関数には画像データを第引数に カーネルサイズを第引数に渡します．指定したカーネルサイズごとの領域で平滑化処理を行うので領域を大きくするとぼかしが強くなります．わかりやすいように×のカーネルサイズでスムージングを行うと次のようになります．  全体的に画像がぼけましたね  この画像で先程のラベリングを行うと     明らかにノイズが減っています 先程のコードに行追加しただけでフィルタリングができました      赤色はつの領域にまたがります    np array  色彩  彩度  明度    各値は適宜設定する        背景のラベルを削除      すべてのラベルを表示   任意の物体を取り出すさて ラベリングができたので最後に目的の物体を取り出します．これにも様々な手法がありますが 今回は最も面積の大きいものを求める物体とすることにします．これは  周囲に検出したい物体と同じ色のものがない あっても相対的に小さい ことを前提  にしてます．      note warn同じ色のものの中で物体を検出したい場合は物体の形から検出する方法などを試してみてください 環境条件によってはきれいに色を抽出できず 求める形を取り出せない場合があります ．最大のラベルが欲しいのでNumPyの argmax 関数で走査して最大面積を持つラベルのインデックスを求めます．  完成したコードが以下になります．  赤色はつの領域にまたがります    np array  色彩  彩度  明度    各値は適宜設定する        背景のラベルを削除          以下最大面積のラベルについて考える        print  目標物が見当たりません    出力結果は   目的の物体を検出できましたね   これで完成です  お疲れ様でした     ．まとめ今回は色情報から特定の物質を検出するプログラムを作成しました．画像を用いて物体を検出しようとすると周囲の環境の影響を大きく受けます．どのような環境下でも画像処理を行えるようにすることがプログラマーの目標ですがなかなか難しいものです    OpenCVを用いると複雑な画像処理でも数行で実現できてしまいます ぜひ一度試してみて下さい,10,2023-04-12
216,216,面倒なスクショ作業を自動化してみた！,画像処理,https://qiita.com/kusaaaaagi/items/d00e99d4a37bcf4442d8,  要はSnapBotという画面変化を検知してスクリーンショット Slackのスレッドへの送信を自動で行うツールを開発しました このBotを使用することで 単純作業であるスクリーンショットのアップロードを自動化し 作業効率を向上させることができます こんな感じでスレッド上でスクショを送り続けます   image png    WhyなぜこのBotを作ろうと思ったのかというと 勉強会などの機会にスクリーンショットのアップロード作業が手間だったからです 毎回手動でスクリーンショットを撮影して それをSlackにアップロードする必要がありました   WhatSnapBotを 作成しました   SlackAppとopenCVを使用して 画面の変化を検知してスクショをアップロードするBot  image png  このBotを使用することで スクリーンショットを自動的に取得し Slackチャンネルにアップロードすることができます 以下に Botの作成手順を説明します   How簡単に技術紹介します 詳しくはレポジトリを参照してください    SlackAppを作成manifest yamlを使用しました 漏れやすいポチポチ設定を省けるのでめっちゃ便利  とりあえず ファイルの読み書きを設定しました   スクリーンショットスクリーンショットコマンドを使用しました    直近のスクリーンショットと差分を比較opencvで画像の特徴量を抽出して差分を定量化しています      画像読み込み      画像をグレースケールに変換      特徴量抽出器の生成      特徴量の検出と記述子の計算      特徴量のマッチング      マッチングされた特徴点の距離の平均値を計算      つの画像の距離を計算   スレッドへ送信    与えられた判定結果と画像を使用して スレッドに画像を送信する関数     判定結果がTrueの場合は 画像をスレッドに送信します         print  判定結果はFalseです 何もしません     気づき  Slack Appの開発ハードルが低いことを知れた    充実したライブラリとパッケージ    manifest ymlを用いた再現性の高いアプリの構築  Slackへのアクションまで一貫すると 自然 に使える    普段から使用するコミュニケーションツール上で動くのは 体験としてめっちゃ良かった  余談CopilotとChatGPTが大活躍して数日で要件通り動くものができましたMVPのハードルが下がっているのをめっちゃ感じました  余談記事に関しても骨子はChatGPTが作ってくれました 楽ちん,0,2023-04-12
217,217,高画質画像を小さく表示すると波打つ件の対処【SwiftUI】【メモ帳アプリ】,画像処理,https://qiita.com/yocomaron/items/651b425d44f0464e6caf,  課題画面の横幅いっぱいに画像を表示するには 普通こういう風に書く   swiftif let uiImage   photoUIImage      Image uiImage  uiImage      resizable       scaledToFit       frame width  screenWidth しかし横向きで撮影した写真を表示すると ただの白い石膏ボードの壁が 図：開発中のメモ帳アプリのスクリーンショットこれはと考えられる このときの画面幅は    写真幅は   だったので 分の以下に無理やり収めたことになる   解決策すれば良さそうだと考えて対策したら うまくいった どれくらい画質を下げるかという問題だが 上記の場合は下記計算式で違和感のない表示になった 倍率　  画面幅   写真幅   最初はシンプルに 画面幅   写真幅 を試したが 画質が荒く見えてイマイチだったので倍した  このあたりの最適値は端末や もしかしたら人によっても違うかもしれない コードはこちら：画質を変更するためのextensionを定義しておいて 表示するときに使う  実装したアプリちなみに こちらは下記のメモ帳アプリ Roamee に実装しました よければ覗いてみてください   参考にした記事,0,2023-04-12
218,218,ST 2084のPQカーブの基になったBarten Ramp輝度対輝度差弁別閾,画像処理,https://qiita.com/ieno98/items/7f067fe56abcd7d8c1b9,  Barten Ramp輝度差弁別閾は ある輝度の光に対して それと輝度の違いを感じられる最小の輝度差のことである 例えば Barten Rampによれば nitのとき   の輝度差 つまりnitの差を区別することができる この輝度差弁別閾に現れる人間の輝度知覚特性の非線形性は SMPTE ST のPQカーブ設定の基礎となった 以下はBT   FIGURE のパラメータに基づく  nitからnitまでの輝度対輝度差弁別閾の一覧である ,0,2023-04-10
220,220,画像の配色をHLS空間で可視化する。【Python】,画像処理,https://qiita.com/Cartelet/items/aa8c026b1de9c79164de,  はじめにおにまい お兄ちゃんはおしまい  の放映が終わってしまった 来週から社会の荒波に放り出されるというのに 何を頼りに生き延びれば良いのか そんな陰鬱とした気分でYoutubeの海を漂っていると おにまいの配色について考察している一本の動画に出会った… この動画では HLS色空間 を球の内側に写像した表現 を用いて配色を可視化しています HLSとは Hue 色相  Lightness 輝度  Saturation 彩度 の頭文字で HLS色空間を使用すると補色など色間の関係性が理解しやすいという利点があります 今回は このHSL球による配色の表現をPythonで実装してみます   仕様色と座標の対応は恐らく動画と同じですが 画像全体に対し一度に処理を行います   色相  ightarrow  z軸周りの角度で表現  輝度  ightarrow  z軸方向の高さで表現  彩度  ightarrow  z軸からの距離で表現  点の大きさ  ightarrow  その色がどれだけ使われているか  画像入力方法  ightarrow  クリップボードにコピーした画像  実装  画像をクリップボードから読み込み リサイズする  近傍の色をまとめて threshold以上含まれる色のみ抽出する  HLSから球内に写像する  球内の座標に対応するRGBを計算する  プロットを行う  使ってみる使い方は 解析したい画像をウェブサイトなどから右クリックでコピーして 以下を実行するだけです ※ OpenCVがインストールされていない場合は橙から黄の淡い色を基調として 度前後離れたパステルカラーが差し色として使われていることが分かります   おわりに円盤を買って期に繋げましょう… ,14,2023-03-30
222,222,論文の勉強17 「Vision Transformer(ViT)」,画像処理,https://qiita.com/tanaka_benkyo/items/3c00d2bea08fcce7d6ee,背景などは説明はしません 以下の論文を読んでいきます 途中GoogleColabのユニット   を使い切ってしまい 動くことは確認していますが 一部出力結果がありません 自然言語処理で事実上の標準技術となっているtransformerを画像に適用したものです 大規模データセットで学習させることでCNNベースの精度を超えることが可能となりました 実装自体は通常のtransformerと変わらないので 比較的簡単ではないかと思います そのためか実装したという記事も多数あります こちらは解説というより勉強のメモ書きなので 学習される方は他の記事を参考にしてください     Method     Vision Transformer ViT 概要を図に示す   image png  標準的なTransformerはトークンembeddingの次元の系列を入力として受け取ります 次元の画像を扱うために 画像を \boldsymbol x \in R  H×W×C  から次元のパッチの系列 \boldsymbol x  p\in R  N× P  C   に変形します ここで   H W  は元の画像の解像度  C はチャネル数   P P  は各パッチの解像度です そして N HW P  はパッチの数であり Transformerへの入力の系列長となります Transformerは  D 次元の潜在ベクトルを使用するため パッチを平坦化して学習可能な線形返還により D 次元に変換します この出力をパッチembeddingと呼びます 位置埋め込みは 位置情報を保持するためにパッチ埋め込みに追加されます ここでは 学習可能な D 位置埋め込みを使用します Transformer encoderはmultihead self attention MSA ブロックとMLPブロックの交互レイヤで構成されます Layernorm LM が各ブロックの前に 残差結合が各ブロックの後に適用されます MLPは層構造で活性化関数としてGELUを使用します 式で表すと次のようになります 生のパッチ画像の代わりに CNNの特徴マップから入力系列を構成することができます hybridモデルでは パッチembeddingの射影 \boldsymbol E  はCNNの特徴マップから抽出されたパッチに適用されます 特別な場合として パッチは×のサイズをとることができます これは 特徴マップが平坦化されTarnsformerの次元に射影されることを意味します      FINE TUNING AND HIGHER RESOLUTION通常 大規模なデータセットでViTを事前トレーニングし より小さなタスクに合わせてファインチューニングします このために 事前トレーニング済みの予測ヘッドを削除し ゼロで初期化された D ×K のフィードフォワード層を追加します ここで  K はクラス数です 事前学習より高解像度にすることで有益になることがあります 高解像度の画像を扱う場合 パッチサイズを同じにするためには系列長が長くなります ViTは任意の系列長を処理できますが 事前に学習されたpositional embeddingは意味をなさない可能性があります そのため 基の画像の位置に合わせるため 事前に学習されたporsitional embeddingを次元補完します     Model  image png  ViTの構造はBERTの構造をもとにしています BaseとLargeはBERTから直接採用され ここではさらに大きなHugeを追加しています モデルを表す表記として たとえばViT L は ×のパッチサイズのLargeモデルを表します 系列長はパッチサイズの乗に反比例するため パッチサイズが小さいモデルは計算コストが高くなります      Training最適化の手法としてはAdamを使用して パラメータは \beta   \beta     weight decayは とします     実装 Keras データはKaggleのDogvsCatの一部を使用します kerasでの実装を行います MultiHeadAttentionなどは実装されたものが提供されていますがここでは使用しません     画像をパッチに分割し線形変換を行いtransformerへの入力とする     cls トークンをパッチの系列の先頭に追加し これがencodingさたものを全結合層の入力とする          畳み込み層のkernelとstrideをpatch sizeとすることで分割と線形変換を同時に行う           class  クラストークン 追加          クラストークンの分も入れたshapeを指定          パッチへの分割→線形変換          クラストークンの拡張          クラストークンをパッチ系列へ追加          入力の線形変換          出力の線形変換          各値を取得          ここで q と k の内積を取ることで query と key の単語間の関連度のようなものを計算します           tf matmulで最後の成分について積を計算 それ以外は形がそろっている必要あり           softmax を取ることで正規化します          input query  の各単語に対して memory key  の各単語のどこから情報を引いてくるかの重み          重みに従って value から情報を引いてきます          input query  の単語ごとに memory value の各単語 に attention weight を掛け合わせて足し合わせた ベクトル 分散表現の重み付き和 を計算          入力と同じ形に変換する          線形変換          各値を取得          splitだが実際は次元を拡張する処理        モデルの実行          ヘッド数に分割する          実際はreshapeで次数をつ増やす          入力と同じ形の出力          層構造          層目：チャンネル数を増加させる          層目：元のチャンネル数に戻す        入力と出力で形が変わらない    残差接続        モデルの実行        AttentionもFFNも入力と出力で形が変わらない      それぞれ残差接続されている        AttentionもFFNも入力と出力で形が変わらない        入力と出力で形式が変わらない        patch size   画像を分割するサイズ          Encoderレイヤを繰り返し適用        patch size   画像を分割するサイズ          全結合層          クラストークン部分のみ使用ViT B を定義します パラメータ数は論文通りおよそMとなりました 次の画像 kaggleのdog vs cat を学習させます 多クラス分類でうまくいかなかったため値分類としました モデルについてはかなり小さくしたものを使用します  こういったことができるのは自分で実装するメリットかと思います    ベストのモデルのみ保存学習の実行結果の可視化を行います 実際には学習済みモデルを使用することになると思います kerasで言えば tensorflowhubかvit kerasというライブラリなどからの利用となります epochですが 近くとなりました   学習させない    略                                                                こちらも同様に少ないepoch数で精度を高くだせました 可視化の関数もありますがうまくだせませんでした 調査中     画像をパッチに分割し線形変換を行いtransformerへの入力とする     cls トークンをパッチの系列の先頭に追加し これがencodingさたものを全結合層の入力とする          畳み込み層のkernelとstrideをpatch sizeとすることで分割と線形変換を同時に行う           class  クラストークン 追加          クラストークンの分も入れたshapeを指定          パッチへの分割→線形変換          クラストークンの拡張          クラストークンをパッチ系列へ追加          入力の線形変換          出力の線形変換          各値を取得          ここで q と k の内積を取ることで query と key の単語間の関連度のようなものを計算します           tf matmulで最後の成分について積を計算 それ以外は形がそろっている必要あり           softmax を取ることで正規化します          input query  の各単語に対して memory key  の各単語のどこから情報を引いてくるかの重み          重みに従って value から情報を引いてきます          input query  の単語ごとに memory value の各単語 に attention weight を掛け合わせて足し合わせた ベクトル 分散表現の重み付き和 を計算          入力と同じ形に変換する          線形変換          各値を取得          splitだが実際は次元を拡張する処理        モデルの実行          ヘッド数に分割する          実際はreshapeで次数をつ増やす          入力と同じ形の出力          層構造          層目：チャンネル数を増加させる          層目：元のチャンネル数に戻す        入力と出力で形が変わらない    残差接続        モデルの実行        AttentionもFFNも入力と出力で形が変わらない      それぞれ残差接続されている        AttentionもFFNも入力と出力で形が変わらない        入力と出力で形式が変わらない        patch size   画像を分割するサイズ          Encoderレイヤを繰り返し適用        patch size   画像を分割するサイズ          クラストークン部分のみ使用     略      New  テストデータに対するエポックごとの処理こちらでも可視化を行います GoogleColabのユニットを使いきってしまいだせませんでした pytorchでもいくつか学習済みモデルが提供されています         patch size   画像を分割するサイズ      New  テストデータに対するエポックごとの処理以上となります Chat GPTつかえば簡単かもしれませんが勉強は続けています ,14,2023-03-25
224,224,【考察】GPT-4はどのように画像まで処理できるようになったのか？【ChatGPT】,画像処理,https://qiita.com/daikiclimate/items/f7711a62fa56e2a1fa16,  背景  本日月日地点では GPT はChatGPT Plus 有料プラン でのみ解禁されています  GPT の一番の目玉はマルチモーダル性で 画像付きで自然言語の質問をすることができます   しかし ChatGPT Plusでの解禁はテキストの部分のみで 画像処理を試すことはできません  さらに GPT は論文を公開したが そこにはデータに関する記載はありませんでした    これは意図的なものと思われます そのため どのようなデータを用いて 画像とテキストを扱ったかは不明です しかも このまま公表されないことすらあり得ます   目的はやり仕組みが気になりますよね どうやって画像処理まで出来るようになっているのか この記事では 私が個人的に 多分こういう風に動いているだろうな と思っていることを文章化します 基本的にモデルの構造についてです データをどういう風に集めたとかは 分かりようもないので気にしませんまた 別の解釈や議論 追加の情報などあればコメントいただけると幸いです 正式な発表などあれば 冒頭にリンクを差し込む予定です    GPT の画像処理とは見飽きたかもしれませんが OpenAIのGPT のページから画像を拝借します ざっくり言うと ユーザー この画像は何がおかしい  GPT  iPhoneは本当はLightningケーブルを使うのに VGAケーブルが刺さってるよ  という感じです   image png  ソース：   考察 どのように動作させているのか   背景知識今回はステップで情報を整理し 考察します   GPT系列の思想  GPTの仕組み  ViTの仕組みここを整理してから 結論に進みます    GPT系列の思想多くの解説記事があるため 簡単にふれます まず GPT やというのは  Fine Turningしなくてもあらゆるタスクの問題が解ける大規模モデルを大規模なデータから構築する というのが基本的なモチベーションです この点 逆に言うと   あまりモデルの構造自体にはモチベーションがありません   実はGPT とGPT は 若干の違いはあるものの 基本はただTransformerを大量に連結させ続けた というシンプルな構造をしてます この点から    GPT とGPT においても Transformerの数を増加させただけ モデルに特別な構造は追加されていないだろう    と想像できます    GPT の仕組み世界一簡潔な図でGPT を記述します 左から文章 文字群 が入ってきて Word Embeddingで各文字が特徴量になり 各文字の特徴量がTransformerに入力されます   image png  ※厳密には 各文字の特徴量ではないです 理解のためシンプルにしてあります Transformerが何かはどうでも良いです 大事なのは こういった    ある単語が ベクトル化されたものがTransformerに入力されれば処理できる   ということです    ViTの仕組みところで Vision Transformerというものがあります Transformerで画像処理してみた という研究です   image png  簡単に言うと  画像をパッチに分解します  画像を一列に並び変えて特徴量にします  dのベクトルになったのでTransformerに突っ込んで分類などのタスクを解きますという感じです ざっくりなので 細かいところは違うかも知れません ここで大事なのは    画像をパッチに分解してTransformerに投入しても高い精度でタスクを解ける   ということです 個人的には感覚的にちょっと不思議 パッチに画像を分けるというのがなんとも美しさに欠ける 気がするのですが とはいえ 画像をTransformerに入力する方法がわかりました    まとめます あらためて 以下の仮定を立てます ①あまりモデルの構造自体にはモチベーションがありません GPT とGPT においても Transformerの数を増加させただけ モデルに特別な構造は追加されていないだろう②ベクトル化されたものがTransformerに入力されれば処理できる③画像をパッチに分解してTransformerに投入しても高い精度でタスクを解けるもう結論はシンプルですね 推定されるGPT の構造は以下の感じです   image png  雑な図ですね笑入力されてきているデータが画像かテキストかくらいは 一目瞭然なので そこをIFで分岐させるのはそんなに難しくないでしょう ですので 入り口だけはさすがにTextとImageで処理が異なるかと思いますが Transformerの入力直前では Text由来かImage由来かは関係なく 同じベクトル状態でTransformerに投入されていると思います また この構造だと  テキスト→画像→テキスト というフォーマットもサポートできます 例えば  この画像にうつっているのは何 　   画像   　左にあるのはコーヒーカップでその右側にあるやつ みたいなケースですね ベクトル化してしまえば全部同じ ということです   最後に以上は 基本的に想像です これまでの傾向から 恐らくそうだろうと思って書いてはいますが 真実は公開されていないため不明です その点 ご了承ください,1,2023-03-18
225,225,あ,画像処理,https://qiita.com/Cosmo06/items/78a347ac72b97cb3383c,あ,0,2023-03-18
227,227,Nx でカラー画像の黒背景を透過する,画像処理,https://qiita.com/RyoWakabayashi/items/99892e5e5bfc86537f6e,   はじめにDiscord の elixir と見習い錬金術師 サーバーにて   SF  さんから質問がりました  カラー画像の黒背景を透過する処理についてです   とりあえず以下コードで処理自体は実現出来たんですが あまりスマートじゃないなと     EvisionやNxでこのあたり上手く扱う方法をご存知の方いらっしゃいませんか というわけで記事に答えを書きます実装したノートブックはこちら   実行環境Livebook を使います  Elixir      Livebook       セットアップ必要なモジュールをインストールします   nx   行列演算   画像の準備今回は以下の画像を使いますpiyopiyo ex のキャラクターを殻から出しています以下のコードを実行するとアップロード用のウィジェットが表示されます  画像アップロード  ドラッグ ドロップ等で画像を選択し アップロードします  画像アップロード  画像を Nx で扱えるテンソルに変換しますevision で読み込んだ画像は RGB ではなく BGR の順に色データが並んでいるので  bgr img  にしています Kino Image new  に画像のテンソルを渡すと Livebook 上に画像として表示してくれます   背景透過BGR に A  アルファ チャネルを付けると透過画像になりますA が  のとき完全な透明  A が  のときは完全な不透明です Nx reduce max axes       により 画像内の各ピクセルについて BGR の最大を計算します黒は          なので最大が  で それ以外の色は  より大きくなります Nx greater    で 最大が  より大きければ 　  そうでなければ  にしますこれによって黒のピクセルは    それ以外のピクセルは  になります Nx multiply    で各ピクセルの値を  倍にします黒は   透明  それ以外は   不透明 になります Nx new axis    でテンソルの形を        に変形します後で BGR とくっつけるためです Nx concatenate axis     で BGR に A をくっつけます   まとめ Enum map  などで処理するよりも  Nx の行列演算の方が圧倒的に速いので 可能な限り行列演算するようにしましょう,4,2023-03-11
229,229,google cloud vision で画像チェックを行った話,画像処理,https://qiita.com/gdc-cto/items/feff9c877c2c7b462569,弊社には 該当商品を購入して頂いたユーザーにインセンティブを与えるサービスがあります これは 商品画像をユーザーにアップロードしてもらってチェックを行ったりしています   概要．画像解析のAPIは Google Cloud Vision   Vision APIを利用．解析の言語は pythonを使用 最もポピュラーですね ．検証時の実行場所は Colab 正式名称 Colaboratory  を利用してます ブラウザ上で実行できるのでとても便利です 　ソースコードやファイル等はgoogleドライブに保存して利用できます   画像ファイルからGoogle Cloud Vision   Vision API を利用こちらのソースコードは 画像ファイルの中の人間の数をカウントしてます     物体検出結果からPersonをカウント    人数カウント結果を表示  print  画像内の人数：    str num    まとめVisionAPIとgoogle colabの利用はとても簡易に便利なのでオススメです   今後今後 データサイエンス周りの記事も含めてアップしていきたいと思ってます モチベーションアップのためフォロー イイねお願いします ,3,2023-03-06
231,231,"バッシバシに""バッシング""するぜ！！ in Azure Tech Hackathon",画像処理,https://qiita.com/betio/items/ff9489f433d0baaf0e7d,  今回の内容今回は Azure Tech Hackathonにおいて 自分がチームリーダ兼開発リーダを務めた制作物    バッシバシに バッシング するぜ     についてご紹介いたします   Azure Tech Hackathonについて顔認証やバーコード読み取りなどの画像認識に注目したハッカソンです 今回はクラウドサービスである  Azure   の   custom vision   を使用しました    バッシバシに バッシング するぜ   　とは   本システムAI×IoTによる中間バッシングを支援するWebアプリとなります    そもそもバッシングとは飲食店の業務である バッシング 退席後に空いたお皿を下げること  のことです    コンセプト本Webアプリのコンセプトは    画像認識で 適切なタイミングでバッシングを   です 本Webアプリによって 飲食店側　   　回転率の向上 ＋ 自然な追加オーダー ＋ 従業員の最適配置 ＝   売上の向上  　お客様側　   　不快でないタイミング ＋ 迷惑でないタイミング ＝   満足度の向上  を実現します 具体的なストーリーに関しては以下のサイトをご覧ください   システム概要図 構成図  概要図 png    システム概要図 png    システム構成図 png    バッシングを行うためのタイミングの計り方バッシングを適切なタイミングで行うために   テーブルごとの皿の状況の把握  テーブルごとの盛り上がり状況の把握を行います これらを把握することにより   空のお皿がある　＋　盛り上がっていない状況が そのテーブルの適切なバッシングを行うためのタイミング  となります   各種機能 png    実際の画面実際のWebアプリの画面となります   Webアプリ画面 png  テーブルごとに空のお皿の検出  枚数  ＋ 人の笑顔  人数 を検出できます 今回は そのテーブルに枚以上の空皿がある   そのテーブルに笑顔の人がいない ＝ そのテーブルは盛り上がっていないという判定をしております そのため Webアプリの画面のテーブル①はされている ということになります 店員側はこのテーブルの赤い表示を確認することにより 今どのテーブルにバッシングに行くべきかを決定することができます   まとめ今回の作品は   最優秀賞  をいただくことができました 本システムが 今後発展されるに貢献できることを願います 最後に一言     バッシバシにバッシング決めていくぜ      関連サイト,2,2023-03-02
232,232,【Python】SG法を2次元に拡張し画像平滑化フィルタとして利用する,画像処理,https://qiita.com/Cartelet/items/63fdb138cdb2077bba05,  はじめに先日 SG法というデータ平滑化の手法を紹介いたしました． SG法をPythonで実装したらえらい簡単だった．  SG法は 等間隔で並んだデータ点の各点に対し 点の前後  N  点 合計  N   点 を最小二乗多項式フィッティングし その中心点を平滑化後の値とする手法です．．この手法の最大の特徴はデータの等間隔を仮定することにより 非常に単純な計算でこれを実現できる点です．より具体的には カーネルサイズ  N  に対応した重みを最初の度のみ計算して あとは同じ重みでデータ全体に  畳み込み  演算を施すだけです．個別のサブデータセットに対して最小二乗多項式フィッティングするなど複雑な演算に感じますが 結局のところ ただの重み付き移動平均の一種として扱えるわけです．ところでニューラルネットワークの流行で   畳み込み  といえば次元のイメージがどうしても付きまといますよね．  …ん これって画像の平滑化フィルタとしても使えるのでは     次元のSG法を考える  image png  点が画素値 メッシュがフィッティングした曲面↑．例として次の変数関数による最小二乗法フィッティングを考えます．関数の値  f  x  y   をフィッティングします．中心座標を        に設定する点がポイントです．これには次元も次元も関係ありません．各点における  f  x  y   を   N   imes   の行列に均した以下の連立方程式を解けば良いので で係数ベクトル  \boldsymbol A   が求まります．SG法では このフィッティングした曲面の中心点        での値が知りたいのでした．さて対応する値はどうなっているでしょうか．式   を見ると．．．あら簡単．つまり   \boldsymbol X T \boldsymbol X      \boldsymbol X T   の行目のみ  \boldsymbol F   にかけてあげれば求まるのです．あとは   \boldsymbol X T \boldsymbol X      \boldsymbol X T   の行目を   N  imes  N    の行列に戻してあげれば   任意の   N  imes  N    の領域を次関数でフィッティングして中心点の値を抽出するフィルタ  の完成です．それを各画素に適用 まさに畳み込みです． f x y   の次数を上げれば より複雑なフィッティングも可能となります．例えば以下は  imes   の領域を次関数でフィッティングするフィルタです．  image png  SG法の魅力の一つに 微分値も得ることができる点があります．この特徴も勿論次元でも活きていて 例えば  x  方向の次フィッティング平滑化微分フィルタは  いざ実装意気込むほど難しくはありません．   pythondef SGFilter m  N      mask    np c   m     np r   m    flatten      m   そのままだとm次まで入ってしまうのでマスクを作成  使ってみる  結論使えなくはないけど フィルターサイズが大きくなると明るさの境界の影響が気になる仕上がり．画像以外ならあるいは．画像系のNNの初期重みとして使ったらどうなるか少し気になる．,1,2023-02-24
233,233,StarDistについて,画像処理,https://qiita.com/JohannPachelbel/items/5ddb096d0ca33076879e,  はじめに　近年における細胞のセグメンテーションには 主に学習ベースであるMask RCNNが使用されてきました．しかし 細胞が重なっている場合において 全ての細胞を正しく検出することが困難であるといった問題がありました．そこで StarDistというアルゴリズムが開発されました．StarDistとは 星型凸多角形 Star convex を用いた細胞核のセグメンテーションを行うための手法です．　StarDistはGithubにて論文やコードがまとめられており チュートリアルが充実しています．特に 学習やテストだけでなく 学習済みモデルの使用も可能なため 気軽に試すことができます．しかし 英語の説明が中心であり日本語での説明が全くないため 本記事で大まかにまとめてみることにしました．  Stardistの仕組み　StarDistは バウンディングボックスに比べ優れた形状表現である星型凸多角形を使用することによる細胞核のセグメンテーション手法です．モデルはU Netに基づく軽量なニューラルネットワークを使用します．モデルを使用し オブジェクト確率 d i   j だけでなく k 方向 デフォルト： k   に対する半径距離 r k   i  j  を予測することにより そのピクセルがどのラベルに属するかを出力します．最後に 非最大抑制 NMS により最終的な出力を行います．  実装　GitHubページから引用し コメントを追加しました．本コードではJupyter Notebookを使用しているため 他の開発環境を使用したい場合はコードの書き換えが必要となることに注意してください．  入力画像をテストデータのインスタンスとして読み込み  StarDistの学習済みモデルをインスタンスとして読み込み  入力画像を正規化した画像に対し StarDist適用によりラベリングを実行  入力画像の描画plt subplot    plt imshow img  cmap  gray  plt axis  off  plt title  input image    ラベリング結果画像の描画出力結果　上の図より 高精度でインスタンスセグメンテーションができていることが確認できます．あとは ブロブ解析によるノイズ判断を行ったり 関心領域 Region of Interest  ROI の設定後に機械学習に回したりすることで細胞を解析 種類を識別することができます．  最後に　本記事では StarDistについて大まかにまとめてみました．自身の研究に関連するためデータを記載できませんが StarDistの精度は非常に高く 体感 割の細胞を捕捉できていました 私は医者でないので あくまでも体感です ．また StarDistを適用する前の段階でノイズ軽減などのフィルタ処理を追加することで セグメンテーションが向上しました．　また 本記事では次元での話を中心に行いましたが 時間を追加した次元バージョンもあるみたいです．今後は そちらがベースとなり 細胞のリアルタイムな評価が可能になるのではと考えています．　ここまで読んでくださり ありがとうございました．  参考,1,2023-02-19
234,234,Python でイラストの塗り残しを塗る,画像処理,https://qiita.com/yuinore/items/8ed0cb191ed926aa0a24,  はじめに下図のようなイラストの塗り残しを塗る処理がペイントソフトでは一発で出来なさそうだったので Python と OpenCV で実装してみます 例えばアクリルキーホルダーなどの白版を入稿する場合 塗り残しがあると品質に影響が出る危険性があるため しっかりと処理していきます   入力画像これは意図的に作成した画像ですが 線画と塗りの境界に塗り残しがあり 背景の緑色が透けてしまっていることが分かります  ※分かりやすいように背景を緑色にしています    アルゴリズムの説明画像のアルファ値が 以下の領域を  \boldsymbol A   画像のアルファ値が  より大きく以下の領域を  \boldsymbol B   とします このとき領域  \boldsymbol A   は本来透過しているべき領域であるとみなすことができます 一方 領域  \boldsymbol B   は塗り残しの可能性があります しかし その中で領域  \boldsymbol A   に連結している部分は イラストの境界部分のアンチエイリアスである可能性があります 従って 領域  \boldsymbol A   に連結していない領域  \boldsymbol B   について 不透明度を   に変更すれば 塗り残しを塗ることができると考えます これをプログラムにします   模式図    Alpha     の領域 および Alpha     の領域を抽出する  画像の連結成分をラベリングする  領域Aに対応する label のラベル番号を抽出する  ラベル番号を unique にする  抽出したラベル番号に対応する連結領域の和領域を求める  入力画像に対し 上記で選択した領域以外のアルファ値を  にする  出力画像     塗り残し検知     塗り足し領域     合成結果無事に塗り残しの部分を塗ることができました   計算量について 改良後ver 領域の連結成分が多い場合 計算量が多くなる可能性があります そこで 連結成分が多い場合は numpy isin   を使うように改良しましょう   Alpha     の領域 および Alpha     の領域を抽出する  画像の連結成分をラベリングする  領域Aに対応する label のラベル番号を抽出する  ラベル番号を unique にする  抽出したラベル番号に対応する連結領域の和領域を求める  入力画像に対し 上記で選択した領域以外のアルファ値を  にする  デモサイト上記のプログラムを使って 実際に動作を試すことができるサイトを作りました 良かったら試してみてください ,3,2023-02-16
235,235,evision で検出した顔に Elixir Image でモザイクをかける,画像処理,https://qiita.com/RyoWakabayashi/items/f634a6874d2f389870e1,   はじめに前回の記事で evision と Image モジュールを連携させました今回はそれを利用して  evision で顔を検出して Image でモザイク加工しますいつものように Livebook を使います実装したノートブックはコチラ   実行環境  Elixir     OTP   Livebook    以下のリポジトリーの Docker コンテナ上で起動しています   note infoDocker が使える環境であれば簡単に実行できますDocker Desktop を無償利用できない場合は Rancher Desktop を使ってください   セットアップ必要なモジュールをインストールします   elixirMix install      image              evision              req              kino         Req は画像やモデルファイルを Web からダウンロードするのに使っています   画像の読込毎度の如くレナさんの画像をダウンロードしてきます Vix Vips Image  の場合 画像自体と属性情報のタブが表示されます   顔検出モデルの準備evision  OpenCV  で使うカスケード分類器のモデルを用意します詳しくは以前の記事を参照してください   顔を検出する顔検出は evision で行うので 画像を  Evision Mat  に変換してからカスケード分類器にかけます今回は顔が一つなのでリストの先頭を取り出し 顔の左上XY座標 幅 高さを取得します実行結果は以下の値になります   elixir   顔にモザイクをかけるImage モジュールの  crop  関数で簡単に画像を切り取れますまた   pixelate  関数で簡単にモザイク処理できます evision でもモザイクは可能ですが 少し手間が多いです モザイク加工した顔画像を  compose  関数で元の位置に貼り付ければ完成ですモザイク処理を全てパイプで繋ぐと以下のようになりますね 簡単でしょ    まとめOpenCV と libvips のいいとこ取りができるので かなり書きやすいです,6,2023-02-13
236,236,evision で画像から顔を検出する,画像処理,https://qiita.com/RyoWakabayashi/items/746c52aaa23a6dbb7e6f,   はじめにElixir の evision  OpenCV  を使って 画像から顔を検出します the haigo さんがすでにやっていたので もう少し詳しくやりますいつものように Livebook を使います実装したノートブックはこちら   実行環境  Elixir     OTP   Livebook    以下のリポジトリーの Docker コンテナ上で起動しています   note infoDocker が使える環境であれば簡単に実行できますDocker Desktop を無償利用できない場合は Rancher Desktop を使ってください   セットアップ必要なモジュールをインストールします   elixirMix install      req              evision              kino         Req はモデルファイルや画像を Web からダウンロードするのに使っています   モデルのダウンロードOpenCV の GitHub リポジトリーから Haar カスケード分類器のモデルファイルをダウンロードしてきます他にも色々なモデルが公開されていますHaar カスケードの原理については以下のサイトが参考になりますざっくり仕組みを解説すると 以下のような流れで物体を検出しています  一つのモデルは複数の分類器をカスケード 連結 したものになっている  一つの分類器は以下の処理を行う    画像から一部の領域 四角形 を切り取る    切り取った領域が対象の物体なのかどうか分類する    領域の位置や大きさを変化させながら画像全体の領域を分類していく     対象の物体である と分類された領域は 次の分類器に送られる  最初はざっくりとした分類器で 進むにつれ厳密な分類器になる  全ての分類器で 対象の物体である と分類された領域を 画像内で対象の物体が存在する領域 とする   モデルファイルの読み込み各モデルファイルを読み込み カスケード分類器を用意します   画像のダウンロードいつものレナさんをダウンロードして画像として読み込みます   正面の顔の検出実行結果は以下のように 位置情報 バウンディングボックス の配列になっています   elixir一つのバウンディングボックスはタプルになっていて 左から以下の値を表します  左上X座標  左上Y座標  幅  高さ検出結果が正しいかどうか バウンディングボックスを画像上に描画します四角形の配列を画像に描画する関数を用意します関数を実行します確かに正しそうです    detectMultiScale物体検出には  detectMultiScale  という関数も用意されています実行結果は以下のようになります   elixir              D  実行結果は      となっており   detectMultiScale  よりも返り値が増えています  D   と言われても何のことが分かりませんが これは領域数の配列が文字リストだと判断されているせいです以下のようにして値を取り出しますすると値が    と返ってきますつまり顔として検出できた領域には 最初のざっくり分類器の時点で個の顔領域候補がありました ということを表していますが その情報は特に何の意味もないので使いませんこちらはつの値が返ってきます実行結果は以下のようになります   elixir     という形です番目の値に意味はありませんが 最終的に排除されなかった領域なので  最後の値は確信度なので利用価値がありそうです   目の検出目の検出モデルを実行してみましょう実行結果は以下のとおりで 両目を検出できています   elixir検出位置を可視化しますちゃんとそれぞれ位置は妥当ですね   横顔の検出横顔のモデルを実行します実行結果は      となり 顔は検出できませんでしたこのモデルは顔の左側しか学習していないため 主に顔の右側を見せているこの写真では検出できないのです向きを変えてみましょう結果は以下のようになり 顔を検出できました   elixirバウンディングボックスを反転した画像に描き込みます位置も正しいですね   まとめ物体検出では深層学習モデル DNN が主流になっているので 実務で使うことはあまりないと思いますが 色々なモデルがあって面白いですね,5,2023-02-12
237,237,anacondaでも楽にyolov5を使いたい！,画像処理,https://qiita.com/jo_nakajo/items/3af4403b1d3bcd3cc0cf,こんにちは 皆さんはyolovをローカルで使用したことはありますか yolovの環境構築をする際に便利なのがrequirements txtです pipで使用すると必要なライブラリをまとめてインストールしてくれます しかし conda系でパッケージ管理をしている場合 pipを使用するのは危険が伴います condaでrequirements txtの代わりを担うのがyamlファイルですが yolovにはありません この記事では yolov環境構築用のyamlファイルを作成したのでコピーして使ってみてください なお この記事は に動作確認を行った環境をyamlファイルとして載せています    ruby qiita rbname  yolovchannels       conda forgedependencies       python         numpy         ipython         matplotlib         opencv         pyyaml        requests         scipy         pytorch         torchvision         tqdm         tensorboard         pandas         seaborn         psutil         gitpython   yamlを作成して上記のコードを書き込みます その後同じ階層で以下のコマンドを実行すればyolovを動かせる環境が出来上がるでしょう 最後まで読んでいただきありがとうございました ぜひ使ってみてください ,1,2023-02-12
238,238,白黒背景画像から透過画像をPythonで生成する,画像処理,https://qiita.com/yuinore/items/b05e76d2f0ac70e9b6af,  はじめにアルファチャンネルの存在しない画像 例えばグリーンバック画像から完全なオリジナルの透過画像を生成するような問題は 条件式が不足していて明らかに解が一意に定まりません ですが 画像が枚あれば話は別です 以下のような枚の画像のが与えられた場合に 透過画像をPythonで生成する問題について説明します   黒背景画像とアルファチャンネル画像の枚  白背景画像とアルファチャンネル画像の枚  黒背景画像と白背景画像    a ae fdd png    問題設定アルファ付きの元画像の各ピクセルを  \vec x   とし 以下のように表すとします このとき 黒背景画像  \vec y   白背景画像  \vec z   は以下のようになります アルファブレンド    黒背景画像とアルファチャンネルから透過画像を生成まずは 黒背景画像  y r  y g  y b  とアルファチャンネル  x a  が与えられた場合について考えてみます このケースでは  x r  x g  x b  が未知なので これらについて解くと以下のようになります その結果 以下のような RGB 画像が得られます このような RGB の表し方を ストレートRGB と また このような処理を アンプリマルチプライ処理 と言うそうです 参考：  コンポジターに必要なアルファチャンネルの知識 後編 あとは この RGB 画素値をアルファ値と組み合わせて出力すれば OK です     白背景画像とアルファチャンネルから透過画像を生成同様に解くと以下のようになります     黒背景画像と白背景画像から透過画像を生成この場合は  x a  も未知変数なのでこれについて解かなければなりません ですがこれは簡単です あとは    の場合と同様になります 完成したものがこちらになります ,9,2023-02-11
240,240,Elixir Image で図形・文字描画,画像処理,https://qiita.com/RyoWakabayashi/items/54e92be2e134e0fde0f5,   はじめにImage モジュールを使って  画像に図形や文字を描画します今回も Livebook を使います実装したノートブックはこちら   実行環境  Elixir     OTP   Livebook    以下のリポジトリーの Docker コンテナ上で起動しています   note infoDocker が使える環境であれば簡単に実行できますDocker Desktop を無償利用できない場合は Rancher Desktop を使ってください    実行環境の注意点Image  Vix  を使うためには libvpc が必要ですUbuntu　の場合は  apt get install libvpc dev  を実行してください   セットアップ必要なモジュールをインストールします   elixirMix install      image              req              kino         Req は画像を Web からダウンロードするのに使っています   画像の生成画像サイズ 幅 高さ を指定して画像を生成できますまた 色を指定することもできますCSS で使われる RGB の進数表示 もしくは RGB　の各値を持つ List で色を指定できますまた CSS 用の名称で指定することも可能です実行結果は以下のような  項目の Map になります   elixir   white      hex    FFFFFF   rgb             navy      hex       rgb             hotpink      hex    FFB   rgb          図形描画の背景にするため   x  の真っ黒な画像を用意しておきます   図形描画    点 Image Draw point  に X 座標  Y 座標 色を指定して点を描画します ピクセルでは見にくいので  倍にして表示します    直線 Image Draw line  に始点と終点の XY 座標 色を指定して直線を描画します現状 線の太さは指定できず   ピクセルでしか描画できませんこちらも  ピクセルでは見にくいので  倍にして表示します    四角形 Image Draw rect  に左上の XY 座標 幅 高さ 色を指定することで四角形を描画します fill  false  を指定することで枠だけを描画できますただし こちらも線の太さは  ピクセル固定です    塗りつぶし Image Draw flood  で指定した XY 座標を含み 指定した色に囲まれている範囲を塗りつぶします枠の色と塗りつぶす色は同じなければなりませんちなみに flood 自体の戻り値は以下のようになっています   elixir  ok    Vix Vips Image ref   Reference     left    top    width    height        ok       ですflood を応用することで太い線の四角形を描画することができます   elixirthickness   img  外側の枠  内側の枠  塗りつぶし    円 Image Draw circle  に中心の XY 座標と半径 色を指定することで円を描画できます fill  false  で枠だけの円を描画できますただし 線の太さは  ピクセル固定です円も塗りつぶしを使って太い枠にすることができます    多角形 Image Shape polygon  で多角形を描画できます第引数に多角形の各点 XY座標 を List で指定します width  と  height  で指定したサイズに収まるよう リサイズされます指定しない場合は  x  になります opacity  　は CSS と同じ不透明度で   から  の範囲で指定しますデフォルトが   なので 指定しないとうっすら透けます stroke width  で枠線の太さが指定可能ですまた 第引数に頂点の数   radius  に半径   rotation  に角度を指定することで正多角形を描画することもできます第引数は尖った点の数です内側の半径 外側の半径を指定して もっとトゲトゲにすることも可能です    文字描画 Image Text text  で文字を描画できますデフォルトで文字色が白 背景色はなし 透明 のため  Livebook だと何も見えなくなります Image Text text  にはかなり多くのオプションがあります 詳しくは公式ドキュメントを参照してください     画像との合成多角形や文字の画像だけあっても仕方ないので 他の画像と合成しますまずいつものレナさんをダウンロードしてきますここで注意しないといけないのが 画像のチャネル数 バンド数 ですつまりレナさんは RGB で  RGBA になっていませんアルファ 不透明度 がないため このままだと他の RGBA の画像と合成できません 現状の Image の実装では そのため アルファを追加します Image new  に画像を与えると 同じサイズの画像が生成できますまた   bands  でチャネル数 バンド数  を指定することで 　チャネルのグレースケール画像になりますこれを Livebook 上で表示すると真っ白なので何も見えませんImage モジュールで内部的に使っている Vix の力を借りて RGB   A を実行します不透明度    なので 見た目は何も変わりません改めて  Image shape lenna img   を実行すると 以下のようになります   elixir単純にアルファチャネルだけ増やすことができました Image compose  で画像を合成します  x  と  y  で左上座標を指定できますまた   Image Draw image  で画像を貼り付けることもできますおまけ   まとめImage はまだ機能追加中なので これからもっと色々できるようになりそうです楽しみですね,6,2023-02-02
241,241,Elixir Image で簡単動画再生,画像処理,https://qiita.com/RyoWakabayashi/items/6a5dcbd5f3131918237c,   はじめに先日記事を書いた Image モジュールを使って  Livebook 上で動画を再生しますImage モジュールの動画関連処理は内部的に evision を使っています   実行環境  Elixir     OTP   Livebook    以下のリポジトリーの Docker コンテナ上で起動しています   note infoDocker が使える環境であれば簡単に実行できますDocker Desktop を無償利用できない場合は Rancher Desktop を使ってください    実行環境の注意点Image  Vix  を使うためには libvpc が必要ですまた  evision の動画関連処理を実行するためには  FFmpeg をインストールしてからビルドする必要がありますUbuntu　の場合は以下のモジュールを  apt get install  してください  unzip  ffmpeg  libavcodec dev  libavformat dev  libavutil dev  libswscale dev   事前準備適当な動画を以下のパスに用意したものとします   tmp sample mp  なんでも良い    セットアップ必要なモジュールをインストールしますevision のコンパイルには非常に時間がかかるため 分程度待ちます   動画の読込オプションとしてフレーム番号とミリ秒単位での時間を指定できますオプションを指定しない場合は前回読んだフレームの次のフレームを読み込みます Image Video stream  で動画ファイルを開くことで 各フレームを列挙型として扱えますまた   frame  や  millisecond  で範囲を指定することができますというわけで 以下のコードで Livebook 上で動画が再生できます   まとめかなり短いコードで簡単に動画が読み込めました次は動画を加工してみましょう,8,2023-02-02
242,242,畳み込みニューラルネットワークのメモ,画像処理,https://qiita.com/kakiuchis/items/29221f2be71911165593,CNNに関してとても参考になったサイトがあった 一部ピックアップして解釈の上 メモ できれば本家の英語サイトをみてほしい 他の情報も結構勉強になったし 英語のサイトはこれほどにも 有益なのか これからは英語でも 頑張って読みたいなと思った    一般的なCNNの構造 畳み込み層 Relu層 のセットを繰り返して その後にプーリング層を重ねる なぜなら プーリング層で情報が失われる前に 複数の畳み込み層を重ねることで 複雑な特徴を見出せるから    大きなサイズのフィルタより 小さなサイズの複数フィルタで畳み込むほうが良い大きなサイズのフィルタを用いた畳み込み層つよりも 小さなフィルタを用いた畳み込み層を複数重ねる なぜなら キャッチできる領域が同じでも 非線形変換を含む分 特徴を見出しやすいから パラメータ数も小さいフィルターを重ねる方が少ないから ただし 畳み込み層の結果を保持するために メモリ消費UPの可能性あり    入力層に関する一般的な経験則の〇乗ピクセルが良い たとえば CIFAR は× STL はxとなっている    畳み込み層に関する一般的な経験則畳み込み層では xフィルターでストライドがおすすめ パディングを利用して 次元を変更しない方がよい    プーリング層に関する一般的な経験則プーリング層ではxサイズでストライドがおすすめ    畳み込み層のストライドは なぜなのか より小さなストライドの方が経験的にうまく機能する ストライドにより ダウンサイズはすべてプーリング層に任せたいから 畳み込み層は深さ方向にボリューム変換するだけ    畳み込み層で なぜパディングを使うのか 畳み込み後のダウンサイズを防ぎたいから 実際にパフォーマンスも上がるケースが多い パディングせずに畳み込んだ場合 橋の情報が失われるリスクもある    VGGNetのケーススタディネットワークの深さが優れた性能のための重要な要素であることを示した 畳み込み層はxフィルタで プーリングはxサイズで一貫している 極めて均質なアーキテクチャの事例 畳み込みネットワークでは一般的だが メモリと計算時間のほとんどは初期の畳み込み層で使われ パラメータのほとんどは最後の全結合層で使われている    なぜ畳み込みネットワークのフィルタの重みとバイアスは なぜ同じなのか  深度方向には異なる 特徴量の検出能を維持できるという仮説のもと パラメータ数を激減させれると分かったから ,0,2023-01-31
243,243,evision で動画から物体検出する,画像処理,https://qiita.com/RyoWakabayashi/items/abc3278c4c7c65897f5a,   はじめに以前  evision と YOLOv で画像 静止画 から物体検出するコードを実装したました今回は動画から物体検出したいと思いますいつものように Livebook で実装していきます実装したノートブックはこちら   参考記事実装にあたっては  the haigo さんの以下の記事を参考にしましたまた  Kino animate で動画を表示する方法については  あんちぽ   さんの以下の記事を参考にしました   実行環境  Elixir     OTP   Livebook    以下のリポジトリーの Docker コンテナ上で起動しています   note infoDocker が使える環境であれば簡単に実行できますDocker Desktop を無償利用できない場合は Rancher Desktop を使ってください    実行環境の注意点コンパイル済の evision  OpenCV では mp ファイルを読み込めないため  evision をソースからコンパイルする必要があります前述の私のコンテナ Ubuntu では 以下のモジュールを追加で  apt get install  しました  unzip  ffmpeg  libavcodec dev  libavformat dev  libavutil dev  libswscale devunzip がないとビルド時にエラーが発生しますそれ以外は FFmpeg で mp などのコーデックを使えるようにするために追加しています   事前準備何らかの mp 形式の動画ファイルを   tmp sample mp  に保存しているものとします   セットアップ   Nx   行列演算evision のコンパイルには非常に時間がかかるため 分程度待ちます   機械学習モデルのロード今回は Darknet の YOLOv を使います公式で配布している重み 設定ファイル COCOのラベル一覧をダウンロードします Req get   は   output  で指定したパスにファイルをダウンロードすることができます yolov weights  だけファイルサイズが大きいため タイムアウト時間を長く指定しています   モデルの読込 Evision DNN DetectionModel detectionModel  を使ってモデルを読み込みます Evision DNN DetectionModel setInputParams  で入力パラメータを指定しています  scale  画像ファイルで  から  になっている色を  から  に変換するため        を指定  size  モデルに入力する際に  ×  にリサイズするよう指定  swapRB  BGR を RGB に変換するため true を指定  crop  リサイズ時に画像を切り抜かないので false を指定検出結果の番号を名称に変換するため ラベルの一覧を読み込みます実行結果は以下のような 長さ  のリストになります   elixir  person    bicycle    car    motorbike    aeroplane    bus    train    truck    boat    traffic light    fire hydrant    stop sign    parking meter    bench    bird    cat    dog    horse    sheep    cow    elephant    bear    zebra    giraffe    backpack    umbrella    handbag    tie    suitcase    frisbee    skis    snowboard    sports ball    kite    baseball bat    baseball glove    skateboard    surfboard    tennis racket    bottle    wine glass    cup    fork    knife    spoon    bowl    banana    apple    sandwich    orange          画像の読込テスト用の画像をダウンロードして読み込みます   静止画からの物体検出 Evision DNN DetectionModel detect  で物体検出を実行します  confThreshold  検出結果の確信度に対する閾値  class ids  物体が何であるかを示す番号のリスト  scores  物体の枠に対する確信度×分類に対する確信度  boxes  物体の位置を示す枠 左上のX座標 Y座標 幅 高さ 後の処理で使いやすいように score は小数点以下桁で丸め  class id は名称に変換します実行結果は以下のようになります   elixir    box            class   bicycle   score         box            class   truck   score         box            class   dog   score    自転車とトラックと犬が一つずつ検出できました   静止画への検出結果の書込検出結果を画像に書き込みます    四角形を描画する    ラベル文字を書く正しい位置に検出できています   動画の読込いよいよ動画から物体検出を実行します Evision VideoCapture videoCapture  に動画ファイルのパスを指定しますここでは     tmp sample mp    に置いてある動画を使っていますが お手元の任意の動画を指定してください実行結果は以下のようになります動画ファイルのフレームレート fps やフレーム数 frame count などが取得できていますもし読み込めていない場合   fps  などの値が     になり   isOpened  が  false  になりますその場合は FFmpeg のインストールなどが上手くいっていないので 環境構築を見直してください実行結果は     になります先ほど一回 read したことにより   番のフレームが読み込まれ 次が  番になっています あんちぽさんの記事   で紹介されていた方法で動画を表示します先頭のフレームから読み込み直すため 対象フレームを  に設定してから動かします動画の最終フレームを過ぎると  Evision VideoCapture read  の結果に false が返り それによって終了するようにしていますゆっくり動画が再生されました全フレームを読み込んで表示しているため どうしても元より遅くなります   動画からの物体検出静止画で行った物体検出を関数にしておきます全フレームに対して検出を実行しますフレーム毎に物体検出結果が書き込まれました   動画への検出結果の書込検出結果を動画ファイルとして保存しますmp 形式で保存するように  Evision VideoWriter fourcc  でコーデックを指定します   elixirfourcc   Evision VideoWriter fourcc hd  m    hd  p    hd      hd  v    Evision VideoWriter videoWriter  で出力の設定をしますフレームレートや画像の縦横サイズを指定します再びフレームを  まで戻して 全フレームに対して物体検出を実行します今回は Livebook 上に結果を表示しないで動画ファイルに書き込みます実行すると以下のように経過が表示されていきます   elixir最終的にフレーム数分の true が入ったリストが返ります全フレーム処理し終わったらファイルを解放します   検出結果の確認ちゃんと書き込めたか確認しましょうちゃんと全フレーム書き込めていました   まとめPython   OpenCV と同じ要領で Elixir   evision で動画処理できましたCPU 環境では遅いので  GPU でどれだけ速くなるかも検証してみます,6,2023-01-30
244,244,Pythonで画像ファイルを読み込み白色以外を検出してメッセージを出力させる方法,画像処理,https://qiita.com/Nao_Ishimatsu/items/fbb9fd35ca7deaf4ec39,  今回は Pythonでopencvライブラリをつかって画像ファイルを読み込み 白色以外を検出して判別メッセージを出力させてみます      note info   白色以外を検出してメッセージを出力するには        cv inRange  関数を使用して白色の範囲に収まらない画素を抽出します        np count nonzero  関数を使用して 抽出された画素の数を数え それがでなければメッセージを出力します        検出結果で白以外の色が検出されたら 画像の中に白以外のものがあります と結果を出し 検出されなければ 画像は真っ白です と表示されます        OpenCVライブラリはpipをつかって事前にインストールしておきます     pip install opencv contrib pythonでは実際にやっていきます      画像イメージの作成  ①白以外の色を含む画像  ペイントで日本の国旗を作成してflag pngとファイルに保存しておきます 次のpythonコードを用意しました   画像の読み込みimg   cv imread  flag png    白色の範囲を指定  白色の範囲に収まらない画素を抽出  抽出された画素の数を数える  白色以外の画素があればメッセージを出力    print  画像の中に白以外のものがあります      print  画像は真っ白です     note info この例では 白色の範囲が BGR の       から      に指定されています 白色の範囲は 画像や撮影条件によって異なる場合があるため 適切な範囲を指定する必要があります      実行する  python colorcheck py画像の中に白以外のものがあります実行後 画像の中に白以外のものがありますとメッセージが出力された 以上になります この情報が皆様の何らかのお役に立てることができますように ,2,2023-01-28
245,245,Rembgでレシート画像を切り抜き,画像処理,https://qiita.com/kazzzu/items/8fec3a86497105e6cde2,  目的  レシートと背景が写った写真からレシートをくりぬきたい  Rembgってすごいらしい  なんか画像処理っぽいことしたい  前回のあらすじ今回はrembgをやってみたいと思いました 原理とか理論とかは参考文献を参照してください 最近のDeepleaningはすごいですね 恐ろしく簡単に背景消せるんだ と思いました   環境  Windows Home  VSCode  Python   pip rembg  本編参考：   Rembgをpipでインストールしますpip install rembg   プログラムを作成します基本的に行で切り抜きしてくれます めちゃくちゃ楽    変換結果前回うまくいかなかったものもあっさり良い感じに切り抜けました  こんなに簡単にできてしまうと逆につまらない感じ    元画像変換後元画像 前回失敗した 変換後   終わりに今回GPUを使用しないで実行しましたが 枚当たり秒くらいの処理時間でした GPU使うともっと速くなるかも ,0,2023-01-25
246,246,【matplotlib】0を含んだ画像をlogスケールで白塗りにならずに表示する方法,画像処理,https://qiita.com/yusuke_s_yusuke/items/163033d272511ce3c172,  はじめにmatplotlibでが含まれ画像を対数 log スケールで表示するとき 以下の図の左側のようにの値が白塗りされてしまう事態に遭遇したことはないだろうか 値にカラーをつけるなら 右側のような色になってほしいですよね   image png  そこで 本記事では   白塗りにならないで正しく表示する方法  を紹介します   実装Google Colabで作成した本記事のコードは  こちら  にあります    各種インポート   Python 各種インポート実行時の各種バージョン  Name   Version    numpy         matplotlib         scipy          サンプル画像    img img   e         本記事用に小さい値をに置換しておくガウシアンを使います ただし 本記事では値がほしいので適当に  e   より小さい値を    に置換しています    まずはそのままで見てみよう   python 何も加工しない場合値が白塗りされているのがわかります    方法白塗りになる原因は 単純にlogスケールでを表現できないためです そのため 値をカラーバーの最小値に置換すれば解決できます ここでは カラーバーの最小値を設定しない場合と設定した場合の両方のケースで白塗りにならない表示方法を紹介します     カラーバーの範囲がオートの場合   python カラーバーの範囲がオートの場合  番小さい値  を番目に小さい値に置換説明   np unique    を使うことで 画像の値を重複を除き昇順でソートされます   その中には 値が最小値として含まれていて その一つ大きい値   np unique img      は以外なので その値を  second min  に代入します   後は   colors LogNorm     とすることで オートで  mask img  の最小値 最大値でカラーをつけてくれます     カラーバーの範囲を任意で指定する場合   python カラーバーの範囲を任意で指定する場合  color minに任意の範囲を指定説明    color min  にカラーバーの最小値を指定します   後は   colors LogNorm vmin color min  vmax None   とすることで 最小値の範囲を指定しカラーをつけてくれます    vmax  には   None  でなく 適当な値を入れることもできます    まとめmatplotlibで 値をlogスケールにしたいけど 白塗りになってしまうという問題を解決する方法を紹介しました   np unique    は が暗黙に含まれて カラーバーをオートでつけるときに役に立つのでぜひ使ってみてください また 単純にカラーの最小値を指定する場合は やはり 普通にその最小値になるように置換するのが良いかなと思います matplotlibでが含まれた画像の対数表示は 少し慎重に置換しながら解決していきましょう   参考資料,4,2023-01-22
247,247,犬、猫、うさぎの画像を分類するWebアプリ,画像処理,https://qiita.com/shinya-shibasaki/items/d17196643a461dbda2b7,AidemyのAI アプリ開発講座の成果物として 種類の動物の画像を識別するWebアプリを作成しました Ｇoogle Colaboratoryでモデルを構築し RenderでＷebアプリを公開しています 画像認識 Webアプリともに初めてとなりますので いろいろとコメントいただけると嬉しいです    Webアプリ作成までの流れ   学習用データの準備   step 学習用データの準備    モデルの構築   step モデルの構築    Webアプリの作成   step webアプリの作成  さいごに   さいごに    STEP  学習用データの準備学習用データはスクレイピングで用意しました  スクレイピングの詳細については省略します フォルダの階層は以下のようになっています 一応それぞれのファイル数を確認 犬 猫 うさぎで各枚程度用意 path    ファイルが入っているフォルダ    フォルダ内のすべてのファイル名をリストfiles   os listdir path count   len files print count    STEP  モデルの構築  モジュール パッケージのインポート  分類するクラスの準備  画像のサイズと 保存されているフォルダを指定cvで画像を読み込むとかなり時間がかかったのでImageDateGeneratorを使ってみました  訓練 検証 テストデータを作成   python  トレーニングデータとバリデーションデータを分割する  テストデータを作成  VGGを使ってモデルを構築する  全結合層を構築  VGGと全結合層を結合  VGGの層目までの重みを固定  コンパイル 最適化関数はSGDがよい らしい   モデルの学習  可視化※はまったところ最初は以下のようにしていたのですが これだとnextをした際にX train  y trainに代入されたデータ 枚分のデータ だけをfitで使うことになって 正解率が全くあがりませんでした   混合行列で分類結果を確認  混合行列  混合行列の可視化  Webアプリで利用するためにモデルをダウンロードモデルのダウンロードval accが で割を超えたので まあまあいい結果が得られた と思う   image png  予測ごとにやや変動しますが うさぎの正解率がやや低い傾向があるようです   image png     STEP  Webアプリの作成構築したモデルを使ってFlaskでWebアプリを作成します             flash  ファイルがありません              flash  ファイルがありません               受け取った画像を読み込み np形式に変換             変換したデータをモデルに渡して予測するなかなかいい確率で分類してくれます    さいごに今回 画像認識 Webアプリと作成しましたが なかなか正解率が上がらなかったりと手探りな部分が多くかなり苦労しました うさぎの画像で正解率が上がりずらい傾向があったのは  元データが犬 猫にくらべて枚数が少なかった 風景の一部にうさぎがいるような うさぎが中心ではない画像も含まれていたあたりが一因かと思ってます データの集め方から工夫が必要かなと思いました 今後も引き続きいろいろと調べてよりよいアプリケーションが作成できるようにしていきたいと思います ,6,2023-01-20
248,248,牛丼と親子丼をもっと見分ける...!!(転移学習編),画像処理,https://qiita.com/plusnine/items/623fc2585238bce18e77,   はじめにこの記事は 前回の記事 CNNで親子丼と牛丼を見分ける       の続編です 前回はテストデータでおよそ の精度を達成しましたが もう少し精度を向上できないかと思いました   そこで 今回は転移学習を実施して精度向上できないか検証してみました 結果として 約 の精度を達成することができました  また モデルが何を見て判断しているか確認するためにGradCAMを用いて可視化してみたので みていただいたら嬉しいです   転移学習   転移学習とは　転移学習は 事前学習済みモデルを利用して出力層部分のみを自分の目的のタスク向けに交換し 出力層部分のみを自分で集めたデータセットで学習するというものです 　例えば 犬と猫を高精度に識別したいとします そして あらかじめカテゴリのマルチクラス分類を大量の画像データを利用して学習したモデルがあるとします 転移学習は この学習済みモデルを利用して犬と猫の画像を識別できるようにするために 出力層だけ変更を加え それまでの重みは固定する 自分が作成したデータセットを用意して犬と猫の画像を学習する手法です 転移学習を行うメリットとしてデータセットが少ない場合でも高い精度を出せることが多いことです 自分でデータセットを作成するにはかなりの労力がかかります そういうとき 転移学習を利用することでデータせと作成にコストをあまりかけずに 高精度の推定結果を実現できるようになります また 大量のデータを使わないため 学習の高速化も期待できるようになります デメリットとして 学習効果を高めるために転移元とのデータの関連性がある程度必要です 例えば 犬猫の判別に車両のデータを学習したモデルなど 関連性が低いとあまり学習効果を期待できません 転移学習とよくセットで出てくる手法としてファインチューニングというものがあります ファインチューニングは 転移学習と違い出力層以外の重みを固定せずモデル全体のパラメータを学習し直すという特徴があります  この辺は記事や書籍でも逆に書かれていたり 抽象的に書かれていたりしてややこしいです笑 転移学習と比較するとモデル全体のパラメータを更新するので それなりのデータ量が必要ですが 自分の目的と関連が強いモデルではなくても利用しやすいなどのメリットがあります   image png   引用 今回は データ量がそこまで多くないので 転移学習を実施しました  ImageNetを学習したモデルを利用したのでデータとの関連性は薄いかもしれませんが       ResNetとは　ResNetとは画像分類アーキテクチャの一つで層から構成されるCNNになります 画像分類におけるCNNにおいて層を深くすればするほど より複雑な特徴を検出できるようになると考えられています しかし 層をただ深くしただけではあまり精度が上がらないことも報告されていました 勾配消失問題  そのため 層前後までしか層を深くできませんでした  ResNetでは残差ブロックという手法を利用して層という深い層を実現しました    残差ブロック 残差ブロックとは 通常のネットワークにスキップ接続を加えて数層ごとにブロック化したものです 一般的なCNNは畳み込み層やReLUなどを直列につなげたシンプルなネットワークです  下図   スクリーンショット       png  　これに対し 残差ブロックは各畳み込み層のまとまり  層ごと に対して 並列にShortcut Connectionを導入させます  下図   スクリーンショット       png  　このように 畳み込み層とShortcut Connectionの組み合わせで構成されています Shortcut Connectionに関しては恒等関数になっていて 最終的にはそれぞれの要素を足し合わせます 残差ブロックによって 出力全体の損失を 個々の経路から少しずつ最適化できるようになり 初期表現 勾配をより深い層までの伝搬を可能にしました 次からは実際にResNetによる学習を実施していきたいと思います    ResNetによる牛丼と親子丼の画像分類　実際に転移学習を実施していきます 今回は GoogleColablatoryでPytorchを利用して学習を行いました また データはCookPadからスクレイピングした親子丼と牛丼のデータを利用しました テストデータは Bing画像検索から別で用意しておきます    ライブラリのインポート  ライブラリの読み込み  前処理つづいて 画像の前処理を実施していこうと思います 画像をランダムにトリミング スケール幅の設定  左右反転 回転 Tensorオブジェクトに変換 標準化 リサイズをできるようにしました    py前処理クラスの定義       画像の前処理クラス 訓練時 検証時で異なる動作をする         train  訓練用のトランスフォーマーオブジェクト        val    検証用のトランスフォーマーオブジェクト           トランスフォーマーオブジェクトを生成する         Parameters         resize int   リサイズ先の画像の大きさ        mean tuple    R  G  B 各色チャネルの平均値        std           R  G  B 各色チャネルの標準偏差          dicに訓練用 検証用のトランスフォーマーを生成して格納                  ランダムにトリミングする                transforms RandomResizedCrop                     resize    トリミング後の出力サイズ                    scale             スケールの変動幅           オブジェクト名でコールバックされる  画像のファイルパスをリストにする学習のために 画像のファイルパスをリストにします 学習用と検証用はあらかじめディレクトリで分けておきます    py親子丼と牛丼の画像のファイルパスをリストにする    データのファイルパスを格納したリストを作成する     Parameters       phase str    train または val     Returns       path list list   画像データのパスを格納したリスト      画像ファイルのルートディレクトリ 各自設定       画像ファイルパスのフォーマットを作成      ファイルパスを格納するリスト      glob  でファイルパスを取得してリストに追加  ファイルパスのリストを生成  訓練データのファイルパスの前後要素ずつ出力  検証データのファイルパスの前後要素ずつ出力  スクリーンショット       png  このようにtrain val 親子丼 牛丼でファイルパスを分けることができている   データセット作成PytorchのDataSetクラスを継承して 画像のデータセットを作成します 先ほど作成した前処理クラスのインスタンスを呼び出し 訓練用 検証用 テスト用の画像とその正解ラベルを返すようにしました    py親子丼と牛丼の画像のデータセットを作成するクラス    牛丼と親子丼の画像のDatasetクラス    PyTorchのDatasetクラスを継承    Attributes       file list list   画像のパスを格納したリスト           インスタンス変数の初期化           len obj で実行されたときにコールされる関数        画像の枚数を返す              obj i のようにインデックスで指定されたときにコールバックされる           Parameters              index int   データのインデックス           Returns           前処理をした画像のTensor形式のデータとラベルを取得          ファイルパスのリストからindex番目の画像をロード          ファイルを開く     高さ  幅  RGB           正解ラベルをファイル名から切り出す          正解ラベルの文字列を数値に変更する  データローダーの作成データローダーを作成していきます データローダーはデータをロードしてミニバッチを作成する処理です    pyデータローダーの生成  ミニバッチのサイズを指定  画像のサイズ 平均値 標準偏差の定数値  画像の前処理と処理済み画像の表示  モデルの入力サイズ タテ ヨコ SIZE     標準化する際の各RGBの平均値MEAN               ImageNetデータセットの平均値を使用  標準化する際の各RGBの標準偏差STD                ImageNetデータセットの標準偏差を使用size  mean  std   SIZE  MEAN  STD  MakeDatasetで前処理後の訓練データと正解ラベルを取得  MakeDatasetで前処理後の検証データと正解ラベルを取得  訓練用のデータローダー  バッチサイズ       を生成  検証用のデータローダー  バッチサイズ       を生成  データローダーをdictにまとめる  テストデータのデータローダー作成テストデータのデータローダーを別で作成しておきます    py  テストデータのデータセット作成  モデル構築モデル構築を行います 今回は層を深くすることで 学習損失が大きくなって精度が落ちてしまう劣化問題を改善できるResNetを利用しました 層の深さも最も深いモデルにしてあります この辺りは 学習に問題があれば層を浅くしてもいいかもしれないです また 出力層はクラスのマルチクラス分類のモデルになっているので 二値分類モデルに変更を加え 他の層の重みは固定します   スクリーンショット       png  モデルの層がかなり深いので出力層だけ出してみると 出力層のout featuresがになっていることが確認できます   学習それでは 実際に学習を実施していきます 今回は エポックを回にしてますが 収束したら学習を打ち切れるようにearlyStoppingを設定していきます   損失と精度エポックごとの損失と精度をプロットしていきます    py  損失と精度の推移をグラフにする  損失  精度若干検証用の損失が下がりきらず 過学習気味な感じがありますがテストデータの精度を確かめてみます   テストデータの精度テストデータの精度を求めてみました 結果は       でした  前回が 約 だったので 以上の精度向上を実現しました  それでは モデルが画像のどのようなところに注目しているのか確認していきましょう   GradCAMによるモデルの可視化 実際にモデルが画像のどの部分に注目して予測しているかをGradCAMを利用してみていきます Grad CAMとは学習済みAIモデルの解釈を行う手法の一つであり モデルのレイヤーから重要な特徴を抽出してどの特徴が有効であったかを把握します           最終層逆伝播時の勾配を記録する          最終層の出力 Feature Map を記録するpath    テストデータの画像パス 各自設定   勾配 重み weights と出力の特徴図 Feature Map の加重合計で CAM を算出して ReLU を通す CAM を可視化するために resize して正規化 元画像に CAM を合成 可視化では 予測結果を確認しその中からいくつかピックアップして紹介していきます   予測結果と一致した例    牛丼を牛丼と予測  image png    image png  上に卵が乗っている牛丼ですが 正しく予測できました モデルは料理の中身よりも器の方に向いているようです ただ 親子丼の方には上に卵が乗っている画像が多く含まれていたのですが 正しく判断できたようです   image png    image png  一方 こちらはオーソドックスな牛丼です こちらは 予測の際には牛丼そのものよりも周りの部分に注目しているようでした 画像の右下部分など      親子丼を親子丼と予測  image png    image png  こちらの画像は親子丼の卵の部分や鶏肉の部分 ご飯の部分を注目して正しく予測できています   image png    image png  こちらの画像は 三つ葉の部分や卵の部分に注目しているようでした 今後 カツ丼とかも見分けるようにしたいときどうなるのかなあと思います なかなか難しそう     　  予測結果間違った例    牛丼を親子丼と予測  image png    image png  こちらの画像は結構明らかに牛丼なのに親子丼と予測してしまいました 若干画像が明るくなっていたことが原因の一つなのかなと思うので 前処理の段階で画像の明るさ等も変化させるようにしたほうが良かったかもしれないなと思いました また 訓練用と検証用の画像データを確認したところ画質が悪くあまり牛丼や親子丼だと特定できない画像も多かったので テストデータで利用した画像を訓練用で一部利用しても良かったかなと思いました     親子丼を牛丼と予測  image png    image png  こちらの画像では 丼の周りと丼のごく一部の肉の部分に注目して予測を間違えています なので 画像認識ではなく物体検出で料理を特定する必要があるのかなあと思いました   まとめ今回は ResNetによる転移学習を実施して推定精度を から まで向上することができました 画像の前処理とか物体検出技術を利用した料理検出などまだまだできそうなことが多いので 今後実施していければなと思います  ,8,2023-01-19
249,249,Custom Visionでリアルタイム判別,画像処理,https://qiita.com/Ishigami100/items/ef74ce0221be44c81272,  Custom Visionでリアルタイム判別今回は Microsoft の Cognitive Services のひとつ   Custom Vision  を使用し カメラを起動させ その映像についてリアル判別を行うプログラムを作成しました Custom Vision API の説明については 本記事では省きますが 実際のリアルタイムでの処理について まとめておきたいと思います    事前準備事前に 今回のプログラムで使用するためのモデルを Custom Vission で作成します 作成後 Predection　URLを発行しておきます    プログラムについて    カメラの起動カメラの起動には MediaDevices getUserMedia  を使用し 起動を行う プログラムについては下記の通りとなっています     フレームごとの処理カメラ映像のフレームごとに処理を行う 各フレームごとの画像をCataURL→blob→Fileオブジェクトの順にデータを変換し APIを呼び出すプログラムへと渡す 本プログラムでは   フレーム  ごとに関数を呼ぶようにしている     フレームごとの処理    APIの呼び出しフレームごとの処理から下記の関数でAPIをたたき 結果を返してきている     Javascript  画像の分析    function getFaceInfo file        Custom Vision の Subscription Key と URL をセット     サブスクリプション画面に表示される URL および Key をコピーしてください     Face API を呼び出すためのパラメーターをセットして呼び出し   プログラムの全体今回のプログラムについて 全体像を示す       処理の低速化  画像の分析    function getFaceInfo file        Custom Vision の Subscription Key と URL をセット     サブスクリプション画面に表示される URL および Key をコピーしてください     Face API を呼び出すためのパラメーターをセットして呼び出し   注意事項今回のプログラムでは 実行時に  高速でCustomVisonAPIをたたくため 使用時の従量課金  に必ず注意の上使用してください    参考,3,2023-01-18
251,251,OpenCVを使わず幾何変換する,画像処理,https://qiita.com/RDProm/items/d9d7e618b013ccda2079,   OpenCVを使わず幾何変換するOpenCVが使えない  使いたくない  その他の場合  reasons      reasons   ライセンスの理由  言語の理由  要求仕様の理由  好みの理由等   参考までにここで紹介する OpenCV を使わない方法より OpenCV を素直に使った方が数十倍速い       画像データ画像をプログラム上で扱う時  通常は 次元配列のデータとして扱うことになる   インデクスの   i  j  k   は前のつ  i    j  がそれぞれ  y    x  座標  ピクセルの位置  を表し  つ目  k  は RGB や CMYK 等の画素値ベクトルのインデクスを示す   次元配列の場合  モノトーン画像等の画素値が変数となる画像データを表す   C 系のプログラミング言語では for ループの入れ子構造とデータアクセスの順序の関係で  第インデクスの  i  が縦方向の位置を表すと考えるため    i    j       y    x   という対応付けがアルファベット順と逆になることに注意       幾何変換画像の幾何変換とは写像  f  によって座標   y  x   を他の座標   w  z    f  y  x    に移すことで  新たな画像に変換することを指す   変換後画像の画素  \mathrm Dst   と元画像の画素  \mathrm Src   は  \mathrm Dst  w  z    \mathrm Src  y  x   で対応する   例えばアファイン変換なら座標の配列  y  x   を行列  A  とベクトル  b  で  f  y x     A y  x    b  と変換して新たな座標が計算される  affine      affine   通常はこれをさらに変形して   A  と  b  から構成されるサイズの行列  T  によって   y  x   を   w  z      T y  x     で変換して得られる   w  z   に移すよう実装される     y  x   として入力する画像データの全画素の座標  インデクス  を渡した時  変換後の座標   w  z   は画像データを構成できるものになるとは限らない   例えば小数座標や負の座標は配列のインデクスにはならないし  配列を構成するのに必要な要素のインデクスが得られないこともある   よって  変換後の画像データを作成するためには  変換後の全座標  インデクス    w  z   に対応する変換前の位置座標   y  x   を逆算する必要がある   この時  逆算された   y  x   もまた元の画像データを構成するインデクスになるとは限らないので  元画像の画素値を使って   y  x   における値を補間しなければいけない   画像データのような次元に配置されたデータの補間手法には最近傍補間  双線型  bilinear  補間  双三次  bicubic  補間等があり  詳細は下のサイト等を参照   cf   画素の補間 Nearest neighbor Bilinear Bicubic 　画像処理ソリューション  また    y  x   が補間できない座標の場合  双線型  双三次補間での負の座標等   外挿も行うことになる   補間が 周囲のデータの中間値を取る という方向性が決まっているのに対して外挿は 画像内に写ってない箇所の画素値を予測する という自由度の高い問題なので  定数で埋める  端の画素値を延長する  元画像の画素値を据え置く等  補間と比較してバリエーションが多い       OpenCV での幾何変換OpenCV を使わない幾何変換の前に OpenCV を使う方法を魚眼変換を例に解説する   画像は  SIDBA  標準画像データベース    より引用したマンドリル      python  画像読み込み  元画像と同じサイズの変換後画像のインデクス配列生成  画像は  から  魚眼変換の逆算  オリジナルのマンドリルと魚眼マンドリル    OpenCV を使わない幾何変換OpenCV を使わない幾何変換では変換前座標   y  x   を算出するところまでは同じだが  補間と外挿で新たな座標での画素値を計算する必要がある   ここでは NumPy  SciPy を使った方法を紹介するが  行列演算を for 文で展開すれば Python 以外の言語でも実装できる        NumPy による双線型補間 埋め外挿双線型補間は周囲ヶ所の格子点  座標が整数となる点  の重み付き平均で補間点の画素値を決める補間となる   重みは各格子点と補間点との近さで決められる   詳細は 上で挙げたサイト  等を参照   NumPy を使うと行列演算で全座標について一括して書くことができる   ただし前述のように補間点と周囲の格子点が元画像のインデクスの範囲を外れる可能性もあるので  パディングして範囲を拡張しておく必要がある   この時  外挿方法として埋めを行うのであればパディングと双線型補間だけで追加の処理が不要になる      python  画像読み込み  元画像と同じサイズの変換後画像のインデクス配列生成  魚眼変換の逆算  双線型補間  埋め外挿  パディング  パディングで拡張した画像データを使って双線型補間  この場合は埋め外挿のための追加処理は不要  NumPY 版魚眼マンドリル欠損値ありや非矩形領域の場合等  グリッドでないデータを扱うなら代わりに scipy interpolate griddata  を使う  griddata      griddata   何でグリッドでない場合の関数が  griddata  なんだろうか  非グリッドの場合をベースに考えていたのでスパース配列を持ち出すつもりだったが結局廃案したスパース配列  のインデクスを float に拡張したもの  を使えば補間や外挿は画像化する時だけで済むというメリットはある  画像読み込み  元画像と同じサイズの変換後画像のインデクス配列生成  魚眼変換の逆算  補間  外挿  OpenCV の場合と渡すデータ形式が少し変わる  SciPy 版魚眼マンドリル,0,2023-01-13
252,252,うまくいかない原因が画像データ不足と感じたときに手っ取り早く９０度回転で４倍にデータを増やす,画像処理,https://qiita.com/s0918/items/6bfa32d180debb0f4927,  サンプル数が少ないとうまく結果を出せない可能性がある    正方形の画像かつ回転等を行っても大丈夫なデータに限るが 画像データの水増しを行ってみる   使用ライブラリ   プログラム 今回はGoogleコラボを使うのでこの一文を入れる 指定した場所から取得 取得したアドレスの確認 枚数の確認 枚数分     一枚に対しての処理     度 度 度で一回転するのでになる         保存先の指定 最後 としないとファイル直下に大量の画像が入るので注意         アドレスの指定と数字を組み合わせ         書き込み処理この処理を行う事で 枚→枚になるのでデータは確保できるはず しかしながら上下左右などデータを変えてしまうと成立しないようなデータは使用するべきではないのでちゅうい  image png  ドライブ上ではこのような配置になる goodは元の画像mizumasiは回転後の画像が入る 画像をフォルダで一緒にしてしまうと元データが分かりにくくなるので分けている 上のような一文を学習用プログラムに入れることで 取得した画像をプログラム上で追加できる ,0,2023-01-12
253,253,iOSのVision Frameworkで画像から文字列を取得、文字列の位置を画像上に矩形表示する,画像処理,https://qiita.com/hamayokokuririn/items/9b1bde323d0adfae0d38,  環境  iOS  Xcode   シミュレーターにて動作を確認  やりたいこと  UIImageViewの画像の文字認識を行う  取得した文字列を下部のUILabelに表示  UIImageView上に赤い矩形で認識した文字列の位置を表示  Vision FrameworkiOSで画像認識を行うことができるフレームワーク 今回は文字列の認識機能を利用した 下記のチュートリアルの通りに進めることで文字列認識が行えた 文字列認識処理は下記で実行できた UIはStoryboardで配置している        文字列認識実行           日本語に設定       文字列認識結果           取得できた文字列配列を表示        label text   recognizedStrings debugDescription              矩形を表示 後述する               showBoundingRect observations   文字列認識した位置を画像上に表示するここがハマりポイントだった 上記のチュートリアルでも矩形の位置を取得する方法が紹介されている VNImageRectForNormalizedRectを用いて 画像上のどの位置かが取得できた しかし 今回はUIImageViewのContentModeをAspectFitに設定しているのでアスペクト比を維持したまま 画像が縮小されて表示されている またx y座標もUIImageViewの中央に移動している   Visionで取れる矩形の位置はy座標は反対向きになっているので反転させる必要があった        こちらの記事で紹介されているようにy軸方向に反転する計算を行うことで対応できた   画像は元画像よりサイズを端末サイズに合わせて縮小されているので その縮小率に合わせて矩形の位置を調整する必要があった       UIImageView内の縮小されたimageのCGRectを取得したい AVFoundationのメソッドを利用することで取得できた    取得できたCGRect sizeと元画像のCGSizeから縮小率を計算できた   画像はUIImageViewの真ん中に表示されるのでその分平行移動させる必要があった       で取得できたx yの座標を足して 並行移動することで対応できた    swift       文字列認識できた位置に赤い矩形を表示           ①　画像のRectを取得 メソッド内にてy座標の反転を行う         let boundingRects   getBoundingRects observations  observations            ② 画像のUIImageView内でのRectを取得         let imageRectInView   imageRect             ② 画像の縮小率を取得                ① Visionのy座標はUIKitとは逆向きなので反転する必要がある,3,2023-01-11
254,254,アルファ画像とRGB画像の一括合成,画像処理,https://qiita.com/supertask/items/a6762fbb496e0a958d66,convert rgb png alpha png  alpha off  compose CopyOpacity  composite rgba pngアルファ画像 実際には深度画像 とRGB画像をRGBA画像に出力したかったので  ImageMagicのconvertでRGBA画像に変換しました．,0,2023-01-11
256,256,変分オートエンコーダでそっくりな漢字ランキングを作った,画像処理,https://qiita.com/tsukemono/items/1c5be5fc4814055f2cf2,  オートエンコーダとはデータを一度モデル エンコーダー にかけ低次元ベクトル 潜在変数 にした後  もう一度モデル デコーダー にかけることで  元のデータと同じものを再構成させる機械学習モデルをオートエンコーダーと呼びます しかしこの手法は  ノイズに対する頑強性がなく  ちょっと元データが異なったときに全然別の出力になるといった難点があります これの解決策として  意図的に途中でノイズを加えノイズへの頑強性を増す手法 デノイジングオートエンコーダー 変分オートエンコーダー が用いられています   変分オートエンコーダとは変分オートエンコーダーでは  エンコーダーで潜在変数の分布を求めます そして  その分布から潜在変数をひとつ抽出し  デコーダーで潜在変数から入力データを再構成させます 損失関数としてはELBOと呼ばれるものが用いられており  L  再構成  は再構成誤差と呼ばれており  入力データと出力データ間の対数尤度関数 例 二値交差エントロピー に相応します  L  正則化  は潜在変数の分布がどれぐらい標準正規分布に似ているかを表す指標で  潜在変数の分布が正規分布 \mathcal N  \vec \mu  \mathrm diag \vec \sigma    とすると  やったことの概要変分オートエンコーダでは  入力データの傾向が近いものほど潜在変数が近くに配置されるようになります この性質を活用することで  入力データに漢字の画像を入れることで漢字同士の意味的な距離を測定し  そっくりな漢字ランキングを作成しました   実装   漢字画像の作成まずはじめに  データセットとして常用漢字である字の画像を作成します フォントはメイリオでサイズはピクセル×ピクセル  色は白地に黒の単色としました PILというライブラリを使えば簡単に描画することができます draw text        鬼      少しずれるため補正完成した画像がこちら↓   変分オートエンコーダの実装変分オートエンコーダの実装はpytorch lightningで行いました               以下略          今回はエンコーダー デコーダーの部分を単純な全結合層だけで作りましたが  畳み込みなどをするとより精度が出るかもしれません logvarの初期値が大きすぎると  ノイズが激しすぎて全然学習しないので注意 バッチサイズを気持ち多めにするとうまく学習しやすそう   結果   ちゃんと再構成できているか左がデコード後の画像で右が元の画像です ちょっと怪しい部分もありますが  無事もとの画像を再構成できているようです めでたし   そっくりな漢字ランキング文字間の潜在変数の距離をはかることで似ている文字ランキングを作成する 距離関数としてはユークリッド距離とコサイン距離のつを採用します  順位   ユークリッド距離   コサイン距離       大 vs 太   大 vs 太       了 vs 丁   了 vs 丁       字 vs 学   字 vs 学       送 vs 迭   玉 vs 王       玉 vs 王   送 vs 迭       聞 vs 間   聞 vs 間       忘 vs 志   責 vs 貴       衷 vs 哀   弁 vs 井       関 vs 開   忘 vs 志       責 vs 貴   村 vs 材   ということで  変分オートエンコーダが選ぶ最も似ている漢字は｢大｣と｢太｣でした ｢玉｣と｢王｣のような納得の結果や  ｢忘｣と｢志｣のように言われてみると確かにというものまで色々ですが  そこそこは納得できる結果なのではないでしょうか 以下コード   おまけ モーフィング変分オートエンコーダを使うとつの画像を自然な形で変形できるそうなのでやってみました 佐  休,3,2023-01-05
